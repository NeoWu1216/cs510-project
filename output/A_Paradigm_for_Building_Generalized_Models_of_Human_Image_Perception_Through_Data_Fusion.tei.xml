<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Paradigm for Building Generalized Models of Human Image Perception through Data Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojing</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">L</forename><surname>Koenig</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Washington University in St. Louis</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Paradigm for Building Generalized Models of Human Image Perception through Data Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many sub-fields, researchers collect datasets of human ground truth that are used to create a new algorithm. For example, in research on image perception, datasets have been collected for topics such as what makes an image aesthetic or memorable. Despite high costs for human data collection, datasets are infrequently reused beyond their own fields of interest. Moreover, the algorithms built from them are domain-specific (predict a small set of attributes) and usually unconnected to one another. In this paper, we present a paradigm for building generalized and expandable models of human image perception. First, we fuse multiple fragmented and partially-overlapping datasets through data imputation. We then create a theoretically-structured statistical model of human image perception that is fit to the fused datasets. The resulting model has many advantages. (1) It is generalized, going beyond the content of the constituent datasets, and can be easily expanded by fusing additional datasets. (2) It provides a new ontology usable as a network to expand human data in a cost-effective way. (3) It can guide the design of a generalized computational algorithm for multi-dimensional visual perception. Indeed, experimental results show that a model-based algorithm outperforms state-of-the-art methods on predicting visual sentiment, visual realism and interestingness. Our paradigm can be used in various visual tasks (e.g., video summarization).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When it comes to understanding visual perception of images, studying human judgment remains the leading technique <ref type="bibr" target="#b0">[1]</ref>. Scientists have studied various high-level image properties that impact human behavior <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Such studies are done with psychophysical experiments in labs or online surveys via crowdsourcing platforms such * Corresponding author.</p><p>as Amazon Mechanical Turk (AMT). Consequently, each study creates an isolated set of data mostly used for a specific research purpose, often containing strong built-in bias <ref type="bibr" target="#b8">[9]</ref>. Furthermore, collecting human data is costly and time-consuming. The average pay to AMT workers is $2-3 per hour, and doing 40 ratings takes about 5 minutes, so to have 10 workers rate each of 10K images for merely 40 attributes would cost around $21K <ref type="bibr" target="#b9">[10]</ref>.</p><p>We believe that the reuse of existing datasets can provide novel insights. Although various datasets were created with different aims, many are manifestations of a shared underlying human psychological process <ref type="bibr" target="#b10">[11]</ref>. We focus on perception of visual images, which we hypothesize to have few dimensions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Indeed, many of the previously studied visual perceptual attributes are interrelated (e.g., image naturalness is related to interestingness <ref type="bibr" target="#b12">[13]</ref>). Based on the above understandings, we propose to reuse and fuse existing isolated datasets to statistically and computationally model visual perception (see <ref type="figure">Fig. 1</ref> for an overview). Our contributions are as follows. 1. We report a method to create "big human data" from smaller datasets. We show how to aggregate heterogeneous and fragmented human datasets that partially overlap in content. The non-overlapping parts of dataset are effectively missing data, whose values we infer using multiple imputation to get a "bigger" dataset. Notably, the fused dataset outperforms the constituent datasets when predicting image perceptions (e.g., visual sentiment, visual realism, and interestingness) across more diverse image semantics. Thus, through data fusion we get extra performance for free. Our method provides another path in the journey of achieving big human data -the fused dataset includes all images and all attributes from the partially overlapping constituent datasets. 2. We propose a paradigm for iteratively increasing the generality of a model of human image perception. Based on structure-revealing data fusion and statistical modeling, we <ref type="figure">Figure 1</ref>: Overview of our approach. First, we fused partially overlapping datasets to form a larger dataset (a, b). Second, we built a generalized and expandable human perception model through structural-revealing statistical modeling, which also informs data fusion (c). Finally, we built a generalized computational algorithm guided by the model, and test its performance by comparing their predictions of multiple image perceptions with alternatives (d).</p><p>build a multi-layered model of human image perception of digital images. The model incorporates commonly studied image properties (e.g., aesthetics, interestingness) with properties seldom studied for digital images, such as familiarity and scene dynamics <ref type="figure" target="#fig_1">(Fig. 3)</ref>. Unlike computer paradigms, our theory-based model characterizes human perception in a more understandable and concise way while reducing the danger of overfitting. It provides an improved ontology that can reduce the amount of human data collection needed and therefore associated costs. Critically, we show that it can guide the design of a generalized computational predictor, which is able to predict multiple high level image attributes simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Predicting high-level image properties: A common approach is to link lower-level image attributes with higherlevel properties, such as aesthetics <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, interestingness <ref type="bibr" target="#b1">[2]</ref>, memorability <ref type="bibr" target="#b15">[16]</ref>, visual realism <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>, and emotions <ref type="bibr" target="#b16">[17]</ref>. Most relevant is <ref type="bibr" target="#b3">[4]</ref>, which proposed a visual sentiment ontology together with a set of sentiment descriptors. Although these studies generated algorithms with considerable prediction performance on specific attributes, few insights were provided to explain why the algorithms work. Furthermore, most of the studies are isolated. Despite the exploration in <ref type="bibr" target="#b15">[16]</ref> on correlations between memorability and other image properties, there is no clear framework that explains the correlational structure of various perceptions. Instead of isolated goal-specific tasks, we propose a comprehensive ontology that can guide the design for predicting various human perceptions based on an intuitively understandable theory-based model of human perception. Visual perception datasets: Two datasets with extensive human annotations are the Visual Realism Dataset <ref type="bibr" target="#b2">[3]</ref> and the Memorability Dataset <ref type="bibr" target="#b4">[5]</ref> (See Sec. 3.1 for details). One related dataset consists of affective images datasets <ref type="bibr" target="#b16">[17]</ref>; another is the Sentiment Dataset <ref type="bibr" target="#b3">[4]</ref>. There are other datasets built for specific goals, such as aesthetics <ref type="bibr" target="#b13">[14]</ref> and interestingness <ref type="bibr" target="#b1">[2]</ref>. However, they either have a single rating or binary labels only, and thus are insufficient for visual perception modeling. These datasets are much smaller than popular image datasets (e.g., ImageNet <ref type="bibr" target="#b17">[18]</ref>) due to high costs of human data collection. Our work fuses some of these datasets into larger sets, to get a more general understanding of human image perception.</p><p>Visual perception structure: The human nervous system processes visual information both hierarchically and in parallel <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. The levels of this processing can be classified as early, intermediate, and late vision <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. The basic features of the image itself, like orientation and color, are processed during the early vision stage in the occipital lobe's visual processing areas <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Intermediate vision segregates visual elements to form coherent regions in what would otherwise be a chaotic and overwhelming sensory array <ref type="bibr" target="#b25">[26]</ref>. Late vision selects which of these coherent regions to scrutinize and evokes emotions and memories from which objects are recognized and meanings attached <ref type="bibr" target="#b19">[20]</ref>. Some studies suggest visual perception is low dimensional <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Data fusion and inference: In many disciplines, from computer vision to neuroscience, data from multiple sources are acquired and jointly modeled for enhanced knowledge discovery <ref type="bibr" target="#b26">[27]</ref>, but joining data often results in missing data. A visual synsets was proposed in <ref type="bibr" target="#b27">[28]</ref> to infer missing attributes through Linear SVM prediction. In psychology and sociology, multiple imputation (MI) is a common method for handling missing data <ref type="bibr" target="#b28">[29]</ref>. It estimates multiple values for missing data to account for the uncertainty surrounding missing values, thus providing unbiased estimates and more validity to missing data <ref type="bibr" target="#b29">[30]</ref>. We believe our work is the first to apply MI to visual perception for computer vision applications. Our MI was guided by comprehensive data exploration and theory-based understanding of the underlying structure of human perception.  <ref type="bibr" target="#b29">[30]</ref> in data fusion in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Empirical modeling on separate datasets</head><p>Our goal was to fuse multiple datasets to get a broader picture of human perception. To prepare for data fusion, we first separately explored two datasets with human annotations to get partial pictures of human image perception. We built human perception models from each dataset through statistical modeling to identify common and distinctive factors across datasets, as well as their latent structures. Such structures also guide data fusion in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets with human perception</head><p>We used the following two datasets in empirical modeling, and the data fusion in Sec. 4. Visual Realism Dataset <ref type="bibr" target="#b2">[3]</ref> was collected to study image visual realism. It includes 2520 images of diverse scenes, half of which are computer generated graphics and the rest are photographs. Each image has 40 human annotations (hereafter referred to as attributes). Memorability Dataset <ref type="bibr" target="#b4">[5]</ref> was collected to study image memorability. It includes 2222 images of general scenes. Each image has 127 human-annotated attributes.</p><p>For both datasets, attributes were collected using AMT. Those used for perception modeling are shown in <ref type="table" target="#tab_0">Table 1</ref>. The complete lists are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Structure</head><p>We designed our model structure based on previous findings in psychology, neuroscience, and linguistics. Human visual perception occurs in a hierarchical fashion <ref type="bibr" target="#b18">[19]</ref>. Correspondingly, we hypothesized that the human perception model is hierarchically structured with three layers. The first layer includes "basic perceptual features" of the image itself, such as texture and shape, that are analogous to those processed in the visual cortex <ref type="bibr" target="#b30">[31]</ref>. The second layer integrates the lower-level information. For example, determining whether the scene in the image is familiar. The third layer corresponds to effective (liking) reactions to images <ref type="bibr" target="#b11">[12]</ref>. Our model emphasizes the structure of the upper two layers, which are higher-level human perception. For the lower level we merely used attributes from the datasets to feed into the second, inferred layer. The structure is also inspired from WordNet <ref type="bibr" target="#b31">[32]</ref>, in which the vocabulary of a language is mapped to a set of concepts. In the next few sections, we conducted statistical modeling to fit the data to the three-level model and test the goodness of fit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Statistical modeling</head><p>Based on the hierarchical structure, we created statistical models of human perception separately for each dataset. We first conducted exploratory factor analysis to determine the number of latent variables (factors) present in the human data and identify a compact set of attributes that underlie latent variables. We then performed confirmatory factor analysis to test relations between latent and observed variables and assess the reliability and validity of measures. Finally, path analysis was conducted to determine standardized regression weights (γ) and correlations (φ) among latent variables <ref type="bibr" target="#b32">[33]</ref> Attribute pruning: Our aim was to build a model that reflects human perception, so we excluded non-subjective attributes, such as "sky present?". We also excluded attributes that were unrelated to perception or likely biased by personal experiences such as "recognize place?". The final number of attributes used to create human perception models for the two datasets were 25 and 32 respectively (see <ref type="table" target="#tab_0">Table 1</ref>). Exploratory and confirmatory factor analysis: To measure how observed variables (attributes) are related to latent variables (higher-level perceptions and reactions), we performed exploratory factor analysis (EFA) followed by confirmatory factor analysis (CFA) <ref type="bibr" target="#b32">[33]</ref>. Factor analysis (FA) is similar to principal components analysis (PCA); both  <ref type="figure">Figure 2</ref>: Human perception models built on two separate datasets. Standardized regression weights (γ) are in bold italic font. Although collected for different research purposes, the two datasets shared two latent factors (weird and artistic), indicating that common perceptual processes were measured in different datasets. The 25 and 32 attributes in each dataset were encapsulated by 5 dimensions respectively (four layer-1 factors in the lower layer and one layer-2 factor in the upper layer, liking), suggesting the multidimensionality and low dimensionality of visual perception.</p><p>are variable reduction techniques. However, in PCA, the components are orthogonal linear combinations that maximize the total variance, whereas in FA, the factors are linear combinations that maximize the shared portion of the variance underlying "latent constructs". We applied maximum likelihood in EFA with oblique transformation (where the new axes can be non-orthogonal, thus allowing factors to correlate <ref type="bibr" target="#b33">[34]</ref>), as we hypothesized that visual perception is multidimensional. A CFA <ref type="bibr" target="#b32">[33]</ref> performed after EFA tested how well the measured variables represent the number of constructs identified in FA. Attributes with poor loadings or fits were eliminated. Detailed results are reported in supplementary material. Path analysis: Guided by the designed model structure in Sec. 3.2, we divided the latent factors from previous CFA into two layers <ref type="figure">(Fig. 2</ref>). The final model was created through a path analysis predicting the liking latent construct from the lower-level perception latent constructs. Path analysis is a straightforward extension of multiple regression, which aims to provide concurrently estimated magnitudes of hypothesized causal connections between sets of variables <ref type="bibr" target="#b34">[35]</ref>. Readers can refer to <ref type="bibr" target="#b35">[36]</ref> for its detailed formulae and derivation. The final models for each dataset are shown in <ref type="figure">Fig. 2</ref>. For all models in the paper, detailed estimates are reported in supplementary material.</p><p>We applied two common metrics to measure the fitness of the model to the data. The first is Comparative Fit Index (CF I), which compares a chi-square for the fit of a target model to the fit of an independent model-one in which the variables are uncorrelated. Higher CF I indicates greater extent to which the model of interest is better than the independence model. Values that approach .90 indicate acceptable fit <ref type="bibr" target="#b32">[33]</ref>. Another model fit metric is Root Mean Square Error of Approximation (RM SEA), which estimates the amount of error of approximation per model degree of freedom and takes sample size into account. Smaller RM SEA values suggest better model fit. A value of .10 or less is indicative of acceptable model fit <ref type="bibr" target="#b32">[33]</ref>. Our models had acceptable fit,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data fusion</head><p>To develop a more general human perception model, we fused the two datasets. Data fusion was enabled by shared perceptions across the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Inferring missing data</head><p>Some attributes are in both datasets, but the attributes in only one dataset result in missing values on the images from the other dataset. One common approach for inferring missing values is to replace them with the mean, the median or the modal value <ref type="bibr" target="#b36">[37]</ref>. However, we hypothesized that human perception has a low-dimentional structure, and we expected components to be highly correlated, so we chose MI which utilizes all available data to preserve sample size and statistical power.</p><p>We started with attributes pruning: we collapsed 10 similar attributes from the datasets (color-paired in <ref type="table" target="#tab_0">Table 1</ref>). We further included 16 additional attributes that showed up in separate models (4 of which were used as auxiliary vari-ables 1 <ref type="bibr" target="#b29">[30]</ref>). We did not include all attributes as doing so would introduce excessive missing data <ref type="bibr" target="#b29">[30]</ref>. Attributes for imputation are marked with * in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We performed MI using the Markov Chain Monte Carlo method with a noninformative prior and a single chain <ref type="bibr" target="#b37">[38]</ref>. The multivariate normal model was used for all imputations. Convergence and independence of imputed values were assessed with time series and autocorrelation plots of means and variances <ref type="bibr" target="#b28">[29]</ref>. Specifically, we denote the variables with missing values for observation i by Y obs , the iteration goes as follows: with a current parameter estimate θ (t) at the tth iteration, a first step will draw Y (t+1) mis from p(Y mis |Y obs , θ (t) ) and a second step draws θ (t+1) from p(θ|Y obs , Y (t+1) mis ). This creates a Markov chain</p><formula xml:id="formula_0">(Y (1) mis , θ (1) ), (Y (2) mis , θ (2) ), . . .<label>(1)</label></formula><p>which converges in distribution to p(Y mis , θ|Y obs ).</p><p>We ran MI five times to minimize standard errors <ref type="bibr" target="#b38">[39]</ref>, resulting in five datasets with both observed and imputed values. We refer to those datasets as multiple fused datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Empirical modeling</head><p>The model structure and modeling steps were the same as in Sec. 3.3. Overall estimates were the means of each fused dataset's estimates <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. This averaging was done for all subsequently reported results of imputed datasets. The resulting perception model (hereafter, fused model 1) is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. It had acceptable fit, CF I = .92, RM SEA = .089. 1 Auxiliary variables are observed variables that correlate with the missing values but are not part of any factors used in the model itself. They can improve missing data imputation <ref type="bibr" target="#b29">[30]</ref>. Their selection was based on prior modeling of separate datasets (see Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MI evaluation</head><p>The validity of MI was evaluated four ways. First we compared frequency distributions of the observed attribute values to those generated by MI. This is the most common evaluation approach <ref type="bibr" target="#b29">[30]</ref>. The observed and imputed data had similar distributions <ref type="figure" target="#fig_3">(Fig. 4)</ref>, suggesting consistency from imputation. Second, we compared estimates produced by the MI model with those produced by models using only the observed variables. The estimates were consistent (see supplementary material), indicating that MI retained the model structure. We further compared MI by replacing missing values by means. Although they produced similar predictions for the layer-2 factor, the attributes from MI are more normally distributed thus better for path analysis <ref type="bibr" target="#b34">[35]</ref>. Finally, we compared cross-factor correlations for factor combinations present in only one of the separate models with their correlations in fused model 1. Such correlations were highly consistent (see <ref type="figure" target="#fig_1">Fig. 3</ref> and 2). The coefficients of different factors to liking were also consistent in terms of strength and sign. This indicates that the fused model is largely similar to the constituent models, except it is better than either because it includes attributes from both. In other words, data fusion allows more latent factors to be included, thus generating a visual perception model more gerneral than its constituents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>Generality and expandability: Fused model 1 and the separate models shared latent structure comprised of a small number of common factors, which indicates generality and low dimensionality of the model. Similarly, research has suggested that human emotions <ref type="bibr" target="#b3">[4]</ref> and brain structures for perceiving visual stimuli <ref type="bibr" target="#b11">[12]</ref> are low dimensional. Notably, our model can be extended easily by adding new nodes or latent factors whenever new datasets are available.</p><p>Digging deeper into the model: We compare our findings with three commonly studied human perceptions: 1. Liking: In psychology, several factors have been found to influence interpersonal attraction, namely proximity, familiarity, similarity, and physical attractiveness <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. This is reminiscent of our model of visual perception, in which the factors familiarity, naturalness, and artistic are analogous to familiarity, similarity, and physical attractiveness, respectively. The strong weights from familiarity to liking is also consistent with previous psychological studies <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. Interestingly, the weight of familiarity to liking is significantly stronger than that of artistic, whereas it is comparable with that of weird <ref type="figure" target="#fig_1">(Fig. 3)</ref>. This suggests that visual preference of an image is more influenced by the specific feelings aroused in humans rather than its aesthetic value per se. What's more, liking has a strong loading node, exciting, which accords with previous psychological findings that exciting is a combination of pleasure and arousal <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Artistic: Image sharpness and image quality strongly loaded on artistic. This provides support to previous studies which used similar features, such as image contrast and edge distribution in aesthetics evaluation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Artistic strongly correlated with familiarity and naturalness <ref type="figure" target="#fig_1">(Fig. 3)</ref>. This again supports <ref type="bibr" target="#b14">[15]</ref> in which familiarity was used as one feature dimension for image aesthetics. 3. Naturalness: Colorfulness, image sharpness, reproduction of shadow detail, and absence of washed-out appearance were found to be important factors for naturalness <ref type="bibr" target="#b44">[45]</ref>. This is in line with our model, in which naturalness is strongly loaded by lighting naturalness and color naturalness. Our model suggests that naturalness strongly correlates with familiarity (φ = .85). Thus the nodes of familiairy (natural objects combination and appearance, common perspective) also contribute to naturalness. Similarly, <ref type="bibr" target="#b12">[13]</ref> suggests that image semantics such as object combination and viewing perspective are crucial for naturalness.</p><p>In summary, our model summarizes and visualizes multidimensional human perception in a concise and understandable way. It provides a new ontology for studies that predict high-level image properties, while also corroborating previous findings in visual perception.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Predicting visual sentiment, visual realism, and interestingness</head><p>In this section, we demonstrate how to build a generalized computer algorithm based on the model and data fusion. We incorporate a new dataset into a fused dataset for the purpose of predicting multiple perceptions: visual sentiment, visual realism, and interestingness. We show that our perception model can be used to guide both survey design and building machine algorithms for predicting various human perceptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data fusion</head><p>Sentiment Dataset: We used the Sentiment Dataset in all our experiments on perception prediction. It includes 603 tweets with photos and was originally collected to evaluate the performance of an automatic sentiment prediction method. Ground truth of binary sentiment values were obtained by human annotation using AMT <ref type="bibr" target="#b3">[4]</ref>.</p><p>Model structure: To leverage of our fused model, we mapped sentiment onto liking. Two observations validate this mapping. First, the makes happy attribute is central to positive sentiment <ref type="bibr" target="#b3">[4]</ref>. Second, the average Spearman's rank correlation between exciting and interesting (which strongly loaded on liking) with makes happy on Visual Realism and Memorability datasets was .71, suggesting that exciting and interesting were highly indicative of positive sentiment. So we regarded the liking factor as positive sentiment and reused our previous model structure.</p><p>Human survey: The Sentiment Dataset had only binary labels so we could not fuse it into our perception model. Therefore, we conducted a survey on AMT, to collect human annotations for the dataset. Our perception model guided the design of the survey: we wanted to restrict our survey questions to the observed attributes in our model because these attributes enable us to predict perceptual factors other than sentiment. We randomly selected a child node of each layer-1 factor in <ref type="figure" target="#fig_1">Fig. 3</ref>, resulting in a six-question survey which had no direct connection to liking in fused model 1 (see <ref type="figure" target="#fig_5">Fig. 5</ref> for selected questions). We will show that using multiple imputation and the empirical models enables these 6 questions not only to predict sentiment, but also to predict multiple perceptual factors in our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception model based on fusion of three datasets:</head><p>The fusion and modeling procedures were the same as in Sec. 4. The result, fused model 2 <ref type="figure" target="#fig_5">(Fig. 5</ref>) has acceptable fit, CF I = .93, RM SEA = .089. Its estimates are highly consistent with fused model 1 <ref type="figure" target="#fig_1">(Fig. 3</ref>). <ref type="figure">Figure 6</ref>: Sentiment prediction using human attribute ratings (left) and machine features (right) as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Computational algorithm design</head><p>To build a generalized computational predictor, we computerized the empirical model by designing computational features to predict its layer-1 factors (see <ref type="figure" target="#fig_5">Fig 5 for</ref> the factors). We modeled natural and familiar (which were highly correlated, φ = .83) using <ref type="bibr" target="#b45">[46]</ref>, in which scale invariant natural image patterns were modeled through high-order image patch statistics. Primarily meant for image quality assessment, we used it here to quantify naturalness. We used Ke's method on aesthetics rating <ref type="bibr" target="#b13">[14]</ref> for the artistic factor, which includes perceptual motivated features such as image composition, blur, and contrast. We applied GIST descriptors <ref type="bibr" target="#b46">[47]</ref> to model space. They provide a statistical summary of the spatial layout properties (e.g. openness, expansion) of the scene. Finally, we detected global outliers as for weird factor by applying the Local Outlier Factor (LOF) algorithm <ref type="bibr" target="#b47">[48]</ref> to global image descriptors (GIST and SIFT <ref type="bibr" target="#b48">[49]</ref>). <ref type="table" target="#tab_4">Table 2</ref> shows the list of our features. The detailed design is reported in supplementary material. We did not find niche features for the dynamics factor, as it is highly abstract. However, scene dynamics is somewhat captured by the Spatial Envelope Model in the GIST descriptor, and the blur component in artistic features. We then combined the features in later fusion according to the weights of the links towards liking in fused model 2 ( <ref type="table" target="#tab_4">Table 2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Predicting sentiment with human data</head><p>First of all, we hoped to test the effects of data fusion. We used human ratings of attributes to predict visual sentiment. We did three experiments. In experiment 1, we used all attributes-observed and imputed-in fused model 2 (14 nodes on the left in <ref type="figure" target="#fig_5">Fig. 5</ref>). In experiment 2, we only used observed attributes-the 6 survey questions. In experiment 3, to see if fusing more datasets made a difference, we built the perception models for fusions of only pairs of datasets (i.e., Sentiment and Visual Realism Dataset, Sentiment and Memorability Dataset). The models' structures were the same as <ref type="figure" target="#fig_5">Fig. 5</ref> except that their coefficients differed. In each experiment, human attribute ratings were used to train SVM <ref type="bibr" target="#b49">[50]</ref> for binary sentiment classification. We used grid search to select cost, RBF kernel parameter γ, and ǫ hyperparameters. We split the data into 80% as a training set and 20% as a test set. We used area under ROC curve (AUC) as an evaluation metric. As shown in <ref type="figure">Fig. 6a</ref>, using all the attributes based on the fusion of three datasets produced the best performance, suggesting the advantage of data fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Predicting sentiment with machine features</head><p>We tested our model-based algorithm on binary classification of sentiment with the same SVM settings as Sec. 5.3. Late fusion was performed as weighted average of the regression outputs for respective perceptual factors. We compared the result with four baselines, namely dense SIFT <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b48">49]</ref>, GIST <ref type="bibr" target="#b46">[47]</ref>, dense HOG2x2 <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, and LBP <ref type="bibr" target="#b53">[54]</ref>. Our method outperformed all the baselines <ref type="figure">(Fig. 6b)</ref>.</p><p>We further compared our algorithm with two state-of-theart algorithms: SentiBank descriptor <ref type="bibr" target="#b3">[4]</ref> and unsupervised feature learning. First, we replicated the experiment in <ref type="bibr" target="#b3">[4]</ref> using the same set of features on the same Twitter Dataset. Second, we constructed an unsupervised feature learning framework with a single-layer triangular K-means encoding <ref type="bibr" target="#b54">[55]</ref> on image patches preprocessed by local intensity and contrast normalization, as well as whitening. We scanned each image with 16-by-16 pixel receptive field and 1 pixel stride, before mapping the preprocessed image patches to 512-dimensional feature vectors. Again our method excelled <ref type="figure">(Fig. 6b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Predicting visual realism and interestingness</head><p>Being generalized and expandable, our model can be applied to predict other perceptual factors such as visual realism (the degree an image appears to be a photo rather than computer generated <ref type="bibr" target="#b2">[3]</ref>) and interestingness, by modifying the layer-2 factor to the corresponding perception (see supplementary material for modified models). We used the same human attributes and computer algorithm as Sec. 5.3 and 5.2, respectively. We performed both support vector regression and binary support vector classification, using the same setting as Sec. 5.3. Features were fused through weighted kernel sum based on their corresponding loadings to layer-2 factors. Ground-truth ratings for visual realism and interestingness were collected using AMT. For binary classification, images with ground-truth realism score &gt; .50, and interestingness score &gt; .75 ([2]) were separately taken as positive, negative otherwise.</p><p>As shown in <ref type="table" target="#tab_5">Table 3</ref>, our complete attributes set from data fusion (1st row) performed the best among all evaluation metrics. Our computational features motivated by the perception model (3rd row) produced consistently better results than the state-of-the-art algorithms (4th row). Sample images with computational predictions are shown in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Discussion</head><p>Power of data fusion: Using all data in fused datasets was consistently better than using only observed data, and the performance based on the fusion of three datasets was better than that based on fusions of two datasets <ref type="figure" target="#fig_1">(Fig. 6a, Table 3</ref>,). This suggests that data imputation boosts performance. Recall that we only collected 6 attributes, each from one latent factor, and after imputation we had 14 attributes which predicted better than the 6 attributes. In other words, we got the extra performance for free out of a representative subset of human annotation through data fusion. Does the performance gain come from information gain due to data imputation? The answer is probably no according to the data processing inequality of information theory <ref type="bibr" target="#b55">[56]</ref>. The information gain most likely comes from the expansion of observables due to the data fusion per se, and data imputation makes the usage of off-the-shelf classifiers possible. Nevertheless, the inferred perception model provides effective guidance for human data collection, if needed, to achieve true information gain.</p><p>Power of the perception model: We have already shown how the perception model guided the human survey design and data fusion. More importantly, the model is able to guide the design of a generalized computer algorithm for various image perceptions. Our algorithm was consistently better than its alternatives in sentiment prediction <ref type="figure">(Fig. 6b)</ref> and visual realism and interestingness estimation <ref type="table" target="#tab_5">(Table 3)</ref>. Notably, our algorithm outperformed the original algorithms built from the constituent datasets. This suggests that our perception model can guide the design of a generalized computational predictor, which produces more human-consistent results. Note that for sentiment prediction, the closest two methods were dense SIFT and SentiBank. However, dense SIFT was sampled at multiple scales and learned a dictionary of codewords, which finally generated a feature of 3000 dims. SentiBank detectors were trained on about 500k Flicker images <ref type="bibr" target="#b3">[4]</ref>. In contrast, our algorithm is based on a fused dataset of only 4603 images and sparse features (599 dim). In short, our approach based on the structure of human perception beats both a highly data-driven and a dense pooling approach that lack of a visual perception model. Holistic perception: By simply changing the weights of the features, the same algorithm can be used to simultaneously predict visual sentiment, interestingness and realism while holding a consistent correlation among these properties. This provides a holistic computation in which different dimensions of human perception share a consistent latent structure. More attributes can be computed by expanding our perception model (e.g., persuasiveness <ref type="bibr" target="#b5">[6]</ref>, trustworthiness <ref type="bibr" target="#b56">[57]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>In this paper, we propose a paradigm for building generalized and expandable models of human image perception. Our approach sheds new light on creating "big human data". The resulting model provides a novel ontology for design of both human surveys and generalized computer algorithm design for multiple visual perceptions. One limitation of our approach is that we used only linear modeling but human perception might be non-linear. Therefore, direct prediction from MI and SEM are inferior to non-linear SVM (see supplementary materials for detailed experiments). Our future work will include non-linear models such as Isomap <ref type="bibr" target="#b57">[58]</ref>, hoping to get better prediction results from the model itself and boost performance through fusing larger scale datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>CF</head><label></label><figDesc>Is ≥ .93, RM SEA ≤ .092.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Human perception model (fused model 1) based on the fusion of two datasets. Those with purple borders are from Visual Realism Dataset, those with green borders are from Memorability Dataset. Similar attributes from both datasets have orange borders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Frequency distribution (by normal fitting) of observed and imputed values of the first six attributes in fused model 1. For a complete list see supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Human perceptual model (fused model 2) based on the fusion of three datasets. In Sentiment Dataset, attributes with blue borders were collected from survey on AMT, other attributes were derived through imputation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Human annotated attributes from two datasets, used for perception modeling. Attributes that are similar across datasets share the same non-black color. Spatial layout: Clean scene? Close-range vs. Distant-view; Neat Space? * † ; Common perspective? * Empty space vs. Full space Enclosed space vs. Open space * ; Perspective view vs. Flat view; Mirror symmetry vs. No mirror symmetry; Empty space vs. Cluttered space Aesthetics and presentation: Sharp vs. Blurry * ; Expert photography? Lighting effect natural * ? Color appearance natural * ? Colors go well together? Colorful * ? High quality vs. Low quality; Attractive to you? Post-card like? Is aesthetic? Pleasant vs. Unpleasant scene * ; Boring vs. Striking colors; Unusual or strange vs. Routine or mundane; High quality (expert photography) vs. Poor quality photo; Attractive vs. Dull photo Feelings: Familiar with the scene? Familiar with the objects? * Unusual or Strange?; Mysterious?; Makes you happy?; Makes you sad? † ; Exciting? Frightening * ? Arousing * † ? Funny? Engaging? Peaceful? Interesting * ? Striking? Strange? Mysterious? Makes you happy? Makes you sad? † Exciting? Semantics: Contain fine details * ? Object appearance natural * ? Naturally-occurring objects combinations? * Dynamic or energetic scene? Is there a storyline? Action going on? Something moving in scene? About to happen? Lot going on? Have a lot to say; Length of description; Dynamic scene? Static scene? * Picture tells a story? * Attributes for imputation in Sec. 4. † Selected as auxiliary variables</figDesc><table>Visual Realism Dataset [3] 
Memorability Dataset [5] 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Features for computational prediction. Sum of the absolute weights of natural and familiar.</figDesc><table>Perception factor Weights 
Computer feature 
Dim 
Natural, Familiar 
.51 1 Natural image statistics [46] 72 
Artistic 
.20 
Ke's method [14] 
12 
Space 
.17 
GIST [47] 
512 
Weird 
.37 
LOF [48] 
3 

1 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Experiment results for visual realism and interestingness. ρ v and A v are the Spearman's rank correlation and AUC for visual realism, respectively. ρ i and A i are for interestingness.Figure 7: Sample images with computational predictions. Images with green border have positive sentiment, red border for negative sentiment. The bottom right image with blue border was misclassified as positve sentiment (due to our algorithm' inability to understand text). The four images in lower right quadrant (with grey background) are misclassified as being interesting (partly due to the high threshold of ratings (.75) for being interesting). The rest are correctly classified on interestingness, visual realism, and sentiment.</figDesc><table>Feature type 
Regression 
Classification 
ρv 
ρi 
Av 
Ai 
All attributes 
.70 
.71 
.90 
.75 
Only observed attributes 
.69 
.70 
.84 
.64 
Our method 
.54 
.31 
.82 
.71 
[3], [2] 1 
.51 
.27 
.77 
.59 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In the same row, ρv and Av are based on the algorithm in<ref type="bibr" target="#b2">[3]</ref>, ρi and Ai are based on the algorithm in<ref type="bibr" target="#b1">[2]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>We would like to thank Robert Kirkpatrick and Michael Neale for helpful discussions on statistical modeling. This research is supported by the National Research Foundation, Prime Ministers Office, Singapore under its International Research Centre in Singapore Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephens-Davidowitz</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The interestingness of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Nater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1633" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An automated estimator of image visual realism based on human cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Shaojing Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheston</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangding</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4201" to="4208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the intrinsic memorability of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2429" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual persuasion: Inferring communicative intents of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungseock</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Steen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding image virality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02318</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning a discriminative model for the perception of realism in composite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>arX- iv:1510.00477</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crowdsourcing annotations for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hierarchial model for visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cognitive Neurodynamics (II)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="607" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyzing dependence structure of the human brain in response to visual stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohan</forename><surname>Fadlallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José C</forename><surname>Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Príncipe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="745" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Naturalness and interestingness of test images for visual quality evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raisa</forename><surname>Halonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stina</forename><surname>Westman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirkko</forename><surname>Oittinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="78670" to="78670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithmic inferencing of aesthetics and emotion in natural images: An exposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritendra</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Ze</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding and predicting image memorability at a large scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhil</forename><forename type="middle">S</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical models of object recognition in cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1025" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The cognitive neuroscience of vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Blackwell Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A neural circuit for spatial summation in visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hillel</forename><surname>Adesnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Bruns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Scanziani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">490</biblScope>
			<biblScope unit="issue">7419</biblScope>
			<biblScope unit="page" from="226" to="231" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Early vs. late vision: The role of early vision in spatial reference systems. New Outlook for the Blind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sensation and perception. Cengage Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse coding and decorrelation in primary visual cortex during natural vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Vinje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="issue">5456</biblScope>
			<biblScope unit="page" from="1273" to="1276" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eye smarter than scientists believed: neural computations in circuits of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Gollisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Meister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="164" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural evidence for intermediate representations in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irving</forename><surname>Hayworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4024" to="4031" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multisensor data fusion: A review of the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahador</forename><surname>Khaleghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fakhreddine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saiedeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="44" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale image annotation using visual synset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A comparison of inclusive and restrictive strategies in modern missing data procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Ming</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">330</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Torsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wordnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Principles and Practice of Structural Equation Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><forename type="middle">B</forename><surname>Kline</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Guilford Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Introduction to factor analysis: What it is and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-On</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">W</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Number 13. Sage</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Latent variable models: An introduction to factor, path, and structural analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John C Loehlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Lawrence Erlbaum Associates Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The method of path coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewall</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="215" />
			<date type="published" when="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Imputing missing values before building an estimator</title>
		<ptr target="http://scikit-learn.org/stable/auto_examples/missing_values.html#example-missing-values-py" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multiple imputation for missing data: Concepts and new development (version 9.0)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>SAS Institute Inc</publisher>
			<biblScope unit="page">49</biblScope>
			<pubPlace>Rockville, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple imputation after 18+ years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">434</biblScope>
			<biblScope unit="page" from="473" to="489" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Similarity and attraction in close relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elaine</forename><surname>Hatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard L Rapson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications Monographs</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attraction and close relationships. The handbook of social psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Berscheid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Development of visual preference for natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John H</forename><surname>Balling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environment and Behavior</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Humans prefer curved visual objects. Psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A description of affective quality attributed to environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geraldine</forename><surname>Russel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="322" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Investigation of large display color image appearance-lll: Modeling image naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pointer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rhodes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIST</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31104" to="31105" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On advances in statistical modeling of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="33" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Building the gist of a scene: The role of global image features in recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in brain research</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="23" to="36" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lof: identifying density-based local outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM sigmod record</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Render me real?: investigating the effect of render style on the perception of animated virtual humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Breidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">91</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Joshua B Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
