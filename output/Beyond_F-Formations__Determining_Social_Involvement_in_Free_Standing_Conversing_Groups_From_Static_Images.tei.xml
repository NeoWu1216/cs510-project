<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond F-formations: Determining Social Involvement in Free Standing Conversing Groups from Static Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
							<email>lu.zhang@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<addrLine>Mekelweg 2</addrLine>
									<settlement>Delft</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<addrLine>Drienerlolaan 5</addrLine>
									<settlement>Enschede</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayley</forename><surname>Hung</surname></persName>
							<email>h.hung@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<addrLine>Mekelweg 2</addrLine>
									<settlement>Delft</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond F-formations: Determining Social Involvement in Free Standing Conversing Groups from Static Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the first attempt to analyse differing levels of social involvement in free standing conversing groups (or the so-called F-formations) from static images. In addition, we enrich state-of-the-art F-formation modelling by learning a frustum of attention that accounts for the spatial context. That is, F-formation configurations vary with respect to the arrangement of furniture and the non-uniform crowdedness in the space during mingling scenarios. The majority of prior works have considered the labelling of conversing group as an objective task, requiring only a single annotator. However, we show that by embracing the subjectivity of social involvement, we not only generate a richer model of the social interactions in a scene but also significantly improve F-formation detection. We carry out extensive experimental validation of our proposed approach by collecting a novel set of multi-annotator labels of involvement on the publicly available Idiap Poster Data; the only multi-annotator labelled database of free standing conversing groups that is currently available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the analysis of mingling scenarios has received growing attention. The potential of studying social patterns of behaviour in visual scenes has great potential with the recent advances in social signal processing <ref type="bibr" target="#b20">[21]</ref>. Potential applications include enabling robots to approach a group and offer assistance in a socially intelligent manner <ref type="bibr" target="#b17">[18]</ref>, or social surveillance <ref type="bibr" target="#b2">[3]</ref>, image interpretation or retreival <ref type="bibr" target="#b13">[14]</ref>.</p><p>A major challenge in visual scene interpretation is addressing the problem of bridging the semantic gap <ref type="bibr" target="#b13">[14]</ref>, which defines the disconnect between information that can be extracted from the pixels in an image and how a human might interpret its contents. Traditionally, this gap has been attributed to the mapping of imagery data to objective interpretations such as the labelling of objects or activities in a scene. However, in recent years, scene analysis has started to consider more complex and subjective concepts such as safety <ref type="bibr" target="#b10">[11]</ref> or ambiance <ref type="bibr" target="#b11">[12]</ref>. Similarly, in the area of social surveillance <ref type="bibr" target="#b2">[3]</ref>, researchers have been trying to ascribe social meaning to social scenes. However, unlike conventional scene analysis, social surveillance bridges a more complex semantic gap that associates observable behavioural cues to social phenomena. We call this the social semantic gap. Since social phenomena are extremely complex, it is desirable to use findings from social psychology to help inform how visually observed behaviours could be linked to social phenomena to help bridge the gap in an informed manner.</p><p>Given the great advances already in person tracking and orientation detection, we focus on how these solutions can be used as behavioural input for bridging the social semantic gap. Specifically, we approach the novel problem of detecting associates of conversing groups (or the so-called Fformations). F-formations are defined by social psychology theory as <ref type="bibr" target="#b7">[8]</ref>; as a spatial organization of people gathered for conversation where each member has an equal ability to sense all other members. The so-called associates of Fformations are defined by psychologists as people who are attached to an F-formation but do not have the same status as full members (see <ref type="figure" target="#fig_0">Figure 1</ref> (a)).</p><p>To the best of our knowledge, state-of-the-art methods for F-formation detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> have made three simplifying assumptions. First, each individual is assumed to have a binary membership to an F-formation and to our knowledge, no work has considered refining and enriching this model to label individuals who are partially involved in it. Second, global parameters for the frustrum of attention of each person have been used for the entire visual scene. However, social psychology theory has cited the relaxation of the geometric model of an F-formation when consider-ing the spatial constraints of a room and the furniture in it <ref type="bibr" target="#b7">[8]</ref>. Finally, aside from Hung et al. <ref type="bibr" target="#b5">[6]</ref>, we believe that no other works have seriously addressed the inherently subjective nature of F-formation detection. Our experiments show that by considering the subjectivity of the task, we are better able to model the social scene. That is, by performing associate detection, we show that we can also significantly improve performance on the F-formation detection task.</p><p>Concretely, we offer the following contributions; First, we address the novel task of detecting associates of Fformations and propose a novel feature representation that copes with learning from sparse training data. We also show that the state-of-the-art model for full members of Fformations <ref type="bibr" target="#b18">[19]</ref> are not appropriate for the modelling of associate behaviour. Second, we model the spatial context of a scene for better F-formation and associate detection by learning a location-dependent frustum of attention of individuals in the scene. Moreover, we address the problem of learning the relative weighting between proximity and orientation given the spatial context of furniture. Third, we contribute new multi-annotator labels on the publicly available Idiap Poster Dataset <ref type="bibr" target="#b5">[6]</ref> for modeling associates. Finally, we carry out a deep evaluation and analysis of associates to investigate the complexity of this novel task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Definitions</head><p>F-formations and their Associates The psychologist Kendon <ref type="bibr" target="#b7">[8]</ref> defined a single conversing group as an Fformation; as a spatial and orientational organization of individuals where each member has equal access to all other members of the group. An F-formation usually consists of three parts, see <ref type="figure" target="#fig_0">Figure 1</ref> (a). The o-space is a convex empty space surrounded by the F-formation members, in which every participant orientates themselves inwards, and no external people are allowed. The participants themselves stand in the p-space, which is a narrow strip surrounding the o-space, while the area beyond is called the r-space. Its definition has made it a popular detection task as it relates well to finding maximal cliques in edge-weighted graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. In practice, a geometric model of a conversing group should be adapted when considering the spatial constraints of a room and the furniture in it <ref type="bibr" target="#b7">[8]</ref>. For instance, people talking in front of a laptop may stand closer and look at the same direction (see <ref type="figure" target="#fig_0">Figure 1</ref>(c)), which maintains an F-formation although their o-space could be violated.</p><p>Unlike full members of F-formations, Kendon <ref type="bibr" target="#b7">[8]</ref> defines associates to be people who are attached to an Fformation but who are not fully involved in the conversation. Associates can be people who try to join an Fformation but are not fully accepted by the group, or can leave an F-formation abruptly without disturbing the conversation. We name these out-group and in-group associates respectively as the former tends to stand in the r-space while the latter tends to stand in the p-space. Another example of an associate could be someone who is waiting for a full member (e.g. their spouse) to leave the F-formation and is not interested in engaging in the conversation.</p><p>While F-formations can easily be modelled by either maximal cliques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> or a joint centre-of-focus in the o-space <ref type="bibr" target="#b1">[2]</ref>, associate behaviours are not so clearly linked to a single set of social cues. Therefore, the associate detection problem requires us to bridge a wider gap and the nature of the problem and how to solve it cannot be so easily translated into a single set of geometric constraints. From the perspective of semantic labelling of a scene, we must also consider that distinguishing full members of Fformations from associates and also singletons is quite important conceptually. Singletons have no social influence on the groups around them. Full F-formation members have the most potential to influence other members of the groups. Meanwhile, associates have the least potential to influence full members but could be influenced by them. Crucially, in-group associates could be mistaken for full F-formation members and out-group associates for singletons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frustum of Attention</head><p>The frustum of attention <ref type="bibr" target="#b18">[19]</ref> (or transactional segment, as defined by Kendon <ref type="bibr" target="#b7">[8]</ref> can be considered as a cone-like region extending from the body that represents the spatial and angular extent at which someone is able to see, hear, and potentially touch something or someone else. It represents a three-dimensional space around the human body in which most of our senses and actions are able to be deployed for social interaction. Prior studies have shown that head pose <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>, body pose <ref type="bibr" target="#b5">[6]</ref>, gaze <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7]</ref>, and proximity <ref type="bibr" target="#b5">[6]</ref> often provide reliable features for F-formation modeling.</p><p>Recent state-of-the-art approaches have tended to use sampling methods to approximate the frustum of attention where the parameters are set carefully by grid search on the entire dataset and the same global model for the frustum of attention is used <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13]</ref>. There are two main drawbacks of this approach. First, the parameters are likely to overfit on a certain dataset due to the same data being used for training and testing. Second, the variation in F-formation shape caused by the furniture arrangement and non-uniform densities in the crowding of the scene cannot be captured. For example, people can tend to crowd more densely around the area of a bar area even if they are not trying to order drinks or lean on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Exploiting the frustum of attention is very important for detecting F-formations, studies have showed that head pose <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>, body pose <ref type="bibr" target="#b4">[5]</ref>, gaze <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7]</ref>, and proximity <ref type="bibr" target="#b5">[6]</ref> often provide reliable patterns. In <ref type="bibr" target="#b21">[22]</ref>, F-formations are detected by estimating people's position and lower body orientation using only their head position and orientation from a single camera. The modularity cut algorithm <ref type="bibr" target="#b8">[9]</ref>   was proposed to identify F-formations from automatically extracted trajectories by <ref type="bibr" target="#b23">[23]</ref>. To our knowledge, in terms of the treatment of hierarchy in groups, the work of <ref type="bibr" target="#b23">[23]</ref>. is quite close to ours as they proposed to used eigendecomposition to find centrality in a large mingling group of people. Unfortunately, the data they used was staged but showed participants with high centrality to be those who mingled with more different people.</p><p>A Hough voting strategy was proposed in <ref type="bibr" target="#b1">[2]</ref>, which estimates the locations of o-spaces by density estimation. The size of F-formation was taken into account using a multiscale Hough voting strategy in <ref type="bibr" target="#b12">[13]</ref>. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, detecting F-formations is considered as a clustering problem, where each person is defined as a node in the graph, and each edge is the "closeness" between a pair of people. The goal is to find a dominant set <ref type="bibr" target="#b9">[10]</ref> in the graph and the edges of the graph are computed based on body orientation and proximity. In <ref type="bibr" target="#b18">[19]</ref>, the temporal information is added in the dominant set based approach. A density-based approach was proposed in <ref type="bibr" target="#b3">[4]</ref> where the final purpose of the task was to dynamically select camera angles for automated event recording. In <ref type="bibr" target="#b16">[17]</ref>, temporal patterns of activities were subsequently analyzed. In this paper, we follow the dominant set framework because it gives reliably good results in general <ref type="bibr" target="#b18">[19]</ref> and enables a systematic explanation of the learned model so we can interpret better the social phenomena at play in the experimental data. In contrast to the growing numbers of works on F-formation detection, to our knowledge, no one has attempted to detect associates before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data</head><p>We used the publicly available Idiap Poster Data [6] 1 , which consists of 3 hours of aerial video of over 50 people during a scientific poster session and coffee break. In this poster session, posters are put around the perimeter of the scene, two small round tables are located in the middle and bottom of the image, a drinks table is located in the bottom right of the image, two entrances are located at the far left and top right of the scene. A screen shot is shown in the left of <ref type="figure" target="#fig_4">Figure 5</ref>. In total, 82 images including 1700 instances of people were annotated by 24 paid annotators, where each image was annotated by 3 annotators. No consecutively selected images contained the same set of formations. We used the positions and body orientation provided separately by Hung et al. <ref type="bibr" target="#b5">[6]</ref>. We augmented this data by adding annotations of associates of the F-formations.</p><p>We analyzed the annotations to see whether there was full agreement between the annotators about all members of an F-formation and associates. 211 instances of associates were annotated. 84 associates were identified with majority agreement (39.8%) and 34 for full agreement (16%). We computed the F1 score considering one annotation as ground truth and one other annotation as detection for each set of data annotated by the same 3 annotators. The mean and standard deviation of the F1 score are 44% and 13% respectively, which shows that associates are not as straight forward to label compared to F-formations (94.74% mean average F-measure when computing the agreement for Fformations from the data). We consider all the annotated associates have different levels of involvement to groups, that can be perceived by annotators.</p><p>To explore the relative angle and orientation relationship between different types of associates of F-formations, we computed histograms of both the distance to, and the relative orientation differences between, an associate and his closest F-formation member as shown in the top and bottom of <ref type="figure" target="#fig_4">Figure 5</ref>(b) on p. 8 respectively. The relative orientation of associates to their closest F-formation member has a peak in probability mass at 0, and π/3 while there is only a single peak in the lower histogram. This shows that associates tend to stand similarly closely to their nearest Fformation member. The double peak seen in the relative orientation highlights the idea of two types of associates; those who stand in the p-space of an F-formation but appear less involved in the conversation (in-group associates) and those that stand in the r-space, facing towards the Fformation (out-group associates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Methodology</head><p>We detect an associate by modeling its social prior with its associated conversational group (F-formation) based on non-verbal cues where a set of scale (group size) and orientation invariant features are used to train the social prior. The flowchart of the methodology is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Given the position and body orientation on the group plane of a set of people, a group detector is first applied to find the conversational groups location (F-formation will be used in the following sections to indicate conversational groups); social prior features are extracted next from every individual; trained classifiers will be used to determine the involvement of a certain people to a F-formation, for instance, Fformation members, associates, or singletons. The modules are described in the following subsections separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Modeling the F-formation as a Dominant set</head><p>Building on prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, we exploit the dominant set framework. In an image, people can be represented as a graph G = (V, E, A), where the nodes V are people, E is the set of connections between people, and A = {a ij } , i, j ∈ V is an affinity function which defines the "closeness" between each pair of people. Given a subset S of the set of of nodes in the graph, the average weighted degree of a node i ∈ S with respect to set S is k S (i) = 1 |S| j∈S,j =i a ij . The relative affinity between node j / ∈ S and i is φ S (i, j) = a ij − k S (i), and the weight of each i with respect to a set S = R ∪ {i} is defined as</p><formula xml:id="formula_0">w S (i) =    1 |S|=1 j∈R φ R (j, i)w R (j) otherwise ,<label>(1)</label></formula><p>which measures the overall relative affinity between i and the rest of the nodes in S. The relationship between internal and external nodes of a dominant set S is defined as</p><formula xml:id="formula_1">w S (i) &gt; 0, ∀i ∈ S (2) w S∪{i} (i) &lt; 0, ∀i / ∈ S.<label>(3)</label></formula><p>Detecting a dominant set is identical to solving the following standard quadratic programme</p><formula xml:id="formula_2">max x x T Ax, s.t. x ∈ ∆, where ∆ = x ∈ R |V | : i∈V x i = 1, x i ≥ 0, i = 1, · · · , |V | .</formula><p>The indexes of non-zero x i should correspond to the an F-formation, in such a way that a F-formation can be identified. This optimization problem can be solved with a method from evolutionary game theory, called replicator dynamics. The first-order replicator can be represented as</p><formula xml:id="formula_3">x i = x i (Ax) i</formula><p>x T Ax . Once x converges, one set of F-formation members are detected. A peeling method is used where the detected group is removed and the replicator dynamics is repeated to find the next F-formation. This peeling method is repeated until the minimum distance of pairwise F-formation members is larger than the maximum distance of detected pairwise F-formation members for a given image. Similar to <ref type="bibr" target="#b5">[6]</ref> this enables a stopping criterion that is sensitive to the global context of the scene. For more details, see <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Social involvement features</head><p>As described in Section 1 associates have a complex behaviour that is strongly related to the F-formation that they are associated with. They can exist in either the p-space or r-space. Moreover, unlike the maximal clique constraint of full members of F-formations, associates should be mathematically defined with respect to the spatial arrangement of a candidate set of full members of an F-formation. Searching the space of all possible solutions for an associate and F-formation is NP. Fortunately, in practice, associates tend to be scatted sparsely enough amongst the F-formations in a scene so that the maximal clique assumption for a single F-formation is not severely disrupted by their presence. Therefore in the first instance, using any existing Fformation detection method to reduce the space of possible hypothesis associate and F-formation pairs is reasonable.</p><p>Despite this simplification, another challenge still remains. Due to its sparsity, it is unlikely that a sufficient set of examples exist to account for all possible spatial configurations of an associate and F-formation. Therefore, applying similar features that were used to define full members will lead to a representation that is too sparse to learn from. To make sufficiently descriptive features, we hypothesise therefore that they must be both invariant to the rotation of the associate relative to the group, and also insensitive to the size of the group.</p><p>To better understand associates and avoid incorrect Fformation detection in the earlier step (e.g., detecting associates as full F-formation members), every individual in the data is considered as an associate candidate, so an associate candidate could be an F-formation member, an associate, or a singleton in reality. Three sets of social prior features f = [f p , f o , f s ], centered at the associate candidate, are extracted to represent the geometric relationship of an associate candidate and its associated F-formation, where the features are based on proximity, body orientation, and group size, respectively. The closest F-formation C to a certain associate candidate p a is considered as the associated F-formation of this associate candidate, and p k indicates the location of the k th F-formation member in C.</p><p>Each set of social prior feature f is a 12-bin histogram, which is defined based on the angle of the vector between F-formation member p k and an associate candidate ∠(p k − p a ), so that every bin covers an angle of π/6. We define the m th bin of the three sets of features as</p><formula xml:id="formula_4">f p m = 1 Z d · |C m | k∈Cm p k − p a ,<label>(4)</label></formula><formula xml:id="formula_5">f o m = 1 Z o · |C m | k∈Cm (∠p k − ∠p a ) ,<label>(5)</label></formula><formula xml:id="formula_6">f s m = 1 Z s |C m | ,<label>(6)</label></formula><p>where the set of F-formation members located in this bin is C m . We use f p m to represent the average distance between Fformation members in C m and p a , f o m to represent the average relative body orientation between F-formation members in C m and p a , and f s m to represent the relative person density in C m . The features are normalized by Z d , Z o , and Z s , where Z d is the maximum proximity between associated F-formation members and associate candidate, Z o = 2π, and Z s is the maximum F-formation size. The middle image in <ref type="figure" target="#fig_1">Figure 2</ref> shows examples of the scale or orientation invariant feature representations of an associate and a singleton, which encode people's relative location, orientation and group size.</p><p>Associatess detection is challenging because they are likely to be detected as full F-formation members compare to singletons who are usually far away from an F-formation. We use a one-vs-the rest strategy to train an associates detector. In the experiment, we compare a set of classifiers, such as Parzen, RBF SVM, Random Forests, and AdaBoost, with 10 fold cross validation. Parzen classifier gave the best performance on our dataset. In our experiment, we used 211 instances of annotated associates, 235 full-agreement singletons and 450 full-agreement F-formations as training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Training the affinity matrix</head><p>To detect F-formations in a complex environment, we need to model the variation of the density of geometric variations of potential F-formations in the space. To capture this variation, the affinity matrix A is key. In this paper, we only consider the proximity and body orientation. The "closeness" between people i and j is defined as</p><formula xml:id="formula_7">a ij = e − d 2 ij σ 2 1 − θ 2 ij σ 2 2 ,<label>(7)</label></formula><p>where d ij is the Euclidean distance between two people, θ ij is the sum of difference between each body orientation and the angle of the vector between two people (see <ref type="figure">Figure</ref> 3), and σ 1 and σ 2 are the parameters to be learned. As the values of σ 1 and σ 2 decrease, a person is likely to stand closer and angle more directly towards the others in the Fformation (see <ref type="figure" target="#fig_4">Figure 5</ref> (a)). Likewise, as σ 1 and σ 2 increase, members of an F-formation will tend to stand further apart and orientate themselves less directly towards others (see <ref type="figure" target="#fig_4">Figure 5</ref> (a)). The objective function is defined as</p><formula xml:id="formula_8">ℓ = N n=1 1 − C {n} ∩Ĉ {n} C {n} ∪Ĉ {n} (8)</formula><p>where n is the index of an F-formation in an image, N is the total number of annotated F-formations, and C {n} and C {n} are the nth detected set of F-formation members and its corresponding annotation respectively. During training, we consider a detection C and an annotationĈ to match with each other if |C ∩Ĉ| |C ∪Ĉ| ≥ 2 3 . Considering that the shape of the F-formation can be influenced by the furniture arrangement, we learn parameters σ 1 and σ 2 as a function of a person's location p. We only update the parameters once per person when the detection goes wrong in a passiveaggressive way <ref type="bibr" target="#b0">[1]</ref>.</p><formula xml:id="formula_9">σ s (p) = σ s (p) − g s (C)∆σ s , s ∈ {1, 2}.<label>(9)</label></formula><p>Here, ∆σ s is the basic step size, which is set to a small value. An adaptive parameter g helps to adapt to different Fformation geometric variations. Given F-formation C, the adaptive parameter g is defined as</p><formula xml:id="formula_10">g 1 (C) =y i,j∈Ĉ {n}dij − i,j∈C {n} d ij i,j∈Ĉ {n}dij ,<label>(10)</label></formula><formula xml:id="formula_11">g 2 (C) =y i,j∈Ĉ {n}θij − i,j∈C {n} θ ij i,j∈Ĉ {n}θij ,<label>(11)</label></formula><p>where y ∈ {−1, 1}, y = 1 indicates a false negative Fformation member in C, while y = −1 indicates a false positive member. Hered andσ are the manually annotated proximity and frustum of attention. In each iteration, we update each person's location in the F-formation.</p><p>6. Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experiment setup</head><p>In the experiment, we initialized σ 1 = 40, σ 2 = 30 for training, whose basic update step sizes were set to ∆σ 1 = 0.1 and ∆σ 2 = π/720 respectively. The number of iterations of training for detecting F-formation and associates were both set to 300. Considering that the training samples in each precise location were not distributed densely over the images , we divided the images into blocks of 45 × 45 pixels where all people located in the same block shared the same learned parameters. We trained using each of the 3 annotations separately, applying 10 fold cross validation for each. Finally, the position and body orientations used to train our models came from the annotations of the Idiap poster data provided by Hung et al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>For evaluation, we consider a group as correctly estimated if at least (T · |C|) of their members are detected, where |C| is the cardinality of the labeled group C, and T ∈ [0, 1] is an arbitrary threshold; in <ref type="bibr" target="#b1">[2]</ref>, the scoring threshold T = 2/3, corresponds to finding at least two thirds of the members of a group. Here we also consider T = 1, to mean that a group is correctly detected only if all members are labeled correctly. From these metrics we calculate the precision, recall and F1 measures in each frame, averaging them over all the frames and the three sets of annotations. Associates are evaluated by calculating precision, recall and F1 score in the same way, where only the harder T = 1 criterion for success is used. Here, a baseline detector global-F is added, which only uses the initialized training value σ 1 = 40, σ 2 = 30 for detecting F-formation. We also compared the performance of our spatially-aware F-formation detector (Spatial-F) with state-of-the-art DSFF <ref type="bibr" target="#b5">[6]</ref>, HFF <ref type="bibr" target="#b1">[2]</ref>, ACCVKL <ref type="bibr" target="#b18">[19]</ref>, and ACCVJS <ref type="bibr" target="#b18">[19]</ref>.</p><p>Since we are the first to approach the task of detecting associates, we create three baseline detectors to compare with our proposed associate detector (social-A). Each baseline result was generated using the annotated data and not detections. First, SA labels all people who are not in an F-formation (mostly singletons) as associates. Second, RA labels people as associates of an F-formation if their distance to it is less than or equal to the average distance between pairwise members of F-formations according to the entire labeled data. Third, ADA is set based on the average disagreement between annotators where for each pair, we treated one annotation as a detected result to compute performance against another annotation. We also compared performances with different feature combinations (p: proximity features, o: orientation features, and s : group size features). The associates detector global-A extracts features based on global-F F-formation detection.</p><p>Finally, we analysed how associate detection can help improve F-formation detection. As the F-formation detector has problems mostly with in-group associates, we used the detected associates to clean up false positives in a detected F-formation. The performance of Spatial-F and global-F was evaluated with the T = 1 hard criterion using F-formations annotated with full agreement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">F-formation Detection Results</head><p>Two examples of the learned values for σ 1 and σ 2 with respect to the spatial context, are shown in <ref type="figure" target="#fig_4">Figure 5</ref> (a). People in the top F-formation standing side-by-side tend to have a large σ 2 , while people in the bottom F-formation standing face-to-face tend to have a small σ 2 .</p><p>From <ref type="table" target="#tab_0">Table 1</ref>, for T = 2/3, our detector (spatial-F) shows competitive performance to the state-of-art. This is because tuning a global value of σ can already produce a good approximation of the clean F-formation shape, particularly as the soft detection threshold already considers partially detected members of an F-formation to be sufficient, enabling a softening of the need for strongly circular formations. However, when considering the harsher criterion T = 1, our detector (spatial-F) significantly out-performs the state-of-the-art, even with a cross-validated comparison. We can also see that the spatial-F detector performs equally good with both criteria (T = 2/3or1), which shows the accuracy of our detector is very high. <ref type="table" target="#tab_1">Table 2</ref> shows that our proposed associate detector (social-A) significantly outperforms the three baselines (SA, RA and ADA), which means there are indeed certain patterns of associate behaviour that differs from the behaviour of singletons. We can also see from the performance ADA that it is also difficult for people to agree on who associates are. It also shows that social-A(p+o)with only proximity and orientation features can almost achieve the performance of the complete set of features. Interestingly, global-A shows features extracted with a less accurate F-formation detector can still obtain a similar performance with social-A where a more accurate F-formation detector spatial-F was used. This can be explained as our feature represents prototype-like F-formation structures, which can tolerate certain errors on less perfect F-formation detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results of Detecting Associates of F-formations</head><p>To understand more about associates, some examples of them are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. The red dots indicate the members' positions in an F-formation, the small red lines indicate everyone's orientation, the yellow dots indicate the correctly detected associates, the blue dots are correctly detected singletons, and the green dots show associates that were missed by the detector. From left to right, the first two images show that our detector can successfully detect associates who are in the r-space (See <ref type="figure" target="#fig_0">Figure 1(a)</ref>) trying to join an F-formation but who are not accepted by its members. The third and fourth images show that our detector can detect associates who are still in the F-formation p-space but not fully involved in the group. This conforms our analysis of the orientation and proximity of associates in Section 4 <ref type="figure" target="#fig_4">Figure 5</ref>(b).</p><p>We simulated tracking drifts on the manual labels of position and body orientation to compare the robustness of our method spatial-F with global-F on noisy test data. <ref type="figure" target="#fig_3">Figure 4</ref> (b) shows that our detector spatial-F in general performs better than the detector with global parameters global-F, however, our detector can tolerate less noise by looking at the decay rate because our learned parameters are sensitive to the location changing. As a person width is approximately 20 pixels in the image, the performance of our method starts to drop faster when the deviation of Gaussian noise is around half person width. It means our method should perform well using a reasonably robust visual tracker.</p><p>From <ref type="table">Table 3</ref>, we can see that using the feedback of the detected associates, false positive F-formation members are  removed, so that the precisions are improved significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we addressed the novel task of detecting associates of F-formations. We introduced a novel multiannotator annotations for associates of F-formations, and two methods for detecting them. Using our model, we were also able to discover patterns in proximity and orientation in the behaviours of associates that enable significant improvement over baseline methods with a detection rate of 71% Fmeasure. We proposed a spatial-context-aware F-formation detector, which models people's frustum of attention in a principled way while considering the influence of the social and spatial context. The method is in general more adaptive to different datasets so for example, different frustum of attention parameters can be learned from scenarios with a non-uniform density of crowding. Our proposed method showed competitive performance, even when training the model parameters on less data.</p><p>By cleaning the detected in-group associates from the detected F-formations, we were also able to significantly improve F-formation detection performance in all cases where there was full agreement amongst annotators on the full-members of each F-formation. Surprisingly, althougth learning a spatial-context specific frustrum of attention led to better F-formation detection, when using the output of this models to detect associates, the performance for associate detection was not better than when F-formations were detected with a spatial-context free frustrum parameters.</p><p>In summary, to our knowledge, this constitutes the first attempt on the challenging problem of automatically estimating conversational involvement levels in visual scenes of mingling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustrations of F-formations. (a) The F-formation spaces, gray people stand in the p-space. Red arrows indicate body orientation. Orange people are associates of the F-formation. (b) and (c) example snapshots: F-formations members, associates, and singletons are circled in red, yellow, and blue respectively according to one of our annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Flow diagram showing the stages of F-formation and associate detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Frustum of attention modeling with body orientation and proximity. (a) Calculation of relative orientation and proximity, (b) frustum of attention map with different parameters. The smaller the σ2 is, the narrower of frustum attention of a person is.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(a): example associate detection results: Red dots -members of an F-formation; red lines -body orientation; yellow dotscorrectly detected associates; blue dots -correctly detected singletons; and green dots -missed associate detections. (b): F1 score of F-formation detectors spatial-F and global-F and associates detectors social-A and global-A with noisy test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a): learned frustum of attention in two cases. (b): histograms of both relative orientation differences between an associate and also distance to closest nearest F-formation member.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>F-formation detection results with soft (T = 2/3) and hard (T = 1) criteria for deciding on whether an F-formation is correctly detected.</figDesc><table>Method 
T=2/3 
T=1 
Prec. Rec. 
F1 
Prec. Rec. 
F1 
DSFF [6] 0.93 0.92 0.92 0.81 0.81 0.81 
HFF [2] 0.93 0.96 0.94 0.81 0.84 0.83 
ACCVKL [19] 0.90 0.94 0.92 
-
-
-
ACCVJS [19] 0.92 0.96 0.94 
-
-
-
global-F 0.87 0.92 0.89 0.72 0.76 0.74 
spatial-F 0.91 0.98 0.94 0.91 0.98 0.94 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Associate detection results. SA: labels all singletons as associates, RA: labels people close to F-formation as associates, UA: performance based on annotator disagreement, global-A: use global-F detector to extract features, and social-A: our proposed detector (details in Sec. 6.1).Table 3. F-formation detection with associate detection feedback, results are evaluated only on F-formations annotated with fullagreement. FB-global-F and FB-spatial-F are detectors with associate detection feedback (details in Sec. 6.1). Method Prec. Rec. F1 global-F 0.75 0.94 0.83 FB-global-F 0.82 0.94 0.88 spatial-F 0.76 1.00 0.86 FB-spatial-F 0.84 1.00 0.91</figDesc><table>Method 
Prec. Rec. 
F1 
SA 
0.06 1.00 0.11 
RA 
0.11 0.84 0.19 
ADA 
0.44 0.44 0.44 
global-A(p+o+s) 0.89 0.59 0.71 
social-A(p) 
0.87 0.58 0.69 
social-A(o) 
0.91 0.55 0.69 
social-A(s) 
0.78 0.53 0.63 
social-A(p+o) 
0.89 0.57 0.70 
social-A(p+s) 
0.85 0.56 0.67 
social-A(o+s) 
0.91 0.56 0.69 
social-A(p+o+s) 0.89 0.59 0.71 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.idiap.ch/dataset/idiap-poster-data</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work has partly been supported by the European Commission under contract number FP7-ICT-600877 (SPENCER). The authors thank Jan van Gemert and Julian Kooij for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Online passive-aggressive algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Social interaction discovery by statistical analysis of F-formations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paggetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tosato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Menegaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human behavior analysis in video surveillance: A social signal processing perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Special issue: Behaviours in video</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal encoded f-formation system for social interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="937" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting social situations from interaction geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Frieß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Second International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Social Computing (Social-Com)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting f-formations as dominant sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kröse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces</title>
		<meeting>the 13th international conference on multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards automatic addressee identification in multi-party dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jovanović</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conducting interaction: Patterns of behavior in focused encounters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CUP Archive</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modularity and community structure in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="8577" to="8582" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dominant sets and pairwise clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting and understanding urban perception with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, MM &apos;15</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia Conference, MM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Loud and trendy: Crowdsourcing impressions of social ambiance in popular indoor urban places</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Santani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, MM &apos;15</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia Conference, MM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale f-formation discovery for group detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Setti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ferrario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3547" to="3551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLI-GENCE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tracking the visual focus of attention for a varying number of wandering people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1212" to="1229" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Putting the pieces together: multimodal analysis of social attention in meetings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kalimeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pianesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Multimedia</title>
		<meeting>the international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="659" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Activity analysis in crowded environments using social cues for group discovery and human interaction modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kakadiaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spencer: A socially aware service robot for passenger guidance and help in busy airports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khambhaita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kucner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lilienthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lohse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Okal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Rooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Field and Service Robotics (FSR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Game-Theoretic Probabilistic Approach for Detecting Conversational Groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Mequanint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting conversational groups in images and sequences: A robust gametheoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Mequanint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bridging the gap between social animal and unsocial machine: A survey of social signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heylen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>D&amp;apos;errico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="87" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing conversation groups in an open space by estimating placement of lower bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kakusho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okadome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funatomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man and Cybernetics</title>
		<imprint>
			<publisher>SMC</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="544" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monitoring, Recognizing and Discovering Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krahnstoever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
