<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressively Parsing Interactional Objects for Fine Grained Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
							<email>nibingbing@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiaotong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
							<email>xkyang@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiaotong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ShanghaiTech University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Progressively Parsing Interactional Objects for Fine Grained Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine grained video action analysis often requires reliable detection and tracking of various interacting objects and human body parts, denoted as Interactional Object Parsing. However, most of the previous methods based on either independent or joint object detection might suffer from high model complexity and challenging image content, e.g., illumination/pose/appearance/scale variation, motion, and occlusion etc. In this work, we propose an end-to-end system based on recurrent neural network to perform frame by frame interactional object parsing, which can alleviate the difficulty through an incremental/progressive manner.</p><p>Our key innovation is that: instead of jointly outputting all object detections at once, for each frame we use a set of long-short term memory (LSTM) nodes to incrementally refine the detections. After passing through each LSTM node, more object detections are consolidated and thus more contextual information could be utilized to localize more difficult objects. The object parsing results are further utilized to form object specific action representation for fine grained action detection. Extensive experiments on two benchmark fine grained activity datasets demonstrate that our proposed algorithm achieves better interacting object detection performance, which in turn boosts the action recognition performance over the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained action analysis has been an emerging research direction during recent years <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b11">14,</ref><ref type="bibr" target="#b17">20,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b33">36]</ref>. The major task of fine-grained interaction action analysis is to detect the interacting objects or human body parts for each video frame (in the rest of the paper, we will simply use the term objects to denote both interactional objects and human body parts). However, detecting and tracking the objects under interaction (also denoted as interactional object parsing) in fine-grained action videos is very challenging due to frequent occlussion/self-occlusion, change of object scale/orientation/appearance, and fast object or background motion, etc.</p><p>Early methods detect and track each of the interacting objects independently. For example, some works <ref type="bibr" target="#b0">[3,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b12">15]</ref> only attempt to detect and track hands in interaction videos, without consideration of the relationship between the hands and the objects being manipulated. In some contextual object recognition works <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b4">7,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b8">11]</ref>, hands and objects are detected using different methods independently, and the detections are further fused via probabilistic graphical models for high level inference, e.g., action detection or joint object and action recognition. With RGB-Depth data, Lei et al. <ref type="bibr" target="#b11">[14]</ref> proposed a system to perform fine-grained kitchen activity recognition. Different objects are detected independently. Obviously, this method is not able to utilize rich contextual information among various objects during interaction, e.g., hands and objects, to improve the joint detection performance.</p><p>Geometrical contextual information among objects, body parts and body poses could be jointly explored to enhance both object and action recognition performance. Packer et al. <ref type="bibr" target="#b17">[20]</ref> presented a joint model for objects, human poses and motion features to recognize complex, finegrained human actions in cooking action sequences. Koppula et al. <ref type="bibr" target="#b9">[12]</ref> jointly detected manipulation activities and object affordances. These works require 3D skeleton data which are not easy to obtain in practice. Ni et al. <ref type="bibr" target="#b15">[18]</ref> recently proposed a joint hand and object tracking framework called interaction tracking, which is based on the observation that there exists rich contextual information between the interaction status and the occurrence of mutual occlusion. Their method outperforms prior art.</p><p>However, joint object detection framework still has two issues. First, joint object detection frameworks are usually based on tree-like graphical models, for example, deformable part-based models <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b29">32]</ref>, And-Or graph models <ref type="bibr" target="#b35">[38,</ref><ref type="bibr" target="#b13">16]</ref>, probabilistic graphical models <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b15">18]</ref>. These models usually can only handle the binary contextual relationship between objects. While a large portion of actions involve high order interaction, e.g., hand, knife, and chopping board for the action cutting, simply considering the mutual geometrical relationship between objects is insufficient to guarantee good object parsing performance. It is infeasible for these algorithms to model higher order contextual relationship due to the high computational complexity nature of these algorithms. Therefore, we need a flexible way to model high order contextual information for better detection of interactional objects. Second, during fine grained interaction there exist frequent occlusion/selfocclusion and object appearance/scale/orientation change, which make the interactional object parsing problem very challenging. Therefore, it is very hard to guarantee the performance of joint object detection <ref type="bibr" target="#b15">[18]</ref>. One key observation is that during interaction, some objects might be "easier" to detect than others. For example, during the chopping action, the chopping board and the hand is often easier to detect than the knife (which is often occluded). If we can confidently identify some "easy" objects first, it would be very helpful for us to further use the contextual information to seek other related and more "difficult" objects. In other words, an easy-to-difficult progressive/incremental detection approach might be more preferable for interactional object parsing. Moreover, the detection order should be varying according to different interaction scenarios. Inspired by the recent success of recurrent neural networks (RNN) <ref type="bibr" target="#b3">[6]</ref> (especially application of the longshort term memory network (LSTM) <ref type="bibr" target="#b5">[8]</ref> in people detection <ref type="bibr" target="#b23">[26]</ref>), we propose an end-to-end system based on recurrent neural network to perform frame by frame interactional object parsing, which can alleviate the above mentioned difficulty through a progressive/incremental detection manner. For each frame, expressive image features from the stateof-the-art deep convolutional models (e.g., VGG-19 <ref type="bibr" target="#b22">[25]</ref>) are input to our proposed interactional object parsing network. Instead of jointly outputting all object detections at once, for each frame, we use a set of LSTM nodes to progressively/incrementally refine the detections. After passing through each LSTM node, more object detections are consolidated and thus more contextual information could be utilized to determine more difficult object detections. By applying the proposed network, all detection and contextual information up to the current nodes could be maximally explored to generating a better detection in the next processing LSTM node. Therefore, "easy" objects have higher probability to be confidently detected and confirmed in the early LSTM nodes of the network. Based on the contextual information between these already "discovered" objects with the uncertain ones, the later LSTM nodes could better identify the "more difficult" objects (e.g., those occluded ones during interaction, or those have large appearance/scale/orientation variation). The detection results of the current frame are also input to the LSTM network associated with the next frame to facilitate inter-frame tracking.</p><p>The object parsing results are further used to compute object specific motion representations for fine grained action detection. We perform extensive experiments on two benchmark fine grained activity datasets. The results demonstrate that our proposed algorithm achieves better interacting object detection performance, which in turn boosts the action recognition performance over the state-of-the-art.</p><p>The rest of this paper is organized as follows. We enumerate some related works in Section 2. The details of our progressive interactional object parsing network along with the detailed implementation and training procedure are described in Section 3. We demonstrate experimental settings and results in Section 5. Conclusions are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Fine-grained Action Analysis. Although general action recognition has a rich literature <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b10">13,</ref><ref type="bibr" target="#b20">23,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b32">35]</ref>, fine-grained action analysis is a relatively new research direction. Rohrbach et al. <ref type="bibr" target="#b19">[22]</ref> provided a large-scale finegrained cooking action dataset with several baseline results. Their dataset has been the most important and challenging benchmark test-bed for evaluating fine-grained action recognition/detection algorithms. Several works <ref type="bibr" target="#b11">[14,</ref><ref type="bibr" target="#b17">20]</ref> utilized RGB-depth information for fine-grained action recognition; however, relying depth channel is a limitation for realistic application. Zhou et al. <ref type="bibr" target="#b34">[37,</ref><ref type="bibr" target="#b33">36]</ref> proposed a series of works on fine-grained action analysis which focus on modeling local contextual information between motion and object of interest. The work mostly related to ours is by Ni. et al. <ref type="bibr" target="#b15">[18]</ref>, where their focus is also object parsing/tracking in fine-grained action video.</p><p>Recurrent Neural Networks and LSTM. Recurrent neural networks especially the long-short term memory models <ref type="bibr" target="#b5">[8]</ref> have achieved great success in a large variety of applications including temporal modeling such as natural language processing <ref type="bibr" target="#b21">[24]</ref> and speech recognition <ref type="bibr" target="#b3">[6]</ref>, and non-temporal modeling such as image caption generation <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b26">29]</ref>.</p><p>Several works have been proposed to model action image sequences using RNN/LSTM models. Veeriah et al. <ref type="bibr" target="#b25">[28]</ref> proposed a differential gating scheme for the LSTM neural network (termed as differential Recurrent Neural Network (dRNN)), which emphasizes on the change in information gain caused by the salient motions between the successive frames. Donahue et al. <ref type="bibr" target="#b1">[4]</ref> developed a novel recurrent convolutional architecture for large-scale visual learning. They applied this model on several tasks including benchmark video recognition, image description, and video narration. Wu et al. <ref type="bibr" target="#b31">[34]</ref> extracted spatial and the short-term motion features by two Convolutional Neural Networks (CNN) to further model longer-term temporal clues. The two types of CNN-based features are further combined in a regularized feature fusion network for video event classification. The above works mostly focus on modeling temporal dependen-cies for action recognition; in contrast, our work focuses on progressive refinement of the interactional object detection within each video frame, i.e., the LSTM model applied to our problem is not focused on temporal modeling, rather, it is applied for sequencing the detection process in an easyto-difficult manner. In this sense, the most related work to ours is by Stewart and Andriluka al. <ref type="bibr" target="#b23">[26]</ref>, where LSTM network is applied for sequencing the human detection problem. Namely, for each frame, their model sequentially outputs the detection bounding boxes by exploring the contextual information between bounding boxes. Their algorithm, however, is designed for detecting only a single type of object (with many instances in the image). In contrast, our algorithm handles different types of objects simultaneously, based on our developed sequential detection refinement algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Interactional Object Parsing Network</head><p>Motivation. The task of interactional object parsing is to infer at each frame the bounding boxes for various interacting objects and human parts. To this end, we propose an interactional object detection network which progressively/incrementally outputs/refines the bounding boxes for various interactional objects at each frame. In other words, instead of confirming the localizations of all the object bounding boxes simultaneously at each frame, the image frame is input to a recurrent neural network and that network progressively refines the object detection results nodeby-node until all detected object bounding boxes cannot be improved further (e.g., when the detection confidence is below some threshold). Through this progressive detection scheme, objects that are "easy" to detect (e.g., without occlusion, with little deformation etc.) could be identified and consolidated in the early output nodes of the recurrent neural network. The detections which are "consolidated" from early nodes of the recurrent network could provide contextual information, which help to detect more "difficult" objects (e.g., those occluded or deformed ones) in later output nodes of the network. For instance, during a frying action, the hand and the fry pan might be detected much easier and their confirmed positions could help to locate the position of the ladder, which is most probably occluded by hand and difficult to localize.</p><p>Network Architecture. We begin with notations. The number of interesting objects (including human body parts, e.g., hands) is denoted as M . At each frame, our parsing network outputs a concatenated vector B, which is composed of M object bounding boxes with detection confidence scores B = (b 1 ; b 2 ; · · · ; b M ). Here each b m = (p m ; c m ) is composed of four dimensional bounding box parameters p m indicating the relative x-y coordinates and height, width of the bounding box for object m, as well as the corresponding detection confidence score c m . High-er value of the confidence score indicates higher probability that the detected bounding box matches the target object. <ref type="figure">Figure 1</ref> overviews our proposed end-to-end interactional object parsing network. Each frame image is input to a VGG-19 <ref type="bibr" target="#b22">[25]</ref> CNN model for extracting image representation. As the original image frame size is usually 640 × 480 × 3, we first re-scale it to 320 × 240 × 3 and we crop the center 224 × 224 × 3 region for processing. Note that in fine grained action, the human subject along with the objects being manipulated are always in the image center. The Conv 5 layer (D = w × h × 512-dimensional, w and h denote the receptive field size) is used as the image level representation. We denote this feature as CNN feature x. We use the VGG-19 network architecture and its ImageNet pre-trained model due to its discriminative capability in image classification task. Note that each pixel in the Conv 5 map has the receptive field size typically smaller than that of any object of interest. Namely, the resolution of the Conv 5 CNN feature map x is sufficient for object localization. The D-dimensional CNN image representation is further input to a recurrent network structure with H L-STM nodes (in this work, H is larger than M , i.e., to allow sufficient number of iterative refinement steps). Each LST-M takes the D-dimensional CNN image representation and the object parsing status C (l−1) vector from the last LSTM node and outputs the detection vector B (l) for the current LSTM node (we use l to index the LSTM node). We set the dimensionality of LSTM cell status vector C (l) as 512. In other words, at each frame t, our LSTM network generates a sequence of gradually refined object parsing vectors, i.e., B</p><p>(1)</p><formula xml:id="formula_0">t , B (2) t , · · · , B (H) t .</formula><p>Inter-frame tracking is naturally handled by inputting the cell status of the last LSTM node (l = H) of the current frame t to the first LSTM node (l = 1) of the next frame t + 1. This interactional object parsing process for frame t could be mathematically expressed as:</p><formula xml:id="formula_1">i l = σ W i x t + U i h (l−1) t + b i , f l = σ W f x t + U f h (l−1) t + b f , o l = σ W o x t + U o h (l−1) t + b o , C (l) t = tanh W c x t + U c h (l−1) t + b c , C (l) t = i l * C (l) t + f l * C (l−1) t , h (l) t = o l * tanh(C (l) t ), B (l) t = sof tmax(W h (l) t + b).</formula><p>(1)</p><p>Here i l , f l , o l and C (l) denote the input gate, forget gate, output gate, and cell status of the LSTM node l, respectively. Note that for each frame, the image CNN feature x t is input to all LSTM nodes. Through this recursive architecture, image features and contextual information (e.g., early consolidated detections) could be jointly explored to gradu-ally detect more and more "difficult" objects. Cost Function. To facilitate our idea of progressive refinement of interactional object detections, we introduce a partial matching cost, mathematically defined as follows:</p><formula xml:id="formula_2">ℓ partial (B, G, r) = m∈N ℓ c (c m , c ′ m ) + m∈P O(r) ℓ c (c m , c ′ m ) + λ m∈P O(r) p m − p ′ m 2 2 . (2)</formula><p>The cost function for a frame is expressed as:</p><formula xml:id="formula_3">ℓ(B, G) = H r=1 ℓ partial (B, G, r).<label>(3)</label></formula><p>Here G is the corresponding ground-truth parsing results, i.e., G = (g 1 ; g 2 ; · · · ; g M ) and g m = (p ′ m ; c ′ m ). N and P denote the un-matched (according to the invisible object entities in the ground-truth G, namely c ′ m = 0) and matched (according to the visible object entities in the ground-truth G, namely c ′ m = 1) subset of detection bounding boxes from B. ℓ c (y,</p><formula xml:id="formula_4">y ′ ) = − i y i log(y ′ i ) − (1 − y i ) log(1 − y ′ i ) is the cross-entropy cost. O(r)</formula><p>is the set of first r elements from the ordered matching list from the subset P. To order the matching list, we define an ordering based on the bounding box overlapping between ground-truths and predictions (IoU ). The last term is a displacement between the predicted bounding boxes' positions with the ground-truth ones. λ is a balance factor which is decided by cross-validation using a validation set. During network training, after each round of object bounding box prediction generation, we perform predicted bounding box matching and ordering with the ground-truth and proceed to further optimize the network model parameters. For testing, we take the last step prediction B (H) t for frame t to be the confirmed object parsing result. Bounding boxes with low confidence values are discarded.</p><p>The working mechanism of our network is well reflected by our designed cost function. For the first iteration (l = r = 1), the cost function only attempts to minimize the displacement between the top matched prediction bounding box O(1) to its corresponding ground-truth. The philosophy is that in the first step, we only need to detect the easiest object. When the iteration index increases, our cost function requires that more predicted bounding boxes should match their corresponding ground-truths. For every next iteration, we add one more object in the must-match list until all detections have been considered. Through this way, the challenging interactional object parsing problem is decomposed into a series of sub-problems with increasing difficulty level. As more and more object detections have been confirmed, we have more confidence to detect more difficult object based on its contextual relationship with previously detected objects. Once more object information gathered in the path, previous difficult object could also become not that difficult to detect. Note that for the predicted bounding boxes which do not have a match in the ground-truth, i.e., occluded part/object, our cost function can directly penalize this erroneous detections based on the first cross-entropy term (to minimize the confidence score associated with that object).</p><p>Discussions. Note that our formulation of progressively object detection scheme is different from the scheme proposed in the work <ref type="bibr" target="#b23">[26]</ref>. In <ref type="bibr" target="#b23">[26]</ref>, only one type of object is considered. Therefore, after each round of prediction during model training, candidate object bounding boxes should be matched to the ground-truth bounding boxes through a bipartite graph matching algorithm. In contrast, for our problem, different types of objects are involved and we have a fixed object indexing scheme, i.e., each b m in B is a fixed entity (hand, oil box, knife, etc). In other words, b m could be only matched to its corresponding ground-truth g m , i.e., hand prediction matched to hand ground-truth. Moreover, the working mechanism of our network and the one proposed in <ref type="bibr" target="#b23">[26]</ref> is different. The purpose of <ref type="bibr" target="#b23">[26]</ref> is to sequentially output all object detections at some receptive field, one at each iteration; in contrast, our purpose is to gradually refine the entire object parsing vector B. In each step, some bounding boxes (might be more than one) in B could be refined (refinement is performed on the entire vector B).</p><p>Second, previous interactional object parsing method cannot well handle the scenario when some parts are occluded. For example, as revealed by the work <ref type="bibr" target="#b15">[18]</ref>, objects and body parts tracking easily gets failed when occlusion is serious. In contrast, since our model naturally handles the occlusion problem. Specifically, our method confirms object/part localizations from a sequential manner with a nonfixed order: simple objects could be consolidated first, difficult objects could be consolidated later, and those totally occluded ones could be explicitly decided as non-detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training Details</head><p>We use the open source package Caffe <ref type="bibr" target="#b6">[9]</ref> (for the image feature learning part) and NLP-Caffe [1] (modified, for the LSTM part, similar to the usage in <ref type="bibr" target="#b23">[26]</ref>) and train the proposed network using stochastic gradient descent (SGD) and back propagation through time (BPTT). The CNN feature (VGG-19) model is initialized by the ImageNet pre-trained model. The parameters of the LSTM part are randomly initialized in the range [−1, 1]. Both the CNN part and LSTM part are jointly trained. The temporal batch size is set as 32. We use an equal learning rate for all layers. The learning rate is initialized at 0.1 (the momentum is set as 0.8) and it is adjusted manually by dividing 10 when the validation error rate stops decreasing with the current learning rate. The network converges Inter-frame tracking Refinement Refinement <ref type="figure">Figure 1</ref>. Overview of the proposed progressive interactional object parsing method based on LSTM network. We note that for the first iteration B</p><p>(1) t (output from the first LSTM node), the inferred position for hand is accurate (since it is easy to detect hand); however, the other two objects (ham and knife) is NOT accurately localized (this is because of the occlusion, illumination variation and lessdiscriminative appearance of these objects). After several iterations (B </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Fine-grained Action Detection</head><p>We develop a fine-grained action detection method based on the interactional parsing results. A video is divided uniformly by overlapped segments of length 30, 60, 90 frames (detection windows). Temporal overlappings are of 10, 20 and 30 frames. Within each video, we pool the motion features within each parsed body part/object to achieve part/object specific motion representation, similar to the scheme used in <ref type="bibr" target="#b15">[18]</ref>. It has been widely proved in previous works that this type of object-specific motion feature pooling method can significantly outperform global motion pooling method. This is because object/body part centric motion pooling is less ambiguous than global pooling method.</p><p>The local motion features we use are the improved dense motion trajectories <ref type="bibr" target="#b28">[31]</ref>. For each trajectory, we extract histogram of oriented gradient (HoG), motion boundary histogram (MBH), histogram of optical flow (HoF) and trajectory shape (TS) descriptors as in <ref type="bibr" target="#b27">[30]</ref>. We perform PCA to reduce the dimension of each descriptor by half. These features are encoded using improved Fisher vector with the number of clusters K = 128. We also apply a second PCA to reduce the overall Fisher vector encoding by a factor of 0.1 (i.e., keep around 90% energy). Assume that the dimensionality of Fisher vector is d, number of objects of interest is M , then a video segment i is represented by a d × M dimensional vector x i . We then use linear SVM learned on this segment level representation to detect the action label of every video segment. The inferred labels are averaged over three detection window scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our experiments are focused on two parts. On the one hand, we compare our fine-grained interactional object parsing result to that of the state-of-the-art object tracking algorithms. On the other hand, we show the finegrained action detection performance based on our proposed framework. Similar to the work <ref type="bibr" target="#b15">[18]</ref>, all experiments are performed on two challenging interaction-intensive finegrained action benchmarks as follows.</p><p>1) ICPR 2012 Kitchen Scene Context based Gesture Recognition dataset (KSCGR) <ref type="bibr">[2]</ref>. There are five candidate cooking menus cooked by five different actors. Each of the videos are from 5 to 10 minutes long containing 9, 000 to 18, 000 frames. The dataset contains eight types of cooking motions such as baking, boiling, breaking, etc. Following <ref type="bibr" target="#b15">[18]</ref>, the objects of interest (which we parse) are fry pan, oil bottle, salt bottle, bowl, knife, spoon, chopstick, spatula, chopping board, egg and ham. For KSCGR dataset, the e-valuation metric of action detection is the mean recognition F -score over all action categories.</p><p>2) MPII Fine-grained Kitchen Activity Dataset (MPI-I) <ref type="bibr" target="#b19">[22]</ref>. It contains 65 different cooking activities, such as cut slices, pour spice, etc., recorded from 12 participants. In total there are 44 videos with a total length of more than 8 hours or 881, 755 frames. The dataset contains a total of 5, 609 annotations of 65 activity categories. Following <ref type="bibr" target="#b15">[18]</ref>, the objects that we parse are bottle, bowl, bread, charger, electric range, cup, cupboard, chopping board, dough, drawer, egg, lid, food wrapper, knife, pan, slicer, plate, pot, blender, seasoning bottle, bottle rack, juicers, tin, tin opener and towel. For MPII dataset, we follow experimental configuration and evaluation metric defined by the dataset developer <ref type="bibr" target="#b19">[22]</ref>. In brief, leave-one-person-out cross validation is used.</p><p>For object/body part annotations, we obtained from the authors of <ref type="bibr" target="#b15">[18]</ref> around 20000 frames of annotations from the training data. These annotations are used to train all comparing detectors/trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on Interactional Object Parsing</head><p>We use the same annotated testing sequences as in <ref type="bibr" target="#b15">[18]</ref> on the testing set of KSCGR to evaluate the interactional object parsing performance. These sequences are manually annotated with object positions (bounding boxes). According to <ref type="bibr" target="#b15">[18]</ref>, these selected testing sequences are representative sequences which contain all types of human object interactions with frequent occlusions. It is a good test bed for evaluating our interactional object parsing algorithm as well as other object tracking algorithms.</p><p>The proposed interactional object parsing method is compared with the state-of-the-art tracker specifically designed for fine-grained action detection (IAT) <ref type="bibr" target="#b15">[18]</ref> (which jointly tracks hand and objects). According to <ref type="bibr" target="#b15">[18]</ref>, we use the same measuring metrics as follows:</p><p>1. Average Distance Error (Err.): the average distance between the center of the identified bounding box and the center of the ground-truth bounding box;</p><p>2. Precision (Prec.): the average percentage of frames for which the overlap between the identified bounding box and the ground-truth bounding box is at least 50 percent. <ref type="table">Table 1</ref> compares the measurements averaged over all target objects and over all frames in the video sequence. <ref type="figure" target="#fig_1">Figure 2</ref> shows several examples of the tracked/parsed results using IAT and our algorithm, for qualitative comparison. To reveal the working mechanism of our progressive object parsing refinement algorithm, <ref type="figure">Figure 3</ref> plots the change of object localization error with respect to the number of LSTM iteration steps. In the meantime, in <ref type="figure">Figure 4</ref> Sequence we also show several examples of the refinement process of our interactional object parsing algorithm.</p><p>We make several key observations from the results in Table 1 and <ref type="figure" target="#fig_1">Figure 2</ref>, 3, 4. First, our interactional object parsing method outperforms prior art (although the IAT tracker also performs quite well in most of the cases). This is because that progressively refining the interactional object detection results can break the difficult parsing task down to easier steps. Quantitatively, <ref type="figure">Figure 3</ref> shows that the average object localization (parsing) error decreases when the refinement process goes deeper. We also note from <ref type="figure">Figure 4</ref> that easy object such as hand is first localized and then the hand position provides rich contextual information to localize other interacting objects such bowl, knife and chopsticks, i.e., following an easy-to-difficult manner. Second, we note that for some action sequences such as cutting, our method outperforms the IAT significantly (see <ref type="table">Table 1</ref>). This is because that it is usually not easy to localize the knife when it is partly occluded by the operating hand, i.e., the knife's color feature often confuses with the background. In contrast, our algorithm first localizes the hand and then uses the contextual information to localize the knife, therefore the parsing accuracy is higher. This could be also observed from the 8-th and 10-th examples in <ref type="figure" target="#fig_1">Figure 2</ref>. We see that for the IAT tracker, when the object knife and chopstick are occluded, it easily gets lost during tracking. In contrast, since our algorithm well models the contextual information between hand and the operated objects, this issue could be alleviated. Third, in some cases when there exist ambiguous objects, both the IAT algorithm and our proposed algorithm could fail. For example, our algorithm also mistakes fry pan handler as spoon in the last example of <ref type="figure" target="#fig_1">Figure 2</ref>. This is because the appearances of both objects are quite similar and their spatial relationships with respect to hand are also similar during certain action.</p><p>We also compare our algorithm with the R-CNN and fast R-CNN algorithms in our off-line experiment. Results show that R-CNN and fast R-CNN perform worse than our proposed method, provided that all the experimental settings   <ref type="figure">Figure 4</ref>. Four examples of the progressive interactional object parsing refinement process of our algorithm. Each column corresponds to a frame for parsing. From top to down, it shows the refinement process, i.e., l = 1, 2, 3. Note that hand is always easily localized first and other objects which have interaction with hands will obtain more and more precise localization after several LSTM iterations.</p><p>are the same. This is due to two reasons: 1) R-CNN based methods do not consider contextual information between objects; 2) Some objects in the fine-grained video are too small or of strange aspect ratio so that R-CNN based methods can not well handle it; in contrast, our proposed method uses a progressive easy-to-difficult detection scheme so that it can deal with these difficult cases. Also, our off-line experiment shows that jointly applying LSTM on consecutive frames (tracking) performs better than simply applying L-STM on individual frames.  <ref type="figure">Figure 3</ref>. Illustration of the interactional object parsing error decreasing procedure using our iterative refinement method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Doman and Kuai [2]</head><p>IDT-IFV-SVM IAT-Action <ref type="bibr" target="#b15">[18]</ref> Ours Mean F-score 0.74 0.76 0.79 0.84  <ref type="table">Table 3</ref>. Detection performance comparisons for MPII dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on Fine-grained Action Detection</head><p>We apply the object-centric motion feature pooling introduced in Section 4 for action detection on the two benchmark fine-grained action datasets. We compare our algorithm with the state-of-the-art algorithms on fine-grained action detection. The comparing algorithms include: 1) the baseline globally pooled Fisher vector representation with linear SVM detector (the parameters of motion feature descriptors, Fisher vector construction <ref type="bibr" target="#b18">[21]</ref> and temporal sliding window are exactly the same as our proposed algorithm, denoted as IDT-IFV-SVM); and 2) the multiple-granularity based fine-grained action detection algorithm (denoted as IAT-Action) <ref type="bibr" target="#b15">[18]</ref>. On the KSCGR dataset, we also compare our method to the best reported result in the contest by Doman and Kuai <ref type="bibr">[2]</ref>. Comparison results are shown in Table 2. Detection mean F-scores are reported on the KSCGR dataset. On the MPII dataset, we also compare our method to the best reported result in <ref type="bibr" target="#b19">[22]</ref> by Rohrbach et al. Multiclass precision (Pr) and recall (Rc). The mean value of single class average precision (AP) are reported in <ref type="table">Table 3</ref>.</p><p>We note that using the object-centric motion pooling method based action detection framework (including ours and IAT-Action) boosts the detection performances, compared with using the traditional global pooling scheme. This demonstrates that the object and body part parsing algorithm is precise enough to achieve high performance objectcentric motion feature pooling and representation. Moreover, our proposed fine-grained action detection framework also outperforms the state-of-the-art IAT-Action method on both benchmarks. This is because our algorithm performs better object parsing and the pooled action representation is therefore more discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we proposed a novel progressive interactional object parsing method based on the recurrent neural network (LSTM), for the application of fine-grained action analysis. Experiments on two benchmark datasets demonstrated good parsing accuracy of our algorithm compared with prior art. Based on the good parsing results, we also achieved the state-of-the-art fine-grained action detection performances on those benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the refined locations of these objects are getting more accurate, thanks to the proposed progressively refinement scheme. In this example, H = M = 3. roughly with 30 epochs, and each epoch contains 10000 iterations. It takes around 10 days on one NVIDIA Tesla K40 GPU. During testing, parsing each frame requires around 2 seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Twelve examples of the interactional object parsing results of our method (solid line bounding boxes) and the IAT method (dashed line bounding boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1. Comparisons of the object localization performances of various object parsing or tracking methods. Numbers of frames are indicated in brackets.</figDesc><table>IAT 
Ours 
Err. Prec. Err. Prec. 
baking (3786) 
28.9 0.56 24.5 0.60 
boiling (3320) 
25.5 0.59 25.0 0.61 
breaking (299) 20.4 0.64 17.3 0.72 
cutting (1373) 
24.8 0.66 20.1 0.70 
mixing (705) 
17.9 0.68 17.4 0.69 
peeling (3241) 30.1 0.62 28.9 0.64 
seasoning (303) 12.3 0.69 13.9 0.66 
turning (3402) 15.4 0.71 15.0 0.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Detection performance (mean F-score for all classes) comparisons for KSCGR dataset.</figDesc><table>Approach 
Prec. Recall 
AP 
Rohrbach et al. [22] 19.8 
40.2 
45.0 
IDT-IFV-SVM 
24.5 
46.8 
50.7 
IAT-Action [18] 
28.6 
48.2 
54.3 
Ours 
34.8 
51.7 
58.9 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modelbased hand tracking with texture, shading and selfocclusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De La Gorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6645" to="6649" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Objects in action: An approach for combining action understanding and object perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>arX- iv:1412.2306</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous visual recognition of manipulation actions and manipulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="336" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-grained kitchen activity recognition using rgb-d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="208" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pixel-level hand detection in ego-centric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning latent spatio-temporal compositional model for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting human actions and object context for recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple granularity analysis for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Paramathayalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="756" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatialtemporal words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A combined pose, object, and feature model for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1593" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning nonregular languages: A comparison of simple recurrent networks and lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<idno>abs/1506.04878</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hand tracking by binary quadratic programming and its application to retail activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gabbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1902" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<idno>abs/1504.06678</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno>arX- iv:1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng-Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hidden part models for human action recognition: Probabilistic versus max margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1310" to="1323" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A scalable approach to activity recognition based on object use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osuntogun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Discriminative video pattern search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>T-PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1728" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interaction part mining: A mid-level approach for finegrained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3323" to="3331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pipelining localized semantic features for fine-grained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="481" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A stochastic grammar of images. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="259" to="362" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
