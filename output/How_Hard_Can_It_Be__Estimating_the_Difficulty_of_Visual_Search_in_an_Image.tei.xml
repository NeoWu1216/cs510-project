<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How hard can it be? Estimating the difficulty of visual search in an image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Mathematical Statistics and Applied Mathematics of the Romanian Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dim</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How hard can it be? Estimating the difficulty of visual search in an image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>3 Institute of Mathematics of the Romanian Academy,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of estimating image difficulty defined as the human response time for solving a visual search task. We collect human annotations of image difficulty for the PASCAL VOC 2012 data set through a crowd-sourcing platform. We then analyze what human interpretable image properties can have an impact on visual search difficulty, and how accurate are those properties for predicting difficulty. Next, we build a regression model based on deep features learned with state of the art convolutional neural networks and show better results for predicting the groundtruth visual search difficulty scores produced by human annotators. Our model is able to correctly rank about 75% image pairs according to their difficulty score. We also show that our difficulty predictor generalizes well to new classes not seen during training. Finally, we demonstrate that our predicted difficulty scores are useful for weakly supervised object localization (8% improvement) and semi-supervised object classification (1% improvement).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans can naturally understand the content of images quite easily. The visual human perception system works by first recognizing the 'gist' of the image almost instantaneously <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, just from a single glance (200 ms) and, then, in a second stage, by recognizing the individual objects in the image <ref type="bibr" target="#b32">[33]</ref> as a result of visual search. Cognitive studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref> show evidence that, for the task of searching for a pattern in an image, the user response time is proportional to the visual search difficulty, which could vary from one image to another. Images are not equal in their difficulty: some images are easy to search and objects are found fast while others are harder, requiring intensive visual processing by humans. The measure of visual search difficulty could be related to several factors such as background clutter, complexity of the scene, number of objects, whether they are partially occluded or not, and so on.</p><p>In this paper, we address the problem of estimating visual search difficulty. This topic is little explored in the computer vision literature with no data sets assessing the difficulty of an image being available. We approach our study by collecting annotations on the PASCAL VOC 2012 data set <ref type="bibr" target="#b14">[15]</ref> as human response times during a visual search task and convert them into difficulty scores (Section 2). While measuring visual search difficulty by human observations might be subject to some user variability, we believe that there are intrinsic image properties that constitute the ingredients in the unknown underlying recipe of making an image difficult ( <ref type="figure">Figure 1</ref>). We use the PASCAL VOC 2012 images annotated with difficulty scores to investigate in depth how different image properties correlate with the ground-truth difficulty scores. We find that higher level features, such as the ones learned with convolutional neural networks (CNN) <ref type="bibr" target="#b24">[25]</ref> are the most effective, suggesting that visual search difficulty is indeed a measure that relates to higher level cognitive processing. Using such features, we train models to automatically predict the human assessment of visual search difficulty in an image (Section 3). We release the human difficulty scores we collected on PASCAL VOC 2012, as well as our code to predict the difficulty of any image at http://image-difficulty.herokuapp.com.</p><p>Measuring image difficulty could have many potential applications that use the primary information that some images are harder to analyze than others. In Section 4, we demonstrate the usefulness of our difficulty measure in two object recognition applications. For the task of weakly supervised object localization, we show how to enhance standard methods based on multiple instance learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> with our measure and obtain an 8% improvement. Similarly, for the task of semi-supervised object classification, we use our measure to improve the accuracy of a classifier based on CNN features <ref type="bibr" target="#b37">[38]</ref> by 1%. Related work. There are many computer vision works analyzing global image properties such as saliency <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, memorability <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, photo quality <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr">Figure 1</ref>. Images with difficulty scores predicted by our system in increasing order of their difficulty. objects' importance <ref type="bibr" target="#b41">[42]</ref>. However, there is little work on the topic of image difficulty <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref>. Russakovsky et al. <ref type="bibr" target="#b33">[34]</ref> measure difficulty as the rank of an object's bounding-box in the order of image windows induced by the objectness measure <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. This basically measures image clutter. However, it needs ground-truth bounding-boxes in order to quantify difficulty (even at test time). Liu et al. <ref type="bibr" target="#b27">[28]</ref> predict the performance of a segmentation algorithm to be applied to an image, based on various features including gray tone, color, gradient and texture (on just 100 images). More closely related to our idea, Vijayanarasimhan and Grauman <ref type="bibr" target="#b45">[46]</ref> try to predict the difficulty of an image in terms of the time needed by a human to segment it, with the specific goal of reducing manual annotation effort. They select candidate low-level features and train multiple kernel learning models to predict easy versus hard images. However, the image segmentation task <ref type="bibr" target="#b45">[46]</ref> is conceptually different from our visual search task. For example, it might be very easy to find a tree in a particular image, although it can be very hard to segment, while a truncated car can be easily segmented but difficult to find and recognize. Jain and Grauman <ref type="bibr" target="#b21">[22]</ref> predict what level of human annotation will be sufficient for interactive segmentation to succeed. Their approach learns the image properties that indicate how successful a given form of user input will be.</p><p>In contrast to these previous works, we approach the problem from a higher level of image interpretation, for the general task of visual search and collect annotations for a much larger data set of over 10K images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Image difficulty from a human perspective</head><p>Supported by cognitive studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>, we consider that the difficulty of an image is related to how hard it is for a human to decide the presence or absence of a given object class in an image. We quantify the difficulty as the time needed by a human to solve this visual search task. This value could depend on several factors such as the amount of irrelevant clutter in the image, the number of objects, their scale and position, their class type, the relevant contextual relationships among them, occlusions and other kinds of noise. We thoroughly investigate how these properties correlate with the visual search difficulty in Section 2.2.</p><p>First, we designed a visual search protocol for collecting human response times on a crowd-sourcing platform, namely CrowdFlower 1 . We collected ground-truth difficulty annotations by human evaluators on a per image basis for all 11, 540 train and validation images in PASCAL VOC 2012 data set <ref type="bibr" target="#b14">[15]</ref>. This data set contains images with object instances from 20 classes (aeroplane, boat, cat, dog, person and so on) annotated with bounding-boxes. The images vary in their difficulty: objects appear against a variety of backgrounds, ranging from uniform to heavily cluttered, and vary greatly in their number, location, size, appearance, viewpoint and illumination. This variety makes this data set very suitable for collecting ground-truth difficulty annotations. We next describe the protocol and present informative statistics about the collected data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Can we measure visual search difficulty?</head><p>Collecting response times. We collected ground-truth difficulty annotations by human evaluators using the following protocol: (i) we ask each annotator a question of the type "Is there an {object class} in the next image?", where {object class} is one of the 20 classes included in the PAS-CAL VOC 2012; (ii) we show the image to the annotator; (iii) we record the time spent by the annotator to answer the question by "Yes" or "No". Finally, we use this response time to estimate the visual search difficulty.</p><p>To make sure the measured time is representative, the annotator has to signal that he or she is ready to see the image by clicking a button (after reading the question first). After seeing the image and analyzing it, the annotator has to signal when he or she made up his mind on the answer by clicking another button. At this moment we hide the image to prevent cheating on the time. Moreover, we made sure the annotation task is not trivial by associating two questions for each image, such that the ground-truth answer for one question is positive (the object class specified in the question is present in the image) and the ground-truth answer for the other question is negative (the object class specified in the question is not present in the image). In this way we prevented a bias in obtaining answers uncorrelated with the image content, constraining the annotator to be focused during the entire task. Each answer ("Yes" or "No") has a 50% chance of being the right choice. Naturally, an annotator could memorize an image and answer more quickly if the image would be presented several times, so we made sure that a person did not get to annotate the same image twice. Each question was answered by three human annotators. Given that we used 11, 540 images and we associated two questions per image, we obtained 69, 240 annotations. The annotations come from 736 trusted contributors. A trusted contributor has an accuracy (percentage of answers that match the ground-truth answers) higher than 90%. Data post-processing and cleanup. When the annotation task was finished, we had 6 annotations per image (3 for each of the two questions) with the associated response times. We removed all the response times longer than 20 seconds, and then, we normalized each annotator's response times by subtracting the annotator's mean time and by dividing the resulted times by the standard deviation. We removed all the annotators with less than 3 annotations since their mean time is not representative. We also excluded all the annotators with less than 10 annotations with an average response time higher than 10 seconds. After removing all the outliers, the difficulty score per image is computed as the geometric mean of the remaining times. It is worth mentioning that by adjusting the accuracy threshold for trusted annotators to 90%, we allow some wrong annotations in the collected data. Wrong annotations provide the ultimate evidence of a difficult image, showing also that the problem of estimating image difficulty is not trivial. We determined the images containing wrong annotations (based on the groundtruth labels from PASCAL VOC 2012) and added a penalty to increase the difficulty scores of these images. Human agreement. We report the inter-human correlations on a subset of 56 images that we used to spot untrusted an- notators in CrowdFlower. We consider only the 58 trusted annotators who annotated all these 56 images. In this setting, we compute the correlation following a one-versus-all scheme, comparing the response time of an annotator to the mean response time of all annotators. For this, we use the Kendall's τ rank correlation coefficient <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref>. Kendall's τ is a correlation measure for ordinal data based on the difference between the number of concordant pairs and the number of discordant pairs among two variables, divided by the total number of pairs. The mean Kendall's τ correlation is reported in <ref type="table">Table 1</ref>, along with the standard deviation, the minimum and the maximum correlations obtained. The mean value of 0.562 means that the average human ranks about 80% image pairs in the same order as given by the mean response time of all annotators. This high level of agreement among humans demonstrates that visual search difficulty can indeed be consistently measured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">What makes an image difficult?</head><p>Images are not equal in their difficulty. In order to gain an understanding of what makes an image more difficult than another, we consider several human interpretable image properties and analyze their correlation with the visual search difficulty assessed by humans. The image properties are derived from the human manual annotations provided for each image in PASCAL VOC 2012 <ref type="bibr" target="#b14">[15]</ref>. All object instances of the 20 classes are annotated with bounding boxes and other several details (viewpoint, truncation, occlusion, difficult flags) regarding the annotated object (for more details see <ref type="bibr" target="#b14">[15]</ref>). In our analysis, we consider the following image properties: (i) number of annotated objects; (ii) mean area covered by objects normalized by the image size; (iii) non-centeredness, defined as the mean distance of the center of all objects' bounding boxes to image center normalized by the square root of image area; (iv) number of different classes; (v) number of objects marked as truncated; (vi) number of objects marked as occluded; (vii) number of objects marked as difficult.</p><p>It is important to remark that these image properties are not available at test time. We only use them in our analysis to study how human interpretable properties correlate with visual search difficulty and also how well these properties could predict difficulty.</p><p>We quantify the correlation between image properties and visual search difficulty assessed by humans (Section 2.1) by measuring how well image properties scores can predict ground-truth human difficulty scores. More precisely, we compute the Kendall's τ correlation between the rankings of the images when ranked either by the image properties scores or by the ground-truth human difficulty scores. Each image property assigns visual search difficulty scores in a range that is different from the range of the ground-truth scores. Kendall's τ is suitable for our analysis because it is invariant to different ranges of the various measurements.</p><p>In all our experiments on visual search difficulty prediction throughout this paper, we divided the 11, 540 samples included in the official training and validation sets of PAS-CAL VOC 2012 into three subsets. We used 50% of the samples for training, 25% for validation and another 25% for testing. <ref type="table">Table 2</ref> shows the Kendall's τ rank correlations between the difficulty scores based on the image properties and the ground-truth difficulty scores on our test set. The results confirm that human interpretable properties are informative for predicting visual search difficulty. The top three most correlated image properties with the ground-truth difficulty score specify some of the ingredients that make an image difficult: the image should contain many instances of different classes scattered all over the image (not just in the center). The next most informative property is the mean area covered by objects. It shows a negative correlation with the ground-truth difficulty score suggesting that, on average, small objects are more difficult to find. Interestingly, difficulty could also be predicted to some degree based on the number of objects marked as truncated, occluded or difficult. However, as most objects appear normally, without being truncated or occluded, these markers are rarely used, which reduces their predictive power. As each image property captures a different characteristic, combining them appears to be promising. We trained a Support Vector Regression (ν-SVR) model <ref type="bibr" target="#b35">[36]</ref> to combine all seven image properties. In our evaluation, we used the ν-SVR implementation provided in <ref type="bibr" target="#b5">[6]</ref>. The combination yields the highest Kendall's τ correlation (0.36). In Section 3, we show that we can learn an even better predictor capable of automatically assessing visual search difficulty based on CNN features, without information derived from image properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Visual search difficulty at the class level</head><p>We can produce some interesting statistics based on our collection of difficulty scores. Perhaps one of the most interesting aspects is to study the difficulty scores at the class level. We compute a difficulty score per object class by averaging the score for the images that contain at least one instance of that class. The difficulty scores for all the 20 classes in PASCAL VOC 2012 are presented in <ref type="table">Table 3</ref>. It appears that bird, cat and aeroplane are the easiest ob-  <ref type="table">Table 3</ref>. Average difficulty scores per class produced by humans versus the classification mean Average Precision (mAP) performance of the best model presented in <ref type="bibr" target="#b6">[7]</ref> for the 20 classes available in PASCAL VOC 2012. Classes are sorted by human scores.</p><p>ject classes in PASCAL that can be found in images by humans. We believe that birds and aeroplanes are easy to find as they usually appear in a simple, uniform background, for example on the sky. On the other hand, cats can appear in various contexts (simple or complex), but their distinctive shape, eyes and other body features are probably very easy to recognize. The most difficult classes in PASCAL, from a human perspective, appear to be potted plant, chair, dining table and tv monitor. We believe that potted plants and chairs are hard to find due to high (intra-class) variability in their appearance. For instance, chairs come in different shapes and sizes, such as stools, armchairs, and so on. Furthermore, all the difficult classes usually appear in complex contexts, such indoor scenes with many objects and varying illumination conditions. Interestingly, the difficulty scores presented in <ref type="table">Table 3</ref> indicate that the human perspective is not very different from the results achieved by state of the art computer vision systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>. <ref type="table">Table 3</ref> includes the mean Average Precision (mAP) performance of the best CNN classifier presented in <ref type="bibr" target="#b6">[7]</ref>. It can be observed that the lowest performance is obtained for the bottle, potted plant and chair classes. These are also among the top 5 most difficult classes for humans according to our findings. Moreover, aeroplane and bird are among the top 4 easiest classes for both humans and machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning to predict visual search difficulty</head><p>So far, we obtained a set of ground-truth difficulty scores based on human annotations. We now go a step further and train a model to predict the difficulty of an input image. We compare our supervised model with a handful of baseline models. We first describe our supervised model and the baseline models and then present experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Our regression model</head><p>We build our predictive model based on CNN features and linear regression with ν-SVR <ref type="bibr" target="#b35">[36]</ref> or Kernel Ridge Regression (KRR) <ref type="bibr" target="#b35">[36]</ref>. We considered two pre-trained CNN architectures provided in <ref type="bibr" target="#b44">[45]</ref>, namely VGG-f <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr">Figure 2</ref>. Visual search difficulty assessed by baselines. We show the global image features used by each baseline for computing a difficulty score. Top row: input images with predicted scores by our method. Following rows: image edge maps <ref type="bibr" target="#b12">[13]</ref>, image segmentation <ref type="bibr" target="#b15">[16]</ref>, top 5 highest scoring objectness windows (colored red to black from highest to lowest).</p><p>VGG-verydeep-16 <ref type="bibr" target="#b37">[38]</ref>. These CNN models are trained on the ILSVRC benchmark <ref type="bibr" target="#b33">[34]</ref>.</p><p>We removed the last layer of the CNN models and used them to extract deep features as follows. The input image is divided into 1 × 1, 2 × 2 and 3 × 3 bins in order to obtain a pyramid representation for increased performance. The input image is also horizontally flipped and the same pyramid is applied over the flipped image. Finally, the 4096 CNN features extracted from each bin are concatenated into a single feature vector for the original input image. The final feature vectors are normalized using the L 2 -norm. The normalized feature vectors are then used to train either a ν-SVR or a KRR model to regress to the ground-truth difficulty scores. We use our learned models as a continuous measure to automatically predict visual search difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baselines</head><p>We try out several baselines. Each baseline can assess the visual search difficulty based on some specific feature: image area, file size, objectness <ref type="bibr" target="#b1">[2]</ref>, edge strengths <ref type="bibr" target="#b12">[13]</ref>, number of segments <ref type="bibr" target="#b15">[16]</ref>. Unlike the image properties analyzed in Section 2.2, these features can be computed at test time (without manual annotations). Random scores. We assign random scores for each image. Image area. Without any prior information about the image content, the visual search task should be more difficult on larger images than on smaller ones. Based on this intuition, without analyzing the pixels inside an image, we quantify the difficulty of an image by its area. File size. A similar feature for quantifying visual search difficulty without looking at the pixels is the image file size. The images in PASCAL VOC 2012 are all compressed, in JPEG format. In this context, we tried recovering the compression rate induced to the original image by normalizing the file size with the image area, but it did not provide better results than the image file size alone. Objectness. The objectness measure <ref type="bibr" target="#b1">[2]</ref> quantifies how likely it is for an image window to contain an object of any class. It is trained to distinguish windows containing an object with a well defined boundary and center, such as cows, cars and telephones, from windows covering amorphous background such as grass, road and sky. We used the official objectness code and obtained, for each image, a difficulty score by summing all the objectness scores of sampled windows. This difficulty score quantifies the image clutter through the objectness distribution in the 4D space of image windows. An easy image <ref type="figure">(Figure 2</ref>, first column) should have a small score as it contains only a small number of windows with high objectness (red colored windows) covering the dominant object. All other windows, not covering objects, have small objectness (black colored windows). Conversely, a harder image <ref type="figure">(Figure 2</ref>, last column) would have several peaks in the objectness distribution in the 4D space of image windows corresponding to objects' positions in the image. We tried several variants for obtaining a difficulty measure by using objectness: (i) entropy of the objectness distribution estimated with kernel density in the 4D space of all image windows; (ii) mean value of the objectness heat map obtained by accumulating objectness scores at each pixel for all windows containing the respective pixel; (iii) entropy of the sampled objectness windows; (iv) sum of all (usually 1000 samples obtained via the NMS sampling procedure <ref type="bibr" target="#b1">[2]</ref>) objectness windows scores. We found out that all variants are essentially the same in terms of performance (Kendall's τ correlations between 0.20 and 0.24), with (iv) being marginally better. Edge strengths. Humans can easily find objects in cluttered scenes by detecting their contours <ref type="bibr" target="#b34">[35]</ref>. We use this idea to provide a measure of difficulty based on edges. Intuitively, an image with a smaller density of edges should be easier to search than another image with higher density. We use the fast edge detector of <ref type="bibr" target="#b12">[13]</ref> to compute the edge map of an image and characterize its visual search difficulty by the sum of edge strengths. Segments. A different way of measuring difficulty rests on using segments as features. Segments divide an image into regions of uniform texture and color. Ideally, each segment should correspond to an object or to a background region. We quantify the complexity of an image by counting  the number of segments. While turbo-pixels <ref type="bibr" target="#b26">[27]</ref> segment the image in regular small regions, essentially providing the same number of superpixels per image, the method of <ref type="bibr" target="#b15">[16]</ref> divides the image into irregular segments covering objects and larger portions of uniform background with fewer superpixels ( <ref type="figure">Figure 2)</ref>. We use the available segmenter tool of <ref type="bibr" target="#b15">[16]</ref> with the default parameters for segmenting an image and characterize the difficulty by the number of segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experimental Analysis</head><p>Evaluation measures. In order to evaluate the proposed regression model for predicting visual search difficulty, we report both the mean squared error (MSE) and the Kendall's τ rank correlation coefficient <ref type="bibr" target="#b43">[44]</ref>. We report only the Kendall's τ correlation coefficient for the baseline models that do not involve regression, since the scores predicted by the baseline models are on a different range compared to the ground-truth difficulty scores and the MSE is a quantitative measure of performance unsuitable in this case. Evaluation protocol. We use the same split of the data set as described in Section 2.2. The validation set is used for tuning the regularization parameters of ν-SVR and KRR.</p><p>Results. <ref type="table" target="#tab_4">Table 4</ref> shows the results of different methods for predicting the ground-truth difficulty. Using random scores to assess difficulty leads to almost zero accuracy, showing that visual search difficulty estimation is not a trivial problem. Baselines that do not analyze image pixels perform a little bit better but are far away from accurately predicting the order of the images based on their difficulty. The methods based on mid-level features offer an increase in accuracy. Objectness and edge strengths perform essentially the same, achieving a correlation rank around 0.24. Using segments further improves the performance to around 0.27. Combining all these baselines with the ν-SVR framework, we obtain a predictor that achieves a Kendall's τ rank correlation of about 0.30. Based on the Kendall's τ definition, this translates in ranking about 65% image pairs correctly.</p><p>In training our regression models, we tested out several configurations, including two neural network architectures (VGG-f and VGG-verydeep-16), various ways of extracting features (standard, pyramid, horizontal flip), and finally, two different regression methods, namely Kernel Ridge Regression and Support Vector Regression. The least accurate configuration (VGG-f + KRR) gives already better performance compared to the baselines, reaching a rank correlation coefficient of 0.345. Changing the regression method, ν-SVR instead of KRR, we obtain a substantial increase to 0.440. The best approach is to combine the pyramid features from both CNN architectures and to train the model using ν-SVR. This combination outperforms by far all the baselines and their combination, and it remarkably achieves better performance than the image properties investigated in Section 2.2, which require knowledge of the number objects, classes, bounding boxes (unavailable at test time). The best approach based on linear regression reaches a Kendall's τ correlation coefficient of 0.472, which means that it correctly ranks about 75% image pairs. We consider the best regression model as our difficulty predictor and use it in two applications in Section 4. <ref type="figure" target="#fig_0">Figure 3</ref> shows the correlation between the ground-truth and the predicted difficulty scores. The cloud of points forms a slanted Gaussian with the principal component oriented almost diagonally, indicating a strong correlation between the predicted and ground-truth scores.</p><p>The examples presented in <ref type="figure">Figure 1</ref> visually confirm the performance of our model: images with small number of objects and uniform backgrounds are ranked lower in difficulty than cluttered images with many objects and complex backgrounds. We explain the high accuracy of our model through the powerful features that capture visual abstrac-  tions at a higher level, close to the level of object class recognition. Since we define difficulty based on human response times for a visual search task that involves object detection and recognition, the fact that the best features are the higher level ones makes perfect sense. Analyzing the image content at lower levels (edge strengths, objectness, segmentation) is not good enough, showing that a higher level of interpretation is needed in order to assess difficulty. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>We demonstrate the usefulness of our difficulty measure in two applications: weakly supervised object localization and semi-supervised object classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Weakly supervised object localization</head><p>In a weakly supervised object localization (WSOL) scenario, we are given a set of images known to contain instances of a certain object class. In contrast to the standard full supervision, the location of the objects is unknown. The task is to localize the objects in the input images and to learn a model that can detect new class instances in a test image. Often, WSOL is addressed as a Multiple Instance Learning (MIL) problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. In the MIL paradigm, images are treated as bags of windows (instances). A negative image contains only negative windows, while a positive image contains at least one positive window, mixed in with a majority of negative ones. The goal is to find the true positives instances from which to learn a window classifier for the object class. This typically happens by iteratively alternating two steps: (i) select instances in the positive images based on the current window classifier; (ii) update the window classifier given the current selection of positive instances and all windows from negative images. Learning protocol. We employ our measure of difficulty as an additional cue in the standard MIL scheme for WSOL. We design a simple learning protocol that integrates the difficulty measure: rank input images by their estimated difficulty and pass them in this order to the standard MIL. We call this Easy-to-Hard MIL. Evaluation protocol. We perform experiments on the training and validation sets of PASCAL VOC 2007 <ref type="bibr" target="#b13">[14]</ref>. The main goal of WSOL is to localize the object instances in the training set. Following the standard evaluation protocol in the WSOL literature, we quantify this with the Correct Localization (CorLoc) measure <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref>. For a given target class, a WSOL method outputs one window in each positive training image. CorLoc is the percentage of images where the returned window correctly localizes an object of the target class according to the PASCAL VOC criterion (intersection-over-union &gt; 0.5 <ref type="bibr" target="#b14">[15]</ref>). Implementation details. We represent each image as a bag of windows extracted using the state-of-the-art object proposal method of <ref type="bibr" target="#b11">[12]</ref>. This produces about 2, 000 windows per image. Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47]</ref>, we describe windows by the output of the second-last layer of the CNN model <ref type="bibr" target="#b24">[25]</ref>, pre-trained for whole-image classification on ILSVRC <ref type="bibr" target="#b33">[34]</ref>, using the Caffe implementation <ref type="bibr" target="#b22">[23]</ref>. This results in 4096-dimensional features. We employ linear SVM classifiers that we train with a hard-mining procedure at each iteration. For our Easy-to-Hard MIL we split the images in k batches according to their difficulty. We use the easiest images (easiest batch) first, in order to update the window classifier, and progressively use more and more difficult batches. We used k = 3 batches and 3 iterations per batch, for a total of 9 iterations. The standard MIL baseline instead uses all images in every iteration. Results. In <ref type="table" target="#tab_6">Table 5</ref>, we compare the performance of our Easy-to-Hard MIL with the standard MIL, in terms of average CorLoc over all 20 classes. From the first iteration the improvement is already noticeable: almost +5% CorLoc. Easier images lead to a better initial class model as the MIL has a higher chance to detect class specific patterns and localize objects correctly. The improvement increases as we add more batches: +7% after the second batch and +8.4% after the third. This increase demonstrates that the order in which images are processed is important in WSOL. Processing easier images in the initial stages results in better class models that in turn improve later stages. Remarkably, our difficulty measure is trained on PASCAL VOC 2012, while here, we used it to quantify difficulty on images from PASCAL VOC 2007. As these two datasets have no images in common, the results show that our measure can generalize across different data sets. Finally, we point out that better CorLoc performance results have been reported on PASCAL VOC 2007 by other works using different WSOL algorithms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semi-supervised object classification</head><p>Here we use our difficulty measure in a second application, namely in predicting whether an image contains a certain object class (without localizing it). Learning protocol. We consider three sets of samples: a set L of labeled training samples, a set U of unlabeled training samples and a set T of unlabeled test samples. Our learning procedure operates iteratively, by training at each iteration a classifier on an enlarged training set L. We enlarge the training set at each iteration by moving k samples from U to L as follows: we select k samples from U based on some heuristic, we label them (positive or negative) using the current classifier and move them from U to L. We stop the learning when L reaches a certain number of samples.</p><p>The final trained classifier is tested on the test set T . Selection heuristics. To select the k samples from U at each iteration, we use one of the following heuristics: (i) select the samples randomly (RAND); (ii) select the easiest k samples based on the ground-truth difficulty scores (GTdifficulty); (iii) select the easiest k samples based on the predicted difficulty scores (PRdifficulty); (iv) select the most confident (farthest from the hyperplane) k examples from U according to the current classifier confidence score (HIconfidence); (v) select the least confident (closest to the hyperplane) k examples from U according to the current classifier (LOconfidence); (vi) select the least confident K examples from U according to the current classifier, and from these K, take the easiest k examples based on our predicted difficulty score (LOconfidence+PRdifficulty). Evaluation protocol. We evaluate the classification performance of several models on PASCAL VOC 2012. All models are linear SVM classifiers based on CNN features <ref type="bibr" target="#b37">[38]</ref>. We use as test set T the official PASCAL validation set, and we partition the PASCAL train set into L and U . We stopped the learning process when L reached 3 times more samples than the initial training set. We choose the initial L to have 500 labeled images randomly selected and repeat each run for 20 times to reduce the amount of variation in the results. We report the mean Average Precision (mAP) performance. We set k to 50 and K to 2000. In addition to the 6 models given by the above heuristics, we include a baseline model (BASIC) trained only on the initial set L. We evaluate all models on the 7 classes (aeroplane, bird, car, cat, chair, dog and person) from PASCAL VOC 2012 that include more than 5% positive samples. If the number of positive samples is not large enough, our semi-supervised learning protocol has trouble capturing feature patterns of the class. Results. <ref type="figure" target="#fig_1">Figure 4</ref> shows the evolution of mAP for the proposed heuristics and the baseline. Randomly choosing 50 examples leads to a decrease in performance (86.1% ± 1.0%) compared to the BASIC method (87.8% ± 0.6%). Adding the most confident examples from U (HIconfidence) does not influence the results because the support vectors remain essentially the same. Using the least confident examples from U (LOconfidence) in order to change the support vectors decreases performance (85.2% ± 1.1%). The only useful information is provided by the difficulty scores, either predicted (88.4% ± 0.6%) or ground-truth (88.5% ± 0.7%), although it improves performance by less than 1%. Interestingly, by taking the least confident 2, 000 examples from U , and the easiest 50 from these examples based on our predicted difficult score (LOcon-fidence+PRdifficulty), we can also improve performance (88.1% ± 0.7%) by a little margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future work</head><p>Curriculum learning <ref type="bibr" target="#b3">[4]</ref> can help to optimize the training of deep learning models. We believe that our difficulty measure can be used in a curriculum learning setting to optimize the training of CNN models for various vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Correlation between ground-truth (x-axis) and predicted (y-axis) difficulty scores. The least squares regression line is almost diagonal suggesting a strong correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The mAP performance (y-axis) as the size of the training set (x-axis) grows by adding automatically labeled samples using different heuristics (compared to the BASIC baseline).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Visual search difficulty prediction results of baseline mod-
els versus our regression models based on deep features extracted 
by VGG-f [7] and VGG-verydeep-16 (VGG-vd) [38]. KRR and ν-
SVR are alternatively used for training our model on 5, 770 sam-
ples from PASCAL VOC 2012. The mean squared error (MSE) 
and the Kendall's τ correlation are computed on a test set of 2, 885 
samples. The best results are highlighted in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 .</head><label>5</label><figDesc>CorLoc results for standard MIL versus Easy-to-Hard MIL.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.crowdflower.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Turning visual search time on its head. Vision Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="86" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="27" to="28" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-fold mil training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Localizing Objects while Learning Their Appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly Supervised Localization and Learning with Generic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internation Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bottom-up saliency is a discriminant process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What makes a photograph memorable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1469" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What makes an image memorable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting sufficient annotation strength for interactive foreground segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional Architecture for Fast Feature Embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rank correlation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<pubPlace>Griffin, London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A model of saliencybased visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Christof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<title level="m">TurboPixels: Fast Superpixels Using Geometric Flows. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimating image segmentation difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MLDM</title>
		<meeting>MLDM</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Photo and video quality evaluation: Focusing on the subject</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A framework for visual saliency detection with applications to image thumbnailing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cifarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning saliency maps for object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gist of the scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of Attnetion, Elsevier</title>
		<imprint>
			<biblScope unit="page" from="251" to="256" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Edge detectors in human vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Shapley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tolhurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="183" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transfer learning by ranking for weakly supervised object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weakly Supervised Object Detector Learning with Model Drift Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Some objects are more equal than others: Measuring and predicting importance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lifespan changes in attention: The visual search task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Trick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Enns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Development</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="386" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A Dictionary of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MatConvNet -Convolutional Neural Networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACMMM</title>
		<meeting>eeding of ACMMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Whats It Going to Cost You?: Predicting Effort vs. Informativeness for Multi-Label Image Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large-scale weakly supervised object localization via latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reaction time distributions constrain models of visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1304" to="1311" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
