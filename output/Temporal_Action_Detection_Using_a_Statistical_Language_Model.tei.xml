<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Action Detection using a Statistical Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
							<email>richard@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<email>gall@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Action Detection using a Statistical Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While current approaches to action recognition on presegmented video clips already achieve high accuracies, temporal action detection is still far from comparably good results. Automatically locating and classifying the relevant action segments in videos of varying lengths proves to be a challenging task. We propose a novel method for temporal action detection including statistical length and language modeling to represent temporal and contextual structure. Our approach aims at globally optimizing the joint probability of three components, a length and language model and a discriminative action model, without making intermediate decisions. The problem of finding the most likely action sequence and the corresponding segment boundaries in an exponentially large search space is addressed by dynamic programming. We provide an extensive evaluation of each model component on Thumos 14, a large action detection dataset, and report state-of-the-art results on three datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition is a major research topic in computer vision since decades due to its applications in various fields like video understanding, human-computer-interaction, or surveillance. In recent years, there has been a major progress on classifying pre-segmented video clips. On challenging large-scale datasets like UCF-101 <ref type="bibr" target="#b22">[23]</ref>, current approaches for video-clip classification already achieve accuracies of eighty percent and more <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref>. However, when the videos are not pre-segmented and the task requires the temporal segmentation of the activities in a video, as it is required for the temporal action detection task of Thumos 14 <ref type="bibr" target="#b7">[8]</ref>, current approaches struggle to achieve good results.</p><p>One reason is that an action class can be arbitrarily long, e.g., a background class, but it can also take only a few frames in a video. Without a pre-segmentation of the video, the duration of the action classes needs to be well modeled. Another difference between video clip classification and temporal action detection is the relevance of the con-text. While video clips can already be well recognized by using only spatial <ref type="bibr" target="#b21">[22]</ref> or context information <ref type="bibr" target="#b9">[10]</ref>, the differences between the frames where an activity occurs and the rest of the video are more subtle. For this task, the context of the temporal change is more important than the spatial context.</p><p>The most successful methods for temporal action detection on a dataset like Thumos 14 <ref type="bibr" target="#b7">[8]</ref> follow a two step approach. They first extract segments from the video using a sliding temporal window and classify each segment in a second step. The final segmentation is then achieved by greedily selecting the segments with the highest scores <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>In this paper, we present an approach to temporal action detection that avoids a greedy approximation and aims to find the globally most likely action sequence in a single step by solving the segmentation and classification task jointly. Our model incorporates information about the duration of an action class by a length model and the temporal context by a language model. The length and language model are combined with a discriminative model for recognizing actions. We further show how inference can be efficiently performed using dynamic programming.</p><p>In our experiments, we provide an extensive analysis of our approach on three datasets where we evaluate the impact of the length and language model in detail. Our model achieves state-of-the-art accuracy for temporal action detection on the challenging Thumos 14 benchmark <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent action recognition systems, which combine Fisher vectors of improved dense trajectories <ref type="bibr" target="#b27">[28]</ref> or CNN features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7]</ref> with a classifier like a support vector machine (SVM), achieve very good performances for video clip classification on various real world datasets.</p><p>For temporal action detection, most approaches incorporate these classifiers into the detector. For instance, a sliding temporal window approach with a greedy non-maximum suppression can be applied to locate the action segments <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9]</ref>. In the context of spatio-temporal action detection, the number of windows can be reduced by finding good action proposals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref> or optimal action tubes using branch-and-bound <ref type="bibr" target="#b31">[32]</ref>. These methods, however, focus on the spatio-temporal localization of a single action in a short video clip and cannot be applied to long videos containing a large number of different actions. Other approaches model the sequence with structured temporal models. Early works use hidden Markov models for action segmentation, see <ref type="bibr" target="#b29">[30]</ref> for a survey. Shi et al. <ref type="bibr" target="#b20">[21]</ref> use a semi-Markov model in combination with three different feature types for segment boundaries, segment content, and interaction between neighboring segments, respectively. Similar to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>, in <ref type="bibr" target="#b20">[21]</ref> action detection is formulated as a max-margin problem, which is solved by an SVM, and specific features that correlate with transitions between classes are proposed. In our work, on the contrary, none of these features are used, but an explicit length and language model are proposed. Also note that the dynamic programming used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref> aims at finding a segmentation that maximizes the individual SVM scores, whereas we use dynamic programming to obtain the solution that maximizes our probabilistic model consisting of action, length, and language model.</p><p>With the publication of datasets providing action annotations on multiple granularity levels like Breakfast <ref type="bibr" target="#b10">[11]</ref> or 50 Salads <ref type="bibr" target="#b23">[24]</ref>, hierarchical models using context free stochastic grammars gained attention <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27]</ref>. Vo and Bobick <ref type="bibr" target="#b26">[27]</ref> use a Bayes network and model the temporal structure of high level activities with a grammar. AND-and OR-rules define possible compositions of actions and the optimal hierarchical activity composition is computed via a message passing algorithm. The approach of Kuehne et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> similarly requires the definition of a context free grammar to model temporal structures. In this work, activities are treated as compositions of smaller sub-actions, each of which is modeled with a hidden Markov model.</p><p>The use of multiple types of granularity has also been explored without grammars. Ni et al. <ref type="bibr" target="#b15">[16]</ref> track hands and object parts to infer hand-object interactions and compute dense trajectories around the tracked positions. They outperform the sliding window approach from <ref type="bibr" target="#b18">[19]</ref> on the MPII-Cooking dataset. In <ref type="bibr" target="#b12">[13]</ref>, mid-level action elements are generated by concatenating low-level actions of different granularity levels. This approach allows for a multiresolution reasoning even if only low level actions are annotated. Sharir and Tuytelaars <ref type="bibr" target="#b19">[20]</ref> propose to divide the video into a spatio-temporal grid and compute the action chain with highest likelihood over the cells. Although the approach is inferior to <ref type="bibr" target="#b18">[19]</ref> or <ref type="bibr" target="#b15">[16]</ref> for temporal localization, it also predicts the spatial location of the actions.</p><p>In order to model long-term relations in complex event detection tasks, the authors of <ref type="bibr" target="#b2">[3]</ref> propose the "sequence memoizer", which is a hierarchical Bayesian nonparametric model, for joint detection and classification of events. In <ref type="bibr" target="#b1">[2]</ref>, events are recognized by modeling temporal dynamics of mid-level concept detectors. Mettes et al. <ref type="bibr" target="#b13">[14]</ref> apply a bag-of-fragments approach to event detection and obtain a precise temporal event localization.</p><p>Finally, Sun et al. <ref type="bibr" target="#b24">[25]</ref> train long short-term memory network (LSTM) based fine-grained action detectors on both weakly labeled videos, where only video-level annotations without segmentation are available, and noisily tagged web images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Action Detection</head><p>We propose a probabilistic model for temporal action detection that jointly models the segmentation and classification task. We first describe the model in Section 3.1 and present in Section 3.2 an approach for exact inference using dynamic programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model</head><p>Given a video with T frames, let x T 1 be a sequence of feature vectors x t ∈ R D from a D-dimensional input space representing the video. Let further C = {1, . . . , C} be the set of C possible action classes. Our goal is to segment the given input sequence into an unknown number of N segments and assign an action class to each of the segments. More specifically, we aim to find the sequence of action end positions t N 1 and corresponding action classes c N 1 that are most likely for the given video, i.e.</p><formula xml:id="formula_0">max N,t N 1 ,c N 1 p(c N 1 , t N 1 |x T 1 ) (1) = max N,t N 1 ,c N 1 p(c N 1 )p(t N 1 |c N 1 )p(x T 1 |c N 1 , t N 1 ) .<label>(2)</label></formula><p>Note that the division by p(x T 1 ) has been dropped in Equation (2) as it does not affect the maximizing arguments. The formulation induces a model consisting of three components. The first one, p(c N 1 ), is a context or language model, providing probabilities for the sequence of action labels assigned to each video segment. We stick to the term language model as this type of model has been developed in the context of natural language processing in order to determine the likelihood of word sequences.</p><p>The second component, p(t N 1 |c N 1 ), determines the ending points of the segments. Note that our model does not allow gaps, i.e. a sequence ending at t n starts at t n−1 + 1, exactly one frame after the previous segment ends. This is no restriction if background is also modeled as an action class. Hence, t N 1 specifies the length of each segment and we call this component the length model.</p><p>The third component is the action model, providing the actual probability of a feature sequence x T 1 being generated by the given segmentation t N 1 and class labeling c N 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Language Model</head><p>As language model, we use an m-gram,</p><formula xml:id="formula_1">p(c N 1 ) = N n=1 p(c n |c n−1 1 ) = N n=1 p(c n |c n−1 n−m ),<label>(3)</label></formula><p>where the action class c n is assumed to depend only on the m preceding action classes. At the beginning of a sequence, the preceding classes are assumed to be virtual sequence start classes, i.e. c k = c $ for k ≤ 0. Maximum likelihood estimation leads to the concrete model</p><formula xml:id="formula_2">p(c|h) = N (h, c) N (h, ·) ,<label>(4)</label></formula><p>where h is a sequence of m preceding classes, e.g. c n−1 n−m for c = c n , and N (h, c) is the count of occurrences of class c with the history h in the training data. Note that particularly for larger histories, N (h, c) may be zero and such a sequence of action classes could never be detected. In order to deal with these unseen events, we use linear discounting with backing-off <ref type="bibr" target="#b14">[15]</ref>, i.e.</p><formula xml:id="formula_3">p(c|h) =    (1 − λ) · N (h,c) N (h,·) , if N (h, c) &gt; 0, λ · p(c|h ′ ) c ′ :N (h,c ′ )=0 p(c ′ |h ′ ) , otherwise.<label>(5)</label></formula><p>The parameter λ assigns a certain amount of probability mass to unseen events and is obtained using maximum likelihood estimation in combination with leaving-one-out, see <ref type="bibr" target="#b14">[15]</ref> for details. For unseen events, we back-off to p(c|h ′ ), an m-gram of lower order, e.g. a bigram (m = 1) if p(c|h) is a trigram (m = 2). The renormalization is required for a proper probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Length Model</head><p>For the length model, we assume a first-order dependence on the ending times, i.e.</p><formula xml:id="formula_4">p(t N 1 |c N 1 ) = N n=1 p(t n |c N 1 , t n−1 1 ) = N n=1 p(t n |c n , t n−1 ).<label>(6)</label></formula><p>Note that we also simplified the distribution to be dependent on the class c n of segment n only rather than on all classes c N 1 . Further, we assume that the ending time t n does not depend on the actual position but only on the distance to t n−1 . Thus,</p><formula xml:id="formula_5">p(t N 1 |c N 1 ) = N n=1 p(l n |c n )<label>(7)</label></formula><p>where l n = t n − t n−1 is the length of the segment. The distribution p(l|c) can be modeled with any discrete probability distribution defined on the natural numbers. We investigate a class-dependent and class-independent Poisson distribution as well as a class-independent length model based on the average length µ of all actions,</p><formula xml:id="formula_6">p(l|c) ∝ 1, if l ≤ µ, α l−µ , otherwise,<label>(8)</label></formula><p>where α is a decay factor. In the following, this model is referred to as mean length model. While the Poisson model prefers segments with lengths that are more likely according to the training data, the mean length model only ensures that no unreasonably long action segments are hypothesized. Without any restriction of the length, the system would tend to hypothesize a small number of long segments in order to avoid the penalty induced by the language model each time a new segment is hypothesized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Action Model</head><p>In action classification, discriminative models such as support vector machines or convolutional neural networks achieve good performance <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10]</ref>. These kinds of models can be viewed as a class posterior distribution p(c|x T 1 ), or p(c|x tn tn−1+1 ) for action segments in the domain of temporal action detection, respectively. Particularly when using Fisher vectors of improved dense trajectories, a linear classifier such as a support vector machine performs well <ref type="bibr" target="#b27">[28]</ref>. We stick to this finding but replace the support vector machine with a log-linear model of the form</p><formula xml:id="formula_7">p(c|x tn tn−1+1 ) = softmax(W T f (x tn tn−1+1 ) + b),<label>(9)</label></formula><p>where W is a weight matrix, b the bias, and f (x tn tn−1+1 ) the Fisher vector computed on the video segment [t n−1 +1, t n ].</p><p>The log-linear model is also a linear classifier but in contrast to support vector machines, it directly models a class posterior distribution. The parameters W and b can be estimated from the pre-segmented training data using gradient based optimization.</p><p>In the following, we show how to incorporate such a segment-based posterior distribution into our action model. We start with a simple factorization of p(x T 1 |c N 1 , t N 1 ). Assuming independence of the video frames, we can rewrite the action model as a product of action segments,</p><formula xml:id="formula_8">p(x T 1 |c N 1 , t N 1 ) = N n=1 p(x tn tn−1+1 |c N 1 , t N 1 ) (10) = N n=1 tn t=tn−1+1</formula><p>p(x t |c t , t n n−1 ).</p><p>Using Bayes' Theorem, Equation <ref type="formula" target="#formula_9">(11)</ref> can be transformed to contain a class posterior distribution,</p><formula xml:id="formula_10">p(x T 1 |c N 1 , t N 1 ) = N n=1 tn t=tn−1+1 p(c t |x t , t n n−1 ) p(x t |t n n−1 ) p(c t |t n n−1 ) .<label>(12)</label></formula><p>Due to the dependence on the segment start and end position, we assume that the class posterior has the same probability for each frame within the segment, i.e.</p><formula xml:id="formula_11">tn t=tn−1+1 p(c t |x t , t n n−1 ) = p(c n |x tn tn−1+1 ) ln ,<label>(13)</label></formula><p>where l n = t n − t n−1 is again the length of segment n.</p><p>Coming back to Equation <ref type="formula" target="#formula_0">(12)</ref>, we further assume that neither the frame prior nor the class prior depend on the segment start and end positions. Together with Equation <ref type="formula" target="#formula_1">(13)</ref>, this leads to</p><formula xml:id="formula_12">p(x T 1 |c N 1 , t N 1 ) = N n=1 p(c n |x tn tn−1+1 ) p(c n ) ln T t=1 p(x t ). (14)</formula><p>In practice, we found that a uniform class prior p(c) works well, so that factor can be omitted. Moreover, the product over the frame priors p(x t ) is independent of the arguments we maximize over and can thus also be omitted in the maximization.</p><p>Inserting all these results into Equation <ref type="formula" target="#formula_0">(2)</ref> leads to the final model</p><formula xml:id="formula_13">max N,c N 1 ,t N 1 N n=1 p(c n |c n−1 n−m ) · p(l n |c n ) · p(c n |x tn tn−1+1 ) ln .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference</head><p>We use dynamic programming to efficiently solve the maximization problem from Equation <ref type="bibr" target="#b14">(15)</ref>. For terms of simplicity, we derive the recursion equations for a bigram language model (m = 1) only. The modifications that are required for higher order language models are straightforward.</p><p>In order to enable dynamic programming over the time frames 1, . . . , T , we transform the product from Equation <ref type="bibr" target="#b14">(15)</ref> to run over the time rather than over the number of segments. To simplify notation, let s(t) be a function that maps frame t onto its corresponding segment number, i.e.</p><formula xml:id="formula_14">s(t) = n ⇔ t n−1 &lt; t ≤ t n .<label>(16)</label></formula><p>Then, Equation <ref type="formula" target="#formula_3">(15)</ref> can be rewritten as</p><formula xml:id="formula_15">max N,c N 1 ,t N 1 T t=1 p(c s(t) |c s(t)−1 ) · p(l s(t) |c s(t) ) · p(c s(t) |x t s(t) t s(t)−1 +1 ) l s(t) δ(t,t s(t) ) ,<label>(17)</label></formula><p>where δ(t, t s(t) ) is the Kronecker delta function and is one if and only if t is the ending time of segment s(t). This way, the N factors from Equation <ref type="bibr" target="#b14">(15)</ref> are sustained and the factors for the times t that are not a segment end time are one. We now define an auxiliary function Q(τ, c) that specifies the best segmentation of the video up to time τ with class c ending at τ . Enforcing c n = c and t n = τ , we obtain</p><formula xml:id="formula_16">Q(τ, c) = max n,c n−1 1 ,t n−1 1 τ t=1 p(c s(t) |c s(t)−1 ) · p(l s(t) |c s(t) ) · p(c s(t) |x t s(t) t s(t)−1 +1 ) l s(t) δ(t,t s(t) ) .<label>(18)</label></formula><p>Isolating the factors of the last segment and renaming c n−1 =c and l n = l leads to the recursive equation</p><formula xml:id="formula_17">Q(τ, c) = max n,c n−1 1 ,t n−1 1 τ −l t=1 p(c s(t) |c s(t)−1 ) · p(l s(t) |c s(t) ) · p(c s(t) |x t s(t) t s(t)−1 +1 ) l s(t) δ(t,t s(t) ) · p(c|c) · p(l|c) · p(c|x τ τ −l+1 ) l = max l,c Q(τ − l,c) · p(c|c) · p(l|c) · p(c|x τ τ −l+1 ) l .<label>(19)</label></formula><p>The score of the best segmentation in the sense of Equation <ref type="bibr" target="#b14">(15)</ref> is now given by max c Q(T, c). In order to reconstruct the best action segmentation, two additional traceback arrays need to be stored:</p><formula xml:id="formula_18">A(τ, c) = arg max l max c Q(τ − l,c) · p(c|c) · p(l|c) · p(c|x τ τ −l+1 ) l<label>(20)</label></formula><p>is the best-scoring length of the segment with class c ending at time τ and</p><formula xml:id="formula_19">B(τ, c) = arg max c max l Q(τ − l,c) · p(c|c) · p(l|c) · p(c|x τ τ −l+1 ) l<label>(21)</label></formula><p>is the best predecessor class of the segment ranging from [τ − A(τ, c) + 1, τ ] with class c. The optimal segmentation can then be reconstructed using Algorithm 1. Starting at the last frame T and the best ending class c at this frame, the best hypothesized segment start frame can be obtained as T −A(T, c)+1. The ending frame of the preceding segment is then T −A(T, c) and the best hypothesized class is stored in B(T, c). The optimal segmentation is reconstructed by iterating this scheme until the first frame is reached.  <ref type="figure">A(τ, c)</ref>, B(τ, c)) 8: end while 9: return segments</p><formula xml:id="formula_20">(τ, c) ← (τ −</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Runtime</head><p>Since for each frame each possible action length needs to be evaluated, and for each class each predecessor class has to be considered, inference is quadratic in the number of frames classes, i.e. O(C 2 T 2 ). However, limiting the maximal action length to a constant L, it can easily be reduced to O(C 2 LT ), allowing to process long videos. For higher order language models, predecessor classes also need to be stored. For a trigram language model, for instance, a function Q(τ, c,c) needs to be computed, which increases the runtime to O(C 3 LT ). In practice, however, the runtime is dominated by the computation of the action model, which is not affected by an increased history in the language model. Thus, the difference of using a bigram or a trigram language model does hardly affect the runtime at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We evaluate our method on three datasets. A detailed analysis is provided on Thumos 14 <ref type="bibr" target="#b7">[8]</ref>, a large dataset for action recognition and temporal action detection. The dataset offers a vast amount of training data, i.e. the videos from UCF101 <ref type="bibr" target="#b22">[23]</ref>, a set of 2, 500 background videos, and a validation set with 1, 010 temporally untrimmed videos of which 200 are temporally annotated with the 20 classes relevant for the detection task. For training, we only use these 200 videos from the validation set and the videos from UCF101 corresponding to the relevant 20 classes. The test set comprises 212 temporally annotated videos.</p><p>MPII-Cooking <ref type="bibr" target="#b18">[19]</ref> is a large dataset for fine grained action detection of cooking activities. It contains more than 8 hours of video with recordings of 12 different persons performing 65 different cooking related actions, including a class for background activity. We follow the protocol of <ref type="bibr" target="#b18">[19]</ref> and use leave-one-person-out cross-validation, resulting in seven splits.</p><p>Finally, we conduct experiments on 50 Salads <ref type="bibr" target="#b23">[24]</ref>, a dataset originally designed for hierarchical activity recognition. There are three high level activities and a set of 17 low level activities of finer granularity. Since our method is not designed for hierarchical activity detection, we report detection results on the low level only. Setup. The action model is trained by segmenting the training data according to the ground truth and computing a Fisher vector of improved dense trajectories <ref type="bibr" target="#b27">[28]</ref> for each segment. The ground truth annotation of the training data is also used to estimate the length-and language model. For detection, we extract unnormalized Fisher vectors of improved dense trajectories for each video frame and store the result as an integral image. This way, the Fisher vector for an arbitrary hypothesized segment can be computed efficiently by looking up the segment start and end time in the integral image and applying the normalization.</p><p>We compare our method to a sliding window baseline similar to the one used in <ref type="bibr" target="#b18">[19]</ref>. Starting with a window size of 30 frames and a step size of 10 frames, both values are increased by a factor of √ 2 until the window size exceeds 1, 000 frames. Non-maximum suppression is then applied to remove all overlapping windows. As classifier, we use the log-linear model from Equation <ref type="formula" target="#formula_7">(9)</ref> that is also used in our method.</p><p>For our approach, the maximal action length is limited to 1, 000 frames and we increment t by 10 instead of 1 in Equation <ref type="formula" target="#formula_5">(17)</ref>, i.e. we only evaluate every 10 th frame to reduce runtime. If not mentioned otherwise, we use the mean length model from Equation <ref type="bibr" target="#b7">(8)</ref>. With these settings, our algorithm needs 7.5h on a CPU with eight 1.2GHz cores for inference on Thumos 14. The code is available online. <ref type="bibr">1 2</ref> Evaluation Protocol. For the evaluation on Thumos 14, we use the official evaluation script provided by the authors of <ref type="bibr" target="#b7">[8]</ref>. The script computes mean average precision (mAP) over the detections. A detection is marked as correct if its intersection over union ratio is larger than some overlap threshold. Since we find this evaluation method very useful as it also gives insight in how a method performs for various overlap ratios, we also apply it to MPII-Cooking and 50 Salads. For MPII-Cooking, we additionally report precision and recall as well as single class mAP based on the midpoint hit criterion as proposed in <ref type="bibr" target="#b18">[19]</ref> to be able to compare to other methods using this dataset.</p><p>Language Model. In this section, we evaluate the effect of the language model on the performance of the system. To this end, we trained different kinds of language models on the temporally annotated validation set and compared their strength and their effect on the detection.</p><p>The strength of a language model can be measured using the perplexity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. For a single sequence with N action classes, it is defined as The perplexity for a dataset consisting of multiple sequences is the product of the language model probabilities for each sequence where N is replaced by the total number of segments in the dataset. Intuitively, the perplexity can be seen as the number of possible choices per position. A small perplexity corresponds to a strong language model. <ref type="table">Table 1</ref> shows the results on Thumos. Note that the perplexities on the training data (i.e. the validation set) and on the test data are very similar, indicating that the learned language model works well for the action context on the test set. Further, the perplexity decreases with increasing mgram order, making the language model stronger.</p><formula xml:id="formula_21">PP = N n=1 p(c n |c n−1 n−m ) − 1 N .<label>(22)</label></formula><p>The system with the unigram language model performs clearly worse than the system without a language model. Since 50% of the classes in Thumos are background, the model has a strong bias towards background. Moreover, background usually gets a high classification score for any segment, so the model tends to predict multiple consecutive background segments. This can be prevented by taking context into account. A bigram already leads to a huge gain in performance. Using more context, e.g. with a trigram, can still boost the performance, although the gain is not as intense as for the bigram. For the remainder of the paper, we use a trigram language model as it produces the best results.</p><p>Model Components. In <ref type="table" target="#tab_2">Table 2</ref>, the impact of each component is analyzed. In addition to the results at each of the five overlap ratios, we also report the average length of the detected segments in frames. The detection result of a video from the Thumos 14 test set in <ref type="figure" target="#fig_1">Figure 1</ref> serves as an example for the cases discussed in the table.</p><p>We start with an analysis of the action model. In Section 3.1.3, we argue that a uniform prior p(c) in Equation <ref type="bibr" target="#b13">(14)</ref> works well in practice. If we use a non-uniform class prior <ref type="figure" target="#fig_2">(Table 2 (b)</ref>), the performance is far below the performance of the uniform prior (Table 2 (a)) and the average segment length is shorter. This is due to the fact that the division by the prior emphasizes infrequent classes. Hence, longer actions are more likely to be split into multiple short segments of rare classes which are then sometimes falsely classified, see <ref type="figure" target="#fig_1">Figure 1</ref> (b).</p><p>Due to the interplay between the action, length, and language model, the impact of the power factor is a little bit more complex. Without the length and language model, the power factor l n penalizes long segments, cf . (f) and (g) in <ref type="table" target="#tab_2">Table 2</ref>. However, when length and language model are included, l n has another effect: It enhances the action model compared to the language-and length model. So, omitting l n increases the impact of the length model. Thus, sequences that are longer than µ (see Equation <ref type="formula" target="#formula_6">(8)</ref>), typically background, are more likely to be split. The language model strongly penalizes consecutive background segments, what explains the short action artifacts in <ref type="figure" target="#fig_1">Figure 1(c)</ref>.</p><p>Without length-and language model, long segments which include multiple short actions are classified as background since most of the frames are actually from the background. Thus, performance drops and the average segment length increases, cf . (f) in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>When using a language model without length model, the performance is still not satisfying and the average segment length is quite large, see (e) in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_1">Figure 1</ref>. The reason is that each time a new segment is hypothesized, a language model probability is multiplied to the probability score of the system. Thus, there is a clear tendency towards few, long segments in order to avoid language model penalties. Adding a length model compensates for this effect since unreasonably long segments are penalized. Note the interdependence of both models. While the complete system which includes both performs well, the effect of the length model is too strong if the language model is omitted, cf . (d) in <ref type="table" target="#tab_2">Table 2</ref>. The hypothesized segments are rather short in this case and the performance also drops again. Moreover, false detections occur due to the loss of context information, see <ref type="figure" target="#fig_1">Figure 1</ref> (d).</p><p>Length Model. We also evaluate our method on MPII-Cooking and 50 Salads in addition to Thumos 14, starting with an investigation of three different kinds of length models. The choice of the length model depends on the dataset and the characteristics of the action classes. A strong length model, such as the class-dependent Poisson model, is superior on 50 Salads and MPII-Cooking but performs worse on Thumos 14, cf . <ref type="table">Table 4</ref>. To analyze this effect, we compare the distribution of the ground truth lengths of each class with the distribution generated by the Poisson model.</p><p>We discretize both distributions as a histogram with 20  <ref type="table" target="#tab_2">Table 2</ref>. Effect of the model components. In the second column, the average length of the detected action segments is given. Evaluation follows the protocol from <ref type="bibr" target="#b7">[8]</ref>.   bins with a width of 50 frames each. Then, we compute the χ 2 -distance between both distributions and report the mean distance over all classes in <ref type="table" target="#tab_3">Table 3</ref>. While the χ 2 -distances for MPII-Cooking and 50 Salads are comparably small, the value for Thumos 14 is twice as large, indicating that the Poisson model is a worse representation of the true length distribution on Thumos 14 than on the other datasets. The mean length model, which only compensates for the bias of the language model towards long segments, performs best on Thumos where the Poisson distribution is a poor model of the underlying distribution as shown in <ref type="table" target="#tab_3">Table 3</ref>. Even on 50 Salads and MPII-Cooking where explicit length modeling is superior, the mean length model outperforms the sliding window baseline. We also investigated the decay factor α from Equation <ref type="bibr" target="#b7">(8)</ref>. For values between 0.5 and 0.9, the results do not change substantially. Only if α is very close to one, the effect of the length model vanishes since long segments are not anymore penalized. In this case, the performance decreases towards the system without length model, cf . <ref type="table" target="#tab_2">Table 2</ref> (e).</p><p>The class-independent Poisson model is a model in between, not as strong as the class-dependent model, but more explicit than the mean length model. Only on Thumos 14, where the true length distribution is hard to fit, it performs better than the class-dependent model. On the other datasets, the class-dependent model is still superior.</p><p>An example detection for each of the three length models on a video from 50 Salads is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. In contrast to the class-dependent model <ref type="figure" target="#fig_2">(Figure 2 a)</ref>, the classindependent Poisson model <ref type="figure" target="#fig_2">(Figure 2</ref> b) tends to avoid short segments, particularly for the background class. The mean length model <ref type="figure" target="#fig_2">(Figure 2</ref> c) prefers short segments, which results in an over-segmentation of long actions.</p><p>A lower bound on the segment lengths is defined by the subsampling of frames. On 50 Salads, the 0.1 overlap mAP slightly decreases from 0.391 to 0.379 and 0.376 for 5, 10, and 20 frame subsampling. For efficiency reasons, we stick to the 10 frame subsampling for all experiments.</p><p>Comparison to State-of-the-art. Our method outperforms the sliding window baseline consistently on all datasets, cf . <ref type="table">Table 4</ref>. On Thumos 14, our system achieves 3% higher mAP for overlap 0.1 to 0.4 and is still 1% better for overlap 0.5 compared to the winning submission from INRIA <ref type="bibr" target="#b16">[17]</ref>. Their approach is based on a sliding window and a model combination of their system from the classifica-   <ref type="table">Table 4</ref>. Comparison of our method to recently published results and the sliding window baseline on the three datasets Thumos 14, MPII-Cooking, and 50 Salads. We use the evaluation protocol proposed for Thumos <ref type="bibr" target="#b7">[8]</ref> and report the results as mAP for different overlap ratios.  <ref type="table">Table 5</ref>. Multi-class precision and recall and single class mAP on MPII-Cooking. We used the evaluation protocol from <ref type="bibr" target="#b18">[19]</ref>.</p><p>tion challenge and a trajectory based classifier trained on the data for the detection task. This has been the best published result on the dataset so far. Our system also outperforms the other challenge submissions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref> which both use a sliding window. Wang et al. <ref type="bibr" target="#b28">[29]</ref> additionally include CNN features in their classifier. <ref type="table">Table 5</ref> shows multi-class precision/recall and single class mAP on MPII-Cooking. The authors of <ref type="bibr" target="#b18">[19]</ref> use dense trajectories as features (holistic) and additional pose features. Ni et al. <ref type="bibr" target="#b15">[16]</ref> use dense trajectory features and detect hand-object interactions. While the existing approaches achieve a high recall at the cost of a comparably low precision, our approach achieves a higher precision at the cost of lower recall. In terms of single class mAP, we are 14% better than <ref type="bibr" target="#b18">[19]</ref> and 4% better than <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new method for temporal action detection that jointly models the segmentation and recognition of actions. Our approach includes a length and language model in addition to an action classifier. Using dynamic programming, we can efficiently infer the globally optimal action segmentation and classification. We have evaluated our method on three recent datasets and outperformed the state-of-the-art on Thumos 14 and MPII-Cooking by 3% and 4%. Moreover, an analysis of the impact of each model component revealed that the combination of length and language model is crucial for good performance. An investigation of three different length models on each of the three datasets revealed that a strong length model, e.g. a class-dependent Poisson model, is beneficial if it represents the true distribution of the action lengths.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Detection result on video test 0001058 of Thumos 14, which contains actions of the class Hammer Throw. The first row contains the ground truth, the other rows show the detection results for the systems (a)-(g) from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Detection result on rgb-03-1 of 50 Salads. Each class is encoded by another color, background is white. The first row contains the ground truth, the other lines show the detection results of our system with (a) a class-dependent Poisson model, (b) a class-independent Poisson model, and (c) the mean length model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Different classes have different colors.</figDesc><table>Thumos 14 
MPII-Cooking 
50 Salads 

2.77 
1.20 
1.31 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>χ 2 -distance between the ground truth length distribu-
tion and the Poisson model averaged over all classes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>ours w/ class-dependent Poisson model 0.248 0.239 0.220 0.192 0.140 ours w/ class-dependent Poisson model 0.379 0.368 0.352 0.312 0.229</figDesc><table>Overlap 

Method 
0.1 
0.2 
0.3 
0.4 
0.5 

Thumos 14 
sliding window 
0.325 
0.279 
0.206 
0.150 
0.086 
Univ. of Florence [9] 
0.046 
0.034 
0.024 
0.014 
0.009 
CHUK &amp; SIAT [29] 
0.182 
0.170 
0.140 
0.117 
0.083 
INRIA (challenge winner) [17] 
0.367 
0.334 
0.270 
0.208 
0.144 
ours w/ mean length model 
0.397 0.357 0.300 0.232 0.152 
ours w/ class-independent Poisson model 
0.337 
0.307 
0.255 
0.191 
0.127 
ours w/ class-dependent Poisson model 
0.251 
0.234 
0.201 
0.144 
0.088 

MPII-Cooking 
sliding window 
0.222 
0.197 
0.158 
0.126 
0.079 
ours w/ mean length model 
0.220 
0.209 
0.180 
0.135 
0.104 
ours w/ class-independent Poisson model 
0.219 
0.200 
0.163 
0.129 
0.098 
50 Salads (low level) 
sliding window 
0.201 
0.158 
0.126 
0.100 
0.080 
ours w/ mean length model 
0.305 
0.295 
0.260 
0.208 
0.153 
ours w/ class-independent Poisson model 
0.375 
0.357 
0.306 
0.237 
0.149 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: Authors acknowledge financial support from the DFG Emmy Noether program (GA 1927/1-1) and the donation of a Titan X by NVidia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A maximum likelihood approach to continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition of complex events: Exploiting temporal dynamics between underlying concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2243" to="2250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal sequence modeling for video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2235" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting social actions of fruit flies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eyjolfsdottir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Hoopfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="772" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint segmentation and classification of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3265" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast saliency based pooling of Fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Florence</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action recognition by hierarchical mid-level action elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bag-of-fragments: Selecting and encoding video fragments for event detection and recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cappallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On structuring probabilistic dependences in stochastic language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple granularity analysis for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Paramathayalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="756" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The LEAR submission at Thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inria</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action in chains: A chains model for action localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="610" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative human action segmentation and recognition using semi-Markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Joint Conf. on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">APT: Action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2641" to="2648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>The Chinese University of Hong Kong and Shenzhen Institutes of Advanced Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of visionbased methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RASR/NN: The RWTH neural network toolkit for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiesler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3281" to="3285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative video pattern search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1728" to="1743" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
