<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Personalizing Human Video Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
							<email>j.charles@leeds.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Leeds</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
								<orgName type="institution" key="instit3">University of Leeds</orgName>
								<orgName type="institution" key="instit4">University of Leeds</orgName>
								<orgName type="institution" key="instit5">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Leeds</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
								<orgName type="institution" key="instit3">University of Leeds</orgName>
								<orgName type="institution" key="instit4">University of Leeds</orgName>
								<orgName type="institution" key="instit5">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Magee</surname></persName>
							<email>d.r.magee@leeds.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Leeds</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
								<orgName type="institution" key="instit3">University of Leeds</orgName>
								<orgName type="institution" key="instit4">University of Leeds</orgName>
								<orgName type="institution" key="instit5">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hogg</surname></persName>
							<email>d.c.hogg@leeds.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Leeds</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
								<orgName type="institution" key="instit3">University of Leeds</orgName>
								<orgName type="institution" key="instit4">University of Leeds</orgName>
								<orgName type="institution" key="instit5">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Leeds</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
								<orgName type="institution" key="instit3">University of Leeds</orgName>
								<orgName type="institution" key="instit4">University of Leeds</orgName>
								<orgName type="institution" key="instit5">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Personalizing Human Video Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person's appearance to improve pose estimation in long videos.</p><p>We make the following contributions: (i) we show that given a few high-precision pose annotations, e.g. from a generic ConvNet pose estimator, additional annotations can be generated throughout the video using a combination of image-based matching for temporally distant frames, and dense optical flow for temporally local frames; (ii) we develop an occlusion aware self-evaluation model that is able to automatically select the high-quality and reject the erroneous additional annotations; and (iii) we demonstrate that these high-quality annotations can be used to fine-tune a ConvNet pose estimator and thereby personalize it to lock on to key discriminative features of the person's appearance. The outcome is a substantial improvement in the pose estimates for the target video using the personalized ConvNet compared to the original generic ConvNet.</p><p>Our method outperforms the state of the art (including top ConvNet methods) by a large margin on three standard benchmarks, as well as on a new challenging YouTube video dataset. Furthermore, we show that training from the automatically generated annotations can be used to improve the performance of a generic ConvNet on other benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in 2D human pose estimation exploit complex appearance models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> and more recently convolutional neural networks (ConvNets) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. However, even the state of the art ConvNets often produce absurdly erroneous predictions in videos -particularly for unusual poses, challenging illumination or viewing conditions, self-occlusions or unusual shapes (e.g. when wearing baggy clothing, or unusual body proportions). This is due to the lack of large quantities of annotated data, which is critical for training ConvNets; and, as we show below, the models failing to exploit personspecific information.</p><p>To address these issues, this paper proposes an occlusion-aware method for automatically learning reliable, person-specific pose estimators in long videos. Using the fact that people tend not to change appearance over the course of a video (same clothes, same body shape), we show that the large quantity of data in the video can be exploited to 'personalize' a pose estimator, thereby improving performance for unusual poses. The key idea is to 'spread' a small number of high quality automatic pose annotations throughout the video using spatial image matching techniques and temporal propagation (see <ref type="figure" target="#fig_0">Fig 1)</ref>, and use this new annotation to fine-tune a generic ConvNet pose estimator. We demonstrate that such personalization yields significant improvements in detection performance over the original generic pose estimation method. Our idea stems from the observation that current pose estimation methods fail to exploit person-specific information, such as jewelery, clothing, tattoos etc. For example, if one learns a person is wearing an item of clothing which is easily tracked, such as a necklace, then this information can be used to help localize the head and shoulders. Similarly, for a distinctive pattern or color on an item of clothing, or a tattoo/watch on a wrist. The personalization algorithm essentially 'locks on' to these person-specific features, and exploits them to more accurately determine the pose.</p><p>Operationalizing personalization requires a novel set of methods: (i) spatial matching for body parts; (ii) temporal propagation based on dense optical flow; and (iii) an occlusion-aware pose model for self-evaluation to verify or excise erroneous annotations. We evaluate the personalization algorithm on both long and short videos from YouTube, sign language TV broadcasts and cooking videos, and show that our method significantly outperforms state of the art generic pose estimators.</p><p>More generally, the approach provides a 'production system' for effortlessly and automatically generating copious quantities of high quality annotated pose data, for example starting from the abundant repository of long video sequences containing the same person on YouTube (e.g. comedy and cooking shows; DJs; single player sports such as golf, aerobics, gymnastics; training videos etc.). These an- Pose annotations are spread throughout the video in two more stages: Stage 2 uses spatial matching (illustrated with the wrist joint), stage 3 propagates annotation temporally. Stage 4 self-evaluates the new annotations to discard errors. These stages are iterated, and resulting annotations used to train a personalized pose estimator. Right: the improvement in wrist accuracy after each stage on the YouTube Pose Subset dataset. Starting from a generic ConvNet based pose estimator <ref type="bibr" target="#b31">[32]</ref>, new annotations are generated over five iterations, and used to fine-tune the ConvNet. Note, the large improvement gains obtained by personalizing.</p><p>notations can be used for large scale training of a generic ConvNet pose estimator, thereby overcoming current limitations due to limited and restricted training regimes which rely upon manual annotation. The code, models and data are available at https://www.robots.ox.ac.uk/ vgg/research/personalization. Related work. The fact that human appearance tends to stay unchanged through videos has been used in the past to aid pose estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. Ramanan et al. <ref type="bibr" target="#b35">[36]</ref> train discriminative body part detectors by first detecting 'easy' poses (such as a 'scissors' walking pose) and then using the appearance learnt from these poses to track the remaining video with a pictorial structure model. Shen et al. <ref type="bibr" target="#b40">[41]</ref> iteratively re-train a pose estimator from confident detections, and also include temporal constraints. In the same spirit as <ref type="bibr" target="#b35">[36]</ref>, we too initialize from high-precision poses and re-train a discriminative model (a ConvNet pose estimator) -but rather than training a personalized part detector from these poses alone, we first spread the initial annotation throughout the whole video <ref type="bibr" target="#b2">[3]</ref> using image matching <ref type="bibr" target="#b41">[42]</ref> and optical flow <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49]</ref> to generate far more annotated frames. Since long video sequences contain an abundance of data, we can simply delete poor pose annotations (rather than trying to correct them); and even if some of the remaining annotations are incorrect, our ConvNet is able to deal with the label noise. Prior work <ref type="bibr" target="#b21">[22]</ref> has used evaluator algorithms to remove entire erroneous pose estimates, whereas here we evaluate individual body part annotations. Furthermore, in a similar manner to poselets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref>, our matching framework captures dependencies between both connected and non-connected body parts.</p><p>More generally, the idea of starting from general classifiers, and then 'personalizing' them has been used in other areas of computer vision, such as pedestrian detec-tion <ref type="bibr" target="#b19">[20]</ref> or object tracking <ref type="bibr" target="#b25">[26]</ref>. Kalal et al. <ref type="bibr" target="#b25">[26]</ref> proposed a tracking-learning-detection paradigm for learning a set of object templates online, e.g. for vehicles. Typically, in this type of approach, object models are initialized from a single frame <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>, and then matched to the next frame before being re-learnt. Supancic and Ramanan <ref type="bibr" target="#b42">[43]</ref> improve this by proposing to revisit tracked frames and re-learn a model to correct previous errors. In a similar way, our approach utilizes the whole video to learn body part appearance, but differs to previous work in that we match to all frames in a video sequence in one step, and not just frame to frame.</p><p>Self-occlusion is a challenging problem for pose estimation, with some methods addressing the issue by incorporating a state in their body model to signal body part occlusion <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref>, or by including an explicit occlusion part template <ref type="bibr" target="#b16">[17]</ref>. Alternatively one can opt to use multiple body models, each one handling a different type of part occlusion <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. Fine-scale occlusion reasoning (pixel level) is also possible when the depth order is known <ref type="bibr" target="#b3">[4]</ref> or unknown <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>. Another approach is to train better discriminative part detectors which learn the occlusion patterns from large datasets <ref type="bibr" target="#b15">[16]</ref>. In our case we use an occlusionaware pose model trained only to signal body part occlusion. In this setting, occlusion inference and pose estimation are decoupled, resulting in fast inference while also handling occluded parts correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Personalizing Pose Estimation</head><p>We start with an overview of the personalization algorithm before going into details below. <ref type="figure" target="#fig_0">Fig 1 shows</ref> an overview of the process, which has six distinct stages: 1. Initial pose annotation. Initial pose annotations for the video are obtained with generic pose estimators for a few frames (yellow regions in <ref type="figure" target="#fig_0">Fig 1)</ref>. By design, these annota- tions have high precision, but low recall (i.e. only cover a very small proportion of the frames in the video). 2. Spatial matching. Image patches from the remaining frames are matched (blue regions in <ref type="figure" target="#fig_0">Fig 1)</ref> to image patches of body joints in frames with annotations (from stage 1). This forms a correspondence between annotated frames and matched frames, allowing the body joint annotations to be transferred to quite temporally distant frames in the video. 3. Temporal propagation. Pose annotations are spread from the annotated frames to temporally near-by (neighboring) frames, by propagating current annotation temporally along tracks using dense optical flow (pink regions in <ref type="figure" target="#fig_0">Fig 1)</ref>. 4. Annotation evaluation. In this stage, an evaluation measure discards annotations from the previous stages that are deemed to be poor. Multiple evaluation measures are employed, the two principal ones are: (i) consistency of overlapping annotations where regions in the video with multiple overlapping annotations (red regions in <ref type="figure" target="#fig_0">Fig 1)</ref> coming from different 'sources' (e.g. propagated from different initial annotations) are tested to see whether the annotations agree -this provides a very natural way to evaluate annotation correctness; and (ii) an occlusion-aware puppet model that renders a layout according to the predicted pose, and measures consistency with the image, similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b48">49]</ref>. 5. Iterating. To maximize frame annotation coverage, stages 2-4 are iterated, with the evaluator used to discard incorrect propagation histories, and propagate further those that are verified. <ref type="figure" target="#fig_2">Fig 3 demonstrates</ref> increased coverage and accuracy as our system iterates. 6. Personalizing a ConvNet. A generic ConvNet pose estimator is personalized by fine-tuning with the annotations acting as training data for the input video. This ConvNet can be applied to frames which the annotation process hasn't reached. The caveat here is that the reasoning about selfocclusions in the annotation is lost, since the ConvNet pose Accuracy of annotation and coverage (% of frames with annotation) across the video increases as the system iterates. Body joints with less appearance variation, such as the shoulders, have consistent accuracy and coverage rapidly approaches 100%. Most notable gains in accuracy are for joints with high appearance variation such as the wrists, improving by 9% from iteration 1. Accuracy is measured as the percentage of estimated annotations within d = 20 pixels from ground truth (approx wrist width 15 pixels). Results are averaged over videos with ground truth from the Youtube Pose Subset dataset. Note, for this and all other figures, results for all joints are given in the supplementary material.</p><p>estimator's predictions are not occlusion-aware. We next describe each of these stages in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generic pose estimator</head><p>In the first stage of the algorithm we obtain highprecision initial pose annotations for a small number of frames. These high-precision pose estimates are obtained by two approaches: first, by using very high confidence pose estimates from a ConvNet pose estimator <ref type="bibr" target="#b31">[32]</ref> (we have determined empirically that the high confidence joint predictions (greater than 80% confidence) are quite accurate). The second approach is poselet like <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> and involves detecting specific limb poses using a Yang and Ramanan <ref type="bibr" target="#b47">[48]</ref> pose detector. This is done by training the detector to only fire on a small number of poses <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref>, such as those with no complex self-occlusions or extreme foreshortening of limbs.</p><p>In the case of the Yang and Ramanan <ref type="bibr" target="#b47">[48]</ref> approach, we modify their release code to detect poses for the left and right arm separately. This effectively squares the total number of detectable upper-body poses, compared to learning separate models for each pose involving both arms. 15 arm models are trained, enabling us to detect up to 225 different poses with high precision. In general, this model captures more poses with arms above the head than the ConvNet.</p><p>Example high-precision detections from the arm posespecific models, and the high confidence ConvNet pose estimates are shown in <ref type="figure" target="#fig_1">Fig 2.</ref> We next discuss how we propagate these pose annotations spatially and temporally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spatial matching</head><p>In this stage, we propagate the small number of highprecision pose annotations, from the generic pose estimators, to new frames using image patch matching. The   matching process is illustrated in <ref type="figure">Fig 4(a-b)</ref>. For each body joint, small image patches in the annotated frames (with the annotated body joint at their center) are matched to new image patches in other frames of the video, and the annotations are then transferred. Note, body part patches rather than entire poses are matched as this allows more flexibility. This spatial matching proceeds in three steps as follows: Candidate matching patches. A random forest classifier similar to <ref type="bibr" target="#b5">[6]</ref> is trained for each body part (e.g. left shoulder or left wrist) using all annotated frames for that joint. This personalized body part detector is applied to all frames in the video to discover candidate (potentially matching) patches. The random forest classifier is trained on raw RGB image patches using multiple window sizes, and is able to take advantage of 'opportunistic features' such as bright colored gloves for detecting the wrists or trouser braces for detecting the shoulders. Small windows lead to very precise location detection and larger windows add global context. We found mixing the window sizes improves generalization, especially when training from a small number of initial annotations. As the forest classifier has the ability to average out possible errors in annotation, it adds robustness to the system and is also very fast to apply. Candidate patch verification. The candidate match is accepted if its HOG similarity to an original annotated patch is above a significance threshold. For this verification step, an exemplar-SVM is trained to match patches with similar types of body joint configuration (i.e. bent elbow, or straight elbow). Configurations are found by k-means clustering RGB patches of annotated joints (typically 200 clusters per joint are used). One exemplar-SVM is trained per configuration medoid. A significance measure can then be computed as in <ref type="bibr" target="#b18">[19]</ref>, between a candidate patch and each exemplar-SVM. Candidate patches with maximum matching significance (over all centroids) falling below a threshold are discarded (see <ref type="figure">Fig 5(a-b)</ref>). Annotation propagation and refinement. Annotations are then transferred to patches that match. However, due to imperfections in the personalized detector, the candidate body joint locations may be a small offset away from the correct location. To rectify this, as shown in <ref type="figure">Fig 5(c)</ref>, propagated annotations are refined by registering the matched patches to the annotated patch using SIFTflow <ref type="bibr" target="#b29">[30]</ref>, and transforming the annotations using this registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Temporal propagation</head><p>In this stage, annotations (from initialization and spatial matching) are further spread temporally (as illustrated in <ref type="figure">Fig 4(c)</ref>). This is achieved by computing dense optical flow <ref type="bibr" target="#b46">[47]</ref> for frames within a temporal window around the annotated frames. Starting from these annotated frames, body joint locations are temporally propagated forwards and backwards along the dense optical flow tracks. This propagation is inspired by <ref type="bibr" target="#b48">[49]</ref>. The outcome of this stage is that all frames within the temporal window (of up to 30 frames before and after) now have pose annotations. Some annotations may be incorrect, however, we are able to filter these away as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Self-evaluation</head><p>In this stage, the quality of the spatially and temporally propagated annotations is automatically evaluated, and incorrect annotations are discarded. To this end, we design a self-evaluation measure which, for a given annotated frame uses temporal information together with an occlusion-aware puppet model to detect erroneous annotation. Below we describe these evaluators in detail: Annotation agreement. When there are multiple annotations per frame, originating from different 'initially annotated' frames in the video, we can use their level of agree-ment as a confidence measure. This measure is formed by looking at the standard deviation of the annotation's 2D location for a given frame and joint. If below a threshold, a single annotation is derived from multiple annotations by selecting the 2D location with maximum annotation density (computed using a Parzen window density estimate with a Gaussian kernel). On average, after one iteration each joint will have at least two annotations per frame.</p><p>Occlusion-aware puppet model. Due to errors in both the matching and temporal propagation stages, it is possible (particularly in cases of self-occlusion) for propagated detections to drift to background content or other non-joint areas on the person. Subsequent iterations of the system would then reinforce these locations incorrectly. Additionally, due to the independence assumption of the forest part detector, errors can occur due to confusion between left and right wrists. Both of these types of errors are alleviated by learning a puppet model for the lower arm appearance, and separate body joint occlusion detectors which check for self-occlusion at head, shoulder and elbow joints. A puppet model of the lower arms is used to infer likelihood of an arm given the position of lower arm joints. Akin to other puppet models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b48">49]</ref>, our lower arm puppets are 'pulled' around the image space according to proposed lower arm joints, and used to evaluate the underlying image content. In our case, the puppet is used to detect when a proposed lower arm position is incorrect or when head, shoulder and elbow joints become occluded. Lower arm puppet construction. The lower arm of a puppet is represented by a rectangle which can be oriented and scaled anisotropically (to model limb foreshortening) according to proposed elbow and wrist joint locations. Two linear SVMs are jointly used to classify the image content encompassed by the rectangle as 'passed' or 'failed', one SVM uses HOG features the other RGB values, both SVMs have to agree on a 'pass' decision for the proposed lower arm to pass the evaluation. A separate model is trained and evaluated independently for left and right arms. The models are trained using initial annotation as positive examples with negative examples generated by adding random offsets to the elbow and wrist annotations. Further negatives are created by swapping left and right wrist locations to simulate a hand swap, similar to the method used in <ref type="bibr" target="#b5">[6]</ref>. Occlusion detection. To check for occlusion at head, shoulder and elbow joints, a square window is considered around a joint of interest. An SVM using HOG features and another using RGB features is applied. A low score from either SVM signifies an occluded joint. The SVMs are trained (a pair per joint) using initial un-occluded annotations, determined by considering the body part layout. Negative examples are generated from random offsets to these annotations. At run-time, if a joint is flagged as occluded we remove its track until flagged as un-occluded. The lower arm and occlusion detectors are retrained at each iteration using updated annotations. Discarding annotations. The above self-evaluation measures are used to discard annotations that are considered 'failed'. An annotation (per joint, per frame) is discarded if any one of the above measures falls below a threshold value or classified as 'failed' by the lower arm evaluator (see supplementary for details). The puppet model is also used to discard some initial annotations prior to subsequent stages. Correcting failed annotations. Sometimes it is possible to correct lower arm joint detections 'failing' the puppet evaluation method. This is done by randomly sampling a pair of wrist and elbow points (25 combinations in practice) around the 'failed' detection and re-evaluating each pair with the lower arm model . If a pair of points 'pass' evaluation we accept them as new annotation. Correcting failed annotations is beneficial as this leads to improved propagation when iterating the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Personalizing a ConvNet pose estimator</head><p>The final distributed annotations are used to fine-tune (with back-propagation) the generic ConvNet-based pose estimator of Pfister et al. <ref type="bibr" target="#b31">[32]</ref>, with which we initialized our system. The ConvNet is fine-tuned using all annotated frames thereby personalizing the ConvNet to the input video. Examples of channels that adapt to detect personalized features for a particular video are shown in <ref type="figure" target="#fig_5">Fig 6.</ref> The net is trained with a fixed learning rate of 1×10 −7 for 2,000 iterations using a batch size of 30 frames and momentum set at 0.95. This pose estimator is used to predict body joints for all frames of the video, possibly correcting any local mistakes that do persevere in the annotation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We first present the datasets; then evaluate gains from each stage in our method; and finally present a comparison to state of the art. Experimental details are included in the supplementary material, and a demo video is online at https://youtu.be/YO1JF8aZ_Do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and evaluation</head><p>Experiments are performed using three datasets that contain long videos suitable for personalization. YouTube Pose. This new dataset consists of 50 videos of different people from YouTube, each with a single person in the video. Videos range from approximately 2,000 to 20,000 frames in length. For each video, 100 frames were randomly selected and manually annotated (5,000 frames in total). The dataset covers a broad range of activities, e.g., dancing, stand-up comedy, how-to, sports, disk jockeys, performing arts and dancing sign language signers. YouTube Pose Subset. A five video subset from YouTube Pose. Example frames from the YouTube Pose Subset are shown in <ref type="figure" target="#fig_7">Fig 8(a)</ref>  <ref type="bibr" target="#b40">[41]</ref> consists of 20 short video clips, each containing approximately 100 consecutive frames with annotations. BBC Pose. This dataset <ref type="bibr" target="#b6">[7]</ref> contains five one-hour-long videos each with different sign language signers, different clothing and sleeve length. Each video has 200 test frames which have been manually annotated with joint locations (1,000 test frames in total). Test frames were selected by the authors to contain a diverse range of poses. Additionally, we annotated the location of the nose-tip on test frames. Evaluation measure. The accuracy of the pose estimator is evaluated on ground truth frames. An estimated joint is deemed correctly located if it is within a set distance of d pixels from the ground truth. Accuracy is measured as the percentage of correctly estimated joints over all test frames. For consistency with prior work, we evaluate on the UYDP dataset using the Average Precision of Keypoints (APK) <ref type="bibr" target="#b47">[48]</ref> at a threshold of 0.2.</p><p>Since automatically propagated annotations may not reach all 'test' frames with manual ground truth, we measure accuracy of the annotation stages by fine-tuning a ConvNet <ref type="bibr" target="#b31">[32]</ref> using all available annotations, and then evaluating this ConvNet's predictions on all test frames. This ensures evaluation consistency and fairness, and is a good indirect measure for annotation performance (as higher-quality training annotations should lead to improved ConvNet pose predictions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Component evaluation</head><p>We first evaluate the stages of the method on the YouTube Pose Subset. Tab 1 and <ref type="figure" target="#fig_2">Fig 3 show</ref>   <ref type="table">Table 1</ref>. Component analysis on YouTube Pose Subset. Accuracy at each stage of the method compared to baseline algorithms. Personalized ConvNet results are shown after 5 iterations. Corresponding curves are given in <ref type="bibr">Fig 7.</ref> in accuracy (at a threshold of 20 pixels) and coverage as the iterations progress, whereas <ref type="figure" target="#fig_6">Fig 7 shows</ref> accuracy as the allowed distance from manual ground truth d is increased. Accuracy: Each stage leads to a significant improvement in performance across all body joints. Even after Stage 1 (initialization) we improve upon the generic ConvNet of Pfister et al. <ref type="bibr" target="#b31">[32]</ref>, demonstrating that fine-tuning with relatively few annotations (from the generic ConvNet and Yang and Ramanan arm detection) brings benefits. Stage 2 (spatial matching) yields further gains in accuracy. One of the reasons spatial matching is so beneficial is that it helps propagate annotation to frames with similar poses but different local background content -in the YouTube videos the person moves against a static scene with some videos using a moving camera and containing shots from different angles. Personalizing a pose estimator from annotations at this stage, therefore, introduces more invariance to background content. Stage 3 (temporal propagation), is another mechanism for reaching unannotated frames containing different poses (more so than stage 2) from previous stages. Thus at this stage, we begin increasing the variation of poses that can be recognized. Again, this leads to an increase in performance.</p><p>The main causes of failure for Stage 3 are heavy self occlusion and optical flow errors, causing propagated annotation to drift to background content. In some cases annotations from different frames in the video can drift to the same background location, and (incorrectly) 'pass' the annotation agreement measure of Stage 4. However, the occlusionaware puppet model effectively detects and removes these errors, permitting subsequent iterations to progress without propagating errors. This is evident from the increase in accuracy when iterating the stages, most notably for the wrist joints which reach near 90% after five iterations. Coverage: The number of initially annotated frames across each video varies greatly and is highly dependent on the types of pose being performed and camera angle used. More initial annotations are obtained for videos where people have their hands down, such as the disc jokey sequences. For videos containing a high pose variation there is a greater A typical video of 10,000 frames, starting with 500 initially annotated, can rapidly increase annotation coverage in just one iteration. Stage 2 (spatial matching) normally doubles the annotations. The biggest boost in coverage comes from stages 3 and 4 (temporal propagation followed by checking with self-evaluation), with annotated frames rapidly increasing to over 60% (6,000 frames) of the video. Gains in coverage after temporal propagation are dependent upon the size of the temporal window (a larger temporal window improves coverage but can result in decreased accuracy (before self-evaluation) as errors in optical flow compound). A temporal window of 30 frames is selected as the tradeoff between coverage and accuracy. Subsequent iterations result in close to 100% of the frames being annotated for head and shoulder joints with wrist and elbow annotations covering 85% (8,500 frames) of the video. Relative gains in coverage decrease at each iteration because the system is tackling increasingly more difficult-to-detect poses. Timings: Timings for training are amortized over the application costs per frame for a typical 10k frame video, processed on a single core Intel Xeon 2.60GHz. Stage 1 (the most expensive stage) takes ∼15 seconds per-frame (s/f ) for initialization as multiple arm-models have to be applied. Stage 2 performs both model training and matching in ∼8s/f . Stage 3 computes in ∼8s/f and self-evaluation in ∼4s/f . Further iterations are quicker as less of the video requires spatial matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison to baselines and state of the art</head><p>As baselines we compare against two ConvNet-based estimators from Chen &amp; Yuille <ref type="bibr" target="#b9">[10]</ref> and Pfister et al. <ref type="bibr" target="#b31">[32]</ref>, and the deformable parts-based model by Yang &amp; Ramanan <ref type="bibr" target="#b47">[48]</ref>. All baseline pose estimators are trained for upper-body pose detection on the FLIC dataset <ref type="bibr" target="#b38">[39]</ref>. Additionally, on YouTube Pose we compare against Cherian et al. <ref type="bibr" target="#b10">[11]</ref> (trained on FLIC); and to Charles et al. <ref type="bibr" target="#b6">[7]</ref>, Rohrbach et al. <ref type="bibr" target="#b36">[37]</ref> and Shen et al. <ref type="bibr" target="#b40">[41]</ref>   <ref type="table">Table 2</ref>. Evaluation of accuracy over the four datasets. Accuracy is the percentage of correctly estimated body joints within a distance d pixels from ground truth (wrist width approx 15 pixels on average on YouTube Pose and MPII cooking, and 8 pixels on BBC pose). Results are averaged over all videos with ground truth from each dataset. Note, for Cherian et al. <ref type="bibr" target="#b10">[11]</ref>, head estimates are not comparable with other methods; and for UYDP there is a problem with the evaluation script for wrists.</p><p>Cooking and UYDP, respectively. On BBC Pose, head location accuracy is evaluated for Charles et al. <ref type="bibr" target="#b6">[7]</ref> using head center of mass ground truth (as this is how the model is trained), all other models are evaluated against nose-tip.</p><p>Results are given in Tab 2 and <ref type="figure">Fig 9.</ref> The results from the baselines indicate that MPII is the most challenging dataset of the three. As is evident, personalization achieves a huge improvement over both the baselines and state of the art. For example, obtaining 86.1% accuracy for wrist detection on YouTube Pose and an astonishing 93.5% accuracy for wrist detection on BBC Pose -significantly increasing over the state of the art results of 59.9% of <ref type="bibr" target="#b6">[7]</ref> and 78.6% of <ref type="bibr" target="#b31">[32]</ref>. The boost in performance of the generic ConvNet estimator by fine-tuning using personalization, that was noted on the YouTube dataset, is also repeated here on the BBC and MPII datasets. For example, increasing average prediction accuracy from 66.6% to 81.7% on MPII. Personalization leverages many frames in long videos; yet even for short videos such as UYDP we see an increase in accuracy and perform particularly well for the elbow joints.  <ref type="figure">Figure 9</ref>. Comparison to the state of the art. Accuracy of pose estimation evaluated on three datasets. Accuracy is averaged over left and right body parts and shown as allowed distance from manual ground truth as d is increased.</p><p>In comparing stages of the algorithm on YouTube Pose Subset <ref type="figure" target="#fig_0">(Tab 1 and Fig 3)</ref>, by Stage 2 (spatial matching) the head and shoulder accuracy already exceeds all baselines. By Stage 3 (temporal propagation), the system outperforms all baselines across all body joints. As mentioned above, spatial matching helps propagate annotations to frames with similar poses but different local background content. This occurs frequently in the BBC Pose dataset since signers are overlaid on a moving background in broadcasts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Boosting a generic ConvNet for other videos</head><p>The previous section showed the boost in performance through personalization on a target video. There remains the question of whether there is an additional benefit for generic pose estimation: if the ConvNet is fine-tuned on personalized annotations over many videos, does this im-prove pose estimation performance when applied to other videos and datasets?</p><p>This question is answered by attempting to boost performance of the generic ConvNet model by supplementing the training data with automatically annotated frames from the YouTube dataset (leaving out the YouTube Pose Subset for testing). For this we use a model that is pre-trained on the FLIC training set, and fine-tune using the full FLIC training set together with an equal number of annotated frames sampled from the YouTube videos.</p><p>The performance of the pre-trained and fine-tuned models is compared on the FLIC, YouTube and MPII Cooking test frames. There is a performance boost in all cases: for FLIC an increase in wrist &amp; elbow accuracy of 4% &amp; 6% respectively (at 0.1 normalized distance); for YouTube an increase in wrist &amp; elbow accuracy of 8% &amp; 5% respectively (at d = 20); and for MPII Cooking an increase of 5% &amp; 8% respectively (at d = 20), demonstrating the benefit of using additional automatically annotated training material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary and extensions</head><p>We have proposed a semi-supervised-like method for personalizing video pose estimation and have shown that this significantly improves performance compared to a nonpersonalized, 'generic', pose estimator, and beats the state of the art by a large margin on four challenging video pose estimation datasets.</p><p>The method can be used to boost the pose estimation performance on any long video sequence containing the same person, and we have also shown that the annotations generated by this personalization can be used to improve the performance of a ConvNet estimator for other videos.</p><p>It is straightforward to extend the method to estimate full body pose by adding the extra joints and limbs to the initialisation and the puppet model. The method can also be extended to deal with multiple people in a video and occlusion, given a suitable ConvNet model. For example, using the generated occlusion-aware annotations to train an occlusion-aware ConvNet pose estimator. One alternative formulation is to train from additional synthetic data as in <ref type="bibr" target="#b30">[31]</ref>, with the data generated using the puppet model. Given the recent success with training ConvNets on synthetic data, this would certainly be worth investigating.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Personalized video pose estimation. Left: Overview. A few video frames are annotated with confident pose estimates from one or more generic pose estimators in stage 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Initial pose estimates with generic pose estimators. Two arm pose specific model estimate examples for (a) a bent arm, and (b) a straight arm -note that the models need not fire on both left and right arms. (c) Poses showing joint detections with high confidence output from the generic ConvNet -in practice not all joints in a pose will have high confidence and therefore not all will be used during initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Annotation accuracy and coverage when iterating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Spreading the annotations spatially and temporally through the video. (a) Example patch of annotated left shoulder joint (white cross) from the generic pose estimator. (b) Patches matched spatially to the patch in (a). Blue arrow illustrates an example propagated annotation, and white crosses show locations in other frames where this annotation has also been propagated. Note, in this stage annotations can propagate to temporally very distant frames within the video. (c) Temporal propagation of annotations to neighboring frames. Spatial matching phase. (a) Annotated frame, with the square delineating the left wrist patch; (b) a candidate matching RGB frame (left) and random forest part-detector confidence map (right), square shows selected candidate left wrist patch. Using SIFTflow, the annotated patch (c) is registered to the candidate patch (e) as shown in (d). This enables annotation to be transferred (blue arrows) to the candidate patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>ConvNet filter response maps. Heatmap responses from two different personalized ConvNets shown overlaid on example input frames. (a) and (b) are filter responses from layers 2 and 3, respectively, of the spatial fusion layers obtained using<ref type="bibr" target="#b31">[32]</ref>. Personalized features for (a) show that the hat and glove are important, and in (b) hairline is important. The original generic ConvNet of the same network shows no personalized features of this type (i.e. person specific features are added by the fine-tuning.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Component evaluation on the YouTube Pose Subset dataset. The graphs show the improvement from each stage of the algorithm. Notice how each stage leads to a very significant increase in accuracy. Accuracy is shown (averaged over left &amp; right body parts) as the allowed distance from ground truth is increased. increase in coverage over the iterations. Fig 3 shows the percentage of video frames (on Youtube Pose Subset) that have been annotated after each iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Example pose estimates on frames from three datasets (a) YouTube Pose , (b) BBC Pose and (c) MPII Cooking. Note the variety of poses, clothing, and body shapes in the YouTube videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>, and further examples from YouTube Pose are shown in the supplementary material. MPII Cooking. This dataset contains video sequences from [37] for recognizing cooking activities. Each video is on average approximately 20,000 frames. 21 videos come manually annotated with upper body pose, with 1,277 testing frames. Each video contains a single person (person varies between videos) captured with a static camera; all sequences are shot in the same kitchen. Upper-body YouTube Dancing Pose (UYDP). This dataset</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>on BBC Pose, MPII Yang &amp; Ramanan [48] 91.6 27.6 66.0 81.0 63.0 Charles et al. [7] 98.2 59.9 85.3 88.6 80.8 Personalized ConvNet 99.5 93.5 95.5 95.9 95.6</figDesc><table>YouTube Pose Accuracy (%) at d = 20 pixels 
Method 
Head Wrsts Elbws Shldrs Average 
Pfister et al. [32] 
89.3 64.2 74.6 85.8 76.9 
Chen &amp; Yuille [10] 
85.7 78.8 83.5 87.3 83.5 
Yang &amp; Ramanan [48] 89.9 38.5 58.3 85.3 64.9 
Cherian et al. [11] 
-54.3 66.9 84.7 
-
Personalized ConvNet 95.4 86.1 86.8 93.9 89.9 
MPII Cooking Accuracy (%) at d = 20 pixels 
Pfister et al. [32] 
53.4 79.0 70.4 57.0 66.6 
Chen &amp; Yuille [10] 
62.3 75.5 73.8 72.7 72.3 
Yang &amp; Ramanan [48] 46.7 37.5 43.7 46.1 43.0 
Rohrbach et al. [37] 80.5 66.2 67.1 72.2 70.2 
Personalized ConvNet 86.7 85.8 80.4 76.3 81.7 
UYDP (%) at APK=0.2 
Pfister et al. [32] 
78.7 -
35.2 63.3 
-
Chen &amp; Yuille [10] 
86.3 -
46.8 80.3 
-
Yang &amp; Ramanan [48] 81.7 -
17.6 66.5 
-
Shen et al. [41] 
90.9 -
33.3 83.5 
-
Personalized ConvNet 91.7 -
57.6 83.8 
-
BBC Pose Accuracy (%) at d = 6 pixels 
Pfister et al. [32] 
97.1 78.6 88.2 83.0 85.3 
Chen &amp; Yuille [10] 
65.9 47.9 66.5 76.8 64.1 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. Financial support was provided by the EPSRC grants EP/I012001/1 and EP/I01229X/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Test-time adaptation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GCPR</title>
		<meeting>GCPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactive feature tracking using K-D trees and dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Buchanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="626" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zisserman. Upper body detection and tracking in extended signing sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning shape models for monocular human pose estimation from the microsoft xbox kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic and efficient human pose estimation for sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptation for upper body pose tracking in signed TV broadcasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Upper body pose estimation with temporal sequential forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixing body-part sequences for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive occlusion state estimation for human pose tracking under selfocclusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="649" to="661" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scene semantics from long-term observation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pixeltrack: a fast adaptive algorithm for tracking non-rigid objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parsing occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection with grammar models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Articulated pose estimation using discriminative armlet classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning and calibrating per-location classifiers for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient non-iterative domain adaptation of pedestrian detectors to video scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Htike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeep: A deep learning framework using motion features for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Has my algorithm succeeded? an evaluator for human pose estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human pose search using deep poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG</title>
		<meeting>FG</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pn learning: Bootstrapping binary classifiers by structural constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tracking-learningdetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient discriminative learning of parts-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with tiny synthetic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for efficient pose estimation in gesture videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Strike a pose: Tracking people by finding stylized poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic affine-invariant shape-appearance handshape features and classification in sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theodorakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pitsikalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised video adaptation for parsing human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recognizing and tracking human action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-paced learning for long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Join training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple tree models for occlusion and spatial constraints in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Estimating human pose with flowing puppets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
