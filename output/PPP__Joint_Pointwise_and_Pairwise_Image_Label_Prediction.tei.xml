<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PPP: Joint Pointwise and Pairwise Image Label Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Wang</surname></persName>
							<email>yilinwang@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Arizona State Univerity</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
							<email>suhang.wang@asu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Arizona State Univerity</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Yahoo Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<email>huan.liu@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Arizona State Univerity</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxin</forename><surname>Li</surname></persName>
							<email>baoxin.li@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Arizona State Univerity</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PPP: Joint Pointwise and Pairwise Image Label Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pointwise label and pairwise label are both widely used in computer vision tasks. For example, supervised image classification and annotation approaches use pointwise label, while attribute-based image relative learning often adopts pairwise labels. These two types of labels are often considered independently and most existing efforts utilize them separately. However, pointwise labels in image classification and tag annotation are inherently related to the pairwise labels. For example, an image labeled with "coast" and annotated with "beach, sea, sand, sky" is more likely to have a higher ranking score in terms of the attribute "open"; while "men shoes" ranked highly on the attribute "formal" are likely to be annotated with "leather, lace up" than "buckle, fabric". The existence of potential relations between pointwise labels and pairwise labels motivates us to fuse them together for jointly addressing related vision tasks. In particular, we provide a principled way to capture the relations between class labels, tags and attributes; and propose a novel framework PPP(Pointwise and Pairwise image label Prediction), which is based on overlapped group structure extracted from the pointwise-pairwise-label bipartite graph. With experiments on benchmark datasets, we demonstrate that the proposed framework achieves superior performance on three vision tasks compared to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The increasing popularity of social media generates massive data at an unprecedented rate. The ever-growing number of images has brought new challenges for efficient and effective image analysis tasks, such as image classification, annotation and image ranking. Based on the types of labels, we can roughly divide the supervised vision tasks into two categories -pointwise label based approaches and pairwise label based approaches. Pointwise approaches adopt pointwise labels such as image categories or tags as training targets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref>. Class labels in classi- <ref type="bibr">Figure 1</ref>. An Illustrative Example of poinwise labels and pairwise labels. Pointwise label "4 door" is better than the pairwise label to describe presence of 4 door in a car, while "sporty" is better to use pairwise label to describe the car style, as the right is more sporty than the left. For example it is hard to label the middle (we ask 10 human viewer -40% agree with the non sporty and 60% agree with sporty, but 100% agree with middle one is more sporty than the left one and less sporty than right one). fication often capture high-level image content, while tags in tag annotation are likely to describe a piece of information in the image, such as "high heel, buckle, leather" in a shoe image. In <ref type="bibr" target="#b21">[21]</ref>, these two tasks are considered together because the labels and tags may have some relations in an image. Recently, due to the semantic gap between lowlevel image features and high-level image concepts, human nameable visual attributes are proposed to solve the vision tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">13]</ref>. However, for a large variety of attributes, the pointwise binary setting is restrictive and unnatural. For example, it is very difficult to assign or not assign "sporty" to the middle car in <ref type="figure">Figure 1</ref> because different people have different opinions. Thus, pairwise approaches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">12]</ref> have been proposed, which aim to learn a ranking function to predict the attribute strength for images. For example, in <ref type="figure">Figure 1</ref>. most of the people would agree that the middle car is more "sporty" than the left one and less "sporty" than the right one Pointwise and pairwise labels have their own advantages as well as limitations in terms of labeling complexity and representational capability. Labeling complexity: given 10 images, we only need 10 sets of class categories/tags. However, we need to label at least 45 image pairs to capture the overall ordering information. (Although the ranking rela-tion is considered as transferable, e.g. A ≻ B&amp;B ≻ C ⇒ A ≻ C). Representational capability: pointwise labels such as tags/class labels imply the presence of content properties such as whether a shoe is made of leather, contains a heel, buckle, etc. While pairwise labels capture the relations in a same property, e.g., A has a higher heel than B. Solely relying on pointwise labels may cause ambiguity or produce noisy data for the models as in the example of assigning "sporty" to the middle car in <ref type="figure">Figure 1</ref>, while only using pairwise labels may also cause problems when the images have very similar properties.</p><p>As pointwise and pairwise labels encapsulate information of different types and may have different benefits for vision problems and recommendation systems <ref type="bibr" target="#b22">[22]</ref> , we develop a new framework for fusing different types of training data by capturing their underlying relations. For example, in <ref type="figure" target="#fig_0">Figure 2</ref>, the tags, "leather, cognac, lace up" may suggest the left shoe with a higher score on the "formal" attribute, while the "high heel" may indicate the right shoe with a lower score on the "comfort" attribute. On the other hand, the higher score on "formal" and "comfort" with tag "Oxford" could help label the left image as "shoe" and enable the rare tag annotation such as "wingtip". To the best of our knowledge, there are only a few recent works that fused pointwise and piarwise labels <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b3">4]</ref>. However, they simply combined regression and ranking in the loss functions for ranking tasks and totally ignored the relations between pointwise labels and pairwise labels.</p><p>In this paper, we investigate the problem of fusing pointwise and pairwise labels by exploiting their underlying relations for joint pointwise label prediction such as, image classification and annotation, and pairwise label prediction, e.g., relative ranking. We derive a unified bipartite graph model to capture the underlying relations among two types of labels. Since traditional approaches cannot take advantages of relations among pointwise and pairwise labels, we proceed to study two fundamental problems: (1) how to capture relations between pointwise and pairwise labels mathematically; and (2) how to make use of the relations for jointly addressing vision tasks. These two problems are tackled by the propose framework PPP and our contributions are summarized as follows:</p><p>• We provide a principled approach to modeling relations between pointwise and pairwise labels;</p><p>• we propose a novel joint framework PPP, which can predict both pointwise and pairwise labels for images simultaneously; and</p><p>• We conduct experiments on various benchmark datasets to understand the working of the proposed framework PPP.</p><p>In the remaining of the paper, we first give a formal problem definition and basic model in Section 2. Then the proposed framework and an optimization method for model learning is presented in Section 3. Experiments and results are demonstrated in Section 4, with further discussion in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Proposed Method</head><p>Before detailing the proposed framework, we first introduce notations used in this paper. We use X ∈ R n×d to denote a set of images in the database where n is the number of images and d is the number of features. Note that there are various ways to extract features such as SIFT, Gist or the features learned via deep learning frameworks. Let Y t ∈ R n×c1 and Y c ∈ R n×c3 be the data-tag and data-label matrices which represent the pointwise labels. Y(i, j) = 1 if the i-th image is annotated/classified with j-th tag/class label, Y(i, j) = 0 otherwise. Given a fixed training set D, a candidate pair set P can be drawn. The pair set implied by the fixed training set D uses pairwise labels. In the proposed framework, given a pair of images &lt; a, b &gt; on the attribute q, if y a ≻ y b , then a has a positive attribute score y(a, q, 1) = |y a − y b |, and a negative score y(a, q, 2) = 0; while b has a positive attribute y(b, q, 1) = 0, and a negative score y(b, q, 2) = |y a − y b |. Thus, the pairwise label is defined as Y r ∈ R m×c2 , where m is the number of pairs drawn from training samples and c 2 = 2q where q is the number of attributes. For example, let &lt; a, b &gt; be the first pair, the pairwise label Y r (1, 2(q − 1) + 1) represents how likely the y a ≻ y b and Y r (1, 2(q − 1) + 2) represents how likely y a ≺ y b on attribute q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline Models</head><p>In our framework, pointwise labels are considered for classification and annotation tasks. For classification, we assume that there is a linear classifier W c ∈ R d×c3 to map X to the pointwise label Y c as Y c = XW c . W c can be obtained by solving the following optimization problem:</p><formula xml:id="formula_0">min Wc Ω(W c ) + L(W c , Y c , D)<label>(1)</label></formula><p>where L() is a loss function and Ω is a regularization penalty to avoid overfitting, D is the training sample set.</p><p>Here we employ least square for loss function L.</p><p>For tag annotation, we also assume that there is a linear function W t ∈ R d×c1 which captures the relation between data X and pointwise label Y t as Y t = XW t . Similarly, the optimization problem to learn W t is:</p><formula xml:id="formula_1">min Wt Ω(W t ) + L(W t , Y t , D)<label>(2)</label></formula><p>For pairwise label based approaches, a simple and successful approach to utilizing the pairwise label is Rank <ref type="figure" target="#fig_0">Figure 2</ref>. The demonstration of capturing the relations between pointwise label and pairwise label via bipartite graph. For example, the attribute "formal" with tags "leather, lace up, congnac" will form a group via the upper bipartite graph, while label "sandal" with attribute "less formal" and tags "high heel, party" will form a group via the lower bipartite graph. SVM, whose goal is to learn a model W that achieves little loss over a set of previously unseen data, using a prediction function. Similar to RankSVM, in our framework, the original distribution of training examples are expanded into a set of candidate pairs and the learning process is over a set of pairwise feature vectors as:</p><formula xml:id="formula_2">min W L(W, Y r , P ) + Ω(W r )<label>(3)</label></formula><p>where P is a set of training pairs. The loss function L is defined over the pairwise difference vector x:</p><formula xml:id="formula_3">L(W, Y r , P ) = ((a,ya,qa),(b,y b ,q b ))∈P l(t(y a −y b ), f (w, a−b))</formula><p>(4) where the transformation function t(y) transforms the difference of the labels <ref type="bibr" target="#b18">[18]</ref>. In our framework, the transformation function is defined as t(y) = sign(y).</p><p>Note that one may form a unified model by simply adding all the above objective functions together. Such an approach would still essentially treat the component models as independent tasks (albeit trade-off among them might be considered via weighting), since no explicit relations among them are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Capturing Relations between Poinwise and Pairwise Labels</head><p>In the previous subsection, we defined three tasks that use pointwise and pairwise labels separately. Capturing the relations between pointwise and pairwise labels can further pave a way for us to develop a joint framework that enables interaction between classification, annotation and ranking simultaneously.</p><p>First, the relations between attributes and tags can be denoted as a bipartite graph as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We assume that B ∈ R c2×c1 is the adjacency matrix of the graph where B(i, j) = 1 if both the i-th tag and the j-th attribute cooccur in the same image and B(i, j) = 0 otherwise. Note that in this paper, we do not consider the concurrence frequencies of tags and attributes and we would like to leave it as one future work. From the bipartite graph, we can identify groups of attributes and tags where attributes and tags in the same group could share similar properties such as semantical meanings. A feature X(:, i) should be either relevant or irrelevant to the attributes and tags in the same group. For example, W r (i, j) indicates the effect of the i-th feature on predicting the j-th attribute; while W t (i, k) denotes the impact of the i-th feature on the k-th tag. Therefore we can impose constraints on W t and W r together, which are derived from group information on the bipartite graph, to capture relations between attributes and tags.</p><p>We can adopt any community detection algorithms to identify groups from the bipartite graph. In this paper, we use a very simple way to extract groups from the bipartite graph -for the j-th attribute, we consider the tags that connect to that attribute in the bipartite graph as a group, i.e., B(i, j) = 1. Note that a tag may connect to several attributes thus extracted groups via the aforementioned process have overlaps. Assume that G is the set of groups we detect from the attribute-tag bipartite graph and we propose to minimize the following term to capture relations between attributes and tags as:</p><formula xml:id="formula_4">Ω G (W t,r ) = d i=1 g∈G α g w i g 2 (5)</formula><p>where W t,r = [W t , W r ] and α g is the confidence of the group g and w i g is a vector concatenating <ref type="bibr" target="#b8">9)</ref>] . Next we discuss the inner workings of Eq. (5). Let us check terms in Eq. (5) related to a specific group g, d i=1 w i g 2 , which is equal to adding a ℓ 1 norm on the vector g = [w 1 g , w 2 g , . . . , w d g ], i.e., g 1 . That ensures a sparse solution of g; in other words, some elements of g could be zero. If g i = 0 or w 2 g 2 = 0, the effects of the i-th feature on both the attribute and tags in the group g are eliminated simultaneously.</p><formula xml:id="formula_5">{W t,r (i, j)} j∈g . For example, if g = {1, 5, 9}, w i g = [W t,r (i, 1), W t,r (i, 5), W t,r (i,</formula><p>Similarly, we build the bipartite graph to capture the underlying relations for the attributes and class labels. In <ref type="bibr" target="#b21">[21]</ref>, it was suggested that the co-occurrence of tags and labels should also be considered. Thus, we build a mixture bipartite graph to extract the group information between class labels, tags, and attributes. The group regularization Ω G2 (W t,r,c ) is similar to Eq. 5 and illustration is shown in <ref type="figure" target="#fig_0">Figure 2</ref>, where a tag or an attribute will connect to the class label if they are associated with each other. Note that a group extracted from <ref type="figure" target="#fig_0">Figure 2</ref> could include a class label, a set of attributes and a set of tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The Proposed Framework</head><p>With the model component to exploit the bipartite graph structures, the proposed framework is to solve the following optimization problem:</p><formula xml:id="formula_6">min W L(W c , Y c , D) + L(W t , Y t , D) + L(W r , Y r , P ) + λ( W c 2 F + W t 2 F + W r 2 F ) + αΩ G1 (W t,r ) + βΩ G2 (W t,r,c )<label>(6)</label></formula><p>In Eq. 6, the first six term is from the basic models to predict the class label, tags and ranking order. The seventh and eighth term are to capture the overlapped structure of the output, which is controlled by α and β respectively. The group regularization is defined as blow:</p><formula xml:id="formula_7">Ω G (Z) = i∈G Z g 2 = d i=1 i∈G z i g 2<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">An Optimization Method for PPP</head><p>Since the group structures are overlapped, directly optimizing the objective function is difficult. We propose to use Alternating Direction Method of Multiplier (ADMM)( <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b1">2]</ref>) to optimize the objective function. We first introduce two auxiliary variables P</p><formula xml:id="formula_8">= [W t , W r ]M 1 and Q = [W t , W r , W c ]M 2 . M 1 ∈ {0, 1} (c1+c2)×c2(c1+c2) is de- fined as: if i − th tag connects to the jth attribute then M 1 (i, (c 1 +c 2 )(j−1)+i) = 1, otherwise it is zero. The def- inition of M 2 ∈ {0, 1} (c1+c2+c3)×c3(c1+c2+c3) is similar to M 1 .</formula><p>With these two variable, solving the overlapped group lasso on W is transfered to the non-overlapped group lasso on P and Q, respectively. Therefore, the objective function becomes:</p><formula xml:id="formula_9">min W,P,Q L(W c , D) + L(W t , D) + L(W r , P ) + αΩ G (P) + βΩ G2 (Q) + λ( W c 2 F + W t 2 F + W r 2 F ) s.t.P = [W t , W r ]M 1 ; Q = [W t , W r , W c ]M 2 ;</formula><p>(8) which can be solved by the following ADMM problem:</p><formula xml:id="formula_10">min W,P,Q L(W c , Y c , D) + L(W t , Y t , D) + L(W r , Y r , P ) + λ( W c 2 F + W t 2 F + W r 2 F ) + αΩ G (P) + βΩ G2 (Q) + Λ 1 , P − [W t , W r ]M 1 + Λ 2 , Q − [W t , W r , W c ]M 2 + µ 2 P − [W t , W r ]M 1 2 F + µ 2 Q − [W t , W r , W c ]M 2 2 F (9)</formula><p>where Λ is the Lagrangian multiplier and µ is a scaler to control the penalty for the violation of equality constrains P = [W t , W r ]M 1 and Q = [W t , W r , W c ]M 2 . Noting that the loss function L has lots of choices, we use the least square loss function in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Updating W</head><p>To update W, we fix the other variable except W and remove terms that are irrelevant to W. Then the Eq. 9 becomes:</p><formula xml:id="formula_11">min W x∈D xW t − y t 2 2 + x∈D xW c − y c 2 2 + xi,xj ∈P (x i − x j )W r − y r 2 2 + λ( W c 2 F + W t 2 F + W r 2 F ) + µ 2 (P + 1 µ Λ 1 ) − [W t , W r ]M 1 2 F + µ 2 (Q + 1 µ Λ 2 ) − [W t , W r , W c ]M 2 2 F<label>(10)</label></formula><p>Setting the derivative of Eq. 10 w.r.t W t to 0, we get:</p><formula xml:id="formula_12">X T D X D W t + λW t + W t (M t 1 M tT 1 + M t 2 M tT 2 ) = X T Y + µ 2 [(P + 1 µ Λ 1 )M t 1 + (Q + 1 µ Λ 2 )M t 2 ]<label>(11)</label></formula><p>where M t 1 is the part of M 1 corresponding to W t . Directly getting the close form solution from Eq. 11 is intractable. On the other hand X T D X D + 1 2 λI and M t 1 M tT 1 +M t 2 M tT 2 + 1 2 λI are symmetric and positive definite. Thus, we employ eigen decomposition for each of them:</p><formula xml:id="formula_13">X T D X D + 1 2 λI = U 1 Σ 1 U T 1 M t 1 M tT 1 + M t 2 M tT 2 + 1 2 λI = U 2 Σ 2 U T 2<label>(12)</label></formula><p>whereU 1 , U 2 are eigen vectors and Σ 1 , Σ 2 are diagonal matrices with eigen value on the diagonal. Substituting Eq. 12 into Eq. 11: </p><formula xml:id="formula_14">U 1 Σ 1 U T 1 W t +W t U 2 Σ 2 U T 2 = X T D Y t + µ 2 (P + 1 µ Λ 1 )M t 1 + µ 2 (Q + 1 µ Λ 2 )M t 2<label>(13)</label></formula><formula xml:id="formula_15">W t = U T 1 W t U 2 and Z t = U T 1 [X T D Y t + µ 2 [(P + 1 µ Λ 1 )M t 1 + (Q + 1 µ Λ 2 )M t 2 ]</formula><p>]U 2 , we can obtain:</p><formula xml:id="formula_16">Σ 1 W t + W t Σ 2 = Z t<label>(14)</label></formula><p>Then, we can get W t and W t as:</p><formula xml:id="formula_17">W t (s, t) = Z t (s, t) σ s 1 + σ t 2<label>(15)</label></formula><formula xml:id="formula_18">W t = U 1 W t U T 2<label>(16)</label></formula><p>Similarly, setting the derivative of Eq. 10 w.r.t W c to zero and apply the eigen decomposition, we have the closed form solution of W c :</p><formula xml:id="formula_19">W c (s, t) = Z c σ s 1 + σ t 3<label>(17)</label></formula><formula xml:id="formula_20">W c = U 1 W c U T 3<label>(18)</label></formula><p>where</p><formula xml:id="formula_21">Z c = U T 3 [X T D Y c + µ 2 (Q + 1 µ Λ 2 )</formula><p>]M c 2 and U 3 , σ 3 are the eigen vector and eigen value for the symmetric and positive definite matrix M c 2 M cT 2 + 1 2 λI. Noting that for W r , which input is data pairs, we can use the same learning process by using the transform label function mentioned above. For example, we regard the pair difference as one data sample for X P and use the positive and negative label for label transformation. Setting the Eq. 10 w.r.t W r to zero, we can obtain:</p><formula xml:id="formula_22">X T P X P W r + λW r + W r (M r 1 M rT 1 + M r 2 M rT 2 ) = X T P Y r + µ 2 [(P + 1 µ Λ 1 )M r 1 + (Q + 1 µ Λ 2 )M r 2 ]<label>(19)</label></formula><p>Similar to W c , with eigen decomposition, we can get the closed form solution for W r as:</p><formula xml:id="formula_23">W r (s, t) = Z r σ s 4 + σ t 5 (20) W r = U 4 W r U T 5<label>(21)</label></formula><p>where</p><formula xml:id="formula_24">Z r = U 4 [X T P Y r + µ 2 [(P + 1 µ Λ 1 )M r 1 + (Q + 1 µ Λ 2 )M r 2 ]U T 5 , U 4 , σ 4</formula><p>are eigen vector and eigen values for X T P X P + 1 2 λI, and U 5 , σ 5 are eigen vector and eigen value for M r 1 M rT 1 + M r 2 M rT 2 + 1 2 λI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Updating P</head><p>After removing terms that are irrelevant to P, Eq. 9 becomes:</p><formula xml:id="formula_25">min P µ 2 P−[W t , W r ]M 1 2 F +αΩ G (P)+T r(Λ 1 P) (22)</formula><p>When applied to the collection of group for the parameters, P, Ω G (P)) no longer have overlapping groups. We denote j −th group in i-th row as P i,j = P(i, (c 1 +c 2 )(j −1)+1 : (c 1 + c 2 )j). Hence, we can solve the problem separately for each row of P within one group by the following optimization:</p><formula xml:id="formula_26">min Pi,j α P i,j 2 2 + µ 2 P i,j − (([W c , W r ]M 1 ) i,j − Λ 1ij µ ) 2 F<label>(23)</label></formula><p>Note that Eq. 23 is the proximal operator <ref type="bibr" target="#b27">[27]</ref> </p><formula xml:id="formula_27">of 1 µ (P ) i,j applied to (([W c , W r ]M 1 ) i,j − Λ1ij µ ). Let Z P i,j = ([W c , W r ]M 1 ) i,j − Λ1ij µ .</formula><p>The solution by applying the proximal operator used in non-overlapping group lasso to each sub-vector is:</p><formula xml:id="formula_28">P i,j = prox(Z P i,j ) =    0 if Z P i,j 2 ≤ α µ Z P i,j 2 − α µ Z P i,j 2 Z P i,j otherwise<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Updating Q</head><p>Similar to P, we can update Q by proximal operator used in non-overlapping group lasso to each sub-vector of Q:</p><formula xml:id="formula_29">Q i,j = prox(Z Q i,j ) =    0 if Z Q i,j 2 ≤ β µ Z Q i,j 2 − β µ Z Q i,j 2 Z Q i,j otherwise (25) where Q ij = Q(i, (c 1 +c 2 +c 3 )(j −1)+1 : (c 1 +c 2 +c 3 )j) and Z Q i,j = ([W t , W r , W c ]M 2 ) i,j − Λi,j µ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Updating Λ 1 , Λ 2 and µ</head><p>After updating the variables, we now need to update the ADMM parameters. According to <ref type="bibr" target="#b1">[2]</ref>, they are updated as follows:</p><formula xml:id="formula_30">Λ 1 = Λ 1 + µ(P − [W t , W r ]M 1 )<label>(26)</label></formula><formula xml:id="formula_31">Λ 2 = Λ 2 + µ(Q − [W t , W r , W c ]M 2 ) (27) µ = min(ρµ, µ max )<label>(28)</label></formula><p>Here, ρ &gt; 0 is a parameter to control the convergence speed and µ max is a large number to prevent µ from becoming too large.</p><p>With these updating rules, the optimization method for our proposed method is summarized in Algorithm 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The algorithm for the proposed framwork</head><p>Input: X D ∈ R N ×d and X P ∈ R m×d and corresponding label Y t , Y c and Y r Output: c 1 tags label c 2 relative score and c 3 class label for each data instance 1: Initialize random Sample training set D and drawn random pair set P from D. Update W t , W r and W c by Eq. 16, Eq. 21, and Eq. 18, respectively. <ref type="bibr">7:</ref> Calculate Z P and Z Q</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Update P and Q 9:</p><p>Update Λ 1 , Λ 2 and µ 10: until convergence 11: Using max pooling for testing use XW to predict tags, relative relation and labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Convergence Analysis</head><p>Since the sub-problems are convex for P and Q, respectively, Algorithm 1 is guaranteed to converge because they satisfy the two assumptions required by ADMM. The proof of the convergence can be found in <ref type="bibr" target="#b1">[2]</ref>. Specially, Algorithm 1 has dual variable convergence. Our empirical results show that our algorithm often converges within 100 iterations for all the datasets we used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Time Complexity Analysis</head><p>The main computation cost for W involves the eigen decomposition on X T X+ 1 2 βI, while other terms that involve eigen decomposition is very fast because the feature dimension of MM T is small. The time complexity for eigen decomposition is O(d 3 ). However, in Algorithm 1 the eigen decomposition is only computed once before the loop and dimension reduction algorithm can be employed to reduce image feature dimensions d. The computation cost for Z is O(nd 2 ) due to the sparsity of M. The computation of P depends on the proximal method within each group. Since there are c 2 groups which have the group size c 1 + c 2 for each feature dimension, the total computation cost for P is O(dc 2 (c 1 + c 2 )) and it is similar for Q. It is worth noting that P and Q can be computed in parallel for each feature dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we conduct experiments to evaluate the effectiveness of PPP. After introducing datasets and experimental settings, we compare PPP with the state-of-the-art methods of tag prediction, classification and ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments Settings</head><p>The experiments are conducted on 3 publicly available benchmark datasets.</p><p>Shoe-Zappo dataset <ref type="bibr" target="#b26">[26]</ref>: It is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories shoes, sandals, slippers, and boots. The tags are functional types and individual brands such as high-heel, oxford, leather, lace up, and pointed toe. The number of tags is 147 and 4 relative attribute is defined as "open" , "pointy", "sporty" and "comfortable". The ground truth is labeled from AmazonTurk.</p><p>OSR-scene dataset <ref type="bibr" target="#b16">[16]</ref>: It is a dataset for out door scene recognition with 2688 images. The images are divided into 8 category named as coast, forest, highway, inside-city, mountain, open-country, street and tallbuilding. 6 attributes with pointwise label and pairwise label are provided by <ref type="bibr" target="#b17">[17]</ref> named by natural, open, perspective, large-objects, diagonal-plane and close-depth.</p><p>Pubfig-face dataset <ref type="bibr" target="#b13">[13]</ref>: It is a dataset containing 800 images from 8 random identities (100 images per person) named Alex Rodriguez, Clive Owen, Hugh Laurie , Jared Leto , Miley Cyrus, Scarlett Johansson , Viggo Mortensen and Zac Efron. We use the 11 attributes with pintwise label and pairwise label provided by <ref type="bibr" target="#b17">[17]</ref>. The example attributes are named as masculine-looking, white, young, smiling and etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison</head><p>We compare PPP with the following representative algorithms:</p><p>• SVM <ref type="bibr" target="#b2">[3]</ref>: It uses the state of the art classifier SVM for classification with linear kernel; We also apply it to tag prediction by considering tags as a kind of labels;</p><p>• GLasso <ref type="bibr" target="#b28">[28]</ref>: The original framework of group lasso is to handle high-dimensional and multi-class data. To extend it for joint classification and tag prediction, we also consider tags as a kind of labels and apply GLasso to learn the mapping of features to tags and label. Note that it does not make use of the pointwise and pairwise label bipartite graph. We use the implementation in <ref type="bibr" target="#b15">[15]</ref>;</p><p>• sLDA <ref type="bibr" target="#b21">[21]</ref>: It is a joint framework based on topic models, which learns both class labels and annotations given latent topics;</p><p>• LS <ref type="bibr" target="#b8">[9]</ref>: A multi-label classification method that exploits the label correlation information. To apply LS for joint classification and tag prediction, we consider tags as a kind of labels and use tag and label relations to replace the label correlation in the original model; and</p><p>• FT <ref type="bibr" target="#b5">[6]</ref>: It is one of the state-of-the art annotation method which is based on linear mapping and coregularized joint optimization. To apply it for classification, we consider labels as tags to annotate; and</p><p>• RD: It predicts labels and tags by randomly guessing.</p><p>• MultiRank <ref type="bibr" target="#b4">[5]</ref>: It is a ranking method based on the assumption that the correlation exists between attributes, where the ranking function learns all attributes together via multi task learning framework.</p><p>• RA <ref type="bibr" target="#b17">[17]</ref>: It is the method for image ranking based on relative attributes.</p><p>Note that for all the baseline methods, none of them can utilize both pointwise and pairwise labels. Although we get the performance of the proposed framework by jointly predicting both pointwise and pairwise labels, we present our results for each task separately for a clear comparison. Moreover, we could use more advanced features, e.g., CNN feature, however, to compare with other methods fairly, we adopt the original feature provided by each datasets, which can easily show the performance gain from the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pointwise label Prediction</head><p>For pointwise label prediction, our method is compared with SVM, Glasso, sLDA, LS, FT, and RD. For all the baseline methods with parameters, we use cross validation to determine their values. For the Shoe dataset, we use the same data split and features (990 gist and color features) in <ref type="bibr" target="#b26">[26]</ref>. It contains 11102 data samples for training and 2400 data sample for testing. For OSR and Pubfig, we use the same data split and features in <ref type="bibr" target="#b17">[17]</ref>. Since OSR and Pubfig contain a small number of attributes, we leave one random-picked attribute for pairwise prediction and use the rest for tag annotation. Especially, to evaluate the performance of tag annotation, we rank all the tags based on their relevant scores and return the top K ranked tags. We use the average precision AP @K as the evaluation metric which has been widely used in the literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">21]</ref>. Meanwhile since the data samples are balanced, we use accuracy as the metric to evaluate the classification performance. The comparison results are shown in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref> for classification and tag annotation, respectively. We repeat 10 times for the training-testing process and report the average performance.</p><p>From the tables, we make the following observations:</p><p>• The proposed method that utilizes pairwise labels to predict pointwise labels tends to outperform the methods which solely rely on pointwise labels. These results support that (1) pairwise attributes can provide evidence for the pointwise label prediction; especially for the Pubfig dataset that contains 8 label classes, our method utilizes information from pairwise attributes significantly improve the classification performance.</p><p>(2) The performance of tag prediction AP @K indicates that the pairwise attributes contain important information for tag prediction;</p><p>• Our method with model components to capture relations between pairwise and pointwise labels outperforms those without. For example, compared to GLasso, the proposed framework, modeling the relations via the bipartite graph, gains remarkable performance improvement for both classification and tag prediction; and</p><p>• Most of the time, the proposed framework PPP performs the best among all the baselines, which demonstrates the effectiveness of the proposed algorithm.</p><p>There are two major reasons. First, PPP jointly performs pointwise and pairwise label prediction. Second, PPP captures relations between labels by extracting group information from the bipartite graph, which works as the bridge for building interactions between pointwise and pairwise labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Pairwise label Prediction</head><p>For pairwise label prediction, we generate pairs drawn from the training set used in the pointwise label prediction. For the Shoe dataset, we use 300 pairs; while for OSR and Pubfig, we use 100 pairs (the number suggested in <ref type="bibr" target="#b4">[5]</ref>) drawn from training set. We compute the average ranking accuracy with standard deviation by running 10 rounds of each implementation. The results are shown in <ref type="table">Table 3</ref>. Moreover, we also plot in <ref type="figure" target="#fig_1">Figure 3</ref> to show how average accuracy changes with different sizes of training samples on the attributes on the Shoe dataset (due to the space limits, we omit the figure on OSR and Pubfig).</p><p>From <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_1">Figure 3</ref>, we can have the following observations:</p><p>• The proposed method that leverages pointwise labels to predict pairwise labels often outperforms the methods which only use pairwise labels. These results support that pointwise labels can help the pairwise label prediction;</p><p>• The performance of the ranking accuracy varies with the number of the training pairs. With a small amount of labeled data, e.g., 10 pairs, the proposed method significantly outperforms relative attribute methods, which demonstrates that the pointwise labels contain important information for attribute ranking;</p><p>• The comparison based on multi-task attribute learning methods and our method demonstrates that simply combining the attributes together fails to differentiate these attributes which are not related to other attributes, while our methods use group structures, which makes the correlated attributes have strong overlaps, providing a discriminative way to capture the correlation between attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel way to capture the relations between pointwise labels and pairwise labels. Moreover, PPP provides a new viewpoint for us to have a better understanding how pointwise and pairwise labels interact with each other. Experiments demonstrated : (1) the advantages of the proposed methods for pointwise label based tasks including image classification, tag annotation and pairwise label based image ranking; and(2) the importance of considering the group correlation between pointwise labels and pairwise labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>The work was supported in part by ONR grants N 00014−15−1−2722 and N 00014−15−1−2344. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of ONR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 :</head><label>2</label><figDesc>Setting µ = 10 −3 , ρ = 1.1, µ max = 10 8 and building M 1 and M 2 3: Precompute the eigen decomposition 4: repeat 5:Calculate W t , W t and W r 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Learning curve of average ranking accuracy with regarding to different numbers of training pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Multiplying U T 1 and U 2 from left to right on both sides, and letting</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison in terms of classification. The number after each dataset means the class label number.</figDesc><table>Method Zappos(4) OSR(8) Pubfig (8) 
SVM 
67.41 % 
42.21 % 
50.77% 
GLasso 
78.31% 
50.11% 
59.13% 
sLDA 
74.32% 
46.33% 
56.21% 
LS 
84.46% 
61.22% 
66.56% 
FT 
84.69% 
59.38% 
67.45% 
RD 
25.01% 
12.51% 
12.50% 
PPP 
89.39% 
62.33% 
74.95% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison in terms of tag recommendation. 57% 40.89% 38.53% 68.51% 63.69% 60.12% 46.21% 38.31% 34.12% GLasso 64.34% 59.81% 55.37% 87.11% 83.34% 80.87% 90.12% 88.76% 86.41% sLDA 62.57% 57.22% 51.63% 90.15% 88.06% 84.78% 91.12% 87.93% 84.17% LS 74.76% 66.62% 61.85% 94.19% 93.19% 92.75% 93.71% 92.</figDesc><table>Method 
Zappo (147 tags) 
OSR (5 tags) 
Pubfig (10 tags) 
AP @3 
AP@5 
AP @10 AP @1 
AP@2 
AP @3 
AP @1 
AP@3 
AP@5 
SVM 
50.66% 91.91% 
FT 
67.37% 59.52% 51.98% 98.62% 94.45% 92.22% 92.45% 91.50% 90.16% 
RD 
1.44% 
1.43% 
1.44% 
20.00% 20.01% 20.01% 10.01% 10.00% 10.01% 
PPP 
77.10% 71.08% 62.95% 96.69% 94.21% 90.14% 94.48% 93.67% 92.71% 

Table 3. The average ranking accuracy on three dataset 

Method 
Zappos 
OSR 
Pubfig 
RA 
70.37% 76.10% 71.23% 
MultiRank 76.12% 84.93% 74.91% 
PPP 
79.67% 88.40% 76.32% 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="663" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fusing pointwise and pairwise labels for supporting user-adaptive image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 5th ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting multiple attributes via relative multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1027" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th international conference on Machine Learning</title>
		<meeting>the 30th international conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1274" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Recommender systems: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felfernig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting shared subspace for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Consumer video understanding: A benchmark database and an evaluation of human and machine performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM International Conference on Multimedia Retrieval</title>
		<meeting>the 1st ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attribute adaptation for personalized image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3432" to="3439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attribute pivots for guiding relevance feedback in image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<title level="m">Sparse learning with efficient projections</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combined regression and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flickr tag recommendation based on collective knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding meaning on youtube: Tag recommendation and category discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paşca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sbaiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3447" to="3454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simultaneous image classification and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1903" to="1910" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring implicit hierarchical structures for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1813" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inferring sentiment from web images with joint inference on visual and social cues: A regulated matrix factorization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Web and Social Media, ICWSM 2015</title>
		<meeting>the Ninth International Conference on Web and Social Media, ICWSM 2015<address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="473" to="482" />
		</imprint>
		<respStmt>
			<orgName>University of Oxford, Oxford</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised sentiment analysis for social media images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-25" />
			<biblScope unit="page" from="2378" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Making the most of bag of words: Sentence regularization with alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-grained visual comparisons with local learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient methods for overlapping group lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="352" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
