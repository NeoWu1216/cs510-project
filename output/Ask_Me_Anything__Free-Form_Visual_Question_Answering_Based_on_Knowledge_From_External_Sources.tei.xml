<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<email>p.wang@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<email>chunhua.shen@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
							<email>anthony.dick@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<email>anton.vandenhengel@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method for visual question answering which combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. This allows more complex questions to be answered using the predominant neural network-based approach than has previously been possible. It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain the whole answer. The method constructs a textual representation of the semantic content of an image, and merges it with textual information sourced from a knowledge base, to develop a deeper understanding of the scene viewed. Priming a recurrent neural network with this combined information, and the submitted question, leads to a very flexible visual question answering approach. We are specifically able to answer questions posed in natural language, that refer to information not contained in the image. We demonstrate the effectiveness of our model on two publicly available datasets, Toronto  and VQA <ref type="bibr" target="#b0">[1]</ref> and show that it produces the best reported results in both cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual question answering (VQA) is distinct from many problems in Computer Vision because the question to be answered is not determined until run time <ref type="bibr" target="#b0">[1]</ref>. In more traditional problems such as segmentation or detection, the single question to be answered by an algorithm is predetermined, and only the image changes. In visual question answering, in contrast, the form that the question will take is unknown, as is the set of operations required to answer it. In this sense it more closely reflects the challenge of general image interpretation.</p><p>VQA typically requires processing both visual information (the image) and textual information (the question and answer). One approach to Vision-to-Language problems,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Internal Textual Representation:</head><p>A group of people enjoying a sunny day at the beach with umbrellas in the sand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>External Knowledge:</head><p>An umbrella is a canopy designed to protect against rain or sunlight. Larger umbrellas are often used as points of shade on a sunny beach. A beach is a landform along the coast of an ocean. It usually consists of loose particles, such as sandâ€¦.</p><p>Question Answering: Q: Why do they have umbrellas? A : Shade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attributes:</head><p>umbrella beach sunny day people sand laying blue green mountain <ref type="figure">Figure 1</ref>. A real case of question answering based on an internal textual representation and external knowledge. All of the attributes, textual representation, knowledge and answer are produced by our VQA model. Underlined words indicate the information required to answer the question.</p><p>such as VQA and image captioning, which interrelate visual and textual information is based on a direct method pioneered in machine language translation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>. This direct approach develops an encoding of the input text using a Recurrent Neural Network (RNN) and passes it to another RNN for decoding.</p><p>The Vision-to-Language version of this direct translation approach uses a Convolutional Neural Network (CNN) to encode the image, and an RNN to encode and generate text. This approach has been well studied. The image captioning methods developed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>, for example, learn a mapping from images to text using CNNs trained on object recognition, and word embeddings trained on large scale text corpora. Visual question answer-ing is a significantly more complex problem than image captioning, not least because it requires accessing information not present in the image. This may be common sense, or specific knowledge about the image subject. For example, given an image, such as <ref type="figure">Figure 1</ref>, showing 'a group of people enjoying a sunny day at the beach with umbrellas', if one asks a question 'why do they have umbrellas?', to answer this question, the machine must not only detect the scene 'beach', but must know that 'umbrellas are often used as points of shade on a sunny beach'. Recently, Antol et al. <ref type="bibr" target="#b0">[1]</ref> also have suggested that VQA is a more "AI-complete" task since it requires multimodal knowledge beyond a single sub-domain. Our proposed system finally gives the right answer 'shade' for the above real example.</p><p>Large-scale Knowledge Bases (KBs), such as Freebase <ref type="bibr" target="#b3">[4]</ref> and DBpedia <ref type="bibr" target="#b1">[2]</ref>, have been used successfully in several natural language Question Answering (QA) systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>. However, VQA systems exploiting KBs are still relatively rare. Gao et al. <ref type="bibr" target="#b10">[11]</ref> and Malinowski et al. <ref type="bibr" target="#b19">[20]</ref> do not use a KB at all. Zhu et al. <ref type="bibr" target="#b32">[33]</ref> do use a KB, but it is created specifically for the purpose, and consequently contains a small amount of very specific, largely image-focused, information. This in turn means that only a specific set of questions may be asked of the system, and the query generation approach taken mandates a very particular question form. The method that we propose here, in contrast, is applicable to general (even publicly created) KBs, and admits general questions.</p><p>In this work, we fuse an automatically generated description of an image with information extracted from an external KB to provide an answer to a general question about the image (See <ref type="figure" target="#fig_1">Figure 2</ref>). The image description takes the form of a set of captions, and the external knowledge is textbased information mined from a Knowledge Base. Given an image-question pair, a CNN is first employed to predict a set of attributes of the image. The attributes cover a wide range of high-level concepts, including objects, scenes, actions, modifiers and so on. A state-of-the-art image captioning model <ref type="bibr" target="#b29">[30]</ref> is applied to generate a series of captions based on the attributes. We then use the detected attributes to extract relevant information from the KB. Specifically, for each of the top-5 attributes detected in the image we generate a query which may be applied to a Resource Description Framework (RDF) KB, such as DBpedia. RDF is the standard format for large KBs, of which there are many. The queries are specified using Semantic Protocol And RDF Query Language (SPARQL). We encode the paragraphs extracted from the KB using Doc2Vec <ref type="bibr" target="#b14">[15]</ref>, which maps paragraphs into a fixed-length feature representation. The encoded attributes, captions, and KB information are then input to an LSTM which is trained so as to maximise the likelihood of the ground truth answers in a training set.</p><p>The approach we propose here combines the generality of information that using a KB allows with the generality of question that the LSTM allows. In addition, it achieves an accuracy of 69.73% on the Toronto COCO-QA, while the latest state-of-the-art is 55.92%. We also produce the best results on the VQA evaluation server (which does not publish ground truth answers for its test set), which is 59.44%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Malinowski et al. <ref type="bibr" target="#b17">[18]</ref> were among the first to study the VQA problem. They proposed a method that combines semantic parsing and image segmentation with a Bayesian approach to sample from nearest neighbors in the training set. This approach requires human defined predicates, which are inevitably dataset-specific. This approach is also very dependent on the accuracy of the image segmentation algorithm and on the estimated image depth information. Tu et al. <ref type="bibr" target="#b26">[27]</ref> built a query answering system based on a joint parse graph from text and videos. Geman et al. <ref type="bibr" target="#b11">[12]</ref> proposed an automatic 'query generator' that was trained on annotated images and produced a sequence of binary questions from any given test image. Each of these approaches places significant limitations on the form of question that can be answered.</p><p>Most recently, inspired by the significant progress achieved using deep neural network models in both computer vision and natural language processing, an architecture which combines a CNN and RNN to learn the mapping from images to sentences has become the dominant trend. Both Gao et al. <ref type="bibr" target="#b10">[11]</ref> and Malinowski et al. <ref type="bibr" target="#b19">[20]</ref> used RNNs to encode the question and output the answer. Whereas Gao et al. <ref type="bibr" target="#b10">[11]</ref> used two networks, a separate encoder and decoder, Malinowski et al. <ref type="bibr" target="#b19">[20]</ref> used a single network for both encoding and decoding. Ren et al. <ref type="bibr" target="#b22">[23]</ref> focused on questions with a single-word answer and formulated the task as a classification problem using an LSTM. A single-word answer dataset COCO-QA was published with <ref type="bibr" target="#b22">[23]</ref>. Ma et al. <ref type="bibr" target="#b16">[17]</ref> used CNNs to both extract image features and sentence features, and fused the features together with another multimodal CNN. Antol et al. <ref type="bibr" target="#b0">[1]</ref> proposed a large-scale openended VQA dataset based on COCO, which is called VQA. They also provided several baseline methods which combined both image features (CNN extracted) and question features (LSTM extracted) to obtain a single embedding and further built a MLP (Multi-Layer Perceptron) to obtain a distribution over answers. Our framework also exploits both CNNs and RNNs, but in contrast to preceding approaches which use only image features extracted from a CNN in answering a question, we employ multiple sources, including image content, generated image captions and mined external knowledge, to feed to an RNN to answer questions.</p><p>The quality of the information in the KB is one of the primary issues in this approach to VQA. The problem is that KBs constructed by analysing Wikipedia and similar   are patchy and inconsistent at best, and hand-curated KBs are inevitably very topic specific. Using visually-sourced information is a promising approach to solving this problem <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref>, but has a way to go before it might be usefully applied within our approach. Thus, although our SPARQL and RDF driven approach can incorporate any information that might be extracted from a KB, the limitations of the existing available KBs mean that the text descriptions of the detected attributes is all that can be usefully extracted.</p><p>Zhu et al. <ref type="bibr" target="#b32">[33]</ref>, in contrast used a hand-crafted KB primarily containing image-related information such as category labels, attribute labels and affordance labels, but also some quantities relating to their specific question format such as GPS coordinates and similar. The questions in that system are phrased in the DBMS query language, and are thus tightly coupled to the nature of the hand-crafted KB. This represents a significant restriction on the form of question that might be asked, but has the significant advantage that the DBMS is able to respond decisively as to whether it has the information required to answer the question. Instead of building a problem-specific KB, we use a pre-built large-scale KB (DBpedia <ref type="bibr" target="#b1">[2]</ref>) from which we extract information using a standard RDF query language. DBpedia has been created by extracting structured information from Wikipedia, and is thus significantly larger and more general than a hand-crafted KB. Rather than having a user to pose their question in a formal query language, our VQA system is able to encode questions written in natural language automatically. This is achieved without manually specified formalization, but rather depends on processing a suitable training set. The result is a model which is very general in the forms of question that it will accept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Extracting, Encoding, and Merging</head><p>The key differentiator of our approach is that it is able to usefully combine image information with that extracted from a KB, within the LSTM framework. The novelty lies in the fact that this is achieved by representing both of these disparate forms of information as text before combining them. <ref type="figure" target="#fig_1">Figure 2</ref> summarises how this is achieved. We now describe each step in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attribute-based Image Representation</head><p>Our first task is to describe the image content in terms of a set of attributes. Each attribute in our vocabulary is extracted from captions from MS COCO <ref type="bibr" target="#b4">[5]</ref>, a large-scale image-captioning dataset. An attribute can be any part of speech, including object names (nouns), motions (verbs) or properties (adjectives). Because MS COCO captions are created by humans, attributes derived from captions are likely to represent image features that are of particular significance to humans, and are thus likely to be the subject of image-based questions.</p><p>We formalize attribute prediction as a multi-label classification problem. To address the issue that some attributes may only apply to image sub-regions, we follow Wei et al. <ref type="bibr" target="#b28">[29]</ref> to design a region-based multi-label classification framework that takes an arbitrary number of sub-region proposals as input. A shared CNN is connected with each proposal, and the CNN outputs from different proposals are  aggregated with max pooling to produce the final prediction over the attribute vocabulary.</p><p>To initialize the attribute prediction model, we use the powerful VggNet-16 <ref type="bibr" target="#b24">[25]</ref> pre-trained on the ImageNet <ref type="bibr" target="#b7">[8]</ref>. The shared CNN is then fine-tuned on the multi-label dataset, the MS COCO image-attribute training data <ref type="bibr" target="#b29">[30]</ref>. The output of the last fully-connected layer is fed into a cway softmax which produces a probability distribution over the c class labels. The c represents the attribute vocabulary size, which here is 256. The fine-tuning learning rates of the last two fully connect layers are initialized to 0.001 and the prediction layer is initialized to 0.01. All the other layers are fixed. We execute 40 epochs in total and decrease the learning rate to one tenth of the current rate for each layer after 10 epochs. The momentum is set to 0.9. The dropout rate is set to 0.5. Then, we use Multiscale Combinatorial Grouping (MCG) <ref type="bibr" target="#b21">[22]</ref> for the proposal generation. Finally, a cross hypothesis max-pooling is applied to integrate the outputs into a single prediction vector V att (I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption-based Image Representation</head><p>Currently the most successful approach to image captioning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref> is to attach a CNN to an RNN to learn the mapping from images to sentences directly. Wu et al. <ref type="bibr" target="#b29">[30]</ref> proposed to feed a high-level attribute-based representation to an LSTM to generate captions, instead of directly using CNN-extracted features. This method produces promising results on the major public captioning challenge <ref type="bibr" target="#b4">[5]</ref> and accepts our attributes prediction vector V att (I) as the input. We thus use this approach to generate 5 different captions (using beam search) that constitute the internal textual representation for a given image.</p><p>The hidden state vector of the caption-LSTM after it has generated the last word in each caption is used to represent its content. Average-pooling is applied over the 5 hidden-state vectors, to obtain a 512-d vector V cap (I) for the image I. The caption-LSTM is trained on the humangenerated captions from the MS COCO training set, which means that the resulting model is focused on the types of image content that humans are most interested in describing. <ref type="figure" target="#fig_2">Figure 3</ref> shows some examples of the predicted attributes and generated captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Relating to the Knowledge Base</head><p>The external data source that we use here is DBpedia <ref type="bibr" target="#b1">[2]</ref>. As a source of general background information, although any such KB could equally be applied, DBpedia is a structured database of information extracted from Wikipedia. The whole DBpedia dataset describes 4.58 million entities, of which 4.22 million are classified in a consistent ontology. The data can be accessed using an SQL-like query language for RDF called SPARQL. Given an image and its predicted attributes, we use the top-5 most strongly predicted attributes 1 to generate DBpedia queries. Inspecting the database shows that the 'comment' field is the most generally informative about an attribute, as it contains a general text description of it. We therefore retrieve the comment text for each query term. The KB+SPARQL combination is very general, however, and could be applied to problem specific KBs, or a database of common sense information, and can even perform basic inference over RDF. <ref type="figure" target="#fig_4">Figure 4</ref> shows an example of the query language and returned text.</p><p>The domestic dog is a furry, carnivorous member of the canidae family, mammal class. Domestic dogs are commonly known as "man's best friend". The dog was the first domesticated animal and has been widely kept as a working, hunting, and pet companion. It is estimated there are between 700 million and one billion domestic dogs, making them the most abundant member of order Carnivora.  Since the text returned by the SPARQL query is typically much longer than the captions generated in the section 3.2, we turn to Doc2Vec <ref type="bibr" target="#b14">[15]</ref> to extract the semantic meanings. Doc2Vec, also known as Paragraph Vector, is an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Le et al. <ref type="bibr" target="#b14">[15]</ref> proved that it can capture the semantics of paragraphs. A Doc2Vec model is trained to predict words in the document given the context words. We collect 100,000 documents from DBpedia to train a model with vector size 500. To obtain the knowledge vector V know (I) for image I, we combine the 5 returned paragraphs in to a single large paragraph, before extracting semantic features using our Doc2Vec model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A VQA Model with Multiple Inputs</head><p>We propose to train a VQA model by maximizing the probability of the correct answer given the image and question. We want our VQA model to be able to generate multiple word answers, so we formulate the answering process as a word sequence generation procedure. Let Q = {q 1 , ..., q n } represents the sequence of words in a question, and A = {a 1 , ..., a l } the answer sequence, where n and l are the length of question and answer, respectively. The log-likelihood of the generated answer can be written as: </p><p>where p(a t |a 1:tâˆ’1 , I, Q) is the probability of generating a t given image information I, question Q and previous words a 1:tâˆ’1 . We employ an encoder LSTM <ref type="bibr" target="#b12">[13]</ref> to take the semantic information from image I and the question Q, while using a decoder LSTM to generate the answer. Weights are shared between the encoder and decoder LSTM.</p><p>In the training phase, the question Q and answer A are concatenated as {q 1 , ..., q n , a 1 , ..., a l , a l+1 }, where a l+1 is a special END token. Each word is represented as a one-hot vector of dimension equal to the size of the word dictionary. The training procedure is as follows: at time step t = 0, we set the LSTM input:</p><formula xml:id="formula_1">x initial = [W ea V att (I), W ec V cap (I), W ek V know (I)] (2)</formula><p>where W ea , W ec , W ek are learnable embedding weights for the vector representation of attributes, captions and external knowledge, respectively. In practice, all these embedding weights are learned jointly. Given the randomly initialized hidden state, the encoder LSTM feeds forward to produce hidden state h 0 which encodes all of the input information. From t = 1 to t = n, we set x t = W es q t and the hidden state h tâˆ’1 is given by the previous step, where W es is the learnable word embedding weights. The decoder LSTM runs from time step n + 1 to l + 1. Specifically, at time step t = n + 1, the LSTM layer takes the input x n+1 = W es a 1 and the hidden state h n corresponding to the last word of the question, where a 1 is the start word of the answer. The hidden state h n thus encodes all available information about the image and the question. The probability distribution p t+1 over all answer words in the vocabulary is then computed by the LSTM feed-forward process. Finally, for the final step, when a l+1 represents the last word of the answer, the target label is set to the END token.</p><p>Our training objective is to learn parameters W ea , W ec , W ek , W es and all the parameters in the LSTM by minimizing the following cost function:</p><formula xml:id="formula_2">C = âˆ’ 1 N N i=1 log p(A (i) |I, Q) + Î» Î¸ Â· ||Î¸|| 2 2 (3) = âˆ’ 1 N N i=1 l (i) +1 j=1 log p j (a (i) j ) + Î» Î¸ Â· ||Î¸|| 2 2<label>(4)</label></formula><p>where N is the number of training examples, and n (i) and l (i) are the length of question and answer respectively for the i-th training example. Let p t (a (i) t ) correspond to the activation of the Softmax layer in the LSTM model for the i-th input and Î¸ represent the model parameters. Note that Î» Î¸ Â· ||Î¸|| 2 2 is a regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our model on two recent publicly available visual question answering datasets, both based on MS COCO images. The Toronto COCO-QA Dataset <ref type="bibr" target="#b22">[23]</ref> contains 78,736 training and 38,948 testing examples, which are generated from 117,684 images. There are four types of questions, relating to the object, number, color and location, all constructed so as to have a single-word answer. All of the question-answer pairs in this dataset are automatically converted from human-sourced image descriptions. Another benchmarked dataset is VQA <ref type="bibr" target="#b0">[1]</ref>, which is a much larger dataset and contains 614,163 questions and 6,141,630 answers based on 204,721 MS COCO images. This dataset provides a surprising variety of question types, including "What is...', "How Many" and even "Why...". The ground truth answers were generated by 10 human subjects and can be single word or sentences. The data train/val split follows the COCO official split, which contains 82,783 training images and 40,504 validation images, each has 3 questions and 10 answers. We randomly choose 5000 images from the validation set as our val set, with the remainder testing. The human ground truth answers for the actual VQA test split are not available publicly and only can be evaluated via the VQA evaluation server. Hence, we also apply our final model on a test split and report the overall accuracy. <ref type="table">Table 1</ref> displays some dataset statistics. We did not test on the DAQUAR dataset <ref type="bibr" target="#b18">[19]</ref> as it is an order of magnitude smaller than the datasets mentioned above, and thus too small to train our system, and to test its generality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>To train the VQA model with multiple inputs in the Section 4, we use Stochastic gradient Descent (SGD) with mini-batches of 100 image-QA pairs. The attributes, internal textual representation, external knowledge embedding size, word embedding size and hidden state size are all 256 in all experiments. The learning rate is set to 0.001 and clip gradients is 5. The dropout rate is set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on Toronto COCO-QA</head><p>Metrics Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>, the accuracy value (the proportion of correctly answered test questions), and the Wu-Palmer similarity (WUPS) <ref type="bibr" target="#b30">[31]</ref> are used to measure performance. The WUPS calculates the similarity between two words based on the similarity between their common subsequence in the taxonomy tree. If the similarity between two words is greater than a threshold then the candidate answer is considered to be right. We report on thresholds 0.9 and 0.0, following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Evaluations To illustrate the effectiveness of our model, we provide a baseline and several state-of-the-art results on the Toronto COCO-QA dataset. The Baseline method is implemented simply by connecting a CNN to an LSTM. The CNN is a pre-trained (on ImageNet) VggNet model from which we extract the coefficients of the last fully connected layer. We also implement a baseline model VggNet+ft-LSTM, which applies a VggNet that has been fine-tuned on the COCO dataset, based on the task of image-attributes classification. We also present results from a series of cut down versions of our approach for comparison. Att-LSTM uses only the semantic level attribute representation V att as the LSTM input. To evaluate the  contribution of the internal textual representation and external knowledge for the question answering, we feed the image caption representation V cap and knowledge representation V know with the V att separately, producing two models, Att+Cap-LSTM and Att+Know-LSTM. We also tested the Cap+Know-LSTM, for the experiment completeness. Our final model is the Att+Cap+Know-LSTM, which combines all the available information.</p><p>GUESS <ref type="bibr" target="#b22">[23]</ref> simply selects the modal answer from the training set for each of 4 question types (the modal answers are 'cat', 'two', 'white', and 'room'). VIS+BOW <ref type="bibr" target="#b22">[23]</ref> performs multinomial logistic regression based on image features and a BOW vector obtained by summing all the word vectors of the question. VIS+LSTM <ref type="bibr" target="#b22">[23]</ref> has one LSTM to encode the image and question, while 2-VIS+BLSTM <ref type="bibr" target="#b22">[23]</ref> has two image feature inputs, at the start and the end. Ma et al. <ref type="bibr" target="#b16">[17]</ref> encode both images and questions with a CNN. <ref type="table">Table 2</ref> reports the results. All of our proposed models outperform the Baseline and all of the competing state-ofthe-art methods. Our final model Att+Cap+Know-LSTM achieves the best results. It surpasses the baseline by nearly 20% and outperforms the previous state-of-the-art methods by around 15%. Att+Cap-LSTM clearly improves the results over the Att-LSTM model. This proves that internal textual representation plays a significant role in the VQA task. The Att+Know-LSTM model does not perform as well as Att+Cap-LSTM , which suggests that the information extracted from captions is more valuable than that extracted from the KB. Cap+Know-LSTM also performs better than Att+Know-LSTM. This is not surprising because the Toronto COCO-QA questions were generated automatically from the MS COCO captions, and thus the fact that they can be answered by training on the captions is to be expected. This generation process also leads to questions which require little external information to answer. The comparison on the Toronto COCO-QA thus provides an important benchmark against related methods, but does not really test the ability of our method to incorporate extra information. It is thus interesting that the additional external information provides any benefit at all.   <ref type="table" target="#tab_4">Table 3</ref> shows the per-category accuracy for different models. Surprisingly, the counting ability (see question type 'Number') increases when both captions and external knowledge are included. This may be because some 'counting' questions are not framed in terms of the labels used in the MS COCO captions. Ren et al. also observed similar cases. In <ref type="bibr" target="#b22">[23]</ref> they mentioned that "there was some observable counting ability in very clean images with a single object type but the ability was fairly weak when different object types are present". We also find there is a slight increase for the 'color' questions when the KB is used. Indeed, some questions like 'What is the color of the stop sign?' can be answered directly from the KB, without the visual cue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on the VQA</head><p>Antol et al. in <ref type="bibr" target="#b0">[1]</ref> provide the VQA dataset which is intended to support "free-form and open-ended Visual Question Answering". They also provide a metric for measuring performance: min{ # humans that said answer 3 , 1} thus 100% means that at least 3 of the 10 humans who answered the question gave the same answer. We have used the provided evaluation code to produce the results in <ref type="table" target="#tab_6">Table 4</ref>.</p><p>Evaluation Inspecting <ref type="table" target="#tab_6">Table 4</ref>, results on the VQA validation set, we see that the attribute-based Att-LSTM is a significant improvement over our VggNet+LSTM baseline. We also evaluate another baseline, the VggNet+ft+LSTM, which uses the penultimate layer of the attributes prediction CNN as the input to the LSTM. Its overall accuracy on the VQA is 50.01, which is still lower than our proposed mod-els. Adding either image captions or external knowledge further improves the result. The model Cap+Know produces overall accuracy 52.31, slightly lower than Att+Know (53.79). This suggests that the attributes representation plays a more important role here. Our final model A+C+K-LSTM produces the best results, outperforming the baseline VggNet-LSTM by 11% overall. <ref type="figure" target="#fig_6">Figure 5</ref> relates the performance of the various models on five categories of questions. The 'object' category is the average of the accuracy of question types starting with 'what kind/type/sport/animal/brand...', while the 'number' and 'color' category corresponds to the question type 'how many' and 'what color'. The performance comparison across categories is of particular interest here because answering different classes of questions requires different amounts of external knowledge. The 'Where' questions, for instance, require knowledge of potential locations, and 'Why' questions typically require general knowledge about people's motivation. 'Number' and 'Color' questions, in contrast, can be answered directly. The results show that for 'Why' questions, adding the KB improves performance by more than 50% (Att-LSTM achieves 7.77% while Att+Know-LSTM achieves 11.88%), and that the combined A+C+K-LSTM achieves 13.53%.  <ref type="table">Table 5</ref> provides some indicative results, more results can be found in the supplementary material, including failure cases.</p><p>We have also tested on the VQA test-dev and teststandard 2 consisting of 60,864 and 244,302 questions (for which ground truth answers are not published) using our final A+C+K-LSTM model, and evaluated them on the VQA evaluation server. <ref type="table">Table 6</ref> shows the server reported results.</p><p>Antol et al. <ref type="bibr" target="#b0">[1]</ref> provide several results for this dataset. In each case they encode the image with the final hidden layer from VggNet, and questions are encoded using a BOW representation. A softmax neural network classifier with 2 hid-What color is the tablecloth? How many people in the photo?</p><p>What is the red fruit?</p><p>What  <ref type="table">Table 6</ref>. VQA Open-Ended evaluation server results for our method. Accuracies for different answer types and overall performances on test-dev and test-standard datasets are shown.</p><p>den layers and 1000 hidden units (dropout 0.5) in each layer with tanh non-linearity is then trained, the output space of which is the 1000 most frequent answers in the training set. They also provide an LSTM model followed by a softmax layer to generate the answer. Two versions of this approach are used, one which is given both the question and the image, and one which is given only the question (see <ref type="bibr" target="#b0">[1]</ref> for details). Our final model outperforms the listed approaches according to the overall accuracy and all answer types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Open-ended visual question answering is an interesting challenge for computer vision, not least because it represents a move away from purely image-based analysis and towards a broader form of artificial visual intelligence. In this paper we have shown that it is possible to extend the state-of-the-art RNN-based VQA approach so as to incorporate the large volumes of information required to an-swer general, open-ended, questions about images. The knowledge bases which are currently available do not contain much of the information which would be beneficial to this process, but none-the-less can still be used to significantly improve performance on questions requiring external knowledge (such as 'Why' questions). The approach that we propose is very general, however, and will be applicable to more informative knowledge bases should they become available. At the time of writing this paper, our system performs the best on two large-scale VQA datasets and produces promising results on the VQA evaluation server. Further work includes generating knowledge-base queries which reflect the content of the question and the image, in order to extract more specifically related information.</p><p>It seems inevitable that successful general visual question answering will demand access to a large external knowledge base. We have shown that this is possible using the LSTM-based approach, and that it improves performance. We hope this might lead the way to a visual question answering approach which is capable of deeper analysis of image content, and even common sense.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>laying on the floor with a bird next to it and a cat behind them, on the other side of a sliding glass door. Cap 2: a brown and black dog laying on a floor next to a bird. Cap 3: the dog, cat, and bird are all on the floor in the room.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Our proposed framework: given an image, a CNN is first applied to produce the attribute-based representation Vatt(I). The internal textual representation is made up of image captions generated based on the image-attributes. The hidden state of the caption-LSTM after it has generated the last word in each caption is used as its vector representation. These vectors are then aggregated as Vcap(I) with average-pooling. The external knowledge is mined from the KB (in this case DBpedia) and the responses encoded by Doc2Vec, which produces a vector V know (I). The 3 vectors V are combined into a single representation of scene content, which is input to the VQA LSTM model which interprets the question and generates an answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Examples of predicted attributes and generated captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; sparql SELECT DISTINCT ?comment WHERE { ?entry rdfs: label "Dog"@en. ?entry rdfs: comment ?comment. }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>An example of SPARQL query language for the attribute 'dog'. The mined text-based knowledge are shown below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Performance on five question categories for different models. The 'Object' category is the average accuracy of question types starting with 'what kind/type/sport/animal/brand...'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Top 5 Attributes: players, catch, bat, baseball, swing Generated Captions: A baseball player swing a bat at a ball. A baseball player holding a bat on a field. A baseball player swinging a bat on a field. A baseball player is swinging a bat at a ball. A batter catcher and umpire during a baseball game.Top 5 Attributes: field, two, tree, grass, giraffe Generated Captions : Two giraffes are standing in a grassy field. A couple of giraffe standing next to each other. Two giraffes standing next to each other in a field. A couple of giraffe standing next to each other on a lush green field.Top 5 Attributes: pizza, bottle, sitting, table, beer Generated Captions : A large pizza sitting on top of a table. A pizza sitting on top of a white plate. A pizza sitting on top of a table next to a beer. A pizza sitting on top of a table next to a bottle of beer.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Toronto COCO-QA accuracy (%) per category.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Results on the open-answer task for various question types 
on VQA validation set. All results are in terms of the evaluation 
metric from the VQA evaluation tools. The overall accuracy for 
the model of VggNet+ft+LSTM is 50.01. Detailed results of dif-
ferent question types for this model are not shown in the table due 
to the limited space. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>Table 5. Some example cases where our final model gives the correct answer while the base line model VggNet-LSTM generates the wrong answer. All results are from VQA. More results can be found in the supplementary material. LSTM Q+I [1] 53.74 78.94 35.24 36.42 54.06 79.01 35.55 36.80</figDesc><table>are these people doing? 
Ours: 
white 
2 
apple 
eating 
Vgg+LSTM: 
red 
1 
banana 
playing 
Ground Truth: 
white 
2 
apple 
eating 

Why are his hands outstretched? 
Why are the zebras in water? 
Is the dog standing or laying down? 
Which sport is this? 
Ours: 
balance 
drinking 
laying down 
baseball 
Vgg+LSTM: 
play 
water 
sitting 
tennis 
Ground Truth: 
balance 
drinking 
laying down 
baseball 

Test-dev 
Test-standard 
All 
Y/N 
Num Others 
All 
Y/N 
Num Others 
Question [1] 
40.09 75.66 36.70 27.14 
-
-
-
-
Image [1] 
28.13 64.01 0.42 
3.77 
-
-
-
-
Q+I [1] 
52.64 75.55 33.67 37.37 
-
-
-
-
LSTM Q [1] 
48.76 78.20 35.68 26.59 48.89 78.12 34.94 26.99 
Human [1] 
-
-
-
-
83.30 95.77 83.39 72.67 
Ours 
59.17 81.01 38.42 45.23 59.44 81.07 37.12 45.83 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We only use the top-5 attributes to query the KB because, based on the observation of training data, an image typically contains 5-8 attributes. We also tested with top-10, but no improvements were observed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.visualqa.org/challenge.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic Parsing on Freebase from Question-Answer Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods in Natural Language Processing</title>
		<meeting>Conf. Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a Recurrent Visual Representation for Image Caption Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods in Natural Language Processing</title>
		<meeting>Conf. Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building Watson: An overview of the DeepQA project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual Turing test for computer vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3618" to="3623" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Don&apos;t Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00333</idno>
		<title level="m">Learning to Answer Questions From Image using Convolutional Neural Network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8027</idno>
		<title level="m">Towards a Visual Turing Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ask Your Neurons: A Neural-based Approach to Answering Questions about Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image Question Answering: A Visual Semantic Embedding Model and a New Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VisKE: Visual Knowledge Extraction and Question Answering by Visual Verification of Relation Phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint video and text parsing for understanding events and answering queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="42" to="70" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">CNN: Single-label to multi-label</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What Value Do Explicit High Level Concepts Have in Vision to Language Problems?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Association for Computational Linguistics</title>
		<meeting>Conf. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.08029</idno>
		<title level="m">Describing videos by exploiting temporal structure</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Building a Largescale Multimodal Knowledge Base for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>RÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05670</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
