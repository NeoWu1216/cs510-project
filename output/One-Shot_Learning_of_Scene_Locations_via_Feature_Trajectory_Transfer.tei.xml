<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Learning of Scene Locations via Feature Trajectory Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kwitt</surname></persName>
							<email>roland.kwitt@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Salzburg</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hegenbart</surname></persName>
							<email>sebastian.hegenbart@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Salzburg</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">UNC Chapel Hill</orgName>
								<address>
									<region>NC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">One-Shot Learning of Scene Locations via Feature Trajectory Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The appearance of (outdoor) scenes changes considerably with the strength of certain transient attributes, such as "rainy", "dark" or "sunny". Obviously, this also affects the representation of an image in feature space, e.g., as activations at a certain CNN layer, and consequently impacts scene recognition performance. In this work, we investigate the variability in these transient attributes as a rich source of information for studying how image representations change as a function of attribute strength. In particular, we leverage a recently introduced dataset with fine-grain annotations to estimate feature trajectories for a collection of transient attributes and then show how these trajectories can be transferred to new image representations. This enables us to synthesize new data along the transferred trajectories with respect to the dimensions of the space spanned by the transient attributes. Applicability of this concept is demonstrated on the problem of oneshot recognition of scene locations. We show that data synthesized via feature trajectory transfer considerably boosts recognition performance, (1) with respect to baselines and (2) in combination with state-of-the-art approaches in oneshot learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning new visual concepts from only a single image is a remarkable ability of human perception. Yet, the predominant setting of recognition experiments in computer vision is to measure success of a learning process with respect to hundreds or even thousands of training instances. While datasets of such size were previously only available for object-centric recognition <ref type="bibr" target="#b1">[2]</ref>, the emergence of the Places database <ref type="bibr" target="#b30">[31]</ref> has made a large corpus of scene-centric data available for research. Coupled with advances in CNNbased representations and variants thereof, the performance of scene recognition systems has improved remarkably in the recent past <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3]</ref>, even on already well established (and relatively small) benchmarks such as "15 Scenes". However, large scale databases are typically constructed In this illustration, the transient attribute is "sunny".</p><p>Scenario B:f u l la t t r i b u t ev a r i a b i l i t yf o rt r a i n i n g <ref type="figure">Figure 1</ref>: Introductory experiment on the Transient Attributes Database of <ref type="bibr" target="#b10">[11]</ref>. The task is to distinguish scenes from different webcams. In Scenario A, images with annotated attribute strengths (in the range of [0, 1]) less than 0.4 are excluded during training and used for testing. Results are averaged over 40 attributes. Scenario B represents a standard five-fold cross-validation setup, using random splits of the data. The size of each training split is set to approximately the size as in Scenario A. In summary, significant drops in recognition accuracy occur when only limited variability is present in the training data with respect to (transient) attributes.</p><p>by fetching images from the web and subsequently crowdsourcing the annotation task. Consequently, a considerable user bias <ref type="bibr" target="#b24">[25]</ref> is expected, e.g., with respect to captured conditions. It is, e.g., highly unlikely that scenes of "city skylines" or "beaches" are captured during rainy or cloudy conditions. Hence, we expect that the observed variability of transient states in readily available scene databases will be limited. The question then is if this existing visual data is rich enough to sufficiently inform the process of learning to recognize scenes from single instances, i.e., the aforementioned task on which humans perform so exceptionally well.</p><p>While the majority of approaches to scene recognition either rely on variants of Bag-of-Words <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, Fisher vectors <ref type="bibr" target="#b21">[22]</ref>, or outputs of certain CNN layers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3]</ref>, several works have also advocated more abstract, semantic (attribute) representations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10]</ref>. Largely due to the absence of fine-grained semantic annotations, the axes of the semantic space typically correspond to the scene category labels. To alleviate this problem and to enable attribute representations of scene images, Patterson et al. <ref type="bibr" target="#b17">[18]</ref> have recently introduced the SUN attribute database. This is motivated by the success of attributes in object-centric recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. However, construction of the attribute vocabulary is guided by the premise of enhancing discriminability of scene categories. Somewhat orthogonal to this objective, Laffont et al. <ref type="bibr" target="#b10">[11]</ref> recently introduced the concept of transient attributes, such as "sunny" or "foggy". This is particularly interesting in the context of studying scene variability, since the collection of attributes is designed with high variability in mind (as opposed to discriminability). Equipped with a collection of trained attribute strength regressors, this enables us to more thoroughly assess variability issues.</p><p>Is limited scene variability a problem? To motivate our work, we start with three simple experiments. For technical details, we refer the reader to Section 4.</p><p>First, we consider the (supposedly) simple recognition problem of distinguishing images from the 101 webcams used in the Transient Attribute Database (TADB) <ref type="bibr" target="#b10">[11]</ref>. We refer to this task as recognition of scene locations, since each webcam records images from one particular location. Each image is hand-annotated by the strength of 40 different transient attributes. We use activations of the final fullyconnected layer ('fc7') in Places-CNN <ref type="bibr" target="#b30">[31]</ref> as our feature space X f c7 ⊂ R 4096 and a linear support vector classifier. In Scenario A, images with annotated attribute strength below a certain threshold are excluded during training and used for testing. In Scenario B, we perform five-fold crossvalidation using random splits of all available data. <ref type="figure">Fig. 1</ref> shows that almost perfect accuracy (98.3%) is achieved in Scenario B. However, the recognition rate drops by almost 20 percentage points in Scenario A. This clearly highlights that limited variability with respect to the transient state of a scene can severely impact recognition accuracy, even when using one of the state-of-the-art CNN representations.</p><p>In our second experiment, the task is to predict the scene category of images in TADB, this time by directly using the 205 category labels as outputted by the pre-trained Places-CNN. In this setup, images from different scene locations (i.e., webcams) could be from the same scene category (e.g., "mountain"). However, under the assumption of invariance against the transient states of a scene, we would expect to obtain consistent predictions over all images from the same webcam. This is not always the case, though, as can be seen from <ref type="figure">Fig. 2</ref> showing scene locations from TADB with highly <ref type="table" target="#tab_4">bayou  boardwalk  bridge  campsite  castle  cemetery  construction_site  cottage_garden  driveway  forest_road  formal_garden  highway  igloo  mountain_snowy  railroad_track  river  ruin  shed  ski_resort  tree_farm  trench  valley  vegetable_garden  water_tower  yard   bar  basement  bowling_alley  butte  campsite  chalet  construction_site  driveway  fairway  fire_station  gas_station  golf_course  highway  parking_lot  pasture  plaza  rice_paddy  runway  ski_resort  ski_slope  trench  windmill   skyscraper   25</ref>  Third, we assess how much variability in transient attributes is covered by existing image corpora for scene recognition. In particular, we train a collection of regressors to predict the strength of all 40 attributes in TADB, using the same CNN-based image representation of the previous two experiments. We then use the trained regressors to map each image of the SUN397 database <ref type="bibr" target="#b26">[27]</ref> into the 40-dimensional attribute space. Note that the transient attributes only apply to outdoor scenes; consequently only images from the outdoor categories in SUN397 were used. For each category and attribute, we record the 5th and the 95th percentile of the predicted attribute strength (denoted by p 5 and p 95 ) and compute the range (r = p 95 − p 5 ). <ref type="figure" target="#fig_1">Fig. 3</ref> shows a series of boxplots, where each column represents the distribution of range values r, collected from all images and attributes in a particular scene category. The median over all range values is 0.33, indicating that the observed variability is limited with respect to the transient attributes. We also conducted the same experiment on 50000 randomly sampled images from 10 outdoor categories of the Places dataset * . While the median increases to 0.41, this is still far below a complete coverage of the transient attribute space. For comparison, when running the same experiment on TADB itself, the median is close to 1 by construction.</p><p>In summary, the first two experiments highlight the negative effect of limited variability in the training data, the  third experiment is a direct illustration of the lack of variability. Even in case of Places, our experiments suggest that the image data does not sufficiently cover variability in transient attributes so that the CNN can learn features that exhibit the required degree of invariance. Note that the most extreme scenario of limited variability is learning from single instances, i.e., one-shot recognition. Only a single transient state is observed per scene category. To the best of our knowledge, this has gained little attention so far in the scene recognition community. The motivating question for this work boils down to asking whether we can artificially increase variability -via data synthesis -by learning how image representations change depending on the strength of transient attributes.</p><p>Organization. In Section 2 we review related work. Section 3 introduces the proposed concept of feature trajectory transfer. Section 4 presents our experimental evaluation of the main parts of our pipeline and demonstrates one-shot recognition of scene locations. Section 5 concludes the paper with a discussion of the main findings and open issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Most previous works in the literature consider the problem of scene recognition in a setting where sufficient training data per scene category is available, e.g., <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref>. To the best of our knowledge, one-shot recognition has not been attempted so far. However, there have been many efforts to one-shot learning in the context of object-centric recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref>. In our review, we primarily focus on these approaches.</p><p>A consensus among most avenues to one-shot learning is the idea of "knowledge transfer", i.e., to let knowledge about previously seen (external) data influence the process of learning from only a single instance of each new class.</p><p>In early work, Miller et al. <ref type="bibr" target="#b16">[17]</ref> follow the idea of synthesizing data for classes (in their case digits) with only a single training instance. This is done through an iterative process called congealing which aligns the external images of a given category by optimizing over a class of geometric transforms (e.g., affine transforms). The learned transforms are then applied to each single instances of the new categories to augment the image data.</p><p>In <ref type="bibr" target="#b4">[5]</ref>, Fei-Fei et al. coin the term one-shot learning in the context of visual recognition problems. They propose a Bayesian approach where priors on the parameters of models for known object categories (in the external data) are learned and later adapted to object categories with few (or only a single) training instances.</p><p>In <ref type="bibr" target="#b6">[7]</ref>, Fink advocates to leverage pseudo-metric learning to find a linear projection of external training data that maximizes class separation. The learned projection is then used to transform the one-shot instances of new categories and a classifier is learned in the transform space. Tang et al. <ref type="bibr" target="#b23">[24]</ref> follow a similar idea, but advocate the concept of learning with micro-sets. The key to achieve better results is to learn over multiple training sets with only a single instance per category, i.e., the micro-sets. This has the effect of already simulating a one-shot recognition setting during metric learning on external data. <ref type="bibr">Bart and Ullmann [1]</ref> propose to use feature adaptation for one-shot recognition. In particular, features (e.g., informative image fragments) in new, single instances of a category are selected based on their similarity to features in the external training data and their performance in discriminating the external categories.</p><p>Yu and Aloimonos <ref type="bibr" target="#b29">[30]</ref> tackle one-shot learning of object categories by leveraging attribute representations and a topic model that captures the distribution of features related to attributes. During learning, training data is synthesized using the attribute description of each new object category.</p><p>Pfister et al. <ref type="bibr" target="#b19">[20]</ref> propose one-shot learning of gestures using information from an external weakly-labeled gesture dataset. Exemplar SVMs <ref type="bibr" target="#b15">[16]</ref> are used to train detectors for the single gesture instances and the detectors are then used to mine more training data from the external database.</p><p>Salakhutdinov et al. <ref type="bibr" target="#b25">[26]</ref> consider one-shot learning by adding a hierarchical prior over high-level features of Deep Boltzmann Machines. The model is trained on external data plus the one-shot instances of the novel categories. While the approach generalizes well on the categories with few (or only a single) training instance(s), information transfer happens during training which requires to retrain in case new categories are added.</p><p>Recently, Yan et al. <ref type="bibr" target="#b28">[29]</ref> have explored multi-task learning for one-shot recognition of events in video data. External data from different categories is added to the single instances of the new categories; then, multi-task learning is used to distinguish the categories with the intuition that tasks with lots of training data inform the process of learning to distinguish the new categories.</p><p>Conceptually, our approach is related to Miller et al. <ref type="bibr" target="#b16">[17]</ref> and Yu and Aloimonos <ref type="bibr" target="#b29">[30]</ref> in the sense that we also synthesize data. However, Miller et al. synthesize in the input space of images, while we synthesize image representations in feature space. The generative model of <ref type="bibr" target="#b29">[30]</ref> also allows feature synthesis, however, does not specifically leverage the structure of the space as a function of a continuous state. With respect to other work, we argue that our approach is not a direct competitor, but rather complementary, e.g., to the pseudo-metric learning approaches of Fink <ref type="bibr" target="#b6">[7]</ref> or Tang et al. <ref type="bibr" target="#b23">[24]</ref>. In Section 4, we present experiments that specifically highlight this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>On an abstract level, the key idea of our approach is to use information obtained from an external training corpus to synthesize additional samples starting from a limited amount of previously unseen data. The objective is to increase the variability of transient states available for training. Specifically, we want to use knowledge about how the appearance of a scene location -as captured by its image representation -varies depending on the state of some transient scene attributes. This concept, illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>, is based on two assumptions: First, we can predict the transient state of a scene image based on its representation. Second, we can model the functional dependency between such a transient state and the elements of the feature representation as a trajectory in feature space. We will provide empirical evidence backing both assumptions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature trajectory transfer</head><p>We adhere to the following conventions. We denote by For a previously unseen image, given by x * ∈ X , we first compute its representation in the transient attribute space by evaluating the learned attribute regressors r k . Second, we iterate over all scene locations (here only one scene location is shown) from the external image corpus and transfer the learned feature trajectories for each attribute to x * . Third, this allows us to predict features along the trajectory for this attribute (over its full range). The final synthesized image representation is a weighted combination of predictions from all scene locations, see Eq. (4). R A + , where A T denotes the set of attributes and A = |A T |. Also, let I * be a previously unseen image, represented by x * ∈ X , and let x[d] denotes the d-th component of x.</p><p>Since no attribute representation is available for unseen data, the idea is to use a collection of learned regressors r k : X → R + , k ∈ [A] to estimate the attribute strength vector [r 1 (x * ), . . . , r A (x * )] from x * . These attribute strength regressors can be learned, e.g., using support vector regression, or Gaussian process regression (cf. Section 4).</p><p>For a given scene location, we wish to estimate the path γ k : R + → X for every attribute in A T . In our case, we rely on a simple linear model to represent this path. Formally, lets fix the scene location c and let S c = {i : y i = c} be the index set of the M = |S c | images from this location. The model, for the k-th attribute, can then be written as</p><formula xml:id="formula_0">x i = w k · a i [k] + b k + ǫ k<label>(1)</label></formula><p>where w k , b k ∈ R D are the slope and intercept parameters and ǫ k denotes (component-wise) Gaussian noise. We can easily estimate this model, for a specific choice of c, using linear regression with data vectors z and v, i.e.,</p><formula xml:id="formula_1">z =    x u1 [d]</formula><p>.</p><p>. .</p><formula xml:id="formula_2">x u M [d]    , v =    a u1 [k] . . . a u M [k]    , u i ∈ S c , d ∈ [D].<label>(2)</label></formula><p>Note that for each dimension d and attribute k, we ob-</p><formula xml:id="formula_3">tain one tuple (w k [d], b k [d]</formula><p>) of slope and intercept that parametrizes the linear model. In summary, we estimate (w 1 , b 1 ), . . . , (w A , b A ) for every scene location c, describing the feature trajectories in the external data corpus.</p><p>Synthesizing new data, starting from a previously unseen instance x * , can now be done in the following way. Lets consider the feature trajectory of the k-th attribute and scene location c (parameterized by w k and b k ). We define the synthesis function s k : R + × X → X as</p><formula xml:id="formula_4">s k (t, x * ) = w k · (t − r k (x * )) + x * .<label>(3)</label></formula><p>Up to this point, we have fixed c for readability. However, in practice the question remains how to select the most informative scene location from the external data to perform trajectory transfer. Conceptually, we follow the principle (cf. <ref type="bibr" target="#b0">[1]</ref>) of identifying informativeness as the similarity of I * to each scene location in the external training corpus. In particular, our objective is to select locations which are similar in appearance, e.g., as measured in the feature space X . To avoid relying on hard decisions for one location, we advocate to take the trajectories of all locations into account. This can be achieved by weighing the contribution of each model with the similarity of I * to a scene location. In our approach, weights are determined by the posterior probability of x * under all locations, i.e., ∀c : p c = P [c|x * ]. This can be obtained, e.g., by using the output of a probabilitycalibrated support vector classifier that is trained (on the external training corpus) to distinguish between locations. The synthesis function of Eq. (3) is then (re)formulated as</p><formula xml:id="formula_5">s k (t, x * ) =    c∈[C] P [c|x * ] pc w c k · (t − r k (x * ))   + x * . (4)</formula><p>Alternatively, the contribution of each location could be ranked by P [c|x * ]. This process of synthesizing additional feature representations as a function of the desired attribute strength t ∈ R + is repeated for every attribute in A T . Relation to data augmentation. While the process of synthesizing image representations in X , depending on attribute strength, is particularly useful in our target application of one-shot recognition, it can potentially be used as an augmentation technique in other scenarios as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our experiments are structured as follows: First, we assess the performance of predicting attribute strength, i.e., the quality of r k (see Section 3.1). Second, we evaluate the quality of our linear model for synthesizing different image representations. Third, we address the problem of one-shot recognition of scene locations as our target application. Datasets. We use the Transient Attributes Database (TADB) <ref type="bibr" target="#b10">[11]</ref> that we already briefly introduced in Section 1 in all recognition experiments. In detail, the dataset contains 8571 images from 101 webcams which serve as our scene locations. The recognition task is to assign an image to the correct scene location. While this might sound artificial and overly simplistic at first, Figs. 1 and 2 reveal that the task is actually fairly difficult, especially when the training data is constrained to only contain a certain amount of variability in the transient states. Each image in TADB is handannotated with 40 transient attributes; this set is denoted as A T and corresponds to the attribute set in Section 3. Further, we also use the SUN Attributes Database (SADB) <ref type="bibr" target="#b17">[18]</ref> which contains 14340 images, each hand-annotated with respect to a collection A S of 102 discriminative attributes. Note that SADB is only used for training attribute detectors (i.e., presence/absence). These detectors are then employed to map images, represented in X f c7 (specified below), into the alternative feature space X S ⊂ R |A S | .</p><p>Image representations. All of our image representations build upon Places-CNN <ref type="bibr" target="#b30">[31]</ref>, one of the state-of-the-art CNN architectures for scene recognition. The CNN is based on the AlexNet architecture of <ref type="bibr" target="#b8">[9]</ref>. In particular, we use the activations of the 4096 neurons in the last fully-connected layer ('fc7') and denote this as our feature space X f c7 . We either represent images directly in that space, use it as an intermediate image representation to detect discriminative attributes from A S , or use it to regress the strength of transient attributes from A T . In all recognition experiments, we further perform dimensionality reduction of features in X f c7 via PCA to 200 dimensions. This retains ≈ 90% of the variance in the data. In case of features in X S , we also perform PCA, not to reduce dimensionality, but to decorrelate the feature dimensions. PCA is followed by componentwise normalization to [−1, 1], similar to <ref type="bibr" target="#b23">[24]</ref>.</p><p>Implementation. To obtain regressors for the strength of the 40 transient attributes in A T , we train a collection of linear support vector regressors (SVR). To obtain detectors for the presence/absence of the 102 attributes in A S , we train a collection of linear support vector classifiers (SVC), configured for probability outputs. We do not binarize the predictions (as in <ref type="bibr" target="#b17">[18]</ref>), but use the probability for an attribute being present to represent images in X S . Probability outputs of a SVC are also used to obtain the weights p c required to compute Eq. (4). In that case, the SVC is trained (one-vs-all) to distinguish scene locations (i.e., webcams) from the external TADB training data. The latter setup is also used for recognition of scene locations in the one-shot learning experiments. In all cases, we use the SVM implementation available as part of the scikit-learn <ref type="bibr" target="#b18">[19]</ref> toolbox; the SVR/SVC cost parameter C is configured using five-fold cross-validation on a left-out portion of the training data. The source code to reproduce the results of the paper is available online at https://github.com/ rkwitt/TrajectoryTransfer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Attribute regression / detection performance</head><p>Attribute regression. We start by assessing the first assumption of Section 3.1, i.e., predicting a transient state of a scene from its image representation, in our case x ∈ X f c7 . We choose the following performance metrics: we report the mean-squared-error (MSE) and the R 2 score per attribute. The R 2 score is an informative measure, capturing how much variation in the dependent variable (i.e., the attribute strength) is explained by the model. A R 2 score of 0 indicates no linear dependency, 1 indicates perfect linear dependency. All results are averaged over 10 random splits of TADB (split by scene location). In particular, the data for 61 webcams is selected for training, the data for the remaining 40 webcams is selected for testing. This is equivalent to the Holdout experiment reported in <ref type="bibr" target="#b10">[11]</ref>. <ref type="table" target="#tab_2">Table 1</ref> lists the performance measures. Overall, the MSE of 0.05 is comparable to the results of <ref type="bibr" target="#b10">[11]</ref>. From the R 2 statistic of 0.28, we see that there is a linear relationship, yet this relationship is not particularly strong. Nevertheless, the computational advantages of using a linear SVR (e.g., training/testing in linear time) nicely balances the tradeoff between model choice and computational resources.</p><p>Attribute detection. Since we will experiment with the attribute-based image representation of <ref type="bibr" target="#b17">[18]</ref> (i.e., images represented in X S ) in our one-shot recognition tests, we first need to assess the performance of detecting the presence/absence of the 102 SUN attributes A S . For this assessment, we use the images from SADB and the original splits provided by <ref type="bibr" target="#b17">[18]</ref>. <ref type="table" target="#tab_2">Table 1</ref> reports the mean average precision (MAP). Note that the MAP of 0.90 is slightly higher than the 0.88 originally reported in <ref type="bibr" target="#b17">[18]</ref>, presumably due to our use of Places-CNN features. Most notably, the AP per attribute rarely drops below 0.8.</p><p>Trajectory estimation. Next, we address the second assumption of Section 3.1, i.e., whether it is reasonable to assume linear dependency between the strength of a transient attribute and the entries of an image representation. In particular, we measure the quality of regressing the entries of representations in X f c7 and X S , as a function of the attribute strength. Our evaluation measures are MSE and the R 2 score. Since, according to Section 3, we fit one trajectory per attribute and scene location, the MSE and R 2 are averaged over all models. From <ref type="table" target="#tab_4">Table 2</ref>, we see that a linear model fits best for data in X f c7 with a MSE of 0.03 and a  R 2 score of 0.14. For representations in the SUN attribute space X S , performance is lower in both measures. We hypothesize that this result is due to the additional layer of indirection, i.e., detecting attribute presence/absence from features in X f c7 . Overall, the R 2 scores are relatively low, however, they do indicate some linear relationship. We argue that these results still warrant the use of a linear model, but obviously leave room for potential improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">One-shot recognition of scene locations</head><p>The objective of our one-shot recognition experiments is to distinguish scene locations from only a single image per location available for training. Due to the characteristics of one-shot learning, i.e., a very limited amount of available data and therefore no variability in the transient states, trajectory transfer is a natural approach to this problem.</p><p>We run all recognition experiments on TADB. We use only this database, since it is specifically designed to have a high degree of variability in the transient attributes. This allows us to assess the quality of trajectory transfer on suitable evaluation data. Note that we deliberately do not use SUN397, or Places to demonstrate our approach, since scene variability (cf. Section 1, <ref type="figure" target="#fig_1">Fig. 3</ref>) is quite limited, certainly not with respect to scene configurations, but with respect to the range of transient attribute strengths. Evaluation setup. We follow a standard cross-validation protocol. In detail, we repeatedly (for 10 times) split the number of scene locations into 61 training and 40 testing locations at random. The 61 locations serve as our external training corpus that is used for (1) training the attribute strength regressors for all attributes in A T , (2) estimating feature trajectories and (3) computing the weights for trajectory transfer. For one-shot recognition, we select one image at random from each of the 40 test locations. Random chance for this classification problem is at 2.5%. We stick to the following notation in <ref type="table">Table 3</ref>: whenever the one-shot instances of the 40 scenes are used as training data, we denote this by (A). Experiments with data synthesized using trajectory transfer are indicated by (+S). Recognition rates are reported for the remaining (i.e., after removal of the oneshot instances) images of the test split, averaged over the cross-validation runs. We synthesize data for all attributes in A T , using R different attribute strengths, linearly spaced in [0, 1]. This is done, since the hand-annotated attribute strengths in <ref type="bibr" target="#b10">[11]</ref> are also normalized to that range.</p><p>Comparison(s). To establish baselines for one-shot recognition, we first train a linear SVC on the single instances of the 40 scene locations in the testing portion of each split and refer to this first baseline as SVC (A). Our second baseline is obtained by adding data synthesized from random trajectories, indicated as SVC (A+Random).</p><p>Regarding one-shot approaches from the literature, we compare against the approaches of Fink <ref type="bibr" target="#b6">[7]</ref> and Tang et al. <ref type="bibr" target="#b23">[24]</ref> which are both based on learning a suitable pseudometric from the external training data. In both cases, we learn a linear transformation. In particular, we implement <ref type="bibr" target="#b6">[7]</ref> using LSML <ref type="bibr" target="#b13">[14]</ref> for metric learning (instead of POLA). Tang et al. <ref type="bibr" target="#b23">[24]</ref> adapt neighborhood component analysis (NCA) to the setting of one-shot learning and advocate the use of so called micro-sets during optimization of the transform. We closely follow the original setup of <ref type="bibr" target="#b23">[24]</ref> by choosing 10 instances of every class for testing, one for training and SGD for optimization. Whenever we report results with metric learning, we add (+ML) and the reference to the specific approach, e.g., SVC (A+ML).</p><p>Additionally, we combine feature trajectory transfer with the aforementioned pseudo-metric learning approaches of <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b23">[24]</ref>. As mentioned earlier, those approaches are not necessarily competitors, but rather provide complementary information. In fact, learning a suitable metric can be naturally combined with data synthesized by our approach: the learned transform is not only applied to the one-shot instances, but also on the synthetic instances. We indicate such combinations as (A+S+ML) when reporting results. <ref type="table">Table 3</ref> lists the results for our one-shot recognition experiments. The number of synthesized samples for trajectory transfer is set to R = 5. This leads to a total of 5 · 40 = 200 additional features per one-shot instance. Interestingly, we found that increasing the number of synthesized samples per attribute does not improve the recognition accuracy. In fact, R ≥ 2 already produces stable results. One reason for this behavior could be that our approach independently synthesizes data for each attribute. This is certainly suboptimal and introduces redundancies. For example, sweeping "daylight" from 0 → 1 might have similar effects than sweeping "night" from 1 → 0. A second reason might be that linearly spacing R = 2 values over the range [0, 1], by design, covers the extreme ends of the transient attribute strengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Results</head><p>A first observation is that using all available data (i.e., all images of the 40 locations in each test split) in a fivefold cross-validation setup, denoted by SVC (Ref, <ref type="bibr">CV)</ref> in <ref type="table">Table 3</ref>, leads to almost perfect recognition accuracy. The observed behavior is in line with our initial experiment in <ref type="figure">Fig. 1</ref>. The small difference in the numerical results can be explained by the fact that in <ref type="figure">Fig. 1</ref> data from all 101 scene locations was used, whereas in <ref type="table">Table 3</ref>   <ref type="table">Table 3</ref>: Accuracy (±1σ, averaged over ten runs) of one-shot recognition;</p><p>(1) using one-shot instances only (A), (2) using oneshot instances + synthesized data (A+S) and <ref type="formula" target="#formula_4">(3)</ref>  performed on data from only 40 locations. Second, we see that using data synthesized via trajectory transfer always improves recognition accuracy over not including synthetic data for image representations in X f c7 . This effect is less pronounced for representations in X S .</p><p>Third, and most notably, the results indicate the complementary nature of synthetic data (obtained via trajectory transfer) in combination with pseudo-metric learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>. While, relying on the learned transformations alone, i.e., (A+ML), already leads to fairly good results, including synthetic data is beneficial with accuracy gains ranging from 1 to 3 percentage points. Overall, the top performance is obtained by SVC (A+S+ML <ref type="bibr" target="#b6">[7]</ref>) with 89.2%. This is a considerable improvement over the baseline SVC (A) and reduces the gap between using all available data and only using the one-shot instances to ≈ 10 percentage points. Extracting the minimum (81.5%) and maximum (93.4%) accuracies obtained with SVC (A+S+ML <ref type="bibr" target="#b6">[7]</ref>) over all splits also shows that performance depends on the specific instances that are used as one-shot examples. This is not surprising, since the transient states of these images essentially anchor the process of trajectory transfer.</p><p>Since, we cannot directly visualize the synthesized feature representations, <ref type="figure">Fig. 6</ref> shows a selection of image retrieval results instead. Starting from our one-shot examples, we synthesized representations using trajectory transfer (for "sunny") and used these to search for the closest (i.e., nearest-neighbor) image in the external data. While, <ref type="figure">Fig. 6</ref> only shows a small selection, the retrieved images appear to reflect the desired change in attribute strength. Combined with the results from <ref type="table">Table 3</ref>, this indicates that trajectory transfer produces meaningful representations.</p><p>As an additional comparison, we note that a simplified ‡ automatic variant of Laffont et al.'s <ref type="bibr" target="#b10">[11]</ref> appearance transfer (to create, for each desired attribute strength and R = 5, additional images) leads to an accuracy of 71.7±4.7. Notably, this result is below the accuracy of training with one-shot instances alone (SVC+A). While, we obtain reasonable synthetic images in many cases, transferring appearance without segment matching <ref type="bibr">[11, §6.2]</ref> (not implemented in our setup) tends to introduce spurious image details in the final synthesis results, see <ref type="figure" target="#fig_3">Fig. 5</ref>; this eventually leads to features in X f c7 that apparently confuse the SVM classifier. Nevertheless, exploring the full pipeline of <ref type="bibr" target="#b10">[11]</ref> is a promising avenue for future work, regardless of the relatively high computational demand (compared to trajectory transfer).</p><p>Finally, we remark that improvements in recognition accuracy for image representations in X S are minor and the the gap between SVC (A) and SVC (Ref, CV) of ≈ 36 percentage points is substantial. In fact, we can only demonstrate marginal improvements of about 3 percentage points. This behavior is somewhat expected, since the additional layer of abstraction when predicting attributes adds another source of uncertainty when estimating feature trajectories. <ref type="table" target="#tab_4">Table 2</ref> also support this explanation. By using X f c7 directly, we (1) avoid this indirection and (2) use image representations that already are highly discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Throughout all experiments of this work, we have seen that changes in the appearance of a scene with respect to transient attributes affect the underlying image representation, e.g., in X f c7 . In the ideal case, features would be totally invariant with respect to the particular state of a transient attribute. Consequently, one-shot recognition would "reduce" to the problem of achieving good generalization with respect to the configuration of a particular scene category (i.e., different variants of beaches, alleys, arches, etc.). However, as we have seen in Figs. 1 and 2, changes in transient states manifest as changes in the image representations and negatively impact scene recognition performance.</p><p>While the introduction of large scene recognition datasets, such as Places, has led to better generalization with respect to variability in scene configuration, the data collection process (e.g., via image search engines) does not seem to achieve complete coverage with respect to transient attribute variability (cf. Section 3). However, the recently released dataset of <ref type="bibr" target="#b10">[11]</ref> allows us to study and eventually model the functional dependency between transient attributes and feature representations. This enables the proposed feature trajectory transfer and allows us to increase variability via synthesized data. As demonstrated in our experiments on one-shot recognition of scene locations, the ‡ We closely follow <ref type="bibr" target="#b10">[11]</ref>, but do not implement segment matching as in <ref type="bibr">[11, §6.2]</ref> and no human is involved in the selection of "target" images. Single-shot instance (x * ∈ X fc7 ) <ref type="figure">Figure 6</ref>: Exemplary retrieval results (left/right) for three singleshot images (middle), when the query is executed based on the feature representation, synthesized for a desired attribute strength (here: "sunny" of 0 and 1) via trajectory transfer. resulting increased coverage of the feature space has a positive effect on recognition accuracy.</p><p>The proposed approach also raises several interesting issues. First, we remark that regressing feature dimensions independently only makes sense after decorrelation via PCA. However, this is done one attribute at a time and ignores potential relationships between the attributes (such as "sunny" &amp; "daylight"). An alternative strategy could be, e.g., to regress feature dimensions from combinations of related transient attributes. A second question is, how trajectory transfer performs as an augmentation technique beyond one-shot recognition scenarios. Finally, it will be interesting to (qualitatively) study the results of strategies, e.g., Mahendran and Vedaldi <ref type="bibr" target="#b14">[15]</ref>, that allow for an inversion of feature representations. While this might be challenging for activations from the 'fc7' layer of Places-CNN, inversion results from features (produced via trajectory transfer) at earlier layers could potentially lead to greater insight into what is being captured by the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of limited scene variability with respect to transient attributes in the outdoor scenes of SUN397. Each column represents the distribution of range values r -defined as r = (p95 − p5), where pi denotes the ith percentile of the data -collected from all images and attributes in a scene category (listed on the x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>I 1 , . . . , I N the external training corpus of N images. Each image I i is assigned to one of C scene locations (categories) with label y i ∈ [C] † and represented by a D-dimensional feature vector x i ∈ X ⊂ R D . Additionally, each image is annotated with a vector of transient attribute strengths a i ∈ † [n] is the set {1, . . . , n} with n ∈ N X xu 1 x * , rk (x * ) = α Previously unseen instance trajectory transfer Instances in external training corpus xi Feature vector in X c Scene location c ∈ [C ] α Strength of the k-th attribute rk (·) Illustration of feature trajectory transfer and synthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Some qualitatively good (top) and bad (bottom) results of a simplified variant of Laffont et al.'s appearance transfer [11]. Retrieval result (query: ssunny(0, x * )) Retrieval result (query: ssunny(1, x * ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Illustration of the two locations (left) from TADB with most inconsistent predictions by Places-CNN. The number in the top-right hand corner of each column denotes the number of different predictions. The rightmost column shows the only scene with a consistent prediction over all its images. The mean number of different category predictions over all TADB webcams is 10.3 ± 4.6.inconsistent predictions, as well as the only scene location where the Places-CNN prediction is consistent over all images from this webcam. On average, we get 10.3 ± 4.6 different predictions per webcam. This is in line with our observation from the previous experiment. It further strengthens our conjecture that features learned by Places-CNN are not invariant with respect to transient states of a scene.</figDesc><table>22 
1 

. 
. 
. 

. 
. 
. 

. 
. 
. 

Figure 2: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Transient attributes AT<ref type="bibr" target="#b10">[11]</ref> Performance of regressing transient attribute strength, evaluated on TADB, and attribute detection, evaluated on SADB, using images represented in X f c7 .</figDesc><table>SUN attributes AS [18] 

MSE 
R 2 
MAP 
0.05 ± 0.01 
0.28 ± 0.17 
0.90 ± 0.04 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of a linear model for regressing image representations, evaluated on TADB, in X f c7 and XS as a function of the transient attribute strengths.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>cross-validation was</figDesc><table>Image representation 

X f c7 
XS 

SVC (A) 
78.0 ± 4.5 61.2 ± 4.9 
→ SVC (A+S) 
80.7 ± 4.5 61.7 ± 5.0 
SVC (A+Random) 
77.7 ± 4.5 60.5 ± 5.1 
SVC (A+ML [7]) 
88.2 ± 3.9 61.6 ± 5.2 
→ SVC (A+S+ML [7]) 
89.2 ± 3.9 61.2 ± 5.2 
SVC (A+ML [24]) 
80.6 ± 4.6 61.8 ± 4.5 
→ SVC (A+S+ML [24]) 84.1 ± 4.1 63.5 ± 4.4 

SVC (Ref, CV) 
99.5 ± 0.1 97.4 ± 0.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>using (A, or A+S) in combination with metric learning (+ML). Random chance is at 1/40 = 2.5%. SVC (Ref, CV) reports the accuracy that can be obtained using all available data. Results obtained with synthesized data are marked by '→'; highlighted cells indicate improvements from adding synthesized data. For comparison, SVC (A+Random) lists the results obtained via random trajectories.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work has been supported, in part, by the Austrian Science Fund (FWF KLI project 429) and the NSF grant ECCS-1148870.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-generalization: learning novel classes from a single example by feature replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vasconcelos. Scene classification with semantic fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot learning for object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object classification from a single example utilizing relevance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scene recognition on the semantic manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transient attributes for high-level understanding and editing of outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Laffont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Metric learning from relative comparisons by minimizing squared residual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-SVMs for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from oneexample through shared density transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The SUN attribute database: Beyond categories for deeper scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain-adaptive discriminative one-shot learning of gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene classification with low-dimensional semantic spaces and weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Image classification with the Fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="222" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving image classification using semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="77" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimizing one-shot recognition with micro-set learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to learn with compound HD models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">SUN database: Exploring a large collection of scene categories. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-task transfer methods to improve one-shot learning for multimedia events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attribute-based transfer learning for object categorization with zero/one training example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using Places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
