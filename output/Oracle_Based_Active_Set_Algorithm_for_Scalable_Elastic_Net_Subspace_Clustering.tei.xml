<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Imaging Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SICE</orgName>
								<orgName type="department" key="dep2">Telecommunications ‡ Applied Mathematics and Statistics</orgName>
								<orgName type="institution" key="instit1">Beijing University of Posts</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Imaging Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art subspace clustering methods are based on expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with ℓ 1 , ℓ 2 or nuclear norms. ℓ 1 regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be connected. ℓ 2 and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed ℓ 1 , ℓ 2 and nuclear norm regularizations offer a balance between the subspacepreserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper studies the geometry of the elastic net regularizer (a mixture of the ℓ 1 and ℓ 2 norms) and uses it to derive a provably correct and scalable active set method for finding the optimal coefficients. Our geometric analysis also provides a theoretical justification and a geometric interpretation for the balance between the connectedness (due to ℓ 2 regularization) and subspace-preserving (due to ℓ 1 regularization) properties for elastic net subspace clustering. Our experiments show that the proposed active set method not only achieves state-of-the-art clustering performance, but also efficiently handles large-scale datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many computer vision applications, including image representation and compression <ref type="bibr" target="#b18">[19]</ref>, motion segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b33">34]</ref>, temporal video segmentation <ref type="bibr" target="#b38">[39]</ref>, and face clustering <ref type="bibr" target="#b17">[18]</ref>, high-dimensional datasets can be well approximated by a union of low-dimensional subspaces. In this case, the problem of clustering a high-dimensional dataset into multiple classes or categories reduces to the problem of assigning each data point to its own subspace and recovering the underlying low-dimensional structure of the data, a problem known in the literature as subspace clustering <ref type="bibr" target="#b36">[37]</ref>.</p><p>Prior Work. Over the past decade, the subspace clustering problem has received a lot of attention in the literature and many methods have been developed. Among them, spectral clustering based methods have become extremely popular <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref> (see <ref type="bibr" target="#b36">[37]</ref> for details). These methods usually divide the problem into two steps: a) learning an affinity matrix that characterizes whether two points are likely to lie in the same subspace, and b) applying spectral clustering to this affinity. Arguably, the first step is the most important, as the success of spectral clustering depends on having an appropriate affinity matrix.</p><p>State-of-the-art methods for constructing the affinity matrix are based on the self-expressiveness model <ref type="bibr" target="#b8">[9]</ref>. Under this model, each data point x j is expressed as a linear combination of all other data points, i.e., x j = i =j x i c ij +e j , where the coefficient c ij is used to define an affinity between points i and j, and the vector e j captures deviations from the self-expressive model. The coefficients are typically found by solving an optimization problem of the form min cj ,ej r(c j ) + γ · h(e j ) s.t. x j = Xc j + e j , c jj = 0, <ref type="bibr" target="#b0">(1)</ref> where X = [x 1 , · · · , x N ] is the data matrix, c j = [c 1j , . . . , c N j ] ⊤ is the vector of coefficients, r(·) is a properly chosen regularizer on the coefficients, h(·) is a properly chosen regularizer on the noise or corruption, and γ &gt; 0 is a parameter that balances these two regularizers.</p><p>The main difference among state-of-the-art methods lies in the choice of the regularizer r(·). The sparse subspace clustering (SSC) method <ref type="bibr" target="#b8">[9]</ref> searches for a sparse representation using r(·) = · 1 . While under broad theoretical conditions (see <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47]</ref>) the representation produced by SSC is guaranteed to be subspace preserving (i.e., c ij = 0 only if x i and x j are in the same subspace), the affinity matrix may lack connectedness <ref type="bibr" target="#b28">[29]</ref> (i.e., data points from the same subspace may not form a connected component of the affinity graph due to the sparseness of the connections, which may cause over-segmentation). Other recently proposed sparsity based methods, such as orthogonal matching pursuit (OMP) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">46]</ref> and nearest subspace neighbor (NSN) <ref type="bibr" target="#b31">[32]</ref>, also suffer from the same connectivity issue.</p><p>As an alternative, the least squares regression (LSR) method <ref type="bibr" target="#b27">[28]</ref> uses the regularizer r(·) = 1 2 · 2 2 . One benefit of LSR is that the representation matrix is generally dense, which alleviates the connectivity issue of sparsity based methods. However, the representation is known to be subspace preserving only when the subspaces are independent, 1 which significantly limits its applicability. Nuclear norm regularization based methods, such as low rank representation (LRR) <ref type="bibr" target="#b25">[26]</ref> and low rank subspace clustering (LRSC) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref>, also suffer from the same limitation <ref type="bibr" target="#b43">[44]</ref>.</p><p>To bridge the gap between the subspace preserving and connectedness properties, <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12]</ref> propose to use mixed norms. For example, the low rank sparse subspace clustering (LRSSC) method <ref type="bibr" target="#b43">[44]</ref>, which uses a mixed ℓ 1 and nuclear norm regularizer, is shown to give a subspace preserving representation under conditions which are similar to but stronger than those of SSC. However, the justification for the improvements in connectivity given by LRSSC is merely experimental. Likewise, <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref> propose to use a mixed ℓ 1 and ℓ 2 norm given by</p><formula xml:id="formula_0">r(c) = λ c 1 + 1 − λ 2 c 2 2 ,<label>(2)</label></formula><p>where λ ∈ [0, 1] controls the trade-off between the two regularizers. However, <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref> do not provide a theoretical justification for the benefits of the method. Other subspace clustering regularizers studied in <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b20">[21]</ref> use the trace lasso <ref type="bibr" target="#b14">[15]</ref> and the k-support norm <ref type="bibr" target="#b0">[1]</ref>, respectively. However, no theoretical justification is provided in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21]</ref> for the benefit of their methods. Another issue with the aforementioned methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref> is that they do not provide efficient algorithms to deal with large-scale datasets. To address this issue, <ref type="bibr" target="#b4">[5]</ref> proposes to find the representation of X by a few anchor points that are sampled from X and then perform spectral clustering on the anchor graph. In <ref type="bibr" target="#b32">[33]</ref> the authors propose to cluster a small subset of the original data and then classify the rest of the data based on the learned groups. However, both of these strategies are suboptimal in that they sacrifice clustering accuracy for computational efficiency. Paper Contributions. In this paper, we exploit a mixture of ℓ 1 and ℓ 2 norms to balance the subspace preserving and connectedness properties. Specifically, we use r(·) as in <ref type="bibr" target="#b1">(2)</ref> and h(e) = 1 2 e 2 2 . The method is thus a combination of SSC and LSR and reduces to each of them when λ = 1 and λ = 0, respectively. In the statistics literature, the optimization program using this regularization is called Elastic Net and is used for variable selection in regression problems <ref type="bibr" target="#b48">[49]</ref>. Thus we refer to this method as the Elastic Net Subspace Clustering (EnSC).</p><p>This work makes the following contributions:</p><p>1. We propose an efficient and provably correct activeset based algorithm for solving the elastic net prob-lem. The proposed algorithm exploits the fact that the nonzero entries of the elastic net solution fall into an oracle region, which we use to define and efficiently update an active set. The proposed update rule leads to an iterative algorithm which is shown to converge to the optimal solution in a finite number of iterations.</p><p>2. We provide theoretical conditions under which the affinity generated by EnSC is subspace preserving, as well as a clear geometric interpretation for the balance between the subspace-preserving and connectedness properties. Our conditions depend on a local characterization of the distribution of the data, which improves over prior global characterizations.</p><p>3. We present experiments on computer vision datasets that demonstrate the superiority of our method in terms of both clustering accuracy and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Elastic Net: Geometry and a New Algorithm</head><p>In this section, we study the elastic net optimization problem, and present a new active-set based optimization algorithm for solving it. Consider the objective function</p><formula xml:id="formula_1">f (c; b, A) := λ c 1 + 1 − λ 2 c 2 2 + γ 2 b − Ac 2 2 ,<label>(3)</label></formula><p>where b ∈ IR D , A = [a 1 , · · · , a N ] ∈ IR D×N , γ &gt; 0, and λ ∈ [0, 1) (the reader is referred to <ref type="bibr" target="#b44">[45]</ref> for a study of the case λ = 1). Without loss of generality, we assume that b and {a j } N j=1 are normalized to be of unit ℓ 2 norm in our analysis. The elastic net model then computes</p><formula xml:id="formula_2">c * (b, A) := arg min c f (c; b, A).<label>(4)</label></formula><p>We note that c * (b, A) is unique since f (c; b, A) is a strongly convex function; we use the notation c * in place of c * (b, A) when the meaning is clear. In the next two sections, we present a geometric analysis of the elastic net solution, and use this analysis to design an active-set algorithm for efficiently solving (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Geometric structure of the elastic net solution</head><p>We first introduce the concept of an oracle point. </p><formula xml:id="formula_3">δ(b, A) := γ · b − Ac * (b, A) .<label>(5)</label></formula><p>When there is no risk of confusion, we omit the dependency of the oracle point on b and A and write δ(b, A) as δ.</p><p>Notice that the oracle point is unique since c * is unique, and that the oracle point cannot be computed until the optimal solution c * has been computed. The next result gives a critical relationship involving the oracle point that is exploited by our active-set method (see <ref type="bibr" target="#b44">[45]</ref> for the proof).  <ref type="figure" target="#fig_0">Figure 1</ref>. Illustration of the structure of the solution c * for a data matrix A containing 100 randomly generated points in IR 2 , which are shown as blue dots in the x-y plane. The z direction shows the magnitude for each coefficient c * j . The red dot represents the oracle point δ(b, A), with its direction denoted by the red dashed line. The value for γ is fixed at 50, but the value for λ varies as depicted.</p><p>Theorem 2.1. The solution c * to problem (4) satisfies</p><formula xml:id="formula_4">(1 − λ)c * = T λ (A ⊤ δ),<label>(6)</label></formula><p>where T λ (·) is the soft-thresholding operator (applied com-</p><formula xml:id="formula_5">ponentwise to A ⊤ δ) defined as T λ (v) = sgn(v)(|v| − λ) if |v| &gt; λ and 0 otherwise.</formula><p>Theorem 2.1 shows that if the oracle point δ is known, the solution c * can be written out directly. Moreover, it follows from <ref type="formula" target="#formula_3">(5)</ref> and <ref type="formula" target="#formula_4">(6)</ref> that δ = 0 if and only if b = 0.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we depict a two dimensional example of the solution to the elastic net problem (4) for different values of the tradeoff parameter λ. As expected, the solution c * becomes denser as λ decreases. Moreover, as predicted by Theorem 2.1, the magnitude of the coefficient c * j is a decaying function of the angle between the corresponding dictionary atom a j and the oracle point δ (shown in red). If a j is far enough from δ such that | a j , δ | ≤ λ holds true, then the corresponding coefficient c * j is zero. We call the region containing the nonzero coefficients the oracle region. We can formally define the oracle region by using the quantity µ(·, ·) to denote the coherence of two vectors, i.e.,</p><formula xml:id="formula_6">µ(v, w) := | v, w | v 2 w 2 .<label>(7)</label></formula><p>Definition 2.2 (Oracle Region). The oracle region associated with the optimization problem (4) is defined as</p><formula xml:id="formula_7">∆(b, A) := v ∈ IR D : v 2 = 1, µ(v, δ) &gt; λ δ 2 .<label>(8)</label></formula><p>The oracle region is composed of an antipodal pair of spherical caps of the unit ball of IR D that are located at the symmetric locations ±δ/ δ 2 , both with an angular radius of θ = arccos(λ/ δ 2 ) (see <ref type="figure">Figure 2</ref>). From the definition of the oracle region and Theorem 2.1, it follows that c * j = 0 if and only if a j ∈ ∆(b, A). In other words, the support of the solution c * are those vectors a j in the oracle region.</p><p>The oracle region also captures the behavior of the solution when columns from the matrix A are removed or new columns are added. This provides the key insight into designing an active-set method for solving the optimization.</p><formula xml:id="formula_8">δ δ 2 λ δ 2 Figure 2. The oracle region ∆(b, A) is illustrated in red.</formula><p>Note that the size of the oracle region increases as the quantity λ/ δ 2 decreases, and vice versa. </p><formula xml:id="formula_9">Proposition 2.1. For any b ∈ IR D , A ∈ IR D×N and A ′ ∈ IR D×N ′ , if no column of A ′ is contained in ∆(b, A), then c * (b, [A, A ′ ]) = [c * (b, A) ⊤ , 0 ⊤ N ′ ×1 ] ⊤ .</formula><formula xml:id="formula_10">Proposition 2.2. For any b ∈ IR D , A ∈ IR D×N and A ′ ∈ IR D×N ′ , denote c * (b, [A, A ′ ]) = [c ⊤ A , c ⊤ A ′ ] ⊤ . If any column of A ′ lies within ∆(b, A) , then c ⊤ A ′ = 0.</formula><p>This result means that the solution to the elastic net problem will certainly be changed by adding new columns that lie within the oracle region to the dictionary.</p><p>In the next section, we describe an efficient algorithm for solving the elastic net problem (4) that is based on the geometric structure and concentration behavior of the solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">A new active-set algorithm</head><p>Although the elastic net optimization problem <ref type="bibr" target="#b48">[49]</ref> has been recently introduced for subspace clustering in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31]</ref>, such prior work does not provide an efficient algorithm that can handle large-scale datasets. In fact, such prior work solves the elastic net problem using existing algorithms that require calculations involving the full data matrix A (e.g., the accelerated proximal gradient (APG) <ref type="bibr" target="#b1">[2]</ref> is used in <ref type="bibr" target="#b10">[11]</ref> and the linearized alternating direction method (LADM) <ref type="bibr" target="#b23">[24]</ref> is used in <ref type="bibr" target="#b30">[31]</ref>). Here, we propose to solve the elastic net problem <ref type="formula" target="#formula_2">(4)</ref> with an active-set algorithm that is more efficient than both APG and LADM, and can handle largescale datasets. We call our new method (see Algorithm 1) ORacle Guided Elastic Net solver, or ORGEN for short. The basic idea behind ORGEN is to solve a sequence of reduced-scale subproblems defined by an active set that is itself determined from the oracle region. Let T k be the active set at iteration k. Then, the next active set T k+1 is selected to contain the indices of columns that are in the oracle region ∆(b, A T k ), where A T k denotes the submatrix of A with columns indexed by T k . We use <ref type="figure" target="#fig_3">Figure 3</ref> for a conceptual illustration. In <ref type="figure" target="#fig_3">Figure 3</ref>(a) we show the columns of A that correspond to the active set T k by labeling the corresponding columns of A T k in red. The oracle region ∆(b, A T k ) is the union of the red arcs in <ref type="figure" target="#fig_3">Figure 3</ref>(b). Notice that at the bottom left there is one red dot that is not in ∆(b, A T k ) and thus must not be included in T k+1 , and two blue dots that are not in T k but lie in the oracle region ∆(b, A T k ) and thus must be included in T k+1 . In <ref type="figure" target="#fig_3">Figure  3</ref>(c) we illustrate T k+1 by green dots. This iterative procedure is terminated once T k+1 does not contain any new points, i.e., when T k+1 ⊆ T k , at which time T k+1 is the support for c * (b, A).</p><formula xml:id="formula_11">(a) Active set T k (b) ∆(b, A T k ) (c) Active set T k+1</formula><p>The next lemma helps explain why ORGEN converges.</p><formula xml:id="formula_12">Lemma 2.1. In Algorithm 1, if T k+1 T k , then f (c * (b, A T k+1 ); b, A T k+1 ) &lt; f (c * (b, A T k ); b, A T k ).</formula><p>The following convergence result holds for ORGEN.</p><p>Theorem 2.2. Algorithm 1 converges to the optimal solution c * (b, A) in a finite number of iterations.</p><p>The result follows from Lemma 2.1, because it implies that an active set can never be repeated. Since there are only finitely many distinct active sets, the algorithm must eventually terminate with T k+1 ⊆ T k . The remaining part of Compute c * (b, A T k ) as in (4) using any solver. <ref type="bibr" target="#b3">4</ref>: <ref type="formula" target="#formula_3">(5)</ref>.</p><formula xml:id="formula_13">Compute δ(b, A T k ) from c * (b, A T k ) as in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Active set update: T k+1 ← {j : a j ∈ ∆(b, A T k )}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>If T k+1 ⊆ T k , terminate; otherwise set k ← k + 1. 7: end loop Output: A vector c such that c T k = c * (b, A T k ) and zeros otherwise. Its support is T k+1 .</p><formula xml:id="formula_14">the proof establishes that if T k+1 ⊆ T k , then c * (b, A T k+1 )</formula><p>gives the nonzero entries of the solution.</p><p>ORGEN solves large-scale problems by solving a sequence of reduced-size problems in step 3 of Algorithm 1. If the active set T k is small, then step 3 is a small-scale problem that can be efficiently solved. However, there is no procedure in Algorithm 1 that explicitly controls the size of T k . To address this concern, we propose an alternative to step 5 in which only a small number of new points-the ones most correlated with δ-are added. Specifically,</p><formula xml:id="formula_15">5' : T k+1 = {j ∈ T k : a j ∈ ∆(b, A T k )} ∪ S k ,<label>(9)</label></formula><p>where S k holds the indices of the largest n entries in</p><formula xml:id="formula_16">{|a ⊤ j δ(b, A T k )| : j / ∈ T k , a j ∈ ∆(b, A T k )};</formula><p>ideally, n should be chosen so that the size of T k+1 is bounded by a predetermined value N max that represents the maximum size subproblem that can be handled in step 3. If N max is chosen large enough that the second set in the union in <ref type="formula" target="#formula_15">(9)</ref> is non-empty, then our convergence result still holds. Initialization. We suggest the following procedure for computing the initial active set T 0 . First, compute the solution to (4) with λ = 0, which has a closed form solution and can be computed efficiently if the ambient dimension D of the data is not too big. Then, the l largest entries (in absolute value) of the solution for some pre-specified value l are added to T 0 . Our experiments suggest that this strategy promotes fast convergence of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Elastic Net Subspace Clustering (EnSC)</head><p>Although the elastic net has been recently introduced for subspace clustering in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref>, these works do not provide conditions under which the affinity is guaranteed to be subspace preserving or potential improvements in connectivity. In this section, we give conditions for the affinity to be subspace preserving and for the balance between the subspacepreserving and connectedness properties. To the best of our knowledge, this is the first time that such theoretical guarantees have been established.</p><p>We first formally define the subspace clustering problem.</p><p>Problem 3.1 (Subspace Clustering). Let X ∈ IR D×N be a real-valued matrix whose columns are drawn from a union of n subspaces of IR D , say n ℓ=1 S ℓ , where the dimension d ℓ of the ℓ-th subspace satisfies d ℓ &lt; D for ℓ = 1, . . . , n. The goal of subspace clustering is to segment the columns of X into their representative subspaces.</p><formula xml:id="formula_17">Let X = [x 1 , · · · , x N ],</formula><p>where each x j is assumed to be of unit norm. Using the same notation as for <ref type="formula" target="#formula_2">(4)</ref>, the proposed EnSC computes c * (x j ,</p><formula xml:id="formula_18">X −j ) for each {x j } N j=1 , i.e., c * (x j , X −j ) = arg min c f (c; x j , X −j ),<label>(10)</label></formula><p>where X −j is X with the j-th column removed. In this section, we focus on a given vector, say x j . We suppose that x j ∈ S ℓ for some ℓ, and use X ℓ −j to denote the submatrix of X with columns from S ℓ except that x j is removed. Since our goal is to use the entries of c * (x j , X −j ) to construct an affinity graph in which only points in the same subspace are connected, we desire the nonzero entries of c * (x j , X −j ) to be a subset of the columns X ℓ −j so that no connections are built between points from different subspaces. If this is the case, we say that such a solution c * (x j , X −j ) is subspace preserving. On the other hand, we also want the nonzero entries of c * (x j , X −j ) to be as dense as possible in X ℓ −j so that within each cluster the affinity graph is wellconnected 2 . To some extent, these are conflicting goals: if the connections are few, it is more likely that the solution is subspace preserving, but the affinity graph of each cluster is not well connected. Conversely, as one builds more connections, it is more likely that some of them will be false, but the connectivity is improved.</p><p>In the next two sections, we give a geometric interpretation of the tradeoff between the subspace preserving and connectedness properties, and provide sufficient conditions for a representation to be subspace preserving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Subspace-preserving vs. connected solutions</head><p>Our analysis is built upon the optimization problem min c f (c; x j , X ℓ −j ). Note that its solution is trivially subspace preserving since the dictionary X ℓ −j is contained in S ℓ . We then treat all points from other subspaces as newly added columns to X ℓ −j and apply Propositions 2.1 and 2.2. We get the following geometric result. </p><formula xml:id="formula_19">if x k / ∈ ∆(x j , X ℓ −j ) for all x k / ∈ S ℓ .</formula><p>We illustrate the geometry implied by Lemma 3.1 in <ref type="figure">Figure 4</ref>, where we assume S ℓ is a two dimensional subspace <ref type="bibr" target="#b1">2</ref> In fact, even when each cluster is well-connected, further improving connectivity within clusters is still beneficial since it enhances the ability of the subsequent step of spectral clustering in correcting any erroneous connections in the affinity graph <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b39">40]</ref>. S ℓ <ref type="figure">Figure 4</ref>. The structure of the solution for an example in IR 3 associated with a point xj (not shown) that lies in the 2-dimensional subspace S ℓ . The blue dots illustrate the columns of X ℓ −j , the union of the two red regions is the oracle region ∆(xj, X ℓ −j ), and the green points are vectors from other subspaces.</p><p>in IR 3 . The dictionary X ℓ −j is represented by the blue dots in the plane and the oracle region ∆(x j , X ℓ −j ) is denoted as the two red circles. The green dots are all other points in the dictionary. Lemma 3.1 says that c * (x j , X −j ) is subspace preserving if and only if all green dots lie outside of the red region.</p><p>To ensure that a solution is subspace preserving one desires a small oracle region, while to ensure connectedness one desires a large oracle region. These facts again highlight the trade-off between these two properties. Recall that the elastic net balances ℓ 1 regularization (promotes sparse solutions) and ℓ 2 regularization (promotes dense solutions). Thus, one should expect that the oracle region will decrease in size as λ is increased from 0 towards 1. Theorem 3.1 formalizes this claim, but first we need the following definition that characterizes the distribution of the data in X ℓ −j .</p><p>Definition 3.1 (inradius). The inradius of a convex body P is the radius r(P) of the largest ℓ 2 ball inscribed in P.</p><p>To understand the next result, we comment that the size of the oracle region ∆(x j , X ℓ −j ) is controlled by the quantity λ/ δ(x j , X ℓ −j ) 2 as depicted in <ref type="figure">Figure 2</ref>.</p><formula xml:id="formula_20">Theorem 3.1. If x j ∈ S ℓ , then λ δ(x j , X ℓ −j ) 2 ≥ r 2 j r j + 1−λ λ ,<label>(11)</label></formula><p>where r j is the inradius of the convex hull of the symmetrized points in X ℓ −j , i.e., r j := r(conv{±x k : x k ∈ S ℓ and k = j}).</p><p>We define the right-hand-side of (11) to be zero when λ = 0.</p><p>The above theorem allows us to determine an upper bound for the size of the oracle region. This follows since a lower bound on the size of λ/ δ(x j , X ℓ −j ) 2 implies an upper bound on the size of the oracle region (see <ref type="bibr" target="#b7">(8)</ref> and <ref type="figure">Figure 2</ref>). Also notice that the right hand side of (11) is in the range [0, r j ) and is monotonically increasing with λ.</p><p>Thus, it provides an upper bound on the area of the oracle region, which decreases as λ increases. This highlights that the trade-off between the subspace-preserving and connectedness properties is controlled by λ. </p><p>and parameter choice γ = 10, that λ/ δ (with λ = 0.88) is larger than λ/ δ (with λ = 0.95).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Conditions for a subspace-preserving solution</head><p>A sufficient condition for a solution to be subspace preserving is obtained by combining the geometry in Lemma 3.1 with the bound on the size of the oracle region implied by Theorem 3.1.</p><formula xml:id="formula_23">Theorem 3.2. Let x j ∈ S ℓ , δ j = δ(x j , X ℓ −j )</formula><p>be the oracle point, and r j be the inradius characterization of X ℓ −j as given by <ref type="bibr" target="#b11">(12)</ref>. Then, c * (x j , X −j ) is subspace preserving if</p><formula xml:id="formula_24">max k:x k / ∈S ℓ µ(x k , δ j ) ≤ r 2 j r j + 1−λ λ .<label>(14)</label></formula><p>Notice that in Theorem 3.2 the quantity δ j is determined from X ℓ −j and that it lies within the subspace S ℓ by definition of δ(x j , X ℓ −j ). Thus the left-hand-side of (14) characterizes the separation between the oracle point-which is in S ℓ -and the set of points outside of S ℓ . On the right-handside, r j characterizes the distribution of points in X ℓ −j . In particular, r j is large when points are well spread within S ℓ and not skewed toward any direction. Finally, note that the right-hand-side of <ref type="bibr" target="#b13">(14)</ref> is an increasing function of λ, showing that the solution is more likely to be subspace preserving if more weight is placed on the ℓ 1 regularizer relative to the ℓ 2 regularizer. Theorem 3.2 has a close relationship to the sufficient condition for SSC to give a subspace preserving solution (the case λ = 1) <ref type="bibr" target="#b34">[35]</ref>. Specifically, <ref type="bibr" target="#b34">[35]</ref> shows that if max k:x k / ∈S ℓ µ(x k , δ j ) &lt; r j , then SSC gives a subspace preserving solution. We can observe that condition <ref type="bibr" target="#b13">(14)</ref> approaches the condition for SSC as λ → 1.</p><p>The result stated in Theorem 3.2 is a special case of the following more general result. Theorem 3.3. Let x j ∈ S ℓ , δ j = δ(x j , X ℓ −j ) be the oracle point, and κ j = max k =j,x k ∈S ℓ µ(x k , δ j ) be the coherence of δ j with its nearest neighbor in X ℓ −j . Then, the solution c * (x j , X −j ) is subspace preserving if</p><formula xml:id="formula_25">max k:x k / ∈S ℓ µ(x k , δ j ) ≤ κ 2 j κ j + 1−λ λ .<label>(15)</label></formula><p>The only difference between this result and that in Theorem 3.2 is that κ j is used instead of r j for characterizing the distribution of points in X ℓ −j . We show in <ref type="bibr" target="#b44">[45]</ref> that r j ≤ κ j , which makes Theorem 3.3 more general than Theorem 3.2. Geometrically, r j is large if the subspace S ℓ is well-covered by X ℓ j , while κ j is large if the neighborhood of the oracle closest to δ j is well-covered, i.e., there is a point in X ℓ −j that is close to δ j . Thus, while the condition in Theorem 3.2 requires each subspace to have global coverage by the data, the condition in Theorem 3.3 allows the data to be biased, and only requires a local region to be well-covered. In addition, condition <ref type="bibr" target="#b14">(15)</ref> can be checked when the membership of the data points is known. This advantage allows us to check the tightness of the condition <ref type="bibr" target="#b14">(15)</ref>, which is studied in more details in <ref type="bibr" target="#b44">[45]</ref>. In contrast, condition <ref type="bibr" target="#b13">(14)</ref> and previous work on SSC <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref> use the inradius r j , which is generally NP-hard to calculate <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b43">44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ORGEN on synthetic data</head><p>We conducted synthetic experiments to illustrate the computational efficiency of the proposed algorithm OR-GEN. Three popular solvers are exploited: the regularized feature sign search (RFSS) is an active set type method <ref type="bibr" target="#b19">[20]</ref>; the LASSO version of the LARS algorithm <ref type="bibr" target="#b7">[8]</ref> that is implemented in the sparse modeling software (SPAMS); and the gradient projection for sparse reconstruction (GPSR) algorithm proposed in <ref type="bibr" target="#b13">[14]</ref>. These three solvers are used to solve the subproblem in step 3 of ORGEN, resulting in three implementations of ORGEN. We also used the three solvers as stand-alone solvers for comparison purposes.</p><p>In all experiments, the vector b and columns of A are all generated independently and uniformly at random on the unit sphere of IR 100 . The results are averages over 50 trials.</p><p>In the first experiment, we test the scaling behavior of ORGEN by varying N ; the results are shown in <ref type="figure" target="#fig_7">Figure 5(a)</ref>. We can see that our active-set scheme improves the computational efficiency of all three solvers. Moreover, as N grows, the improvement becomes more significant.</p><p>Next, we test the performance of ORGEN for various values of the parameter λ that controls the tradeoff between the subspace preserving and connectedness properties; the running times and sparsity level are shown in <ref type="figure" target="#fig_7">Figures 5(b)</ref> and 5(c), respectively. The performance of SPAMS is not  reported since it performs poorly even for moderately small values of λ. For all methods, the computational efficiency decreases as λ becomes smaller. For the two versions of ORGEN, this is expected since the solution becomes denser as λ becomes smaller (see <ref type="figure" target="#fig_7">Figure 5</ref>(c)). Thus the active sets become larger, which leads directly to larger and more time consuming subproblems in step 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">EnSC on real data</head><p>In this section, we use ORGEN to solve the optimization problems arising in EnSC, where each subproblem in step 3 is solved using the RFSS method. To compute the coefficient vectors c * (x j , X −j ), the parameter λ is set to be the same for all j, while the parameter γ is set as γ = αγ 0 where α &gt; 1 is a hyperparameter and γ 0 is the smallest value of γ such that c * (x j , X −j ) is nonzero. The algorithm is run for at most 2 iterations, as we observe that this is sufficient for the purpose of subspace clustering and that subsequent iterations do not boost performance. We measure clustering performance by clustering accuracy, which is calculated as the best matching rate between the label predicted by the algorithm and that of the ground truth. Datasets. We test our method on the four datasets presented in <ref type="table" target="#tab_0">Table 1</ref>. The Coil-100 dataset <ref type="bibr" target="#b29">[30]</ref> contains 7,200 grayscale images of 100 different objects. Each object has 72 images taken at pose intervals of 5 degrees, with the images being of size 32 × 32. The PIE dataset <ref type="bibr" target="#b15">[16]</ref> contains images of the faces of 68 people taken under 13 different poses, 43 different illuminations, and 4 different expressions. In the experiments, we use the five near frontal poses and all images under different illuminations and expressions. Each image is manually cropped and normalized to 32 × 32 pixels. The MNIST dataset <ref type="bibr" target="#b21">[22]</ref> contains 70,000 images of handwritten digits 0-9. For each image, we extract a feature vector of dimension 3,472 via the scattering convolution network <ref type="bibr" target="#b2">[3]</ref>, and then project to dimension 500 using PCA. Finally, the Covtype database 3 has been collected to predict forest cover type from 54 cartographic variables.</p><p>Methods. We compare our method with several state-ofthe-art subspace clustering methods that may be categorized into three groups. The first group contains TSC <ref type="bibr" target="#b16">[17]</ref>, OMP <ref type="bibr" target="#b6">[7]</ref>, NSN <ref type="bibr" target="#b31">[32]</ref>, and SSC <ref type="bibr" target="#b8">[9]</ref>. TSC is a variant of the k-nearest neighbors method, OMP and NSN are two sparse greedy methods, and SSC is a convex optimization method. These algorithms build sparse affinity matrices and are computationally efficient, and therefore can perform large-scale clustering. For TSC and NSN we use the code provided by the respective authors. We note that the code may not be optimized for computational efficiency considerations. For OMP, we use our implementation, which has been optimized for subspace clustering. For SSC we use the SPAMS solver described in the previous section.</p><p>The second group consists of LRSC and SSC (with a different solver). We use the code provided by their respective authors, which uses the Alternating Direction Method of Multipliers (ADMM) to solve the optimization problems. To distinguish the two versions of SSC, we refer to this one as SSC-ADMM and to the previous one as SSC-SPAMS.</p><p>The final group consists of ENSC <ref type="bibr" target="#b30">[31]</ref> and KMP <ref type="bibr" target="#b20">[21]</ref>, and are the closest in spirit to our method. Our method and ENSC both balance the ℓ 1 and ℓ 2 regularizations, but ENSC uses h(e) = e 1 to penalize the noise (see <ref type="bibr" target="#b0">(1)</ref>) and the linearized alternating direction method to minimize their objective. In KMP, the k-support norm is used to blend the ℓ 1 and ℓ 2 regularizers. We implemented ENSC and KMP according to the descriptions in their original papers.</p><p>Results. To the best of our knowledge, a comparison of all these methods on large scale datasets has not been reported in prior work. Thus, we run all experiments and tune the parameters for each method to give the best clustering accuracy. The results are reported in <ref type="table">Table 2</ref>. <ref type="table">Table 2</ref>. Performance of different clustering algorithms. The running time includes the time for computing the affinity matrix and for performing spectral clustering. The sparsity is the number of nonzero coefficients in each representation cj averaged over j = 1, · · · , N . The value "M" means that the memory limit of 16GB was exceeded, and the value "T" means that the time limit of seven days was reached. We see that our proposed method achieves the best clustering performance on every dataset. Our method is also among the most efficient in terms of computing time. The methods SSC-ADMM, ENSC, LRSC and KMP cannot handle large-scale data because they perform calculations over the full data matrix and put the entire kernel matrix X ⊤ X in memory, which is infeasible for large datasets. The method of SSC-SPAMS uses an active set method that can deal with massive data, however, it is computationally much less efficient than our solver ORGEN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TSC OMP NSN SSC-SPAMS SSC-ADMM LRSC ENSC KMP EnSC-ORGEN</head><p>For understanding the advantages of our method, in Table 2 we report the sparsity of the representation coefficients, which is the number of nonzero entries in c j averaged over all j = 1, . . . , N . For TSC, OMP and NSN, the sparsity is directly provided as a parameter of the algorithms. For SSC and our method EnSC-ORGEN, the sparsity is indirectly controlled by the parameters of the models. We can see that our method usually gives more nonzero entries than the sparsity based methods of TSC, OMP, NSN, and SSC. This shows the benefit of our method: while the number of correct connections built by OMP, NSN and SSC are in general upper-bounded by the dimension of the subspace, our method does not have this limit and is capable of constructing more correct connections and producing well-connected affinity graphs. On the other hand, the affinity graph of LRSC is dense, so although each cluster is self-connected, there are abundant wrong connections. This highlights the advantage of our method, which is flexible in controlling the number of nonzero entries by adjusting the trade-off parameter λ. Our results illustrate that this tradeoff improves clustering accuracy.</p><p>Finally, ENSC and KMP are two representatives of other methods that also exploit the trade-off between ℓ 1 and ℓ 2 regularizations. A drawback of both works is that the solvers for their optimization problems are not as effective as our ORGEN algorithm, as they cannot deal with large datasets due to memory requirements. Moreover, we observe that their algorithms converge to modest accuracy in a few iterations but can be very slow in giving a high precision solution. This may explain why their clustering accuracy is not as good as that of EnSC-ORGEN. Especially, we see that ENSC gives dense solutions although the true solution is expected to be sparser, and this is explained by the fact that the solution paths of their solver are dense solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We investigated elastic net regularization (i.e., a mixture of the ℓ 1 and ℓ 2 norms) for scalable and provable subspace clustering. Specifically, we presented an active set algorithm that efficiently solves the elastic net regularization subproblem by capitalizing on the geometric structure of the elastic net solution. We then gave theoretical justifications-based on a geometric interpretation for the trade-off between the subspace preserving and connectedness properties-for the correctness of subspace clustering via the elastic net. Extensive experiments verified that that our proposed active set method achieves state-of-the art clustering accuracy and can handle large-scale datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2. 1 (</head><label>1</label><figDesc>Oracle Point). The oracle point associated with the optimization problem (4) is defined to be</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The interpretation for Proposition 2.1 is that the solution c * (b, A) does not change (modulo padding with additional zeros) when new columns are added to the dictionary A, as long as the new columns are not inside the oracle region ∆(b, A). From another perspective, c * (b, [A, A ′ ]) does not change if one removes columns from the dictionary [A, A ′ ] that are not in the oracle region ∆(b, [A, A ′ ]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Conceptual illustration of the ORGEN algorithm. All the dots on the unit circle illustrate the dictionary A. (a) active set T k at step k, illustrated by red dots. (b) The oracle region ∆(b, AT k ) illustrated by red arcs. (c) The new active set T k+1 illustrated in green, which is the set of indices of points that are in ∆(b, AT k ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>ORacle Guided Elastic Net (ORGEN) solver Input: A = [a 1 , . . . , a N ] ∈ IR D×N , b ∈ IR D , λ and γ.1: Initialize the support set T 0 and set k ← 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 3 . 1 .</head><label>31</label><figDesc>Suppose that x j ∈ S ℓ . Then, the vector c * (x j , X −j ) is subspace preserving if and only</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Remark 3. 1 .</head><label>1</label><figDesc>It would be nice if λ/ δ(x j , X ℓ −j ) 2 was increasing as a function of λ (we already know that its lower bound given in Theorem 3.1 is increasing in λ). However, one can show using the data x j = [0.22, 0.72, 0.66] ⊤ ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Performance with varying N and λ: (a) λ = 0.9 and N ∈ [5000, 10 6 ]; and (b, c) N = 100, 000 and λ ∈ [0.05, 0.999].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Dataset information.</figDesc><table>N (#data) D (ambient dim.) n (#groups) 
Coil-100 
7,200 
1024 
100 
PIE 
11,554 
1024 
68 
MNIST 
70,000 
500 
10 
CovType 581,012 
54 
7 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Subspaces {Sκ} are independent if dim( κ Sκ) = κ dim(Sκ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://archive.ics.uci.edu/ml/datasets/Covertype</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse prediction with the k-support norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1466" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkagethresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral curvature clustering (SCC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering with landmark-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multibody factorization method for independently moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="179" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Greedy feature selection for subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2765" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph-oriented learning via automatic group sparsity for data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph-based learning via auto-grouped sparse regularization and kernelized extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A closed form solution to robust subspace estimation and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="586" to="597" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trace lasso: a trace norm regularization for correlated designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-PIE. Image Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Robust subspace clustering via thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bölcskei</surname></persName>
		</author>
		<idno>abs/1307.4891</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering appearances of objects under varying illumination conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiscale hybrid linear models for lossy image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3655" to="3671" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Elastic-net regulariztion: error estimates and active set methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schiffler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient ksupport matrix pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured sparse subspace clustering: A unified optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linearized alternating direction method with adaptive penalty for low rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Correlation adaptive subspace segmentation by trace lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph connectivity in sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nasihatkon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Columbia object image library (COIL-100)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<idno>CUCS-006-96</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Elastic net subspace clustering applied to pop/rock music structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Greedy subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="430" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A geometric analysis of subspace clustering with outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Filtrated spectral algebraic subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tsakiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Robust Subspace Learning and Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Low rank subspace clustering (LRSC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47" to="61" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalized Principal Component Analysis (GPCA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Generalized Principal Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiframe motion segmentation with missing data using PowerFactorization, and GPCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="105" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Noisy sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Provable subspace clustering: When LRR meets SSC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Oracle based active set algorithm for scalable elastic net subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalable sparse subspace clustering by orthogonal matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Geometric conditions for subspacesparse recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1585" to="1593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hybrid linear modeling via local best-fit flats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="240" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
