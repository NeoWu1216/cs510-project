<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multicamera calibration from visible and mirrored epipoles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Bushnevskiy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technicolor Research &amp; Innovation Karl-Wiechert Allee 74</orgName>
								<address>
									<postCode>30625</postCode>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Sorgi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technicolor Research &amp; Innovation Karl-Wiechert Allee 74</orgName>
								<address>
									<postCode>30625</postCode>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Leibniz University</orgName>
								<address>
									<addrLine>Hannover Appelstr 9A</addrLine>
									<postCode>30169</postCode>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multicamera calibration from visible and mirrored epipoles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multicamera rigs are used in a large number of 3D Vision applications, such as 3D modeling, motion capture or telepresence and a robust calibration is of utmost importance in order to achieve a high accuracy results. In many practical configurations the cameras in a rig are arranged in such a way, that they can observe each other, in other words a number of epipoles correspond to the real image points. In this paper we propose a solution for the automatic recovery of the external calibration of a multicamera system by enforcing only simple geometrical constraints, arising from the epipole visibility, without using any calibration object, such as checkerboards, laser pointers or similar. Additionally, we introduce an extension of the method that handles the case of epipoles being visible in the reflection of a planar mirror, which makes the algorithm suitable for the calibration of any multicamera system, irrespective of the number of cameras and their actual mutual visibility, and furthermore we remark that it requires only one or a few images per camera and therefore features a high speed and usability. We produce an evidence of the algorithm effectiveness by presenting a wide set of tests performed on synthetic as well as real datasets and we compare the results with those obtained using a traditional LED-based algorithm. The real datasets have been captured using a multicamera Virtual Reality (VR) rig and a spherical dome configuration for 3D reconstruction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Camera calibration is the specification of a mathematical model describing the image formation process realized by an imaging system. For a single camera it comprises two steps, the photometric and the internal calibration that specifically addresses the estimation of the camera response function <ref type="bibr" target="#b5">[6]</ref> and the system lens-sensor ( and optionallymirror ) geometry <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>. When multiple cameras are used simultaneously, an additional calibration step is required. It copes with the estimation of the camera cluster geome-try, namely the set of camera poses in a common reference frame (rf.) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4]</ref>; which is generally referred to as external calibration. In many calibration algorithms and tools this task is blended in a joint process together with the internal calibration of the cameras, using a calibration object <ref type="bibr" target="#b1">[2]</ref>. This solution, however, requires a simultaneous visibility of the pattern in many or even in all calibration images, which in practice is not always feasible and does not scale well with the increasing number of cameras and complexity of the cluster geometry. Eventually, in the recent years the advantage of decoupling these two operations into two different calibration phases has become clear <ref type="bibr" target="#b9">[10]</ref>. A different approach actually neglects the external calibration and postpones it to the further operation of the system, that is the video capture in an uncontrolled environment. In <ref type="bibr" target="#b4">[5]</ref> the multicamera geometry is extracted only enforcing the rigidity constraint, which is the constraint of the cameras having a fixed orientations and translations between each other. The elegant formulation based on the definition of a virtual camera makes the system suitable also for image sequences with non-overlapping content, however it requires the multicamera system to undergo a consistent motion and relies on the performance of the feature tracker, which is itself dependent on the image content. Therefore this is not the optimal solution for the calibration of an indoor immovable systems. A more flexible alternative is provided by a single-point calibration techniques, namely the algorithms based on the projection of a single moving point in the parallel video sequences. The latter is usually provided by a laser pointer, which can be easily detected in each image <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref>. These techniques however, despite being very powerful, are still designed as a joint framework for external and internal calibration which limits the flexibility in case of variation of the cluster geometry, removal of cameras or addition of the new ones.</p><p>In this paper we present a novel approach for the multicamera systems calibration, based only on the geometrical constraints arising from the visible epipoles and featuring important advantages. The size of the calibration dataset is highly reduced, therefore is less error-prone and easy to col-lect, automatize and review for possible outliers in the set of detected epipoles. Therefore, the system becomes flexible enough to enable a frequent re-arrangement of the camera geometry with a small amount extra work required for recalibration, making it a good candidate for an integration into a really practical, handy and user-friendly calibration tool. The system also has some limitations, which, however, can be easily circumvented. The accuracy of the recovered geometry is high enough to provide a reliable initialization of any 3D reconstruction system, but needs to be further refined using a bundle adjustment (BA) together with the reconstructed model, which is anyway a common approach in any 3D reconstruction architecture. A second issue is a possible lack of a visible epipoles, which may happen for some camera configurations as well as for cameras with a narrow field-of views lenses. For this reason, inspired by the work proposed in <ref type="bibr" target="#b12">[13]</ref>, we have designed an extension, based on a planar mirror, which enables the joint exploitation of mirrored and directly visible epipoles for the multicamera camera pose recovery and eventually makes the system flexible enough for any camera cluster configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Notation</head><p>In order to ease the paper understanding, we first introduce the mathematical notation. We will consider a multicamera system comprised of N cameras, which projection centers are denoted by {C i } i=1,...,N . The absolute reference frame will be identified with the index w and the Euclidean transformation converting the vector representation of a 3D point from w to the rf. of the i-th camera will be denoted as T wi = [R wi |t wi ] : R wi ∈ SO(3), t wi ∈ R 3 , where SO(3) is the group of 3D rotation matrices, <ref type="figure" target="#fig_0">(Fig. 1)</ref> and I 3×3 ∈ SO(3) is the null rotation, namely the identity matrix. The action of the transformation T wi on a vector X will be synthetically expressed as T wi • X = R wi X + t wi . Inner product and cross product between 3D vectors will be denoted as v ⊤</p><formula xml:id="formula_0">a v b and [v a ] × v b , where [v]</formula><p>× is the skew symmetric matrix built using the vector v. We assume working in the calibrated camera conditions, which implies that the image points can be represented as a unit-norm vectors, that is points on unit sphere S2 or equivalently 3D directions in the 3D space. Occasionally, the projection of a 3D vector on S2 will be explicitly expressed using the notation</p><formula xml:id="formula_1">N {v} = v √ v T v .</formula><p>The epipoles of the multicamera systems will identified by the letter e, and specifically e ij denotes the projection of the point C i on the j-th camera; epipoles will be represented as unit-norm vectors as well.</p><p>Our aim will be to estimate the set of camera poses, namely the set of transformations {T wi } i=1,...,N and the estimation will be formulated as the minimization of an objective function built using the geometrical constraints between the visible epipoles. The minimization will be performed by means of the Levenberg-Marquardt iterative algorithm (LM). By the formalization of the LM iterations, we will use the a + in order to identify the update of the variable a. We will also use the notationsã andâ in order to distinguish the actual measurement from the estimate of a. The working dataset will consist of the set of visible epipoles, denoted as V = {(i, j) : ∃ẽ ij }. The epipoles measurements in the set V will be also grouped pairwise in a second set M, the set of pairs of mutually visible epipoles,</p><formula xml:id="formula_2">M = {(i, j) : ∃ẽ ij ∃ẽ ji }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Calibration of the multicamera geometry</head><p>Without loss of generality we assume that the rf. w is aligned with the first camera, that is T w1 = [I 3×3 | 0]. The calibration process consequently reduces to the estimation of the transformation set {T wi } i=2,...,N . In the next sections we show how this task can decoupled in two consecutive steps aimed at estimation of rotational and translational components of the camera poses, namely {R wi } i=2,...,N and {t wi } i=2,...,N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Recovery of the rotational components</head><p>Let us consider two cameras i and j that can observe each other, as in the model in <ref type="figure" target="#fig_0">Fig.1</ref>. By applying the coordinate transformation between the i-th camera rf. and w, the epipole e ji can be then expressed as</p><formula xml:id="formula_3">e ji ∼ T wi • C j = R ⊤ wi (C j − C i ) .<label>(1)</label></formula><p>A similar relation can be established also for the corresponding epipole e ij , which together with (1) allows for the removal of the dependency from the (unknown) camera centers:</p><formula xml:id="formula_4">e ji = −R wi R ⊤ wj e ij .<label>(2)</label></formula><p>Equation <ref type="formula" target="#formula_4">(2)</ref> could have been derived also by considering the epipoles to be the unit norm vectors, representation of two points on S2.</p><p>Each pair of epipoles in the set M can be used to build an equation in a form <ref type="bibr" target="#b1">(2)</ref>, which stacked together lead to the formulation of the estimation of the multicamera geometry rotational components as an optimisation problem</p><formula xml:id="formula_5">{R wi } i=2,...,N = arg min R wi ∈SO(3)    (i,j)∈M ρ i,j 2    ,<label>(3)</label></formula><p>where each error contribution is defined as:</p><formula xml:id="formula_6">ρ i,j =ẽ ij + R wj R ⊤ wiẽji<label>(4)</label></formula><p>A closed form solution for single rotation estimation given a set of corresponding vectors is a well known result of matrix analysis <ref type="bibr" target="#b10">[11]</ref>; however we are not aware of similar solutions for the simultaneous recovery of multiple rotations as in <ref type="formula" target="#formula_5">(3)</ref>. In many approaches rotations and translations are estimated in a non-iterative manner, however those approaches rely on the external feature points, which are not available in our calibration case scenario <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12]</ref>. The same observation applies to the other state of the art works addressing the translation estimation under the assumption of known rotation <ref type="bibr" target="#b8">[9]</ref>. Therefore, we attempt to solve the problem within an iterative minimization context using the LM algorithm.</p><p>For this purpose we parameterize a differential rotation with a 3D vector dω, which is used to compute the corresponding unit quaternion and then the rotation update in the matrix representation:</p><formula xml:id="formula_7">             dq = (1 − dω ⊤ dω) | dω ⊤ dR = dR (dq) = I 3×3 + 2 [dω] × R + = R · dR<label>(5)</label></formula><p>Using equations <ref type="bibr" target="#b4">(5)</ref> in the definition (4) in place of both the camera rotations R wj and R wi , and discarding the second order terms in dp, one obtains:</p><formula xml:id="formula_8">ρ + i,j =ẽ ij + R + wj R +⊤ wiẽ ji = ρ + i,j + J ρ dp<label>(6)</label></formula><p>where dp is the update vector collecting all the rotation update parameters stacked in a single vector, dp</p><formula xml:id="formula_9">= dω ⊤ 2 , ... , dω ⊤ k , ... , dω ⊤ N ⊤</formula><p>. After a few mathematical manipulations the Jacobian J ρ can be arranged in a compact matrix form as</p><formula xml:id="formula_10">   J ρ = [0..., J b , ...0..., −J b , ...0] J b = 2R wj R ⊤ wiẽ ji ×<label>(7)</label></formula><p>The update equations <ref type="formula" target="#formula_10">(7)</ref> and <ref type="formula" target="#formula_8">(6)</ref> are then used within the LM optimization framework to build the normal equation that is solved in each LM iteration, with the trivial initialization R wj = I 3×3 for each camera. As each camera pose has three degrees of freedom and each vector equation <ref type="bibr" target="#b1">(2)</ref> provides only two independent linear equations, one can infer that the number of visible epipoles must satisfy the condition</p><formula xml:id="formula_11">2|M| ≥ 3(N − 1),<label>(8)</label></formula><p>where |M| is the cardinality of the set M. As N cameras define a maximum number of N 2 N 2 pairs of mutually visible epipoles, one can also conclude that a necessary, but not sufficient condition to meet the constraint (8) is N ≥ 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recovery of the translational components</head><p>Once the set of camera orientations {R wi } i=2,...,N is available, one can use it as an additional input to tackle the estimation of the camera translational components. Let us rewrite equation <ref type="formula" target="#formula_3">(1)</ref>, or rather its complementary equation for the epipole e ij , explicitly considering the normalization factor</p><formula xml:id="formula_12">e ij = R wj · N {C i − C j } .<label>(9)</label></formula><p>Now the camera orientation R wj can be assumed to be known, therefore (9) becomes a non-linear equation in the unknowns C i and C j . As each visible epipole provides an equation in a form of (9), one can formulate the estimation of the camera translation components as the solution of the optimization problem</p><formula xml:id="formula_13">Ĉ i i=2,...,N = arg min C i ∈R 3 V τ i,j 2 ,<label>(10)</label></formula><p>where each error contribution is defined as:</p><formula xml:id="formula_14">τ i,j =ẽ ij −R wj · N {C i − C j } .<label>(11)</label></formula><p>Similarly to the rotational component estimation, we solve the problem (10) using the LM algorithm. For a comfortable formulation of the LM iterations, still keeping a minimal dimensionality of the optimization space, we parameterize the projection center of each camera using the corresponding epipolar ray on the reference camera,</p><formula xml:id="formula_15">C i = λ i e i1 ,<label>(12)</label></formula><p>and we express the parameters update as</p><formula xml:id="formula_16">   λ + i = λ i (1 + δλ i ) e + i1 = e i1 + B i δe i ,<label>(13)</label></formula><p>where B i is the 3 × 2 matrix representing any basis for the plane tangent to the unit sphere at the epipole e i1 , δe i is a 2D vector and δλ i is a scalar. Notice that the definition of the epipoles as unit norm vectors justifies the update equation <ref type="bibr" target="#b12">(13)</ref>, which describes a local update of each epipole e i1 on S2 <ref type="figure" target="#fig_1">(Fig.2)</ref>. We omit the description of a basis retrieval for a given 3D plane for brevity. Using equations <ref type="formula" target="#formula_3">(13)</ref> and <ref type="formula" target="#formula_3">(12)</ref>, we can rewrite the update of the error term (11) as a function of the parameter vector</p><formula xml:id="formula_17">dp = δe ⊤ 2 , δλ 2 , ..., δe ⊤ i , δλ i , ..., δe ⊤ N , δλ N ⊤</formula><p>, collecting the parameters of each camera center stacked in a single vector. After a few mathematical manipulations one can arrange a linear equation in the form</p><formula xml:id="formula_18">τ + i,j = τ i,j + J τ dp ,<label>(14)</label></formula><p>where similarly to <ref type="formula" target="#formula_10">(7)</ref> the Jacobian matrix J τ can be expressed in a relatively compact matrix form by applying the basic derivation rules for the square root and quotient of functions. We omit this derivation in order to keep the notation reasonably light. Similarly to <ref type="formula" target="#formula_4">(2)</ref>, also equation <ref type="formula" target="#formula_12">(9)</ref> provides only two independent linear equations in the unknowns C i and C j ; consequently one can show that the number of visible epipoles must satisfy the condition</p><formula xml:id="formula_19">2|V| ≥ 3(N − 1),<label>(15)</label></formula><p>where |V| is the cardinality of the set V. The latter is anyway always greater than |M|, therefore the set of visible epipoles always provides enough constraints for the translation components estimation when the condition <ref type="formula" target="#formula_11">(8)</ref> is met. In contrast to the rotational components, for the recovery of the camera center locations a smarter initialization of the LM iteration can also be achieved, as shown in <ref type="figure" target="#fig_2">Fig.3</ref>. The reference camera is naturally located in the origin, while all the other cameras are located on one epipolar ray corresponding to one visible epipole. For example given that e ij ∈ V, then C i is located on the ray identified byẽ ij at unit distance from the C j , C i = 1 ·ẽ ij , the camera center is then converted in the rf. of the reference camera and the corresponding pose is computed according to <ref type="bibr" target="#b11">(12)</ref>. As multiple epipolesẽ ij for a single view i may be available, one is randomly selected for the pose initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Epipole Mirroring</head><p>In many practical scenarios the geometrical configuration of the multicamera system and the type of lens mounted  on each camera may result in a calibration image set with a number of visible epipoles, which is not large enough to meet the conditions <ref type="formula" target="#formula_11">(8)</ref> and <ref type="bibr" target="#b14">(15)</ref>. An example is given by 360 • camera rigs for VR and panorama capture, such as Google JUMP. In order to cope with such configurations we provide a simple solution based on using a planar mirror. It is placed multiple times in front of the camera system, allowing the cameras to observe each other in the reflection and consequently extending the size of the set of visible epipoles. In the next sections the additional epipoles will be denoted as mirrored epipoles <ref type="figure" target="#fig_3">(Fig. 4)</ref> and we will show how these can be used on their own or in combination with the directly visible epipoles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Recovery of the rotational components</head><p>In order to simplify the derivation of the geometrical constraints pertaining to the mirrored epipoles, it is convenient to introduce an additional a reference system m with the XY plane aligned with the mirror surface, related to the rf. w by the Euclidean transformation T wm = [R wm |t mw ]. By applying the basic laws of the reflection geometry one can show that in the rf. m a vector X and its mirrored version X ′ are related by the equation X = F Z X ′ , where F z = diag (0, 0, −1) is the reflection transformation in-duced by the plane XY . Therefore, the mirrored epipole e ′ ij in the j-th view can be accounted as the image of a virtual cameras located in C ′ i :</p><formula xml:id="formula_20">e ′ ij ∼ T wj • C ′ i = R ⊤ wj (C ′ i − C j ) ,<label>(16)</label></formula><p>where</p><formula xml:id="formula_21">C ′ i = T −1 wm • (F z · (T wm • C i )) .<label>(17)</label></formula><p>Following the same steps as in section 2.1, one can easily derive a relation similar to (2) for the mirrored epipoles,</p><formula xml:id="formula_22">e ′ ij = −R mj F z R T mi e ′ ji = −R wj R T wm F z R wm R T wi e ′ ji .<label>(18)</label></formula><p>Equation <ref type="formula" target="#formula_3">(18)</ref> can be expanded by decomposing the rotation matrix R wm by means of the Euler ZYX parameterization,</p><formula xml:id="formula_23">R wm = Z wm Y wm X wm , e ′ ij = −R wj X T wm Y T wm Z T wm F z Z wm Y wm X wm R ⊤ wi e ′ ji ,<label>(19)</label></formula><p>and then simplified in</p><formula xml:id="formula_24">e ′ ij = −R wj X T wm Y T wm F z Y wm X wm R ⊤ wi e ′ ji ,<label>(20)</label></formula><p>using the equivalence Z T wm F z Z wm = F z . Each pair of mutually visible mirrored epipoles provides an additional equation in the form of (20), which can be incorporated into the optimization problem (3). One can therefore reformulate (3) by extending the optimization space and including the additional error terms, as:</p><formula xml:id="formula_25">{R wi ,X wm ,Ŷ wm } i=1,...,N , m=1,...,M = arg min Rwi∈SO(3) Xwm∈SO X (3) Ywm∈SO Y (3) E ρ ,<label>(21)</label></formula><p>where SO X (3) and SO Y (3) are the subspace of 3D rotations about the X-axis and the Y-axis and the objective function E ρ is defined as:</p><formula xml:id="formula_26">   E ρ = (i,j)∈M ρ i,j 2 + (i,j)∈M ′ ρ ′ i,j 2 ρ ′ i,j =ẽ ′ ij + R wj X ⊤ wm Y ⊤ wm F z Y wm X wm R ⊤ wiẽ ′ ji .<label>(22)</label></formula><p>In (22) the error term ρ i,j is defined according to equation (4), M ′ is the set of pairs of mirrored mutually visible epipoles and M is the number of mirror snapshots, that is the number of different mirror poses.</p><p>The Jacobian of the objective function can be analytically computed in a (relatively compact) matrix using the same procedure, previously described for the derivation of equation <ref type="formula" target="#formula_10">(7)</ref>, which is omitted for brevity. The solution is then obtained by means of the Levenberg-Marquardt algorithm, where the additional parameters of the mirror poses are initialized to zero, that is X wm and Y wm are initialized as identity matrices.</p><p>We remark that the formulation (21) provides the lowest dimensionality of the optimization space as each mirror pose increases the number of parameters by two, namely the additional parameters of the rotations X wm and Y wm .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recovery of the translational components</head><p>Taking advantage of the knowledge of the rotational components of the camera and the mirror poses (21), one can significantly reduce the complexity of the translational component estimation. We first expand the mirror pose components as</p><formula xml:id="formula_27">   R T wm = [r1 wm , r2 wm , r3 wm ] t wm = [tx wm , ty wm , tz wm ] .</formula><p>Then by plugging the previous equations in <ref type="formula" target="#formula_3">(17)</ref> and after a few manipulations, one obtains</p><formula xml:id="formula_28">   C ′ i = F m C i + tz wm r3 wm F m = X ⊤ wm Y ⊤ wm F z Y wm X wm .<label>(23)</label></formula><p>By using equation <ref type="formula" target="#formula_4">(23)</ref> in <ref type="formula" target="#formula_3">(16)</ref> one can represent the mirrored epipoles as a function of only the real poses of the cameras and the mirror:</p><formula xml:id="formula_29">e ′ ij = R wj · N {F m C i + tz wm r3 wm − C j } .<label>(24)</label></formula><p>Each visible mirrored epipole provides an equation in the form of (24), which can be incorporated into the optimization problem <ref type="bibr" target="#b9">(10)</ref>. This is reformulated using the estimates of the camera and mirror rotations in place of R wm , X wm and Y wm , extending the optimization space and including the additional error terms:</p><formula xml:id="formula_30">Ĉ i ,tz wm i=1,...,N , m=1,...,M = arg min Ci∈R 3 tzwm∈R E τ ,<label>(25)</label></formula><p>where the objective function E τ is defined as:</p><formula xml:id="formula_31">   E τ = V τ i,j 2 + V ′ τ ′ i,j 2 τ ′ i,j =ẽ ′ ij −R wj · N F m C i + tz wmr 3 wm − C j .</formula><p>(26) In (26), τ i,j is defined according to equation <ref type="bibr" target="#b10">(11)</ref> and V ′ is the set of mirrored visible epipoles. Once again, we remark that the formulation (25) provides the lowest dimensionality of the optimization problem as each mirror pose increases the number of parameters only by one, namely the additional parameter tz wm .</p><p>The objective function (25) is minimized by means of the LM algorithm, using the same parameterization and the same initialization for the camera centers as in section 2.2. For the mirror one can also derive a simple initialization following the same model shown in <ref type="figure" target="#fig_2">Fig.3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The proposed algorithm has been evaluated on the synthetic and real datasets. In both cases we have tested the algorithm first using only directly visible epipoles and then also introducing the mirrored epipoles in the calibration dataset. The synthetic tests have been organized with the objective of assessing the algorithm stability against increasing noise corrupting the visible epipoles. This is an important point, as in a real scenario the visible epipoles can be localized only with some uncertainty within the image regions corresponding to the visible camera lenses. The algorithm has been further tested using the real multicamera systems, a dome and VR-rig configuration, both comprised of wide angle cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic dataset</head><p>For the synthetic test a scene has been generated with a cluster of cameras, each one modeled as a super wide angle lens (150 • horizontal FoV) and 1024 × 768 pixel sensor.</p><p>In the first test we have simulated a system consisting of seven virtual cameras randomly located on a unit sphere and pointing towards its origin. The visible epipoles location has been corrupted by a zero-mean Gaussian noise, with variance ranging in the interval [0; 2] pixels. For each level of noise a new scene has been generated 100 times and the mean and standard deviation of the geometry estimation error has been computed ( <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>). The estimation error of the rotational and translation component of the camera poses has been defined as</p><formula xml:id="formula_32">   ǫ Ri = Φ R wi ·R ⊤ wi ǫ ti = t wi −t wi ,<label>(27)</label></formula><p>where Φ(R) is the conversion of a rotation matrix in the axis-angle representation.</p><p>In the next test, addressing the case of mirrored epipoles, the virtual cameras have been randomly located on a unit sphere pointing outwards in divergent configuration, so that they cannot see each other directly, but only in the reflection of a virtual mirror, which has been also randomly posed in 3D space. The estimation error has been calculated similar to the previous test and the results are presented in <ref type="figure" target="#fig_4">Fig.5 (b)</ref>.</p><p>Both tests on the synthetic data confirm a high robustness against the image projection noise, despite the small size of the calibration dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Real dataset</head><p>The method has been further evaluated on two real multicamera systems, a real dome and a VR-rig, both comprised of GoPro HERO Black 3+ cameras. The corresponding system geometry has been estimated using the proposed algorithm ( with only visible epipoles or only mirrored epipoles according to the geometry ), followed by a BA refinement. For a comparison purpose we have calibrated the system also using the Multi-Camera Self-Calibration tool (MCSC) <ref type="bibr" target="#b15">[16]</ref>. In order to ensure a fair comparison the intrinsic parameters of the cameras have been estimated beforehand and used for both calibration pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Dome configuration</head><p>The dome configuration comprises 8 GoPro Hero 3+ cameras arranged on a rigid metal frame, in two circular groups located one on top of the other, each one with 4 cameras <ref type="figure">(Fig.6 (a)</ref>). The system geometry has been estimated first using MCSC with an input of 32000 images (4000 images/points per camera), extracted from a video recording of a green LED moving in the inner volume of the dome. The system geometry has been then recovered using the proposed epipole-based method, after manual selection of the visible epipoles from only 8 input images (1 image per camera). The recovered geometry has been further refined using BA on the set of LED points, <ref type="figure">(Fig.6 (b)</ref>). LED points have been only used in order to allow a direct comparison with MCSC, however in the real scenarios the camera positions can be bundle adjusted using the feature points coming from the actual target scene. To evaluate the estimated geometry the LED point structures have been back-projected onto the camera views using the system geometry, estimated using the two tested approaches. In each view the error has been measured as a Euclidean distance in pixels between the detected LED points and their backprojections and the overall statistical distribution of the backprojection error has been shown in <ref type="figure">Fig.7</ref> and <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We observe that this comparison has the shortcoming of computing the evaluation measure from the same data used for calibration. Therefore, we suggest to perform a second test on an independent set of points, provided by a traditional black and white checkerboard pattern, which has been presented to the pairs of camera sharing the same field of view. The grid points have been detected in each view, triangulated in 3D space using the estimated camera geom-LED err., px Pattern err., px MCSC µ = 2.2, σ = 1.9 µ = 3.6, σ = 2.1 Epipole µ = 1.7, σ = 1.3 µ = 1.1, σ = 0.7  <ref type="figure" target="#fig_7">Fig.9</ref> and the error measures collected in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>From the inspection of <ref type="table" target="#tab_0">Table 1</ref> we can conclude that our technique is able to successfully recover the system geometry and slightly outcome the performance of MCSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">VR rig configuration</head><p>The second test has been performed using a VR rig composed of 15 GoPro Hero Black 3+ cameras, 12 pairwise located on a hexagon and 3 positioned on top of the rig at the approximate angle of 45 • <ref type="figure" target="#fig_6">(Fig.8 (a)</ref>).</p><p>We have attempted to calibrate the system using MCSC with an input of 180000 images (12000 images per camera), however the system failed due to the peculiarity of this particular configuration, that implies a reduced overlap between the cameras field of views. Our approach instead has successfully recovered the camera system geometry as well as the 3D pose of the mirrors using a small set of views, 12 snapshots, each with a different mirror position. The overall recovered geometry is shown in <ref type="figure" target="#fig_6">Fig.8 (b)</ref> and <ref type="figure" target="#fig_6">Fig.8 (c)</ref>. We have further refined the system geometry using BA on a set of 5000 feature points extracted from the actual scene. Similar to the dome configuration, the accuracy of the estimated geometry has been evaluated by computing the backprojection error from the reconstructed 3D point cloud and from the pairwise transfer of the BW checkerboard points. The   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed an algorithm aimed at the estimation of the geometry of a multicamera system, based on the enforcement of simple constraints arising only from the visible epipoles. This approach is particularly suitable for dome-like indoor multicamera systems, typically used for motion capture and 3D reconstruction. In such configurations indeed the number of visible epipoles is usually large enough to provide sufficient constraints, and an acceptable solution can be reached very fast, whereas other techniques based on calibration object are not as flexible and easy to use. A great advantage of our solution is the capability of a fast recovery of the external calibration of a multicamera system in case of rearrangement, removal or inclusion of cameras, with almost no overhead. Other algorithms instead require each time completion of a complex and time consuming calibration routine. We have also presented an extension of the calibration algorithm aimed at the integration of the epipoles visible in a planar mirror reflection into the calibration dataset. This solution leads to a sensible enlargement of the epipole set and in practice makes the system suitable for any camera geometry, dome-like, frontoparallel and divergent circular VR-rig configuration, such as Google JUMP rig. We have shown how the mirrored epipoles can be simply integrated within a unified estimation framework by including additional terms into a single calibration objective function. The evaluation on the synthetic and real datasets has shown a high robustness of the proposed approach against image noise, which implies a capability of reaching in a quasi automatic way an accuracy of the camera system geometry high enough for initialization of a successive BA 3D reconstruction pipeline. This is particularly useful, as a typical bundle adjustment pipeline requires a rather precise initial guess in order to converge. The practical advantages such as simplicity and time efficiency of the calibration procedure, reduced dataset collection and high robustness enable the integration of a flexible calibration architecture, capable of handling the rearrangement of the camera geometry without requiring an extra effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Multicamera model with visible epipoles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Epipole update parameterization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>LM Initialization for the translation component recovery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Mirrored Epipoles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Epipole-based (a) and Mirrored epipole-based approaches, synthetic test. Rotation and translation errors vs. the epipole projection noise variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Dome multicamera setup (a). Geometry of the dome multicamera system reconstructed using the proposed Epipolebased approach (b). Backprojection error distribution for MCSC (a) and the Epipole-based (b) approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>VR rig multicamera system (a). VR rig geometry recovered using the proposed approach, based on mirrored epipoles (b). Top view of the same geometry together with the recovered mirror positions (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Checkerboard with the detected (circle) and backprojected (dot) grid corners, using the system geometry, estimated with epipoles.Feature err., px Pattern err., px Mirrored epipoles µ = 1.4, σ = 1.5 µ = 2.4, σ = 0.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Backprojection error for MCSC and the Epipolebased approach. µ -mean, σ -standard deviation . etry and projected back to the source views. Similarly to the LED point test, we have computed the mean value and the standard deviation of the backprojection error between the detected grid points and their backprojected counterpart. A visual representation of this test is shown in</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Backprojection error for the proposed Mirrored Epipole-based approach.numerical results are shown inTable 2.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work has been partially supported by the ERC-Starting Grant (Dynamic MinVIP) and the Cluster of Excellence rebirth. The authors gratefully acknowledge the support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Complete calibration of a multicamera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Calibration of a multicamera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW 2003. Conference on</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient and robust largescale rotation averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Govindu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Geometry of Multiple Images: The Laws That Govern The Formation of Images of A Scene and Some of Their Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Papadopoulou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pose estimation for multi-camera systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3175</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="286" to="293" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling the Space of Camera Response Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1272" to="1282" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rotation averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="305" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">521540518</biblScope>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple-view geometry under the linfinity-norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1603" to="1617" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Easy-to-use calibration of multiple-camera setups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahlesz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lilge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analysis of 3-d rotation fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kanatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="543" to="549" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Plane-based projective reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Dano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="420" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Simple calibration of non-overlapping cameras with a mirror</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust rotation and translation estimation in multiview reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martinec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linear multi view reconstruction and camera recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convenient multicamera self-calibration for virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martinec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoper. Virtual Environ</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging Topics in Computer Vision</title>
		<editor>G. Medioni and S.B. Kang</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
