<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HD Maps: Fine-grained Road Segmentation by Parsing Ground and Aerial Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gellért</forename><surname>Máttyus</surname></persName>
							<email>gellert.mattyus@dlr.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute German Aerospace Center</orgName>
								<orgName type="institution" key="instit2">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
							<email>slwang@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute German Aerospace Center</orgName>
								<orgName type="institution" key="instit2">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<email>fidler@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute German Aerospace Center</orgName>
								<orgName type="institution" key="instit2">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<email>urtasun@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute German Aerospace Center</orgName>
								<orgName type="institution" key="instit2">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HD Maps: Fine-grained Road Segmentation by Parsing Ground and Aerial Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present an approach to enhance existing maps with fine grained segmentation categories such as parking spots and sidewalk, as well as the number and location of road lanes. Towards this goal, we propose an efficient approach that is able to estimate these fine grained categories by doing joint inference over both, monocular aerial imagery, as well as ground images taken from a stereo camera pair mounted on top of a car. Important to this is reasoning about the alignment between the two types of imagery, as even when the measurements are taken with sophisticated GPS+IMU systems, this alignment is not sufficiently accurate. We demonstrate the effectiveness of our approach on a new dataset which enhances KITTI [8]  with aerial images taken with a camera mounted on an airplane and flying around the city of Karlsruhe, Germany.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We are in an exciting time for computer vision, and more broadly AI, as the development of fully autonomous systems such as self-driving cars seems possible in the near future. These systems have to robustly estimate the scene in 3D, its semantics as well as be able to self-localize at all times. Key to the success of these tasks is the use of maps containing detailed information such as road location, number of lanes, speed limit, traffic signs, parking spots, traffic rules at intersections, etc.</p><p>Current maps, however, have been created with the use of semi-automatic systems that employ many man-hours of laborious and tedious labeling. An alternative to this costly labeling is to employ existing maps and correct/enhance them based on ground imagery or LIDAR point clouds, captured, for example, by a Velodyne/cameras mounted on top of a car. Systems like TESLA auto-pilot [1] are currently using their deployed fleet of cars, which are equipped with cameras, to perform such corrections. However, it is difficult to create full coverage of the world as we will need access to imagery/LIDAR from millions of cars in order to reliably enhance maps at a world-scale.</p><p>Alternatively, aerial images provide us with full coverage of a significant portion of the world, but at a much lower resolution than ground images. This makes semantic segmentation from aerial images a very difficult task. In this paper, we propose to use both aerial and ground images to jointly infer fine grained segmentation of roads. Towards this goal, we take advantage of the OpenStreetMap (OSM) project, which provides us with freely available maps of the road topology in the form of piece-wise linear road segments. We formulate the problem as energy minimization, inferring the number and location of the lanes for each road segment, parking spots, sidewalks and background, as well as the alignment between the ground and aerial images. We employ deep learning to estimate semantics from both aerial and ground images, and define a set of potentials exploiting these semantic cues, as well as road constraints, relationships between parallel roads, and the smoothness of both the estimations along the road as well as the alignment between consecutive ground frames.</p><p>We demonstrate the effectiveness of our approach in a new dataset which covers a wide area of the city of Karlsruhe in Germany, both from the ground and from the air. We provide pixel-level annotations for the aerial images in terms of fine-grained road categories. We call our dataset Air-Ground-KITTI. We show that our approach is able to estimate these categories reliably, while significantly reducing the alignment error between the ground and aerial images when compared to a sophisticated GPS+IMU system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>For several decades, researchers from various communities (e.g., vision, remote sensing) have been working on automatic extraction of semantic information from aerial images. In the following, we summarize the approaches most relevant to our work. <ref type="figure">Figure 1</ref>. Illustration of our model: (a) Parameterization of our approach. Our random variables are the absolute location of the different region boundaries (e.g., sidewalk) as well as the alignment between air and ground. (b) Our formulation allows a random variable to take the same state as the previous node, collapsing a region to have 0 width. (c). For each ground-view image, a random variable models the alignment noise. (d). Projection of our parameterization on the ground-view.</p><p>Aerial image parsing: Early approaches employed probabilistic models that aimed to produce topologically connected roads. <ref type="bibr" target="#b0">[2]</ref> defined a probabilistic model that tiled the image into patches, performed road inference inside each patch via dynamic programming, and then "stitched" together high-confidence patches to ensure road connectivity. Recent work exploits learned classifiers to perform semantic segmentation. <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16]</ref> trained a neural net to classify pixels in local patches as road. They employ a postprocessing step to ensure a consistent road topology across the patches, which is, however, prone to block-effects. <ref type="bibr" target="#b24">[26]</ref> segments the road by defining an MRF on superpixels. High-order cliques are sampled over straight segments or junctions to encourage a road-like network structure. Due to complexity of high order terms a sampling scheme is used to concentrate on more important cliques. <ref type="bibr" target="#b2">[4]</ref> samples graph junction-points using image consistency and shape priors. A full review of this large field is out of scope of this paper, and we refer the reader to <ref type="bibr" target="#b12">[14]</ref> for a detailed review.</p><p>Aerial parsing with maps: While proven useful in many computer vision and robotics applications <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b23">25]</ref>, few works employ map information for parsing aerial images. <ref type="bibr" target="#b18">[20]</ref> uses a screenshot of the vector map as a weak source of ground-truth for training a road classifier. <ref type="bibr" target="#b25">[27]</ref> exploit road center-lines from OSM maps as a ground-truth road location and performs road segmentation by estimating the width of the road. This is done by finding boundaries of superpixels along the direction of the road, and ignoring dependencies across different line (road) segments. However, the alignment between OSM and aerial images is far from perfect. To solve this problem, <ref type="bibr" target="#b10">[12]</ref> proposed a MRF which reasons about re-positioning the road centerline and estimating the width of the road. Smoothness is incorporated between consecutive line segments by encouraging their widths to be similar. In our work we go beyond this approach by introducing a formulation that reasons about more fine-grained road semantics such as lanes, sidewalks and parking spots, and exploits simultaneously aerial images as well as ground imagery to infer this information.</p><p>Fine-grained road parsing: Very few works exist that extract detailed segmentation. <ref type="bibr" target="#b15">[17]</ref> propose a hierarchical probabilistic grammar to parse smaller-scale aerial regions into roads, buildings, vehicles and parking lots. Classifiers are first employed to generate object/building/vegetation proposals while the grammar imposes semantic and geometric constraints in order to derive the final parse. Learning and inference are both hard in grammars, and computationally expensive sampling techniques typically need to be employed. In our work, we are aiming at a detailed parsing of the roads into sub-categories. Unlike <ref type="bibr" target="#b15">[17]</ref>, we exploit OSM information in order to derive an efficient formulation.</p><p>The work most related to ours is <ref type="bibr" target="#b19">[21]</ref> which exploits the map as a screenshot of the road vector map to perform road and lane estimation. The authors take a pipeline approach, where, in the first step, road lane hypotheses are generated based on the output of the road classifier and detected lane markings. In the second step, the authors provide heuristics to "track" the lane hypotheses and connect them into a single lane labeling.</p><p>Aerial-to-ground reasoning: Recent work aims to exploit both aerial and the ground-view, mainly for the problem of geo-localization. In <ref type="bibr" target="#b9">[11]</ref>, a deep neural network is used to match ground images with aerial images in oblique views. The matches come from facade to facade matching and therefore can not be extended to orthographic aerial images. In <ref type="bibr" target="#b20">[22]</ref>, 3D reconstructions from the ground images are matched to oblique views of aerial images. <ref type="bibr" target="#b8">[10]</ref> learn cross-view matching between ground images, aerial orthographic photos and land cover attributes. This extends the image geolocalization to areas not covered by ground images. Forster et al. <ref type="bibr" target="#b5">[7]</ref> match the computed 3D maps of MAVs and ground robots for localization and map augmentation. This method relies on matching 3D information and therefore needs multiview images both from above and on ground. In our work, we exploit the maps as well as ground and aerial imagery to perform fine-grained road parsing. We are not aware of prior work that tackles this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fine-grained Semantic Parsing of Roads</head><p>We now describe our model that infers fine-grained semantic categories of roads from aerial and ground images. In particular, we are interested in estimating sidewalks, parking, road lanes as well as background (e.g., vegetation, buildings). Towards this goal we exploit freely available cartographic maps (we use OSM), that provide us with the topology of the road network in the area of interest. Our approach takes as input an aerial image x A , a road map x M and a set of ground stereo images x G , which are taken by a calibrated stereo pair mounted on top of a car. The map x M is composed of a set of roads, where each road is defined as a piece-wise linear curve representing its centerline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Formulation</head><p>We formulate the problem as the one of inference in a Markov random field (MRF), which exploits deep features encoding appearance in both aerial and ground images, edge information, smoothness in the direction of the road as well as restrictions between parallel roads to avoid double counting the evidence. Our model encodes each street segment in the aerial image with 15 random variables encoding all possible combinations of background (B), sidewalk (S), road lanes (L) and parking (P). In particular,</p><formula xml:id="formula_0">y = (y 1 , · · · , y 15 ) = (B 1 , S 1 , B 2 , S 2 , P 1 , L, P 2 , S 3 , B 3 , S 4 , B 4 )</formula><p>with B 1 , B 4 the rightmost (leftmost) border of the background. We model roads with up to 6 lanes, i.e., L = (L 1 , L 2 , L 3 , L 4 , L 5 , L 6 ). We allow all variables (but L 6 ) to take the state of the previous random variable in the sequence (i.e., y i = y i−1 ), encoding the fact that some of these regions might be absent, e.g., there is no parking or sidewalk. This is not the case for L 6 forcing the fact that at least one lane should be present. We define the states of each random variable to be [− <ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b13">15]</ref>m from the projection of the OSM centerline in the aerial image ( <ref type="figure">Fig. 1</ref>). This discretization represents pixel increments. Note that while there are 15 random variables, y defines 16 different regions as B 1 and B 4 are not limited on the left (right). Each region width is simply defined by w i = y i − y i−1 , while the width of B 1 is defined as w 1 = −15m + y 1 , and the width of B 4 as w 16 = 15m − y 16 , since −15m and 15m are the beginning and end of the state space. Note that the combination (B, S, B, S) is necessary (both on the left and right), as there are many bike lines in Germany (where our imagery is captured), and it is not possible to distinguish them from the sidewalk. <ref type="figure">Fig. 1</ref> illustrates the model. Each of our ground images comes with a rough alignment with the aerial image as we have access to a GPS+IMU and the cameras are registered w.r.t these sensors. This alignment is, however, noisy with 1.67m error on average. Thus, our model reasons about the alignment when scoring the ground images. Towards this goal, we define t = (t 1 , · · · , t n ) to be a set of random variables (one per ground image) representing the displacement in the direction perpendicular to the OSM road segment. We define the state space of each misalignment to be t i ∈ (−4m, 4m). This is discretized to represent pixel increments.</p><p>We define the energy of the MRF as to encode the information contained in the ground and aerial images as well as smoothness terms and constraints on the possible solutions:</p><formula xml:id="formula_1">E(y, t, x A , x M , x G ) = E air (y, x A ) + E ground (y, t, x G ) + E smooth (y, t, x M ) + E const (y)<label>(1)</label></formula><p>We now define the potentials we employ in more detail.</p><p>Aerial semantics: We take advantage of deep learning in order to estimate semantic information from aerial images.</p><p>In particular, we create pixel-wise estimates of 5 semantic categories: road, sidewalk, background, building and parking. We exploit the CNN for segmentation <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b17">19]</ref> trained GPS+IMU Our alignment on ILSVRC-2014, which we fine-tune for a 5-label classification task: road, parking spot, sidewalk, building and background. To train the network we created training examples by extracting patches centered on the projection of the OSM road segments. If the road segment is too long (i.e., long straight road) we create an example every 20m. We further perform data augmentation by applying small rotations, shifts and flips to the training examples. The output of the soft-max is a downsampled segmentation. To create our features, we upsample the softmax output using linear interpolation as in <ref type="bibr" target="#b3">[5]</ref>. To save computation, we only apply the network in the region of interest (regions of the image that are close to OSM roads). The aerial semantic potential then encodes the fact that our final segmentation should agree with the semantics estimated by the deep net. Towards this goal, we define 5 features for each of our 16 regions, one per label of the deep net. Each feature simply aggregates the output of the softmax in that region. Recall that each region is defined by two consecutive random variables, e.g. the first sidewalk is defined by y 1 , y 2 , that is B 1 , S 1 . We refer the reader to <ref type="figure">Fig. 1</ref> for an illustration. While this potential seems pairwise in nature, we can further decompose it into unary potentials via accumulators A perpendicular to the road direction. These are simply generalizations of integral images from axis aligned accumulators to accumulators over arbitrary directions. We thus define</p><formula xml:id="formula_2">φ cl (y j i , y j i−1 ) = p∈Ω j i (y j i ,y j i+1 ) ϕ(p) = A(y j i+1 ) − A(y j i )</formula><p>with y j i the i-th variable of the j-th road segment, and ϕ(p) the softmax output interpolated at pixel p. To compute this features, we only need 5 accumulators per road segment, one for each semantic class that the deep net predicts.</p><p>Aerial edges: This potential encodes the fact that the location of the boundaries between regions should be close  to image edges. We thus apply the edge detector of <ref type="bibr" target="#b4">[6]</ref> to detect edges in our aerial images. We then define the potential to be the sum of the edges on the boundary between consecutive regions. To make it more robust we thicken the boundary to be of size 3 pixels.</p><p>Along the road smoothness: We encode smoothness along the road by encouraging consecutive road segments to be similar. In particular, we use the ℓ 1 distance between consecutive road estimations in the direction of the road, i.e.</p><formula xml:id="formula_3">φ sm (y j i , y j+1 i ) = |y j i − y j+1 i |</formula><p>Parallel roads: The regions of close by parallel roads can overlap. To avoid double counting the evidence, we incorporate an additional constraint that forces S 1 of the second road to be bigger or equal to B 4 of the first road or vice versa. We refer the reader to <ref type="figure">Fig. 1</ref> for an illustration.</p><p>Road collapse constraints: We force each variable y i to have a state higher or equal than the previous variable, so that the order is preserved. Note that equal means that a road can collapse (i.e., does not exist)</p><formula xml:id="formula_4">φ coll (y i , y i+1 ) = ∞ if y i+1 &lt; y i 0 otherwise</formula><p>The only exception is L 6 , which we force to have non-zero width as otherwise we could have a road segment without road. Thus  we only have two lanes, there is no sidewalk on the highway). The intervals for the lanes are estimated based on the standards of German roads, while the sidewalk and parking intervals are computed based on empirical estimates.</p><p>Centerline prior: As our images are well registered with OSM, we include a prior that the centerline of our model should be close to the centerline of OSM. In particular,</p><formula xml:id="formula_5">φ cen (L 3 ) = ||L 3 − l|| 2 if − 7.5 ≤ L 3 ≤ 7.5 ∞ otherwise</formula><p>with l the location of the centerline.</p><p>Ground semantics: We take advantage of deep learning in order to estimate semantic information from ground im-ages. We exploit the VGG <ref type="bibr" target="#b21">[23]</ref> implementation of <ref type="bibr" target="#b17">[19]</ref> trained on PASCAL VOC, which we fine-tuned to predict the same 5 classes as the aerial semantics (road, parking, sidewalk, building and background). We estimate the ground plane from the stereo image and project pixels belonging to this plane to the aerial image via a homography. We then define our ground semantic potential to encourage the segmentation to agree with the aligned ground image segmentation projected to the aerial image. Towards this goal, we define 5 features for each of our road regions, each counting the amount of softmax output for the given class:</p><p>φ ground (t k , y j i , y j i−1 ) = G(t k , y j i+1 ) − G(t k , y j i ) Note that via the integral accumulator the 3-way potential decomposes into pairwise terms G(t, y). In this case we Aerial Ground <ref type="figure">Figure 6</ref>. Left: The ground road detection with red projected into the aerial image after alignment and road layout estimation. Right: The semantic lanes projected back into the aligned ground image. These scenes are all challenging with parallel roads, parking spots and intersections. The bottom image is especially difficult since it is an urban pedestrian area. Note that the aerial and ground images were taken with several years difference in different seasons. Pink is road, blue is sidewalk and yellow marks parking spots. only need 5 integral accumulators per ground image.</p><p>Ground alignment smoothness: This potential encodes the fact that two consecutive alignments should be similar.</p><formula xml:id="formula_6">φ gsm (t k , t k+1 ) = |t k − t k+1 |</formula><p>This assumes that GPS+IMU have smooth errors and no outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference via Block Coordinate Descent (BCD)</head><p>Inference in our model can be performed by minimizing the energy function:</p><formula xml:id="formula_7">y * , t * = argmin y,t E(y, t, x A , x M , x G ) with E(y, t, x A , x M , x G ) defined as in Eq.</formula><p>(1). Unfortunately, inference in our model is NP-hard, as our graphical model contains many loops. We thus take advantage of block coordinate descent to perform efficient inference. We refer the reader to Alg. 1 and <ref type="figure" target="#fig_0">Fig. 2</ref> for inference steps.</p><p>Our block coordinate descent algorithm (BCD) alternates by doing inference in the direction along the road, doing inference in the direction perpendicular to the road and aligning the ground and aerial images. Note that when a road is not connected to a parallel road, the second step results in a graphical model with 15 variables, while when there are k parallel roads, this involves doing inference over a graphical model with 15k variables. Note also that in order to minimize the same objective, each of these iterations is performing conditional inference, and the pairwise potentials involving variables that are not optimized collapse to unaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training with S-SVM</head><p>We employ structured SVM (S-SVM) <ref type="bibr" target="#b22">[24]</ref> to learn the weights of the aerial unaries and the smoothness in our model. In particular, we use the parallel cutting plane implementation of <ref type="bibr" target="#b16">[18]</ref>. We employ a combination of two loss functions. The first is a truncated L 2 loss: ℓ data = min(||y j i −ŷ j i || 2 , 100m 2 ), encouraging our prediction y j i to be close to the ground truthŷ j i . We computeŷ j i by performing inference in our model with features computed from the ground truth annotation (segmentation). The second loss term encourages smoothness of the prediction along the Algorithm 1 Block coordinate descent inference (BCD).</p><p>1: Set all alignments t = 0, and initialize y by minimizing Eq.</p><p>(1) ignoring the along road smoothness. 2: repeat <ref type="bibr">3:</ref> for for all y j do 4:</p><p>Minimize Eq. (1) along the road w.r.t y j , holding the rest fixed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We collected a new dataset which we call Air-Ground-KITTI, which is composed of both ground images from the KITTI tracking benchmark <ref type="bibr" target="#b6">[8]</ref> and newly acquired orthorectified aerial images over the same area. We neglected the KITTI sequences where the car is mostly static, resulting in 20 KITTI sequences for a total of 7603 ground stereo images. We annotated every 30th ground image with 4 semantic classes (parking, sidewalk, road, building). The aerial images were acquired by a DSLR camera mounted on an airplane and projected on the earth surface with 9 cm/pixel Ground Sampling Distance (GSD). We split the data into 10 training and 10 test aerial image/KITTI sequences, with special care to avoid overlaps in the aerial images. We manually annotated the aerial images with 4 categories (parking, sidewalk, road, building) as closed polygons and the lane markings as polylines. This took 70h of annotation, at a mean of 21h/km 2 , the area is 3.23 km 2 .</p><p>To perform fine-grained segmentation using both aerial and ground images, we estimate a homography that transforms the ground plane in KITTI to the UTM coordinate system based on the KITTI's GPS+IMU measurements and the camera calibration. We assign each ground image to the closest parallel road segment. Our model then refines this estimate in the direction perpendicular to each road segment. We process every 5th ground image in the sequence.</p><p>As metrics for the fine-grained segmentation we calculate the pixelwise Intersection over Union (IoU), Precision, Recall and F1 metrics for three classes (i.e. road, parking, sidewalk). Note that we only measure the areas laying in the area of interest (i.e. ±15m around the road map centerline). We consider two parallel roads overlapping over the same area as a serious error. To reflect this, we handle these areas as if they were background. The metrics in <ref type="table">Table 1</ref> are calculated according to this.</p><p>For the roads, we additionally compute whether we have estimated the correct number of lanes. This is measured as the average ℓ 1 error in terms of number of lanes (EN). Note that if there are no lane markings, estimating the number of lanes is very difficult. <ref type="figure" target="#fig_7">Fig. 7</ref> (a-b) shows this difficulty.</p><p>In our experiments, we compare our approach to the state-of-the-art method of <ref type="bibr" target="#b10">[12]</ref>, which uses OSMs to estimate road width. We also tested different model configurations for our approach. We refer to Lane as a model that employs Aerial semantics, Aerial Edges, Road collapse constraints, Lane size constraint and Centerline prior energy terms. Inference is done independently for each road segment via dynamic programming along the y j = y j 1 , · · · , y j 15 chains. We refer by LaneParallel to a model where we additionally include the constraint between nearby parallel road. We refer by LaneRoad as a model that contains all the potentials in Lane plus smoothness along the road. We apply BCD inference by alternating between the chains perpendicular to the road (the lanes) and along the roads (segments). We refer by LaneRoadParallel a model that contains all potentials but the ground. Finally, Ground contains all potentials. We evaluate this case only where ground images are available. Comparison to the state-of-the-art: As shown in Table 1, our method outperforms <ref type="bibr" target="#b10">[12]</ref> in almost all metrics, even when we apply our deep features instead of their road classifier in their method. Furthermore, we retrieve more semantic categories such as sidewalk, individual road lanes and parking. The constraint between parallel roads is important to achieve good results on roads. Without it, our model cannot outperform <ref type="bibr" target="#b10">[12]</ref>, which has this constraint.</p><p>Deep semantic features in aerial Images: We show the performance of our Deep Network in <ref type="figure" target="#fig_3">Fig. 4</ref>. Note that it is much better than the road classifier of <ref type="bibr" target="#b10">[12]</ref>.</p><p>Alignment between aerial and ground images: As shown in <ref type="table">Table 2</ref> and <ref type="figure" target="#fig_1">Fig. 3</ref> reasoning about the alignment between ground and aerial images while doing fine-grained segmentation improves the alignment significantly.</p><p>Qualitative Results: We visualize our results when using only aerial images in <ref type="figure" target="#fig_5">Fig. 5</ref>, and when using joint aerial and ground reasoning in <ref type="figure">Fig. 6</ref>. Our approach is able to estimate well the lanes, sidewalk and parking as well as the alignment between the ground and the aerial images.</p><p>Ablation studies: As shown in <ref type="table">Table 1</ref>, the metrics for different versions of our model are fairly similar, however qualitatively, as we add more potentials, the results get better. This is illustrated in <ref type="figure" target="#fig_7">Fig. 7 (c)</ref>, where the OnlyLane model moves the middle road to a parallel road resulting in a noncontinuous structure. In contrast, the LaneRoadParallel model prevents overlaps and favors smoothness, see the <ref type="figure" target="#fig_7">Fig. 7 (d)</ref>. Including the ground images only slightly improves performance. We believe this could be overcome by using stronger features in the ground images, i.e., leveraging the full 3D point cloud, not just the ground plane. Note that since our approach gives us very precise alignments between the ground and the aerial images it could be used to enhance OSM with object locations, e.g. traffic signs.</p><p>Inference time: Inference in our full model takes 6 seconds per km of road, with a single thread on a laptop computer. Note that BCD can easily be parallelized.</p><p>Limitations: Our model is designed for individual roads and it does not reason about turning lanes connecting different roads at intersections (see <ref type="figure" target="#fig_7">Fig. 7</ref> (f)). Dealing with such scenarios is part of our future work. Semantic segmentation from aerial images reasons mainly about the visible parts of the street. Therefore covered areas (e.g. by building, bridges, trees) can be a problem. However, when ground images are available, our approach can handle this problem. Our aerial images were acquired in early spring, and thus trees occluding the roads is not a big problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed an approach to enhance existing freely available maps with fine grained segmentation categories such as parking spots and sidewalk, as well as the number and location of road lanes. Towards this goal, we proposed an efficient method that produces very accurate estimates by performing joint inference over both, monocular aerial imagery captured by a plane and ground images taken from a stereo pair mounted on top of a car. We have demonstrated the effectiveness of our approach on a new dataset which enhances KITTI with aerial images taken with a camera mounted on an airplane and flying around the city of Karlsruhe. In the future, we plan to reason about other fine grained categories such as traffic signs in order to further enhance the maps. As our method reasons about the accurate alignment between the map and the ground images, we envision its use for precise, lane-wise self localization of the vehicle on the road.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>BCD: The graph shows a simplified network with two parallel roads (each with 3 random variables) and one ground image per segment connected to the right road. BCD alternates between three types of updates. (a) Along the road updates: we optimize over each chain with the same color (while holding all other variables fix). The pairwise terms fold to unaries (see dashed black lines). (b) Perpendicular to the road updates: we do inference for the nodes with the same color (holding the rest fix). (c) Along the ground alignments: We minimize only the t variables which are depicted in green. The y variables are fixed and are depicted in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Effect of reasoning about alignment: (left) alignment given by GPS+IMU, (right) alignment inferred by our model. (top) Ground road classifier projected into the aerial image (shown in red). (bottom) Our semantic classes projected on the ground image. Our joint reasoning significantly improves alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Precision-Recall curves for our deep classifier and the road classifier of<ref type="bibr" target="#b10">[12]</ref> marked with * and in dashed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>φ</head><label></label><figDesc>ex (L 5 , L 6 ) = ∞ if L 6 ≤ L 5 0 otherwise Lane size constraint: This constraint forces each region, if present, (i.e., if it is not taking state 0) to have a minimal and maximal size. In particular, we use (1m-3m) for sidewalk, (1.8m-4.5m) for parking and (2.3m-4.6m) for each road lane. Note that width 0 is allowed so that regions can disappear if they are not present in the road segment (e.g., (a) Intersection with tram line. (b) Small town. (c) A road with three lanes. (d) Two roads with tram stop in between. (e) Dense urban area. (f) Splitting road plus a bike lane along the street.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of our semantic road parsing results using only aerial images. The road lanes are shown with shades of pink, the sidewalk with blue and the parking spots with yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>12:  until no energy reduction or max number iterationsroad, ℓ sm = |y j i − y j+1 i |.Note that the geometrical constraints in our model are either 0 or ∞ and are not trained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>It is hard to estimate the number of lanes if there are no lane markings. (a) Our method, (b) Oracle (i.e., our method with ground truth potentials). (c) The OnlyLane model without the parallel constraint allows the road to "jump" to the nearby parallel road. (d) The parallel constraints of LaneRoadParallel prevents this from happing. (e) Dense, urban pedestrian streets are difficult to estimate. (f) Our model is not intended for intersections, as it does not reason about turn lanes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. Performance for the semantic classes (i.e. road, parking spot, sidewalk) with various models and the two baselines. The values are in %, except EN which is the average road lane number l1 error with respect to the oracle. * Marks the method of<ref type="bibr" target="#b10">[12]</ref> with our deep road classifier. The last two rows marked with ** evaluate only over areas where ground images are also available.Table 2. Ground to air image misalignment based on the camera calibrations (GPS+IMU) and after our alignment measured in meters. Using ±4 meter interval.</figDesc><table>Model 

Average 
Road 
Sidewalk 
Parking 
IoU 
F1 
IoU 
F1 
Pr. 
R. 
EN 
IoU 
F1 
Pr. 
R. 
IoU 
F1 
Pr. 
R. 
Mattyus et al. [12] 
-
-
62.1 76.4 68.0 87.0 
-
-
-
-
-
-
-
-
-
[12] Deep Un* 
-
-
64.4 78.4 66.7 94.7 
-
-
-
-
-
-
-
-
-
Lane 
43.6 59.6 
61.9 76.5 82.8 71.0 0.730 
31.8 48.3 67.2 37.7 
37.0 54.1 58.5 50.3 
LaneParallel 
44.8 60.3 
66.5 79.9 85.0 75.4 0.543 
31.6 48.0 69.8 36.6 
36.1 53.1 70.8 42.4 
LaneRoad 
45.4 61.6 
61.9 76.4 82.7 71.0 0.707 
38.3 55.4 62.4 49.7 
36.1 53.1 52.2 54.1 
LaneRoadParallel 
48.6 64.3 
68.0 80.9 83.5 78.5 0.555 
39.5 56.6 63.5 51.1 
38.4 55.5 63.8 49.1 

LaneRoadParallel** 41.9 58.5 
54.9 70.9 86.9 59.9 0.559 
34.9 51.7 68.7 41.5 
35.8 52.7 69.9 42.3 
Full** 
42.0 58.6 
55.3 71.2 86.8 60.4 0.556 
34.9 51.7 68.7 41.5 
35.8 52.7 69.9 42.3 

GPS+IMU [m] Ours [m] 
Alignment error 
1.67 
0.57 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Viktoria Zekoll, Stefan Turzer and Márk Bagdy for making the laborious annotation work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic finding of main roads in aerial images by using geometric-stochastic models and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barzohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lost! leveraging the crowd for probabilistic visual self-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recovering linenetworks in images by junction-point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Forstner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lafarge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Air-ground localization and map augmentation using monocular dense reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pizzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image sequence geolocation with human travel priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vesselova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-view image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhancing road maps by parsing aerial images around the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nyc3dcars: A dataset of 3d vehicles in geographic context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A test of automatic road extraction approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baltsavias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect roads in highresolution aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hierarchical and contextual model for aerial image parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Porway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">W</forename><surname>Song Chun Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="283" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Box In the Box: Joint 3D Layout and Object Reasoning from Single Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting publicly available cartographic resources for aerial image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Urmson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wettergreen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGSPATIAL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ortho-image analysis for producing lane-level highway maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Urmson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wettergreen</surname></persName>
		</author>
		<idno>CMU-RI-TR-12-26</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Accurate geo-registration by ground-to-aerial image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Holistic 3d scene understanding from a single geo-tagged image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A higher-order crf model for road network extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Montoya-Zegarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Road segmentation in aerial images by exploiting road vector data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheriyadat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COM.geo</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
