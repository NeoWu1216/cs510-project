<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inverting Visual Representations with Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
							<email>dosovits@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg Freiburg im Breisgau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg Freiburg im Breisgau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inverting Visual Representations with Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A feature representation useful for pattern recognition tasks is expected to concentrate on properties of the input image which are important for the task and ignore the irrelevant properties of the input image. For example, handdesigned descriptors such as HOG <ref type="bibr" target="#b2">[3]</ref> or SIFT <ref type="bibr" target="#b16">[17]</ref>, explicitly discard the absolute brightness by only considering gradients, precise spatial information by binning the gradients and precise values of the gradients by normalizing the histograms. Convolutional neural networks (CNNs) trained in a supervised manner <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> are expected to discard information irrelevant for the task they are solving <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>In this paper we propose a new approach to analyze which information is preserved by a feature representation and which information is discarded. We train neural networks to invert feature representations in the following sense. Given a feature vector, the network is trained to predict the expected pre-image, that is, the (weighted) average of all natural images which could have produced the HOG SIFT AlexNet-CONV3 AlexNet-FC8 <ref type="figure">Figure 1</ref>: We train convolutional networks to reconstruct images from different feature representations. Top row: Input features. Bottom row: Reconstructed image. Reconstructions from HOG and SIFT are very realistic. Reconstructions from AlexNet preserve color and rough object positions even when reconstructing from higher layers.</p><p>given feature vector. The content of this expected pre-image shows image properties which can be confidently inferred from the feature vector. The amount of blur corresponds to the level of invariance of the feature representation. We obtain further insights into the structure of the feature space, as we apply the networks to perturbed feature vectors, to interpolations between two feature vectors, or to random feature vectors.</p><p>We apply our inversion method to AlexNet <ref type="bibr" target="#b12">[13]</ref>, a convolutional network trained for classification on ImageNet, as well as to three widely used computer vision features: histogram of oriented gradients (HOG) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>, scale invariant feature transform (SIFT) <ref type="bibr" target="#b16">[17]</ref>, and local binary patterns (LBP) <ref type="bibr" target="#b20">[21]</ref>. The SIFT representation comes as a nonuniform, sparse set of oriented keypoints with their corresponding descriptors at various scales. This is an additional challenge for the inversion task. LBP features are not differentiable with respect to the input image. Thus, existing methods based on gradients of representations <ref type="bibr" target="#b18">[19]</ref> could not be applied to them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Our approach is related to a large body of work on inverting neural networks. These include works making use of backpropagation or sampling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref> and, most similar to our approach, other neural networks <ref type="bibr" target="#b1">[2]</ref>. However, only recent advances in neural network architectures allow us to invert a modern large convolutional network with another network.</p><p>Our approach is not to be confused with the Decon-vNet <ref type="bibr" target="#b27">[28]</ref>, which propagates high level activations backward through a network to identify parts of the image responsible for the activation. In addition to the high-level feature activations, this reconstruction process uses extra information about maxima locations in intermediate maxpooling layers. This information has been shown to be crucial for the approach to work <ref type="bibr" target="#b21">[22]</ref>. A visualization method similar to DeconvNet is by Springenberg et al. <ref type="bibr" target="#b21">[22]</ref>, yet it also makes use of intermediate layer activations.</p><p>Mahendran and Vedaldi <ref type="bibr" target="#b18">[19]</ref> invert a differentiable image representation Φ using gradient descent. Given a feature vector Φ 0 , they seek for an image x * which minimizes a loss function -the squared Euclidean distance between Φ 0 and Φ(x) plus a regularizer enforcing a natural image prior. This method is fundamentally different from our approach in that it optimizes the difference between the feature vectors, not the image reconstruction error. Additionally, it includes a hand-designed natural image prior, while in our case the network implicitly learns such a prior. Technically, it involves optimization at test time, which requires computing the gradient of the feature representation and makes it relatively slow (the authors report 6s per image on a GPU). In contrast, the presented approach is only costly when training the inversion network. Reconstruction from a given feature vector just requires a single forward pass through the network, which takes roughly 5ms per image on a GPU. The method of <ref type="bibr" target="#b18">[19]</ref> requires gradients of the feature representation, therefore it could not be directly applied to non-differentiable representations such as LBP, or recordings from a real brain <ref type="bibr">[20]</ref>.</p><p>There has been research on inverting various traditional computer vision representations: HOG and dense SIFT <ref type="bibr" target="#b23">[24]</ref>, keypoint-based SIFT <ref type="bibr" target="#b25">[26]</ref>, Local Binary Descriptors <ref type="bibr" target="#b3">[4]</ref>, Bag-of-Visual-Words <ref type="bibr" target="#b10">[11]</ref>. All these methods are either tailored for inverting a specific feature representation or restricted to shallow representations, while our method can be applied to any feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>Denote by (x, φ) random variables representing a natural image and its feature vector, and denote their joint probability distribution by p(x, φ) = p(x)p(φ|x). Here p(x) is the distribution of natural images and p(φ|x) is the distribu-tion of feature vectors given an image. As a special case, φ may be a deterministic function of x. Ideally we would like to find p(x|φ), but direct application of Bayes' theorem is not feasible. Therefore in this paper we resort to a point estimate f (φ) which minimizes the following mean squared error objective:</p><formula xml:id="formula_0">E x,φ ||x − f (φ)|| 2<label>(1)</label></formula><p>The minimizer of this loss is the conditional expectation:</p><formula xml:id="formula_1">f (φ 0 ) = E x [x | φ = φ 0 ],<label>(2)</label></formula><p>that is, the expected pre-image. Given a training set of images and their features {x i , φ i }, we learn the weights w of an an up-convolutional network f (φ, w) to minimize a Monte-Carlo estimate of the loss (1):ŵ</p><formula xml:id="formula_2">= arg min w i ||x i − f (φ i , w)|| 2 2 .<label>(3)</label></formula><p>This means that simply training the network to predict images from their feature vectors results in estimating the expected pre-image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature representations to invert</head><p>Shallow features. We invert three traditional computer vision feature representations: histogram of oriented gradients (HOG), scale invariant feature transform (SIFT), and local binary patterns (LBP). We chose these features for a reason. There has been work on inverting HOG, so we can compare to existing approaches. LBP is interesting because it is not differentiable, and hence gradient-based methods cannot invert it. SIFT is a keypoint-based representation, so the network has to stitch different keypoints into a single smooth image.</p><p>For all three methods we use implementations from the VLFeat library <ref type="bibr" target="#b22">[23]</ref> with the default settings. More precisely, we use the HOG version from Felzenszwalb et al. <ref type="bibr" target="#b6">[7]</ref> with cell size 8, the version of SIFT which is very similar to the original implementation of Lowe <ref type="bibr" target="#b16">[17]</ref> and the LBP version similar to Ojala et al. <ref type="bibr" target="#b20">[21]</ref> with cell size 16. Before extracting the features we convert images to grayscale. More details can be found in the supplementary material.</p><p>AlexNet. We also invert the representation of the AlexNet network <ref type="bibr" target="#b12">[13]</ref> trained on ImageNet, available at the Caffe <ref type="bibr" target="#b9">[10]</ref> website. <ref type="bibr" target="#b0">1</ref> It consists of 5 convolutional layers and 3 fully connected layers, with rectified linear units (ReLUs) after each layer, and local contrast normalization or max-pooling after some of them. Exact architecture is shown in the supplementary material. In what follows, when we say 'output of the layer', we mean the output of the last processing step of this layer. For example, the output of the first convolutional layer CONV1 would be the result after ReLU, pooling and normalization, and the output of the first fully connected layer FC6 is after ReLU. FC8 denotes the last layer, before the softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network architectures and training</head><p>An up-convolutional layer, also often referred to as 'deconvolutional', is a combination of upsampling and convolution <ref type="bibr" target="#b5">[6]</ref>. We upsample a feature map by a factor 2 by replacing each value by a 2 × 2 block with the original value in the top left corner and all other entries equal to zero. Architecture of one of our up-convolutional networks is shown in <ref type="table" target="#tab_0">Table 1</ref>. Architectures of other networks are shown in the supplementary material.</p><p>HOG and LBP. For an image of size W × H, HOG and LBP features of an image form 3-dimensional arrays of sizes ⌈W/8⌉ × ⌈H/8⌉ × 31 and ⌈W/16⌉ × ⌈H/16⌉ × 58, respectively. We use similar CNN architectures for inverting both feature representations. The networks include a contracting part, which processes the input features through a series of convolutional layers with occasional stride of 2, resulting in a feature map 64 times smaller than the input image. Then the expanding part of the network again upsamples the feature map to the full image resolution by a series of up-convolutional layers. The contracting part allows the network to aggregate information over large regions of the input image. We found this is necessary to successfully estimate the absolute brightness.</p><p>Sparse SIFT. Running the SIFT detector and descriptor on an image gives a set of N keypoints, where the i-th keypoint is described by its coordinates (x i , y i ), scale s i , orientation α i , and a feature descriptor f i of dimensionality D. In order to apply a convolutional network, we arrange the keypoints on a grid. We split the image into cells of size d × d (we used d = 4 in our experiments), this yields ⌈W/d⌉ × ⌈H/d⌉ cells. In the rare cases when there are several keypoints in a cell, we randomly select one. We then assign a vector to each of the cells: a zero vector to a cell without a keypoint and a vector (f i , x i mod d, y i mod d, sin α i , cos α i , log s i ) to a cell with a keypoint. This results in a feature map F of size ⌈W/d⌉×⌈H/d⌉×(D+5). Then we apply a CNN to F, as described above.</p><p>AlexNet. To reconstruct from each layer of AlexNet we trained a separate network. We used two basic architectures: one for reconstructing from convolutional layers and one for reconstructing from fully connected layers. The network for reconstructing from fully connected layers contains three fully connected layers and 5 up-convolutional layers, as shown in <ref type="table" target="#tab_0">Table 1</ref>. The network for reconstructing from convolutional layers consists of three convolutional and several up-convolutional layers (the exact number depends on the </p><formula xml:id="formula_3">Layer Input InSize K S OutSize fc1 AlexNet-FC8 1000 − − 4096 fc2 fc1 4096 − − 4096 fc3 fc2 4096 − − 4096 reshape fc3 4096 − − 4×4×256 upconv1 reshape 4×4×256 5 2 8×8×256 upconv2 upconv1 8×8×256 5 2 16×16×128 upconv3 upconv2 16×16×128 5 2 32×32×64 upconv4 upconv3 32×32×64 5 2 64×64×32 upconv5 upconv4 64×64×32 5 2 128×128×3</formula><formula xml:id="formula_4">(x) = x if x 0 and r(x) = 0.2 · x if x &lt; 0.</formula><p>Training details. We trained networks using a modified version of Caffe <ref type="bibr" target="#b9">[10]</ref>. As training data we used the Ima-geNet <ref type="bibr" target="#b4">[5]</ref> training set. In some cases we predicted downsampled images to speed up computations. We used the Adam <ref type="bibr" target="#b11">[12]</ref> optimizer with β 1 = 0.9, β 2 = 0.999 and minibatch size 64. For most networks we found an initial learning rate λ = 0.001 to work well. We gradually decreased the learning rate towards the end of training. The duration of training depended on the network: from 15 epochs (passes through the dataset) for shallower networks to 60 epochs for deeper ones.</p><p>Quantitative evaluation. As a quantitative measure of performance we used the average normalized reconstruction error, that is the mean of ||x i − f (Φ(x i ))|| 2 /N , where x i is an example from the test set, f is the function implemented by the inversion network and N is a normalization coefficient equal to the average Euclidean distance between images in the test set. The test set we used for quantitative and qualitative evaluations is a subset of the ImageNet validation set.  <ref type="table" target="#tab_2">Table 2</ref>. Clearly, our method significantly outperforms existing approaches. This is to be expected, since our method explicitly aims to minimize the reconstruction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments: shallow representations</head><p>Hoggles <ref type="bibr" target="#b23">[24]</ref> HOG   <ref type="figure">Figure 2</ref>: Reconstructing an image from its HOG descriptors with different methods.</p><p>Colorization. As mentioned above, we compute the features based on grayscale images, but the task of the networks is to reconstruct the color images. The features do not contain any color information, so to predict colors the network has to analyze the content of the image and make use of a natural image prior it learned during training. It does successfully learn to do so, as can be seen in <ref type="figure" target="#fig_0">Figures 1  and 3</ref>. Quite often the colors are predicted correctly, especially for sky, sea, grass, trees. In other cases, the network cannot predict the color (for example, people in the top row of <ref type="figure" target="#fig_0">Figure 3</ref>) and leaves some areas gray. Occasionally the network predicts the wrong color, such as in the bottom row of <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>HOG. <ref type="figure">Figure 2</ref> shows an example image, its HOG representation, the results of inversion with existing methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19]</ref> and with our approach. Most interestingly, the network is able to reconstruct the overall brightness of the image very well, for example the dark regions are reconstructed dark. This is quite surprising, since the HOG descriptors are normalized and should not contain information about absolute brightness.</p><p>Normalization is always performed with a smoothing 'epsilon', so one might imagine that some information about the brightness is present even in the normalized features. We checked that the network does not make use of this information: multiplying the input image by 10 or 0.1 hardly changes the reconstruction. Therefore, we hypothesize that the network reconstructs the overall brightness by 1) analyzing the distribution of the HOG features (if in a cell there is similar amount of gradient in all directions, it is probably noise; if there is one dominating gradient, it must actually be in the image), 2) accumulating gradients over space: if there is much black-to-white gradient in one direction, then probably the brightness in that direction goes from dark to bright and 3) using semantic information.</p><p>SIFT. <ref type="figure">Figure 4</ref> shows an image, the detected SIFT keypoints and the resulting reconstruction. There are roughly Image HOG our SIFT our LBP our  3000 keypoints detected in this image. Although made from a sparse set of keypoints, the reconstruction looks very natural, just a little blurry. To achieve such a clear reconstruction the network has to properly rotate and scale the descriptors and then stitch them together. Obviously it successfully learns to do this.</p><p>For reference we also show a result of another existing method <ref type="bibr" target="#b25">[26]</ref> for reconstructing images from sparse SIFT descriptors. The results are not directly comparable: while we use the SIFT detector providing circular keypoints, Weinzaepfel et al. <ref type="bibr" target="#b25">[26]</ref> use the Harris affine keypoint detector which yields elliptic keypoints, and the number and the locations of the keypoints may be different from our case. However, the rough number of keypoints is the same, so a qualitative comparison is still valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments: AlexNet</head><p>We applied our inversion method to different layers of AlexNet and performed several additional experiments to better understand the feature representations. More results are shown in the supplementary material. <ref type="figure">Figure 5</ref> shows reconstructions from various layers of AlexNet. When using features from convolutional layers, the reconstructed images look very similar to the input, but lose fine details as we progress to higher layers. There is an obvious drop in reconstruction quality when going from CONV5 to FC6. However, the reconstructions from higher convolutional layers and even fully connected layers preserve color and the approximate object location very well. Reconstructions from FC7 and FC8 still look similar to the input images, but blurry. This means that high level features  are much less invariant to color and pose than one might expect: in principle fully connected layers need not preserve any information about colors and locations of objects in the input image. This is somewhat in contrast with the results of <ref type="bibr" target="#b18">[19]</ref>, as shown in <ref type="figure">Figure 6</ref>. While their reconstructions are sharper, the color and position are completely lost in reconstructions from higher layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Reconstructions from different layers</head><p>For quantitative evaluation before computing the error we up-sample reconstructions to input image size with bilinear interpolation. Error curves shown in <ref type="figure" target="#fig_4">Figure 7</ref> support the conclusions made above. When reconstructing from FC6, the error is roughly twice as large as from CONV5. Even when reconstructing from FC8, the error is fairly low because the network manages to get the color and the rough placement of large objects in images right. For lower layers, the reconstruction error of <ref type="bibr" target="#b18">[19]</ref> is still much higher than of our method, even though visually the images look somewhat sharper. The reason is that in their reconstructions the color and the precise placement of small details do not perfectly match the input image, which results in a large overall error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Autoencoder training</head><p>Our inversion network can be interpreted as the decoder of the representation encoded by AlexNet. The difference to an autoencoder is that the encoder part stays fixed and only the decoder is optimized. For comparison we also trained autoencoders with the same architecture as our reconstruction nets, i.e., we also allowed the training to fine-tune the parameters of the AlexNet part. This provides an upper bound on the quality of reconstructions we might expect from the inversion networks (with fixed AlexNet).</p><p>As shown in <ref type="figure" target="#fig_4">Figure 7</ref>, autoencoder training yields much lower reconstruction errors when reconstructing from higher layers. Also the qualitative results in <ref type="figure">Figure 6</ref> show much better reconstructions with autoencoders. Even from CONV5 features, the input image can be reconstructed almost perfectly. When reconstructing from fully connected layers, the autoencoder results get blurred, too, due to the compressed representation, but by far not as much as with the fixed AlexNet weights. The gap between the autoencoder training and the training with fixed AlexNet gives an estimate of the amount of image information lost due to the training objective of the AlexNet, which is not based on reconstruction quality. An interesting observation with autoencoders is that the reconstruction error is quite high even when reconstructing from CONV1 features, and the best reconstructions were actually obtained from CONV4. Our explanation is that the convolution with stride 4 and consequent max-pooling in CONV1 loses much information about the image. To decrease the reconstruction error, it is beneficial for the network to slightly blur the image instead of guessing the details. When reconstructing from deeper layers, deeper networks can learn a better prior resulting in slightly sharper images and slightly lower reconstruction error. For even deeper layers, the representation gets too compressed and the error increases again. We observed (not shown in the paper) that without stride 4 in the first layer, the reconstruction error of autoencoders got much lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Case study: Colored apple</head><p>We performed a simple experiment illustrating how the color information influences classification and how it is preserved in the high level features. We took an image of a red apple <ref type="figure" target="#fig_5">(Figure 8 top left)</ref>  Fixed AlexNet Autoencoder <ref type="figure">Figure 9</ref>: Reconstructions from different layers of AlexNet with disturbed features.</p><p>hue to make it green or blue. Then we extracted AlexNet FC8 features of the resulting images. Remind that FC8 is the last layer of the network, so the FC8 features, after application of softmax, give the network's prediction of class probabilities. The largest activation, hence, corresponds to the network's prediction of the image class. To check how class-dependent the results of inversion are, we passed three versions of each feature vector through the inversion network: 1) just the vector itself, 2) all activations except the 5 largest ones set to zero, 3) the 5 largest activations set to zero. This leads to several conclusions. First, color clearly can be very important for classification, so the feature representation of the network has to be sensitive to it, at least in some cases. Second, the color of the image can be precisely reconstructed even from FC8 or, equivalently, from the predicted class probabilities. Third, the reconstruction quality does not depend much on the top predictions of the network but rather on the small probabilities of all other classes. This is consistent with the 'dark knowledge' idea of <ref type="bibr" target="#b7">[8]</ref>: small probabilities of non-predicted classes carry more information than the prediction itself. More examples of this are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Robustness of the feature representation</head><p>We have shown that high level feature maps preserve rich information about the image. How is this information represented in the feature vector? It is difficult to answer this question precisely, but we can gain some insight by perturbing the feature representations in certain ways and observing images reconstructed from these perturbed features. If perturbing the features in a certain way does not change the reconstruction much, then the perturbed property is not important. For example, if setting a non-zero feature to zero does not change the reconstruction, then this feature does not carry information useful for the reconstruction.</p><p>We applied binarization and dropout. To binarize the feature vector, we kept the signs of all entries and set their absolute values to a fixed number, selected such that the Euclidean norm of the vector remained unchanged (we tried several other strategies, and this one led to the best result). For all layers except FC8, feature vector entries are nonnegative, hence, binarization just sets all non-zero entries to a fixed positive value. To perform dropout, we randomly set 50% of the feature vector entries to zero and then normalize the vector to keep its Euclidean norm unchanged (again, we found this normalization to work best). Qualitative results of these perturbations of features in different layers of AlexNet are shown in <ref type="figure">Figure 9</ref>. Quantitative results are shown in <ref type="figure" target="#fig_4">Figure 7</ref>. Surprisingly, dropout leads to larger decrease in reconstruction accuracy than binarization, even in the layers where it had been applied during training. In layers FC7 and especially FC6, binarization hardly changes the reconstruction quality at all. Although it is known that binarized ConvNet features perform well in classification <ref type="bibr" target="#b0">[1]</ref>, it comes as a surprise that for reconstructing the input image the exact values of the features are not important. In FC6 virtually all information about the image is contained in the binary code given by the pattern of non-zero activations. <ref type="figure" target="#fig_4">Figures 7 and 9</ref> show that this binary code only emerges when training with the classification objective and dropout, while autoencoders are very sensitive to perturbations in the features.</p><p>To test the robustness of this binary code, we applied binarization and dropout together. We tried dropping out 50% random activations or 50% least non-zero activations and then binarizing. Dropping out the 50% least activations reduces the error much less than dropping out 50% random activations and is even better than not applying any dropout for most layers. However, layers FC6 and FC7 are the most interesting ones: here dropping out 50% random activations decreases the performance substantially, while dropping out 50% least activations only results in a small decrease. Possibly the exact values of the features in FC6 and FC7 do not affect the reconstruction much, but they estimate the importance of different features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Interpolation and random feature vectors</head><p>Another way to analyze the feature representation is by traversing the feature manifold and by observing the corre- sponding images generated by the reconstruction networks. We have seen the reconstructions from feature vectors of actual images, but what if a feature vector was not generated from a natural image? In <ref type="figure" target="#fig_6">Figure 10</ref> we show reconstructions obtained with our networks when interpolating between feature vectors of two images. It is interesting to see that interpolating CONV5 features leads to a simple overlay of images, but the behavior of interpolations when reconstructing from FC6 is very different: images smoothly morph into each other. More examples, together with the results for autoencoders, are shown in the supplementary material.</p><p>Another analysis method is by sampling feature vectors randomly. Our networks were trained to reconstruct images given their feature representations, but the distribution of the feature vectors is unknown. Hence, there is no simple principled way to sample from our model. However, by assuming independence of the features (a very strong and wrong assumption!), we can approximate the distribution of each dimension of the feature vector separately. To this end we simply computed a histogram of each feature over a set of 4096 images and sampled from those. We ensured that the sparsity of the random samples is the same as that of the actual feature vectors. This procedure led to low contrast images, perhaps because by independently sampling each dimension we did not introduce interactions between the features. Multiplying the feature vectors by a constant factor α = 2 increases the contrast without affecting other properties of the generated images.</p><p>Random samples obtained this way from four top layers of AlexNet are shown in <ref type="figure">Figure 11</ref>. No pre-selection was performed. While samples from CONV5 look much like abstract art, the samples from fully convolutional layers are much more realistic. This shows that the networks learn a natural image prior that allows them to produce somewhat realistically looking images from random feature vectors. We found that a much simpler sampling procedure of CONV5 FC6 FC7 FC8 <ref type="figure">Figure 11</ref>: Images generated from random feature vectors of top layers of AlexNet.</p><p>fitting a single shifted truncated Gaussian to all feature dimensions produces qualitatively very similar images. These are shown in the supplementary material together with images generated from autoencoders, which look much less like natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have proposed to invert image representations with up-convolutional networks and have shown that this yields more or less accurate reconstructions of the original images, depending on the level of invariance of the feature representation. The networks implicitly learn natural image priors which allow the retrieval of information that is obviously lost in the feature representation, such as color or brightness in HOG or SIFT. The method is very fast at test time and does not require the gradient of the feature representation to be inverted. Therefore, it can be applied to virtually any image representation.</p><p>Application of our method to the representations learned by the AlexNet convolutional network leads do several conclusions: 1) Features from all layers of the network, including the final FC8 layer, preserve the precise colors and the rough position of objects in the image; 2) In higher layers, almost all information about the input image is contained in the pattern of non-zero activations, not their precise values; 3) In the layer FC8, most information about the input image is contained in small probabilities of those classes that are not in top-5 network predictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figures 1 and 3</head><label>3</label><figDesc>show reconstructions of several images from the ImageNet validation set. Normalized reconstruction error of different approaches is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Inversion of shallow image representations. Note how in the first row the color of grass and trees is predicted correctly in all cases, although it is not contained in the features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Reconstructing an image from SIFT descriptors with different methods. (a) an image, (b) SIFT keypoints, (c) reconstruction of<ref type="bibr" target="#b25">[26]</ref>, (d) our reconstruction. Reconstructions Reconstructions from layers of AlexNet with our method (top),<ref type="bibr" target="#b18">[19]</ref> (middle), and autoencoders (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Average normalized reconstruction error depending on the network layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>The effect of color on classification and reconstruction from layer FC8. Left to right: input image, reconstruction from FC8, reconstruction from 5 largest activations in FC8, reconstruction from all FC8 activations except the 5 largest ones. Below each row the network prediction and its confidence are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Interpolation between the features of two images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Network for reconstructing from AlexNet FC8 features. K stands for kernel size, S for stride. layer to reconstruct from). Filters in all (up-)convolutional layers have 5 × 5 spatial size. After each layer we apply leaky ReLU nonlinearity with slope 0.2, that is, r</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Normalized error of different methods when reconstructing from HOG.</figDesc><table>Image 

HOG 
Hoggles [24] 
HOG −1 [19] 
Our 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>from Flickr and modified its</figDesc><table>Image 

CONV3 
CONV4 
CONV5 
FC6 
FC7 
FC8 
CONV3 
CONV4 
CONV5 
FC6 
FC7 
FC8 

No 
per-
turb 

Bin 

Drop 
50 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More precisely, we used CaffeNet, which is almost identical to the original AlexNet.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We acknowledge funding by the ERC Starting Grant VideoLearn (279401). We are grateful to Aravindh Mahendran for sharing with us the reconstructions achieved with the method of Mahendran and Vedaldi <ref type="bibr" target="#b18">[19]</ref>. We thank Jost Tobias Springenberg for comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural Networks for Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Oxford Uni. Press</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From bits to images: Inversion of local binary descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="874" to="887" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distilling the knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inversion of feedforward neural networks: Algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Sharkawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miyamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1536" to="1549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image reconstruction from bag-ofvisual-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inverse mapping of continuous functions using local and global information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Kil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="423" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inversion of multilayer nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kindermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Neural Networks</title>
		<meeting>Int. Conf. on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inverting feedforward neural networks using linear and nonlinear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1271" to="1290" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reconstructing visual experiences from brain activity evoked by natural movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="1641" to="1646" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Track</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vlfeat: an open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1469" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hoggles: Visualizing object detection features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Observer-based iterative fuzzy and neural network model inversion for measurement and control applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Vrkonyi-Kczy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards Intelligent Engineering and Information Technology</title>
		<editor>I. J. Rudas, J. C. Fodor, and J. Kacprzyk</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">243</biblScope>
			<biblScope unit="page" from="681" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reconstructing an image from its local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inverting a connectionist network mapping by back-propagation of error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Annual Conference of the Cognitive Society</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="859" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
