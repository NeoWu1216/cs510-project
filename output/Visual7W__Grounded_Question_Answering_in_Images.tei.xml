<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual7W: Grounded Question Answering in Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Dresden University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual7W: Grounded Question Answering in Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent development of deep learning technologies has achieved successes in many perceptual visual tasks such as object recognition, image classification and pose estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43</ref>]. Yet the status quo of computer vision is still far from matching human capabilities, especially when it comes to understanding an image in all its details. Recently, visual question answering (QA) has been proposed as a proxy task for evaluating a vision system's capacity for deeper image understanding. Several QA datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref> have been released since last year. They contributed valuable data for training visual QA systems and introduced various tasks, from picking correct multiple-choice answers <ref type="bibr" target="#b0">[1]</ref> to filling in blanks <ref type="bibr" target="#b48">[49]</ref>.</p><p>Pioneer work in image captioning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>, sentence-based image retrieval <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40]</ref> and visual QA <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36]</ref> shows promising results. These works aimed at establishing a global association between sentences and images. However, as Flickr30K <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref> and Visual Madlibs <ref type="bibr" target="#b48">[49]</ref> Which paw is lifted?</p><p>What is the dog doing?</p><p>Why is there foam?</p><p>What is the dog standing on?</p><p>Where does this scene take place? <ref type="figure">Figure 1</ref>: Deep image understanding relies on detailed knowledge about different image parts. We employ diverse questions to acquire detailed information on images, ground objects mentioned in text with their visual appearances, and provide a multiple-choice setting for evaluating a visual question answering task with both textual and visual answers.</p><p>suggest, a tighter semantic link between textual descriptions and corresponding visual regions is a key ingredient for better models. As <ref type="figure">Fig. 1</ref> shows, the localization of objects can be a critical step to understand images better and solve image-related questions. Providing these image-text correspondences is called grounding. Inspired by Geman et al.'s prototype of a visual Turing test based on image regions <ref type="bibr" target="#b7">[8]</ref> and the comprehensive data collection of QA pairs on COCO images <ref type="bibr" target="#b24">[25]</ref> such as VQA <ref type="bibr" target="#b0">[1]</ref> and Baidu <ref type="bibr" target="#b6">[7]</ref>, we fuse visual QA and grounding in order to create a new QA dataset with dense annotations and a more flexible evaluation environment. Object-level grounding provides a stronger link between QA pairs and images than global image-level associations. Furthermore, it allows us to resolve coreference ambiguity <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref> and to understand object distributions in QA, and enables visually grounded answers that consist of object bounding boxes.</p><p>Motivated by the goal of developing a model for visual QA based on grounded regions, our paper introduces a dataset that extends previous approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36]</ref> and proposes an attention-based model to perform this task. We collected 327,939 QA pairs on 47,300 COCO images <ref type="bibr" target="#b24">[25]</ref>, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories. Our data collection was inspired by the age-old idea of the W questions in journalism to describe a complete story <ref type="bibr" target="#b21">[22]</ref>. The 7W questions roughly correspond to an array of standard vision tasks: what <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>, where <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">50]</ref>, when <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>, who <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42]</ref>, why <ref type="bibr" target="#b32">[33]</ref>, how <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> and which <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. The Visual7W dataset features richer questions and longer answers than VQA <ref type="bibr" target="#b0">[1]</ref>. In addition, we provide complete grounding annotations that link the object mentions in the QA sentences to their bounding boxes in the images and therefore introduce a new QA type with image regions as the visually grounded answers. We refer to questions with textual answers as telling questions (what, where, when, who, why and how) and to such with visual answers as pointing questions (which). We provide a detailed comparison and data analysis in Sec. 4.</p><p>A salient property of our dataset is the notable gap between human performance (96.6%) and state-of-the-art LSTM models <ref type="bibr" target="#b27">[28]</ref> (52.1%) on the visual QA tasks. We add a new spatial attention mechanism to an LSTM architecture for tackling the visually grounded QA tasks with both textual and visual answers (see Sec. 5). The model aims to capture the intuition that answers to image-related questions usually correspond with specific image regions. It learns to attend to the pertinent regions as it reads the question tokens in a sequence. We achieve state-of-the-art performance with 55.6%, and find correlations between the model's attention heat maps and the object groundings (see Sec. 6). Due to the large performance gap between human and machine, we envision our dataset and visually grounded QA tasks to contribute to a long-term joint effort from several communities such as vision, natural language processing and knowledge to close the gap together.</p><p>The Visual7W dataset constitutes a part of the Visual Genome project <ref type="bibr" target="#b19">[20]</ref>. Visual Genome contains 1.7 million QA pairs of the 7W question types, which offers the largest visual QA collection to date for training models. The QA pairs in Visual7W are a subset of the 1.7 million QA pairs from Visual Genome. Moreover, Visual7W includes extra annotations such as object groundings, multiple choices and human experiments, making it a clean and complete benchmark for evaluation and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vision + Language. There have been years of effort in connecting the visual and textual information for joint learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52]</ref>. Image and video captioning has become a popular task in the past year <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. The goal is to generate text snippets to describe the images and regions instead of just predicting a few labels. Visual question answering is a natural extension to the captioning tasks, but is more interactive and has a stronger connection to real-world applications <ref type="bibr" target="#b2">[3]</ref>.</p><p>Text-based question answering. Question answering in NLP has been a well-established problem. Successful applications can be seen in voice assistants in mobile devices, search engines and game shows (e.g., IBM Waston). Traditional question answering system relies on an elaborate pipeline of models involving natural language parsing, knowledge base querying, and answer generation <ref type="bibr" target="#b5">[6]</ref>. Recent neural network models attempt to learn end-to-end directly from questions and answers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>Visual question answering. Geman et al. <ref type="bibr" target="#b7">[8]</ref> proposed a restricted visual Turing test to evaluate visual understanding. The DAQUAR dataset is the first toy-sized QA benchmark built upon indoor scene RGB-D images. Most of the other datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref> collected QA pairs on Microsoft COCO images <ref type="bibr" target="#b24">[25]</ref>, either generated automatically by NLP tools <ref type="bibr" target="#b35">[36]</ref> or written by human workers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">49]</ref>. Following these datasets, an array of models has been proposed to tackle the visual QA tasks. The proposed models range from probabilistic inference <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51]</ref> and recurrent neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref> to convolutional networks <ref type="bibr" target="#b25">[26]</ref>. Previous visual QA datasets evaluate textual answers on images while omitting the links between the object mentions and their visual appearances. Inspired by Geman et al. <ref type="bibr" target="#b7">[8]</ref>, we establish the link by grounding objects in the images and perform experiments in the grounded QA setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Creating the Visual7W Dataset</head><p>We elaborate on the details of the data collection we conducted upon 47,300 images from COCO <ref type="bibr" target="#b24">[25]</ref> (a subset of images from Visual Genome <ref type="bibr" target="#b19">[20]</ref>). We leverage the six W questions (what, where, when, who, why, and how) to systematically examine a model's capability for visual understanding, and append a 7th which question category. This extends existing visual QA setups <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36]</ref> to accommodate visual answers. We standardize the visual QA tasks with multi-modal answers in a multiple-choice format. Each question comes with four answer candidates, with one being the correct answer. In addition, we ground all the objects mentioned in the QA pairs to their corresponding bounding boxes in the images. The object-level groundings enable examining the object distributions and resolve the coreference ambiguity <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Collecting the 7W Questions</head><p>The data collection tasks are conducted on Amazon Mechanical Turk (AMT), an online crowdsourcing platform. The online workers are asked to write pairs of question and answer based on image content. We instruct the workers to be concise and unambiguous to avoid wordy or speculative questions. To obtain a clean set of high-quality QA pairs, we ask three AMT workers to label each pair as good or bad independently. The workers judge each pair by whether an average person is able to tell the answer when seeing the  image. We accept the QA pairs with at least two positive votes. We notice varying acceptance rates between categories, ranging from 92% for what to 63% for why. The overall acceptance rate is 85.8%. VQA <ref type="bibr" target="#b0">[1]</ref> relied on both human workers and automatic methods to generate a pool of candidate answers. We find that human-generated answers produce the best quality; on the contrary, automatic methods are prone to introducing candidate answers paraphrasing the ground-truth answers. For the telling questions, the human workers write three plausible answers to each question without seeing the image. To ensure the uniqueness of correct answers, we provide the ground-truth answers to the workers, and instruct them to write answers of different meanings. For the pointing questions, the workers draw three bounding boxes of other objects in the image, ensuring that these boxes cannot be taken as the correct answer. We provide examples from the 7W categories in <ref type="figure" target="#fig_0">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Collecting Object-level Groundings</head><p>We collect object-level groundings by linking the object mentions in the QA pairs to their bounding boxes in the images. We ask the AMT workers to extract the object mentions from the QA pairs and draw boxes on the images. We collect additional groundings for the multiple choices of the pointing questions. Duplicate boxes are removed based on the object names with an Intersection-over-Union threshold What is the man wearing? A black shirt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>question ambiguity answer ambiguity</head><p>What is the vehicle with ads on it? A red bus. <ref type="figure">Figure 3</ref>: Coreference ambiguity arises when an object mention has multiple correspondences in an image, and the textual context is insufficient to tell it apart. The answer to the left question can be either gray, yellow or black, depending on which man is meant. In the right example, the generic phrase red bus can refer to both buses in the image. Thus an algorithm might answer correctly even if referring to the wrong bus.</p><p>of 0.5. In total, we have collected 561,459 object bounding boxes, on average 12 boxes per image. The benefits of object-level groundings are three-fold: 1) it resolves the coreference ambiguity problem between QA sentences and images; 2) it extends the existing visual QA setups to accommodate visual answers; and 3) it offers a means to understand the distribution of objects, shedding light on the essential knowledge to be acquired for tackling the QA tasks (see Sec. 4).</p><p>We illustrate examples of coreference ambiguity in <ref type="figure">Fig. 3</ref>. Ambiguity might cause a question to have more than one plausible answers at test time, thus complicating evaluation. Our online study shows that, such ambiguity occurs in 1% of the accepted questions and 7% of the accepted answers. This illustrates a drawback of existing visual QA setups <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref>, where in the absence of objectlevel groundings the textual questions and answers are only loosely coupled to the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparison and Analysis</head><p>In this section, we analyze our Visual7W dataset collected on COCO images (cf. <ref type="table" target="#tab_1">Table 1</ref>, COCO), present its key features, and provide comparisons of our dataset with previous work. We summarize important metrics of existing visual QA datasets in <ref type="table" target="#tab_1">Table 1</ref>. <ref type="bibr" target="#b0">1</ref> Advantages of Grounding The unique feature of our Vi-sual7W dataset is the grounding annotations of all textually mentioned objects (cf. <ref type="table" target="#tab_1">Table 1</ref>, Grounding). In total we have collected 561,459 object groundings, which enables the new type of visual answers in the form of bounding boxes (cf. <ref type="table" target="#tab_1">Table 1</ref>, VisualAns). Examining the object distribution in the QA pairs sheds light on the focus of the questions and the essential knowledge to be acquired for answering them. Our object groundings spread across 36,579 categories (distinct object names), thereby exhibiting a long tail pattern where 85% of the categories have fewer than 5 instances (see <ref type="figure">Fig. 4</ref>). The open-vocabulary annotations of objects, in contrast with traditional image datasets focusing on predefined categories and salient objects <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38]</ref>, provide a broad coverage of objects in the images.</p><p>Human-Machine Performance Gap We expect that a good QA benchmark should exhibit a sufficient performance gap between humans and state-of-the-art models, leaving room for future research to explore. Additionally a nearly perfect human performance is desired to certify the quality of its questions. On Visual7W, we conducted two experiments to measure human performance (cf. Table 1, HumanPerf ), as well as examining the percentage of questions that can be answered without images. Our results show both strong human performance and a strong interdependency between images and QA pairs. We provide the <ref type="bibr" target="#b0">1</ref> We report the statistics of VQA dataset <ref type="bibr" target="#b0">[1]</ref> with its real images and Visual Madlibs <ref type="bibr" target="#b48">[49]</ref> with its filtered hard tasks. The fill-in-the-blank tasks in Visual Madlibs <ref type="bibr" target="#b48">[49]</ref>, where the answers are sentence fragments, differ from other QA tasks, resulting in distinct statistics. We omit some statistics for Baidu <ref type="bibr" target="#b6">[7]</ref> due to its partial release.  <ref type="figure">Figure 4</ref>: Object distribution in telling and pointing QA. The rank of an object category is based on its frequency with rank #1 referring to the most frequent one. The pointing QA pairs cover an order of magnitude more objects than the telling QA pairs. The top 20 object categories indicate that the object distribution's bias towards persons, daily-life objects and natural entities. detailed analysis and comparisons with the state-of-the-art automatic models in Sec. 6.  <ref type="table" target="#tab_3">Table 2</ref> compares Visual7W with VQA <ref type="bibr" target="#b0">[1]</ref> and Facebook bAbI <ref type="bibr" target="#b45">[46]</ref>, which have reported model and human performances. Facebook bAbI <ref type="bibr" target="#b45">[46]</ref> is a textual QA dataset claiming that humans can potentially achieve 100% accuracy yet without explicit experimental proof. For VQA <ref type="bibr" target="#b0">[1]</ref>, numbers are reported for both multiple-choice and open-ended evaluation setups. Visual7W features the largest performance gap (∆), a desirable property for a challenging and long-lasting evaluation task. At the same time, the nearly perfect human performance proves high quality of the 7W questions. QA Diversity The diversity of QA pairs is an important feature of a good QA dataset as it reflects a broad coverage of image details, introduces complexity and potentially requires a broad range of skills for solving the questions. To obtain diverse QA pairs, we decided to rule out binary questions, contrasting Geman et al.'s proposal <ref type="bibr" target="#b7">[8]</ref> and VQA's approach <ref type="bibr" target="#b0">[1]</ref>. We hypothesize that this encourages workers to write more complex questions and also prevents inflating answer baselines with simple yes/no answers. When examining the richness of QA pairs, the length of questions and answers (cf. <ref type="table" target="#tab_1">Table 1</ref>, AvgQLen, AvgALen) is a rough indicator for the amount of information and complexity they contain. The overall average question and answer lengths are 6.9 and 2.0 words respectively. The pointing questions have the longest average question length. The telling questions exhibit a long-tail distribution where 51.2%, 21.2%, and 16.6% of their answers have one, two or three words respectively. Many answers to where and why questions are phrases and sentences, with an average of 3 words. In general, our dataset features long answers where 27.6% of the questions have answers of more than two words (cf. <ref type="table" target="#tab_1">Table 1</ref>, LongAns). In contrast, 89% of answers in VQA <ref type="bibr" target="#b0">[1]</ref>, 90% of answers in DAQUAR <ref type="bibr" target="#b26">[27]</ref> and all answers in COCO-QA <ref type="bibr" target="#b35">[36]</ref> are a single word. We also capture more long-tail answers as our 1,000 most frequent answers only account for 63.5% of all our answers (cf. Table 1, TopAns). Finally we provide human created multiplechoices for evaluation (cf. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Attention-based Model for Grounded QA</head><p>The visual QA tasks are visually grounded, as local image regions are pertinent to answering questions in many cases. For instance, in the first pointing QA example of <ref type="figure" target="#fig_0">Fig. 2</ref> the regions of the window and the pillows reveal the answer, while other regions are irrelevant to the question. We capture this intuition by introducing a spatial attention mechanism similar to the model for image captioning <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Recurrent QA Models with Spatial Attention</head><p>LSTM models <ref type="bibr" target="#b10">[11]</ref> have achieved state-of-the-art results in several sequence processing tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41]</ref>. They have also been used to tackle visual QA tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28]</ref>. These models represent images by their global features, lacking a mechanism to understand local image regions. We add spatial attention <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47]</ref> to the standard LSTM model for visual QA, illustrated in <ref type="figure" target="#fig_1">Fig. 5</ref>. We consider QA as a two-stage process <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>. At the encoding stage, the model memorizes the image and the question into a hidden state vector (the gray box in <ref type="figure" target="#fig_1">Fig. 5</ref>). At the decoding stage, the model selects an answer from the multiple choices based on its memory (the softmax layer in <ref type="figure" target="#fig_1">Fig. 5)</ref>. We use the same encoder structure for all visual QA tasks but different decoders for the telling and pointing QA tasks. Given an image I and a question Q = (q 1 , q 2 , . . . , q m ), we learn the embeddings of the image and the word tokens as follow:</p><formula xml:id="formula_0">v 0 = W i [F (I)] + b i (1) v i = W w [OH(t i )], i = 1, . . . , m<label>(2)</label></formula><p>where F (·) transforms an image I from pixel space to a 4096-dimensional feature representation. We extract the activations from the last fully connected layer (fc7) of a pre-trained CNN model VGG-16 <ref type="bibr" target="#b38">[39]</ref>. OH(·) transforms a word token to its one-hot representation, an indicator column vector where there is a single one at the index of the token in the word vocabulary. The W i matrix transforms the 4096-dimensional image features into the d i -dimensional embedding space v 0 , and the W w transforms the one-hot vectors into the d w -dimensional embedding space v i . We set d i and d w to the same value of 512. Thus, we take the image as the first input token. These embedding vectors v 0,1,...,m are fed into the LSTM model one by one. The update rules of our LSTM model can be defined as follow:</p><formula xml:id="formula_1">i t = σ(W vi v t + W hi h t−1 + W ri r t + b i ) (3) f t = σ(W vf v t + W hf h t−1 + W rf r t + b f ) (4) o t = σ(W vo v t + W ho h t−1 + W ro r t + b o ) (5) g t = φ(W vg v t + W hg h t−1 + W rg r t + b g ) (6) c t = f t ⊙ c t−1 + i t ⊙ g t (7) h t = o t ⊙ φ(c t )<label>(8)</label></formula><p>where σ(·) is the sigmoid function, φ(·) is the tanh function, and ⊙ is the element-wise multiplication operator. The attention mechanism is introduced by the term r t , which is a weighted average of convolutional features that depends upon the previous hidden state and the convolutional features. The exact formulation is as follows: where C(I) returns the 14 × 14 512-dimensional convolutional feature maps of image I from the fourth convolutional layer from the same VGG-16 model <ref type="bibr" target="#b38">[39]</ref>. The attention term a t is a 196-dimensional unit vector, deciding the contribution of each convolutional feature at the t-th step. The standard LSTM model can be considered as a special case with each element in a t set uniformly. W i , b i , W w and all the W s and bs in the LSTM model and attention terms are learnable parameters.</p><formula xml:id="formula_2">e t = w T a tanh(W he h t−1 + W ce C(I)) + b a (9) a t = softmax(e t ) (10) r t = a T t C(I)<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Learning and Inference</head><p>The model first reads the image v 0 and all the question tokens v q1 , v q2 , . . . , v qm until reaching the question mark (i.e., end token of the question sequence). When training for telling QA, we continue to feed the ground-truth answer tokens v a1 , v a2 , . . . , v an into the model. For pointing QA, we compute the log-likelihood of an candidate region by a dot product between its transformed visual feature (fc7) and the last LSTM hidden state (see <ref type="figure" target="#fig_1">Fig. 5</ref>). We use crossentropy loss to train the model parameters with backpropagation. During testing, we select the candidate answer with the largest log-likelihood. We set the hyperparameters using the validation set. The dimensions of the LSTM gates and memory cells are 512 in all the experiments. The model is trained with Adam update rule <ref type="bibr" target="#b17">[18]</ref>, mini-batch size 128, and a global learning rate of 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluate the human and model performances on the QA tasks. We report a reasonably challenging performance delta leaving sufficient room for future research to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experiment Setups</head><p>As the 7W QA tasks have been formulated in a multiplechoice format, we use the same procedure to evaluate hu-man and model performances. At test time, the input is an image and a natural language question, followed by four multiple choices. In telling QA, the multiple choices are written in natural language; whereas, in pointing QA, each multiple choice corresponds to an image region. We say the model is correct on a question if it picks the correct answer among the candidates. Accuracy is used to measure the performance. An alternative method to evaluate telling QA is to let the model predict open-ended text outputs <ref type="bibr" target="#b0">[1]</ref>. This approach works well on short answers; however, it performs poorly on long answers, where there are many ways of paraphrasing the same meaning. We make the training, validation and test splits, each with 50%, 20%, 30% of the pairs respectively. The numbers are reported on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">7W QA Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Human Experiments on 7W QA</head><p>We evaluate human performances on the multiple-choice 7W QA. We want to measure in these experiments 1) how well humans can perform in the visual QA task and 2) whether humans can use common sense to answer questions without seeing the images.</p><p>We conduct two sets of human experiments. In the first experiment (Question), a group of five AMT workers are asked to guess the best possible answers from the multiple choices without seeing the images. In the second experiment (Question + Image), we have a different group of five workers to answer the same questions given the images. The first block in <ref type="table" target="#tab_5">Table 3</ref> reports the human performances on these experiments. We measure the mean accuracy over the QA pairs where we take the majority votes among the five human responses. Even without the images, humans manage to guess the most plausible answers in some cases. Human subjects achieve 35.3% accuracy, 10% higher than chance. The human performance without images is remarkably high (43.9%) for the why questions, indicating that many why questions encode a fair amount of common sense that humans are able to infer without visual cue. However, images are important in the majority of the questions. Human performance is significantly improved when the images are provided. Overall, humans achieve a high accuracy of 96.6% on the 7W QA tasks. <ref type="figure" target="#fig_2">Fig. 6</ref> shows the box plots of response time of the human subjects for telling QA. Human subjects spend double the time to respond when the images are displayed. In addition, why questions take a longer average response time compared to the other five question types. Human subjects spend an average of 9.3 seconds on pointing questions. However, that experiment was conducted in a different user interface, where workers click on the answer boxes in the image. Thus, the response time is not comparable with the telling QA tasks. Interestingly, longer response time does not imply higher performance. Human subjects spend more  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Model Experiments on 7W QA</head><p>Having examined human performance, our next question is how well the state-of-the-art models can perform in the 7W QA task. We evaluate automatic models on the 7W QA tasks in three sets of experiments: without images (Question), without questions (Image) and with images (Question + Image). In the experiments without images (questions), we zero out the image (questions) features. We briefly describe the three models we used in the experiments:</p><p>Logistic Regression A logistic regression model that predicts the answer from a concatenation of image fc7 feature and question feature. The questions are represented by 200dimensional averaged word embeddings from a pre-trained model <ref type="bibr" target="#b28">[29]</ref>. For telling QA, we take the top-5000 most frequent answers (79.2% of the training set answers) as the class labels. At test time, we select the top-scoring answer candidate. For pointing QA, we perform k-means to cluster training set regions by fc7 features into 5000 clusters, used as class labels. At test time, we select the answer candidate closest to the centroid of the predicted cluster.</p><p>LSTM The LSTM model in Malinowski and Fritz <ref type="bibr" target="#b27">[28]</ref> for visual QA with no attention modeling, which can be considered as a simplified version of our full model with the attention terms set to be uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM-Att</head><p>Our LSTM model with spatial attention introduced in Sec. 5, where the attention terms in Eq. (10) determines which region to focus on at each step.</p><p>We report the results in <ref type="table" target="#tab_5">Table 3</ref>. All the baseline models surpass the chance performance (25%). The logistic regression baseline yields the best performance when only the question features are provided. Having the global image features hurts its performance, indicating the importance of understanding local image regions rather than a holistic representation. Interestingly, the LSTM performance (46.2%) significantly outperforms human perfor-</p><formula xml:id="formula_3">A cat.</formula><p>Why is the person holding a knife? To cut the cake with.</p><p>What kind of animal is in the photo?</p><p>At the top.</p><p>Where are the carrots?</p><p>Three.</p><p>How many people are there? We show qualitative results of human experiments and the LSTM models on the telling QA task in <ref type="figure" target="#fig_3">Fig. 7</ref>. Human subjects fail to tell a sheep apart from a goat in the last example, whereas the LSTM model gives the correct answer. Yet humans successfully answer the fourth why question when seeing the image, where the LSTM model fails in both cases.</p><p>The object groundings help us analyzing the behavior of the attention-based model. First, we examine where the model focuses by visualizing the attention terms of Eq. (10). The attention terms vary as the model reads the QA words one by one. We perform max pooling along time to find the maximum attention weight on each of the 14×14 image grid, producing an attention heat map. We see if the model attends to the mentioned objects. The answer object boxes occupy an average of 12% of image area; while the peak of the attention heat map resides in answer object boxes 24% of the time. That indicates a tendency for the model to attend to the answer-related regions. We visualize the atten- For instance, the model attends to the four corners and the borders of the image to look for the carrots in <ref type="figure" target="#fig_4">Fig. 8(c)</ref>.</p><p>Furthermore, we use object groundings to examine the model's behavior on the pointing QA. <ref type="figure" target="#fig_5">Fig. 9</ref> shows the impact of object category frequency on the QA accuracy. We divide the object categories into different bins based on their frequencies (by power of 2) in the training set. We compute the mean accuracy over the test set QA pairs within each bin. We observe increased accuracy for categories with more object instances. However, the model is able to transfer knowledge from common categories to rare ones, generating an adequate performance (over 50%) on object categories with only a few instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we propose to leverage the visually grounded 7W questions to facilitate a deeper understanding of images beyond recognizing objects. Previous visual QA works lack a tight semantic link between textual descriptions and image regions. We link the object mentions to their bounding boxes in the images. Object grounding allows us to resolve coreference ambiguity, understand object distributions, and evaluate on a new type of visually grounded QA. We propose an attention-based LSTM model to achieve the state-of-the-art performance on the QA tasks. Future research directions include exploring ways of utilizing common sense knowledge to improve the model's performance on QA tasks that require complex reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Examples of multiple-choice QA from the 7W question categories. The first row shows telling questions where the green answer is the ground-truth, and the red ones are human-generated wrong answers. The what, who and how questions often pertain to recognition tasks with spatial reasoning. The where, when and why questions usually involve high-level common sense reasoning. The second row depicts pointing (which) questions where the yellow box is the correct answer and the red boxes are human-generated wrong answers. These four answers form a multiple-choice test for each question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Diagram of the recurrent neural network model for pointing QA. At the encoding stage, the model reads the image and the question tokens word by word. At each word, it computes attention terms based on the previous hidden state and the convolutional feature map, deciding which regions to focus on. At the decoding stage, it computes the log-likelihood of an answer by a dot product between its transformed visual feature (fc7) and the last LSTM hidden state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Response time of human subjects on the telling QA tasks. The boxes go from the first quartile to the third quartile of the response time values. The bars in the centers of the boxes indicate the median response time of each category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of human subjects and the state-of-the-art model (LSTM-Att) on multiple-choice QAs. We illustrate the prediction results of six multiple-choice QAs, with and without images. The green answer corresponds to the correct answer to each question, and the rest three are wrong answer candidates. We take the majority votes of five human subjects as the human predictions (H) and the top predictions from the model (M). The correct predictions are indicated by check marks. time on questions with lower accuracy. The Pearson correlation coefficient between the average response time and the average accuracy is −0.135, indicating a weak negative correlation between the response time and human accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Object groundings and attention heat maps. We visualize the attention heat maps (with Gaussian blur) on the images. The brighter regions indicate larger attention terms, i.e., where the model focuses. The bounding boxes show the object-level groundings of the objects mentioned in the answers. mance (35.3%) when the images are not present. Human subjects are not trained before answering the questions; however, the LSTM model manages to learn the priors of answers from the training set. In addition, both the questions and image content contribute to better results. The Question + Image baseline shows large improvement on overall accuracy (52.1%) than the ones when either the question or the image is absent. Finally, our attention-based LSTM model (LSTM-Att) outperforms other baselines on all question types, except the how category, achieving the best model performance of 55.6%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Impact of object category frequency on the model accuracy in the pointing QA task. The x-axis shows the upper bound object category frequency of each bin. The y-axis shows the mean accuracy within each bin. The accuracy increases gradually as the model sees more instances from the same category. Meanwhile, the model manages to handle infrequent categories by transferring knowledge from larger categories. tion heat maps on some example QA pairs in Fig. 8. The top two examples show QA pairs with answers containing an object. The peaks of the attention heat maps reside in the bounding boxes of the target objects. The bottom two examples show QA pairs with answers containing no object. The attention heat maps are scattered around the image grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparisons on Existing Visual Question Answering Datasets</figDesc><table># QA 
# Images 
AvgQLen 
AvgALen 
LongAns 
TopAns 
HumanPerf 
COCO 
MC 
Grounding 
VisualAns 
DAQUAR [27] 
12,468 
1,447 
11.5 ± 2.4 
1.2 ± 0.5 
3.4% 
96.4% 
Visual Madlibs [49] 
56,468 
9,688 
4.9 ± 2.4 
2.8 ± 2.0 
47.4% 
57.9% 
COCO-QA [36] 
117,684 
69,172 
8.7 ± 2.7 
1.0 ± 0 
0.0% 
100% 
Baidu [7] 
316,193 
316,193 
-
-
-
-
VQA [1] 
614,163 
204,721 
6.2 ± 2.0 
1.1 ± 0.4 
3.8% 
82.7% 
Visual7W (Ours) 
327,939 
47,300 
6.9 ± 2.4 
2.0 ± 1.4 
27.6% 
63.5% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Model and Human Performances on QA Datasets</figDesc><table>Model Human 
∆ 

VQA (open-ended) [1] 
0.54 
0.83 
0.29 
VQA (multiple-choice) [1] 
0.57 
0.92 
0.35 
Facebook bAbI [46] 
0.92 
∼1.0 
0.08 
Ours (telling QA) 
0.54 
0.96 
0.42 
Ours (pointing QA) 
0.56 
0.97 
0.41 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 ,</head><label>1</label><figDesc>MC).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Human and model performances in the multiple-choice 7W QA tasks (in accuracy)</figDesc><table>Method 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We would like to thank Carsten Rother from Dresden University of Technology for establishing the collaboration between the Computer Vision Lab Dresden and the Stanford Vision Lab which enabled Oliver Groth to visit Stanford to contribute to this work. We would also like to thank Olga Russakovsky, Lamberto Ballan, Justin Johnson and anonymous reviewers for useful comments. This research is partially supported by a Yahoo Labs Macro award, and an ONR MURI award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vizwiz: nearly real-time answers to visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tatarowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23nd annual ACM symposium on User Interface Software and Technology</title>
		<meeting>the 23nd annual ACM symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Building Watson: An overview of the DeepQA project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual turing test for computer vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Referit game: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What are you talking about? text-to-image coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>arXiv preprint arxiv:1602.07332</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Neveu</surname></persName>
		</author>
		<title level="m">Political journalism: New challenges, new practices. Routledge</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. 2014. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00333</idno>
		<title level="m">Learning to answer questions from image using convolutional neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dating historical color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>ECCV. 2012. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Seeing the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Inferring the why in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5472</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linking people with &quot;their&quot; names using coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint video and text parsing for understanding events and answering queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE MultiMedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: a set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visual Madlibs: Fill in the blank Image Generation and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Reasoning about object affordances in a knowledge base representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
