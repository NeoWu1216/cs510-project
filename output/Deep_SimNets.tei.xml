<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep SimNets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
							<email>cohennadav@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The Hebrew University of Jerusalem</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Sharir</surname></persName>
							<email>or.sharir@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The Hebrew University of Jerusalem</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
							<email>shashua@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The Hebrew University of Jerusalem</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep SimNets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a deep layered architecture that generalizes convolutional neural networks (ConvNets). The architecture, called SimNets, is driven by two operators: (i) a similarity function that generalizes inner-product, and (ii) a log-mean-exp function called MEX that generalizes maximum and average. The two operators applied in succession give rise to a standard neuron but in "feature space". The feature spaces realized by SimNets depend on the choice of the similarity operator. The simplest setting, which corresponds to a convolution, realizes the feature space of the Exponential kernel, while other settings realize feature spaces of more powerful kernels (Generalized Gaussian, which includes as special cases RBF and Laplacian), or even dynamically learned feature spaces (Generalized Multiple Kernel Learning). As a result, the SimNet contains a higher abstraction level compared to a traditional ConvNet. We argue that enhanced expressiveness is important when the networks are small due to run-time constraints (such as those imposed by mobile applications). Empirical evaluation validates the superior expressiveness of SimNets, showing a significant gain in accuracy over ConvNets when computational resources at run-time are limited. We also show that in large-scale settings, where computational complexity is less of a concern, the additional capacity of SimNets can be controlled with proper regularization, yielding accuracies comparable to state of the art ConvNets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks, and convolutional neural networks (ConvNets) in particular, have had a dramatic impact in advancing the state of the art in computer vision, speech analysis, and many other domains (cf. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17]</ref>). It has been demonstrated time and time again, that when ConvNets are trained in an end-to-end manner, they deliver significantly better results than systems relying on manually engineered features.</p><p>The goal of this paper is to introduce a generalization of ConvNets we call Similarity Networks (SimNets), that preserves the simplicity and effectiveness of ConvNets, yet has a higher abstraction level. In a nutshell, the inner-product operator, which lies at the core of the ConvNet architecture, is replaced by an inner-product in "feature space". The feature spaces are controlled by a family of kernel functions which include in particular the conventional (linear) innerproduct as a special case.</p><p>We argue that the incentive for designing deep networks with a higher abstraction level than ConvNets, arises from the need for small networks that could fit into mobile platforms in terms of space and run-time. With small networks the approximation error becomes a limiting factor, which could be ameliorated through network architectures that are based on a higher level of abstraction.</p><p>The SimNet architecture is based on two operators. The first is analogous to, and generalizes, the inner-product operator of neural networks. The second, as special cases, plays the role of non-linear activation and pooling, but has additional capabilities that take SimNets far beyond Con-vNets. In a detailed set of experiments, the SimNet architecture achieves state of the art accuracy using networks with complexity comparable to that of top performing ConvNets. However, when network complexity is limited, SimNets deliver a significant boost in accuracy.</p><p>Recently, the task of reducing run-time complexity of ConvNets is receiving increased attention. For example, a method named FitNets ( <ref type="bibr" target="#b28">[29]</ref>), based on the knowledge distillation principle ( <ref type="bibr" target="#b17">[18]</ref>), has been suggested in order to assist in compressing deep networks. In <ref type="bibr" target="#b33">[34]</ref>, a form of gating inspired by Long Short-Term Memory recurrent networks is introduced, allowing training of very deep and narrow networks. Another line of work considers imposing structural constrains on network weights, such as sparsity, in order to improve run-time efficiency <ref type="bibr">([11, 9, 16, 3, 4]</ref>). Alternatively, network weights may be factorized using matrix or tensor decompositions, reducing storage and computational complexity, at the expense of marginal deterioration in accuracy ( <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b4">5]</ref>). All of these approaches consider ConvNets (or neural networks) as a baseline, and use supplementary techniques to reduce run-time complexity. In this work, we propose the alternative (generalized) SimNet architecture, and argue that it is inherently more efficient than ConvNets. The techniques listed here for reducing run-time complexity of ConvNets could just as well be applied to SimNets, thereby resulting in even more computationally efficient models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The SimNet architecture</head><p>A feed-forward fully-connected neural network, also known as a multilayer perceptron (MLP), is based on a single operator. Given x ∈ R d as input to a layer of neurons, the output of the r'th neuron in the layer is σ(w ⊤ r x + b r ), where σ(·) is a non-linear activation function. An MLP is constructed by forward chaining the input/output operation to create a layered network. The learned parameters of the network are the weight vectors w r and biases b r , per neuron.</p><p>The SimNet architecture consists of two operators. The first operator is a weighted similarity function between an input x ∈ R d and a template z ∈ R d :</p><formula xml:id="formula_0">similarity operator : u ⊤ φ(x, z)</formula><p>where u ∈ R d + is a weight vector and φ : R d × R d → R d is a point-wise similarity mapping. We consider two forms of similarity mappings: the "linear" form φ lin (x, z) i = x i z i , and the "ℓ p " form φ ℓp (x, z) i = −|x i − z i | p defined for p &gt; 0. Note that when setting u = 1, the corresponding similarities reduce to inner-product and p-distance (by the power of p) respectively. Note also that unlike the MLP operator, the similarity does not include a bias term. This functionality is covered, in a much more general sense, by the second operator described below.</p><p>For the second SimNet operator we define MEX -a logmean-exp function:</p><formula xml:id="formula_1">M EX β i=1,...,n {c i } := 1 β log 1 n n i=1 exp{β·c i }<label>(1)</label></formula><p>The parameter β ∈ R spans a continuum between maximum (β → +∞), average (β → 0) and minimum (β → −∞), and for a fixed value of β the function is smooth and exhibits the following "collapsing" property 1 :</p><formula xml:id="formula_2">M EX β {M EX β {c ij } 1≤j≤m } 1≤i≤n = M EX β {c ij } 1≤j≤m,1≤i≤n</formula><p>Given the definition in eqn. 1, the second SimNet operator consists of taking MEX over an input x ∈ R d with a bias vector b ∈ R d -one per input coordinate 2 :</p><formula xml:id="formula_3">MEX operator : M EX β&gt;0 {x i + b i } i=1,..,d</formula><p>Note that unlike a conventional MLP unit which has a bias scalar, a MEX unit has a vector of biases. We may choose to omit part or all of the biases as part of a network design. For example, when all biases are dropped the MEX operator implements a soft trade-off between maximum and average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SimNet MLP</head><p>A SimNet analogy of an MLP with a single hidden layer is obtained by applying the two operators defined in sec. 2 one after the other -similarity followed by MEX. The resulting network is illustrated in <ref type="figure" target="#fig_0">fig. 1(a)</ref>. It includes n hidden similarity units corresponding to weighted templates {(z l , u l )} n l=1 , and k output MEX units associated with bias vectors {b r } k r=1 . Denote by h r (x) the value of the r'th output unit when the network is fed with input x ∈ R d :</p><formula xml:id="formula_4">h r (x) := M EX β {u ⊤ l φ(x, z l ) + b rl } n l=1</formula><p>3 . As a classifier of x into one of k categories, the network predicts the label r for which h r (x) is maximal:</p><formula xml:id="formula_5">y(x) = argmax r=1,...,k M EX β {u ⊤ l φ(x, z l ) + b rl } n l=1</formula><p>As it turns out, SimNet MLP is closely related to kernel machines. In particular, with linear similarity, i.e. with the <ref type="bibr" target="#b0">1</ref> The collapsing property, as well as smoothly generalizing maximum and average, will prove to be essential for us. We are not aware of other functions that meet these three requirements. Specifically, the common softmax function 1 β log i exp{β·c i } collapses and generalizes maximum but does not generalize average, and the alternative softmax function i c i e βc i / i e βc i generalizes maximum and average but does not collapse. <ref type="bibr" target="#b1">2</ref> The MEX operator can be viewed as an "inner-product in log-space". More accurately, if x and b are log-space representations of two vectors c and d respectively (i.e. x i = log c i and b i = log d i ), then</p><formula xml:id="formula_6">M EX β=1 {x i + b i } i = log c, d − log d.</formula><p>In words, the MEX operator (with β = 1) taken over the log-space representations of c and d is equal (up to an additive constant) to the log-space representation of their inner-product. <ref type="bibr" target="#b2">3</ref> Note that with uniform weights (u l ≡ 1), linear similarity mapping φ and β → +∞ we have hr(x) = max z ⊤ l x + b rl n l=1 , i.e. the network outputs are "maxout" units ( <ref type="bibr" target="#b12">[13]</ref>). SimNet MLP is not the first to generalize maxout. Other generalizations have been suggested, notably the recently proposed Lp unit ( <ref type="bibr" target="#b14">[15]</ref>), which is defined by ( l |z ⊤ l x + b rl | p ) 1/p , and tends to max l |z ⊤ l x + b rl | as p → +∞. The differences between SimNet MLP and Lp unit as maxout generalizations are: (i) Lp unit generalizes maximum of absolute values which only coincides with maxout if the arguments are non-negative, and (ii) Lp unit tries to realize maxout with a single operator whereas SimNet MLP implements maxout with a succession of two operators.</p><p>inner-product operator on which neural networks are based, it is a support vector machine (SVM) based on the Exponential kernel. Replacing the linear similarity with ℓ p boosts the abstraction level of SimNet MLP, by lifting it to a Generalized Multiple Kernel Learning (GMKL, <ref type="bibr" target="#b36">[37]</ref>) engine with a Generalized Gaussian kernel. The remainder of this section provides the details.</p><p>SimNet MLP outputs can be written as:</p><formula xml:id="formula_7">h r (x) = M EX β {u ⊤ l φ(x, z l ) + b rl } n l=1 = 1 β ln 1 n n l=1 α rl exp β d i=1 u l,i φ(x, z l ) i = σ n l=1 α rl · K θ (x, z l )</formula><p>where α rl := exp{βb rl }, θ = (φ, u), and σ(t) = (1/β) ln(t/n) is a non-linear activation function. The mapping K θ for the linear and ℓ p similarities takes the following forms:</p><formula xml:id="formula_8">K lin (x, z) = exp βx ⊤ z K ℓp (x, z l ) = exp −β d i=1 u l,i |x i − z l,i | p</formula><p>K lin is known as the Exponential kernel ( <ref type="bibr" target="#b29">[30]</ref>), and K ℓp is a GMKL. Specifically, fixing uniform weights (u l ≡ 1) and p ≤ 2 reduces K ℓp to what is known as the Generalized Gaussian kernel. For the particular cases p = 2 and p = 1 we get the radial basis function (RBF) and Laplacian kernels respectively. When the weights u l and/or order p are learned, the exact underlying kernel is selected during training and we amount at a GMKL. Denoting by ψ θ a feature mapping associated with K θ , we get:</p><formula xml:id="formula_9">h r (x) = σ ( ψ θ (x), w r )</formula><p>where w r := n l=1 α rl ψ θ (z l ) is a learned vector in feature space. We thus conclude that SimNet MLP output units are "neurons in feature space", where the space corresponds to the Exponential kernel in the case of linear similarity, and to the Generalized Gaussian kernel in the case of ℓ p similarity with fixed weights u l and order p. When the weights and/or order are learned, the feature space is selected during training, which is equivalent to saying that SimNet MLP is a GMKL.</p><p>One may ask if perhaps a different choice of kernel, more elaborate than Generalized Gaussian, will suffice in order to capture SimNet MLP with ℓ p similarity and learned weights as a simple kernel machine. Apparently, as theorem 1 (proven in <ref type="bibr" target="#b7">[8]</ref>) shows, such a kernel does not exist, i.e. a GMKL is indeed necessary in order to represent Sim-Net MLP in all its glory. Theorem 1. For any dimension d ∈ N, and constants c &gt; 0 and p &gt; 0, there are no mappings Z :</p><formula xml:id="formula_10">R d → R d and U : R d → R d + and a kernel K : (R d × R d + ) × (R d × R d + ) → R d × R d + , such that for all z, x ∈ R d and u ∈ R d + : K ([Z(x), U (x)], [z, u]) = exp −c d i=1 u i |x i − z i | p .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep SimNets for processing images</head><p>In the previous section we presented the basic MLP version of SimNets. In this section we describe two (orthogonal) directions of extension. The first is the addition of locality, sharing and pooling for processing images (Sim-Net MLPConv, sec. 4.1), while the second focuses on deepening the network (adding layers) for enhanced capacity (sec. 4.3). In this context we introduce a "whitened" ℓ p similarity layer through a succession of a convolution (linear similarity) followed by ℓ p similarity with receptive field 1 × 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">SimNet MLPConv</head><p>The extension of SimNet MLP for processing images follows the line of the MLPConv structure suggested in <ref type="bibr" target="#b25">[26]</ref>, and we accordingly refer to it as SimNet MLPConv. In particular, <ref type="bibr" target="#b25">[26]</ref> convolved a standard MLP across an incoming 3D array by successively applying it to patches and stacking the outputs in a spatially coherent manner. This results in a bank of feature maps, which may be summarized into prediction scores through global average pooling. Sim-Net MLPConv follows the same principles -a SimNet MLP is convolved across an incoming 3D array, and the resulting feature maps are summarized via global MEX pooling. An illustration of SimNet MLPConv is provided in <ref type="figure" target="#fig_0">fig. 1</ref>(c). In the figure, x ij ∈ R hwD refers to the input patch in location ij, z l ∈ R hwD and u l ∈ R hwD + denote similarity templates and weights respectively, φ : R hwD × R hwD → R hwD is the similarity mapping (linear or ℓ p ), β 1 ∈ R and b rl ∈ R are the MEX parameter and offsets of the underlying Sim-Net MLP, and β 2 ∈ R is the MEX parameter of the final global pooling layer.</p><p>When used to classify images, the prediction rule associated with SimNet MLPConv is given by:</p><formula xml:id="formula_11">ŷ(input) = argmax r M EX β2 M EX β1 u ⊤ l φ(x ij , z l ) + b r,l l i,j .</formula><p>Setting β 1 = β 2 = β, and using the collapsing property of MEX, we get a "patch-based" version of SimNet MLP's classification:</p><formula xml:id="formula_12">y(input) = argmax r M EX β i,j,l {u ⊤ l φ(x ij , z l ) + b r,l }</formula><p>It can be shown ( <ref type="bibr" target="#b7">[8]</ref>) that all results put forth in sec. 3 for relating SimNet MLP to kernel machines apply to SimNet MLPConv as well, but with the underlying kernels being based on "patch-representations". In other words, SimNet MLPConv -a "patch-based" extension of SimNet MLP, maintains all kernel relations of the latter, with a "patchbased" extension of the underlying kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Whitening with convolutional layer</head><p>We now describe a simple yet powerful addition to the ℓ p similarity operator. Recall that the ℓ p similarity between an input x ∈ R d and a template z ∈ R d with weights</p><formula xml:id="formula_13">u ∈ R d + , is defined by − d i=1 u i |x i − z i | p .</formula><p>Up to a constant that depends on u (and p), this is equal to the log probability density of the input x being drawn from a Generalized Gaussian distribution with independent components, shape p, mean z, and scales u −1/p . These ideas are further developed in sec. 5, however it is clear at this point that in order to capture this probabilistic model, it would be desirable for the input x to have statistically independent coordinates. Common practice in such cases is to seek for a matrix W for which the linearly transformed vector W x has independent coordinates. This is referred to in the literature as ICAindependent component analysis ( <ref type="bibr" target="#b18">[19]</ref>). Assuming such a matrix is found, it would then be natural to "whiten" inputs, i.e. multiply them by W , before measuring their ℓ p similarities to weighted templates. Besides better compliance with the coordinate independence assumption, this also gives rise to the possibility of dimensionality reduction. In particular, we may set the matrix W to cancel-out low-variance principal components of x, thereby producing whitened vectors of a lower dimension. This can be useful for both noise reduction and computational efficiency.</p><p>In the context of SimNet MLPConv, adding support for whitening before ℓ p similarity is simple -it merely requires a convolutional layer (linear similarity) followed by an ℓ p similarity layer with receptive field 1 × 1. Such a construct, which we refer to as conv→ ℓ p -sim, is illustrated in <ref type="figure" target="#fig_0">fig. 1(b)</ref>. In this figure, input patches x ij are transformed into d-dimensional vectors y ij by a convolutional layer with d filters w t that hold the rows of the whitening matrix W . The whitened vectors y ij are then matched against n weighted templates in the ℓ p similarity layer, producing n similarity maps as output. To recap, one may add whitening to ℓ p similarity by replacing the similarity layer with a conv→ ℓ p -sim structure, which consists of convolution followed by 1 × 1 similarity.</p><p>In sec. 5 we describe how to pre-train a conv→ ℓ p -sim structure, and in particular how to initialize the filters so that they perform the whitening transformation they are intended for. Before that however, we show how SimNet MLPConv can be extended into an image processing Sim-Net of arbitrary depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Going deep with SimNet MLPConv</head><p>After laying out the basic SimNet construct (SimNet MLP -sec. 3), equipping it with spatial structure (SimNet MLPConv -sec. 4.1), and adding whitening to its ℓ p similarity (conv→ ℓ p -sim -sec. 4.2), we are finally in a position to define an arbitrarily deep SimNet for processing images.</p><p>Our starting point is SimNet MLPConv with whitened ℓ p similarity. This network accounts for a single layer (conv→ ℓ p -sim) followed by a classifier (classification MEX and global MEX pooling). Adding depth to the network simply amounts to appending preceding conv→ ℓ p -sim layers, optionally separated by MEX pooling. A general L-layer Sim-Net following this architectural prescription is illustrated in <ref type="figure" target="#fig_0">fig. 1(d)</ref>. In this structure, conv→ ℓ p -sim layers measure whitened ℓ p similarities of incoming patches to weighted templates, MEX pooling operations summarize spatial regions in similarity maps by MEX'ing them together (note that both average pooling and max pooling are special cases of this), the MEX classification uses its offsets b rl to classify each location in the final similarity maps, and the final global MEX pooling summarizes the local classifications into global class scores. The parameters that may be learned during training are: W (1) . . .W (L) -linear filters in conv→ ℓ p -sim; z </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Pre-training</head><p>In this section we briefly describe a method for pretraining an L-layer SimNet as illustrated in <ref type="figure" target="#fig_0">fig. 1(d)</ref>. and orders p (1) , ..., p (L) ), assuming predetermined local MEX pooling parameters (β (1) , ..., β (L) ). Two attractive properties of the scheme are: (i) it is unsupervised (does not require any labels), and (ii) it gives rise to automatic selection of the number of channels in the convolutions and similarities of conv→ ℓ p -sim layers.</p><p>The initialization is applied layer by layer in a forward sweep, thus in order for it to be defined, it suffices to consider a single conv→ ℓ p -sim layer ( <ref type="figure" target="#fig_0">fig. 1(b)</ref>). Recall from sec. 4.2 that we interpret the convolution in conv→ ℓ psim as a linear transformation that whitens (and possibly reduces the dimension of) input patches prior to similarity measurements. Accordingly, we initialize its filters w 1 , ..., w d as the rows of a whitening matrix W estimated via ICA ( <ref type="bibr" target="#b18">[19]</ref>) on patches.</p><p>Turning to the initialization of similarity templates (z 1 , ...z n ), weights (u 1 , ..., u n ) and order (p), we recall that an ℓ p similarity between an input y ∈ R d and a template z ∈ R d with weights u ∈ R d + , is defined to be − d t=1 u t |y t − z t | p . Consider now a probability distribu-tion over R d defined by a mixture of n Generalized Gaussians (with priors λ l ≥ 0, l λ l = 1), all having the same shape parameter (β &gt; 0), and each having independent coordinates with separate scales and means (α l,t &gt; 0 and µ l,t ∈ R respectively, for coordinate t of component l):</p><formula xml:id="formula_14">P (y) = n l=1 λ l d t=1 β 2α l,t Γ(1/β) e −(|yt−µ l,t |/α l,t ) β</formula><p>The log probability density of a vector drawn from this distribution being equal to y and originating from component l is:</p><formula xml:id="formula_15">log P (y ∧ comp. l) = − d t=1 α −β l,t |y t − µ l,t | β + c l , where c l := log λ l d t=1 β 2α l,t Γ(1/β)</formula><p>is a constant that does not depend on y. This implies that if we model whitened patches y ij with a Generalized Gaussian mixture as above, initializing the similarity templates via z l,t = µ l,t , the weights via u l,t = α −β l,t and the order via p = β would give:</p><formula xml:id="formula_16">u ⊤ l φ ℓp (y ij , z l ) = log P (y ∧ comp. l) − c l</formula><p>In words, similarity channel l would hold, up to a constant, the probabilistic heat map of component l and the whitened patches y ij . This observation suggests estimating the parameters of the mixture (shape β, scales α l,t and means µ l,t ) based on whitened patches (via EM, cf. <ref type="bibr" target="#b0">[1]</ref>), and initializing the similarity parameters accordingly. We note in passing that it is possible to append additive biases b l to the similarity (through offsets of the succeeding MEX operator), in which case initializing these via b l = c l would make the probabilistic heat maps exact (not up to a constant). Finally, as stated above, the initialization scheme presented induces an automatic selection of the number of convolution and similarity channels in conv→ ℓ p -sim. The number of convolution channels corresponds to the dimension to which input patches are reduced during whitening, thus may be set via methods for estimating effective dimensionality of data (e.g. <ref type="bibr" target="#b30">[31]</ref>). Similarity channels correspond to components in the mixture estimated for whitened patches, thus may be set via methods for estimating the number of components in a mixture (e.g. <ref type="bibr" target="#b1">[2]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>To evaluate the effectiveness of SimNets, we compared them against alternative ConvNets in three experiments of increasing complexity. In the first experiment, we ran a single layer SimNet against an equivalent single layer Con-vNet, and studied the effect of model size (number of convolution/similarity channels) on the accuracy of the two networks. In a second experiment, we compared compact two layer SimNets against the best performing publicly available ConvNet we are aware of that has comparable complexity. In the third and final experiment, we constructed a large three layer SimNet designed to compete against state of the art ConvNets. Our experiments demonstrate that Sim-Nets are significantly more accurate than ConvNets when networks are constrained to be compact, i.e. when computational load at run-time is limited. This complies with our theoretical analysis in sec. 3, which shows that weighted ℓ p similarity exhibits an expressive power that goes beyond kernel machines, whereas linear similarity (the case associated with ConvNets) is fully captured by the Exponential kernel. Asymptotically as the dimension increases, even a simple kernel machine becomes expressive enough for a given problem, and more elaborate expressiveness may actually be a burden, as it aggravates overfitting. Nonetheless, we see in our experiments that with proper regularization, large-scale SimNets achieve accuracies comparable to state of the art ConvNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental details</head><p>The datasets used in our experiments are CIFAR-10 and CIFAR-100 ( <ref type="bibr" target="#b21">[22]</ref>), as well as SVHN ( <ref type="bibr" target="#b26">[27]</ref>). These three datasets together form an image recognition benchmark that is diverse and challenging on one hand, yet simple enough to enable granular controlled experiments such as those needed to evaluate a new architecture. All datasets consist of 32x32 color images. SVHN (Street View House Numbers) represents a rather simple classification benchmark, where various methods are known to produce nearhuman accuracies. It contains approximately 600K images for training and 26K images for testing, partitioned into 10 categories that correspond to the digits 0 through 9. CIFAR-100 contains 50K images for training and 10K images for testing, equally partitioned into 100 categories. With a relatively large number of categories, and only a few hundred training examples per class, CIFAR-100 represents a challenging classification task. CIFAR-10 contains 50K images for training and 10K images for testing, equally par-titioned into 10 categories. It brings forth a balanced tradeoff between the simplicity of SVHN and the complexity of CIFAR-100, and accordingly served as the central dataset throughout our experiments. Namely, all cross-validations were carried out on CIFAR-10 (with 10K training images held out for validation), with SVHN and CIFAR-100 used for final evaluation only. In terms of implementation, we have integrated SimNets into Caffe toolbox ( <ref type="bibr" target="#b20">[21]</ref>), with the aim of making our code publicly available in the near future.</p><p>In all our experiments, we trained both SimNets and ConvNets by minimizing softmax loss using SGD with Nesterov acceleration ( <ref type="bibr" target="#b34">[35]</ref>). Batch size, momentum, weight decay and learning rate were chosen through crossvalidation, though we observed, at least for the case of SimNets, that the following choices consistently produced good results: batch size 128, momentum 0.9, weight decay 0.0001 and learning rate 0.01 decreasing by a factor of 10 after 200 and 250 epochs (out of 300 total). Unlike ConvNets which are mostly initialized randomly nowadays ( <ref type="bibr" target="#b22">[23]</ref>), SimNets are naturally pre-trained using statistical estimation methods (sec. 5). For computational efficiency, we implemented stochastic versions of these algorithms. Unless otherwise stated, all reported SimNet results were obtained using its pre-training scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Single layer SimNet</head><p>As an initial experiment we compared a single layer Sim-Net, i.e. a SimNet MLPConv with whitened ℓ p similarity (conv→ ℓ p -sim), to an equivalent single layer ConvNet defined for this purpose. We chose to design the ConvNet in accordance with the prescription given by Coates et al. in their study of single layer networks ( <ref type="bibr" target="#b6">[7]</ref>). The resulting network is illustrated in <ref type="figure" target="#fig_3">fig. 2(a)</ref>. As can be seen, it includes a single convolutional layer with 5x5 receptive field and ReLU activation, followed by max pooling over quadrants and dense linear classification. To align the SimNet with this structure, we applied the whitened similarity to patches with spatial size 5x5, and since these have relatively low dimension already (75), we did not reduce it further during whitening.</p><p>To compare the networks as they vary in size (and run-time complexity), we set the number of convolution/similarity channels (denoted n in <ref type="figure" target="#fig_0">fig. 2(a) and fig. 1(c)</ref>) to 50, 100, 200, 400 and 800. Since the ConvNet requires less computations for a given number of channels, we also tried it with 1600 and 3200 channels. CIFAR-10 crossvalidation accuracies produced by the ConvNet, the SimNet with ℓ 1 similarity, and the SimNet with ℓ 2 similarity, are plotted in <ref type="figure" target="#fig_3">fig. 2(b)</ref> against the number of FLOPs (floatingpoint operations) required to classify an image <ref type="bibr">4 5</ref> . As can be seen, for a given computational budget, the accuracies of ℓ 1 and ℓ 2 SimNets are comparable, whereas the ConvNet falls significantly behind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Two layer SimNet</head><p>The purpose of this second experiment was to compare SimNets against the best publicly available compact Con-vNet we could find. We are interested in a clean SimNet vs. ConvNet architectural comparison, and thus did not include in the experiment model compression techniques such as those listed in sec. 1 (e.g. FitNets <ref type="bibr" target="#b28">[29]</ref>), which may be applied to both architectures. An additional reason to exclude these techniques, as well as other works dealing with compact ConvNets (e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>), is that all results they report relate to networks that are significantly larger than those we are interested in evaluating, in many cases too large to fit a real-time mobile application. With the stated purpose of this experiment being a comparison against an off-the-shelf ConvNet that was not altered by us, we eventually chose to work against the compact CIFAR-10 ConvNet that comes built-in to Caffe, the structure of which is illustrated in <ref type="figure" target="#fig_3">fig. 2(c)</ref>. As the figure shows, the network includes three 5x5 convolutions, each followed by ReLU activation and pooling. Two dense linear layers (separated by ReLU) map the last convolutional layer into network outputs (class scores). The SimNet to which we compared Caffe ConvNet is a two layer network that follows the general structure outlined in <ref type="figure" target="#fig_0">fig. 1</ref> receptive field and 32 channels in the first similarity layer, 5x5 receptive field and 64 channels in the second similarity layer, and MEX pooling between the similarities fixed to 3x3 max pooling with stride 2.</p><p>The networks were initially evaluated on CIFAR-10. Training hyper-parameters for the SimNet were configured via cross-validation, whereas for Caffe ConvNet we used the values that come built-in to Caffe. After measuring CIFAR-10 test accuracies, the same settings (network architectures and training hyper-parameters) were used to evaluate test accuracies on SVHN. For evaluation of test accuracies on CIFAR-100, we again used the exact same settings as in CIFAR-10, but this time increased the number of output channels in both networks from 10 to 100. The results of this experiment are summarized in table 1. As can be seen, the SimNet is roughly twice as efficient as Caffe ConvNet, yet achieves significantly higher accuracies on the more challenging benchmarks (CIFAR-10 and CIFAR-100). On SVHN accuracies are comparable, the reason being that in this simple benchmark classification error is dominated by overfit, to which the enhanced expressiveness of SimNets does not contribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Three layer SimNet</head><p>In the previous experiments we have seen that SimNets are more accurate than ConvNets when networks are constrained to be compact, i.e. when classification run-time is limited. In such a setting, the lower approximation error of SimNets plays an important role. In contrast, when networks are over-specified (i.e. are much larger than necessary in order to model the problem at hand) -standard practice for achieving state of the art accuracy, the approximation error is virtually zero, and the advantage of the Sim-Net architecture fades. Moreover, the additional expressive power of SimNets could actually be a burden, as additional regularization for controlling overfit would be required. It is therefore of interest to explore the ability of SimNets to reach state of the art accuracy with over-specified networks. This is the aim of our third and final experiment, carried out on CIFAR-10.</p><p>In this experiment we used a three layer SimNet as described in <ref type="figure" target="#fig_0">fig. 1(d)</ref>, with the following architectural choices (determined via cross-validation): ℓ 2 similarities; 192 similarity channels in all three layers with receptive field sizes 5x5, 5x5 and 3x3 (respectively); max pooling after layer 1, average pooling after layer 2, in both cases pooling windows are 3x3 in size with stride 2 between them. We trained the network with basic data augmentation, and regularized using multiplicative Gaussian noise 6 in conv→ ℓ p -sim layers. We did not make use of ensembles ( <ref type="bibr" target="#b5">[6]</ref>) or aggressive data augmentation that includes rescaling images ( <ref type="bibr" target="#b13">[14]</ref>). These practices are known to improve accuracy, but are orthogonal to the SimNet vs. ConvNet distinction. We did not include them in our study in order to facilitate a simpler comparison between the two architectures. <ref type="table" target="#tab_1">Table 2</ref> draws a comparison between the test accuracy reached by the Sim-Net and reported state of the art results that did not make use of ensembles or aggressive data augmentation. As the table shows, SimNets compare to state of the art ConvNets, even in the over-specified setting.</p><p>As a final sanity check, we compared extremely compact versions of our three layer SimNet and Network in Network (NiN, <ref type="bibr" target="#b25">[26]</ref>) <ref type="bibr" target="#b6">7</ref> . Specifically, we changed the number of channels in all layers of both networks to 10, and removed dropout (NiN) and multiplicative Gaussian noise (SimNet), leaving all other hyper-parameters intact. The resulting networks had only 5K parameters each, and required just 3.5M FLOPs to classify an image. With such limited resources we expect the SimNet to benefit from its inherent expressiveness, and indeed, it outperformed NiN significantly, providing 76.8% accuracy compared to 72.3% reached by NiN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented a deep layered architecture called SimNets that generalizes convolutional neural networks. The architecture is driven by two operators: (i) the similarity operator, which is a generalization of the inner-product operator on which ConvNets are based, and (ii) the MEX operator, that can realize non-linear activation and pooling, but has additional capabilities that make SimNets a powerful generalization of ConvNets. An interesting property of the Sim-Net architecture is that applying its two operators in succession -similarity followed by MEX, results in what can be viewed as an artificial neuron in a high-dimensional feature space (sec. 3). This also holds for the more elaborate image processing SimNet incorporating locality, sharing and pooling (sec. 4.1). The feature spaces realized by SimNets depend on the choice of similarity type: linear or ℓ p with/without weights. We have shown that the simplest setting using linear similarity (corresponding to regular convolution) realizes the feature space of the Exponential kernel, while ℓ p settings realize feature spaces of more powerful kernels (Generalized Gaussian, which includes as special cases RBF and Laplacian), or even dynamically learned feature spaces (Generalized Multiple Kernel Learning). These observations suggest that SimNets, when equipped with ℓ p similarity, have higher abstraction level than ConvNets, which correspond to linear similarity.</p><p>We argue that a higher abstraction level for the basic network building blocks carries with it the advantage of obtaining higher accuracies with small networks, an important trait for mobile and real-time applications. Through a detailed set of experiments we validated the conjecture of higher accuracy for small networks, and we have also shown that SimNets can achieve state of the art accuracy in large-scale settings where computational efficiency is not a concern (and thus the higher abstraction per given network size is not an advantage).</p><p>Finally, the SimNet architecture is endowed with a natural pre-training scheme based on unlabeled data. Besides its aid in training, the scheme also has the potential of determining the number of channels in hidden layers based on statistical analysis of patterns generated in previous layers. This implies that the structure of SimNets can potentially be determined automatically based on (unlabeled) training data. Future work includes a study of this capability, and more generally, further analysis of probabilistic properties of SimNets and unsupervised/supervised algorithms derived thereof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) SimNet MLP -SimNet analogy of MLP with single hidden layer (sec. 3) (b) conv→ ℓp-sim structure -implements whitened ℓp similarity (sec. 4.2) (c) SimNet MLPConv -single layer SimNet for processing images (sec. 4.1) (d) L-layer SimNet for processing images (sec. 4.3). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and weights in conv→ ℓ p -sim; p<ref type="bibr" target="#b0">(1)</ref> . . .p (L)similarity orders in conv→ ℓ p -sim; β<ref type="bibr" target="#b0">(1)</ref> . . .β (L) -MEX parameters in local pooling; β (c) -MEX parameter in classification; b rl -MEX offsets in classification; β (p) -MEX parameter in global pooling. In the following section we describe methods for initializing these parameters prior to training (pre-training).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Our initialization scheme covers the parameters of conv→ ℓ p -sim layers (linear filters W (1) , ..., W (L)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>(a) Single layer ConvNet compared against single layer SimNet on CIFAR-10 (b) CIFAR-10 cross-validation accuracies of single-layer networks as a function of the number of floating-point operations required to classify an instance (c) Caffe ConvNet compared against two layer SimNet on CIFAR-10 and SVHN (for CIFAR-100, number of output units increased from 10 to 100). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>(d), with ℓ 2 similarity and architectural choices taken to maximize the alignment with Caffe ConvNet: 5x5</figDesc><table>Network 
Acc. (%) FLOP Param. 
CIFAR-10 
Caffe ConvNet 
81.1 
24.8M 145.6K 
Two layer SimNet 
85.5 
14.2M 
64.6K 
SVHN 
Caffe ConvNet 
94 
24.8M 145.6K 
Two layer SimNet 
93.8 
14.2M 
64.6K 
CIFAR-100 
Caffe ConvNet 
52.4 
24.8M 151.4K 
Two layer SimNet 
54.6 
14.6M 
70.3K 

Table 1. Two layer SimNet vs. Caffe ConvNet on CIFAR-10, 
SVHN and CIFAR-100 -comparison of test accuracies, number of 
floating-point operations required to classify an image, and num-
ber of learned parameters. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Three layer SimNet vs. state of the art ConvNets on CIFAR-10 (ensemble and aggressive data augmentation methods excluded) -comparison of test accuracies.</figDesc><table>Method 
Acc. (%) 
Network in Network ([26]) 
91.19 
Deeply Supervised Nets ([25]) 
92.03 
Highway Network ([34]) 
92.4 
ALL-CNN ([32]) 
92.75 
Three layer SimNet 
92.18 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In this paper, we consider FLOPs to be a measure of computational complexity. We do not compare actual run-times, as our implementation of SimNets is relatively naïve, not nearly as efficient as the highly optimized ConvNet code that comes built-in to Caffe. One may argue that like Caffe, many other hardware or software platforms are specifically designed for convolutions, and therefore ConvNets have a computational edge over SimNets. While this is true for some off-the-shelf systems, our goal in this paper is to address inherent algorithmic complexities, not specific platforms currently in the market.<ref type="bibr" target="#b4">5</ref> To circumvent the computational price of exp and log functions included in SimNets, we used approximations that require up to 10 FLOPs per operation. The resulting degradation in accuracy is marginal.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">This regularization technique was shown to be more effective than dropout (<ref type="bibr" target="#b32">[33]</ref>), and better suits the nature of SimNets (zeroing out an input coordinate does not neutralize its effect on ℓp similarity).<ref type="bibr" target="#b6">7</ref> We chose to work against NiN since it bears an architectural resemblance to our SimNet, thus it was clear how both networks can be made compact in an analogous way.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Ronen Tamari for his dedicated contribution to the experiments. The work is partly funded by Intel grant ICRI-CI 9-2012-6133 and ISF grant 1790/12. Nadav Cohen is supported by a Google Fellowship in Machine Learning.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image thresholding based on the em algorithm and the generalized gaussian distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakoub</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="619" to="634" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An entropy criterion for assessing the number of clusters in a mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilda</forename><surname>Soromenho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="212" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compressing Neural Networks with the Hashing Trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Compressing Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno>CoRR abs/1506.04449</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An exploration of parameter redundancy in deep networks with circulant projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multicolumn deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simnets: A generalization of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Memory Bounded Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohli</surname></persName>
		</author>
		<idno>CoRR abs/1412.1442</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Per-foratedCNNs: Acceleration through Elimination of Redundant Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CoRR abs/1202.2745, cs.CV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7155</idno>
		<title level="m">Learning compact convolutional neural networks with nested dropout</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Fractional max-pooling. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learned-norm pooling for deep feedforward and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning both Weights and Connections for Efficient Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CoRR abs/1202.2745, cs.NE</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<title level="m">Scaling up end-to-end speech recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speeding up Convolutional Neural Networks with Low Rank Expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto, Tech. Rep</orgName>
		</respStmt>
	</monogr>
	<note>Computer Science Department</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speedingup Convolutional Neural Networks Using Fine-tuned CP-Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Deeply-supervised nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tensorizing Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">FitNets: Hints for Thin Deep Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1412.6550</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian estimation of the number of principal components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abd-Krim</forename><surname>Seghouane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="562" to="568" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting. The Journal of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training Very Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">More generality in efficient multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bodla Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep Fried Convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. Efficient and accurate approximations of nonlinear convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Compact deep convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Nagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosung</forename><surname>Kang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
