<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from the Mistakes of Others: Matching Errors in Cross-Dataset Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktoriia</forename><surname>Sharmanska</surname></persName>
							<email>sharmanska.v@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">SMiLe CLiNiC</orgName>
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Novi</forename><surname>Quadrianto</surname></persName>
							<email>n.quadrianto@sussex.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">SMiLe CLiNiC</orgName>
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from the Mistakes of Others: Matching Errors in Cross-Dataset Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Can we learn about object classes in images by looking at a collection of relevant 3D models? Or if we want to learn about human (inter-)actions in images, can we benefit from videos or abstract illustrations that show these actions? A common aspect of these settings is the availability of additional or privileged data that can be exploited at training time and that will not be available and not of interest at test time. We seek to generalize the learning with privileged information (LUPI) framework, which requires additional information to be defined per image, to the setting where additional information is a data collection about the task of interest. Our framework minimizes the distribution mismatch between errors made in images and in privileged data. The proposed method is tested on four publicly available datasets: Image+ClipArt, Image+3Dobject, and Im-age+Video. Experimental results reveal that our new LUPI paradigm naturally addresses the cross-dataset learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vapnik et al. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref> introduced learning with privileged information (LUPI) as a learning with teacher paradigm, where at the training stage, a teacher gives some additional explanation x ⋆ i about an example x i . LUPI has been shown useful in a variety of learning scenarios such as ranking <ref type="bibr" target="#b27">[28]</ref>, categorization <ref type="bibr" target="#b36">[37]</ref>, structured prediction <ref type="bibr" target="#b8">[9]</ref>, data clustering <ref type="bibr" target="#b7">[8]</ref>, metric learning <ref type="bibr" target="#b9">[10]</ref>, face/gesture recognition <ref type="bibr" target="#b37">[38]</ref>, glaucoma detection <ref type="bibr" target="#b6">[7]</ref>, and recently learning with annotation disagreements <ref type="bibr" target="#b26">[27]</ref>. Most LUPI methods (e.g. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14]</ref>) follow the assumption that the extra information is useful to discriminate between easy and difficult examples. This knowledge is then used to determine the influence of each instance in the training process. Specifically, one puts less emphasis or even ignores difficult instances during training in hope that this will improve performance. Reflecting on how per-instance privileged information can be used to identify whether this instance is  <ref type="figure">Figure 1</ref>: In this work, we propose a framework to solve vision tasks in images by acquiring knowledge from the mistakes committed by other data collections (videos, clip arts, and 3D models) when learning the same concepts.</p><p>an easy or a difficult one, we ask the following question: is it possible to transfer easiness and hardness in a class sense instead of per-instance?</p><p>This paper seeks to advice the learner to acquire knowledge from the mistakes committed by others when learning the same concept. One will learn that making errors on an example-by-example basis is unavoidable, but one will make the right decisions at a larger scale by minimizing divergence between own mistakes and others'. We will explore a distribution matching over the mistakes in original and privileged representations as a principled approach to achieve the class-level information transfer. We will use the regularized risk functional framework and replace the empirical risk with an (empirical) divergence term characterizing the mismatch between error distributions on the privileged and original spaces. This approach is innovative in two senses: (1) Prior knowledge is normally encoded in the regularization term, instead we introduce bias into the risk (loss) term. <ref type="bibr" target="#b1">(2)</ref> Almost all distribution matching methods match input features and/or function outputs, instead we match error distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In the literature, distribution matching has been proposed for, among others, transduction learning (e.g. <ref type="bibr" target="#b24">[25]</ref>) and domain adaptation (e.g. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref>). Quadrianto et al. <ref type="bibr" target="#b24">[25]</ref> matched the distribution between function outputs on the training data f (X) := {f (x 1 ), . . . , f (x N )} and outputs on the test data f (X ′ ) := {f (x ′ 1 ), . . . , f (x ′ N ′ )} to devise a general transduction algorithm for classification, regression, and structured prediction settings, whereas we propose to match error distributions on privileged and original domains. The empirical Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b11">[12]</ref> is employed as the nonparametric metric of difference between two distributions. In the domain adaptation setting, Pan et al. <ref type="bibr" target="#b22">[23]</ref> used the MMD metric to project data from a target domain X := {x 1 , . . . , x N } and a related source domain X ′ := {x ′ 1 , . . . , x ′ N ′ } into a common subspace such that the difference between the distributions of source and target domains is reduced. Recently, Zhang et al. <ref type="bibr" target="#b39">[40]</ref> proposed to also use the MMD metric to project data X and X ′ as well as function outputs f (X) and f (X ′ ) in the framework of deep neural networks.</p><p>In general, finding projection matrices involves either transformation of the data from source and target into a common subspace (two projection matrices) or transformation of the data from source to target (one projection matrix). The projection methods can be expensive in both computational complexity and memory requirement (if the data dimensionality is high). Our method offers a refreshing look on domain adaptation problems that sidestep the process of finding projection matrices. The cross-dataset scenario in this paper overlaps with the work of, for example, <ref type="bibr" target="#b31">[32]</ref>, which aims to overcome dataset bias across multiple image datasets in the domain adaptation scenario. In contrast, we explore cross-modal transfer in the cross-dataset learning. Complementary to us, <ref type="bibr" target="#b12">[13]</ref> recently use a distillation framework for cross-modal representation learning.</p><p>Model distillation or compression <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2]</ref> has attracted attention in the domain adaptation setting with deep architectures (e.g. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b12">13]</ref>). The aim is to learn representations for an unlabeled or sparsely labeled target domain by using a large labeled source domain as a supervisory signal. The framework uses output probability predictions on source domain as training labels for target domain and shows that this training is more accurate than using the original target labels. Instead we propose to match error distributions across domains as knowledge distillation in a LUPI framework. This is also supported by a work of <ref type="bibr" target="#b21">[22]</ref> that connects distillation and LUPI with one-to-one correspondences. In Section 3, we will describe related work on LUPI, followed by our proposed generalization of the LUPI paradigm in Section 4. We choose to motivate our work in terms of LUPI as it offers a unified framework for learning with additional information that is only available at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LUPI with one-to-one correspondence</head><p>We formalize the LUPI setup for the task of supervised binary classification with a single source of privileged information. Assume that we are given a set of N training examples, represented by feature vectors X = {x 1 , . . . , x N } ⊂ X = R d , their label annotations, Y = {y 1 , . . . , y N } ∈ Y = {+1, −1}, and additional information, also in the form of feature vectors,</p><formula xml:id="formula_0">X ⋆ = {x ⋆ 1 , . . . , x ⋆ N } ⊂ X ⋆ = R d ⋆ , where x ⋆</formula><p>i encodes the additional information we have about sample x i . This additional information is only available at training time, thus is referred as the privileged information. We now have X as the original data space and X ⋆ as the privileged data space. What we want is to learn a binary classification function f : X → Y from a large space of possible functions F that can then be used to infer the label y new for a new input instance x new . The goal of LUPI is to exploit the privileged information in the learning process of the latent function f . This exploitation, however, should not involve the usage of X ⋆ information as a direct input to the function f , because X ⋆ is not available for yet to be seen instances.</p><p>For this, a common approach found in the literature is to consider that the privileged information can be used to distinguish between easy and difficult instances <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14]</ref>. This extra knowledge can be used to bias the learning process towards finding a latent function f with better generalization properties.</p><p>Slack based methods. Vapnik and Vashist <ref type="bibr" target="#b34">[35]</ref> introduced an SVM+ method as a generalization of the SVMbased framework to solve LUPI. SVM+ tries to upper bound the mistake at i-th data point in the original space X based on the privileged data X ⋆ . Intuitively, we try to predict the difficulty of each data point based on the additional privileged data for that particular instance, thereby creating a data dependent upper bound ξ i on the hinge loss. In the context of binary classification with a linear classifier, f (x; w) := w, x + b, the SVM+ optimization admits the following form:</p><formula xml:id="formula_1">minimize w,w ⋆ ,b,b ⋆ w 2 regularization +C ⋆ w ⋆ 2 regularization + C N i=1 [ w ⋆ , x ⋆ i + b ⋆ ]</formula><p>loss:=upper bound of own mistakes (1a) subject to, for all i = 1, . . . , N ; w ⋆ ,</p><formula xml:id="formula_2">x ⋆ i + b ⋆ ≥ 0, 1 − y i [ w, x i + b] own mistake ≤ w ⋆ , x ⋆ i + b ⋆ .</formula><p>data dependent upper bound (1b) SVM+ parameterizes the slack value for each sample ξ i = w ⋆ , x ⋆ i + b ⋆ with unknown w ⋆ and b ⋆ parameters. These slack variables indicate which instances are easy and which are difficult to classify based on privileged information. Specifically, a difficult instance x ⋆ i has a large slack variable ξ i , which makes the corresponding constraint in (1b) have little impact or none at all in the estimation of w. If an instance is easy, its slack variable is close to zero, and the optimization task would concentrate on satisfying the corresponding constraint in (1b).</p><p>Remark In a variant of SVM+, called dSVM+ <ref type="bibr" target="#b34">[35]</ref>, Vapnik and Vashist first train a standard SVM parameterized byŵ andb on X ⋆ , Y and then compute deviation values d ⋆ i of each training instance. These are defined as</p><formula xml:id="formula_3">d ⋆ i = 1 − y i [ ŵ, x ⋆ i +b].</formula><p>Finally, an SVM+ is trained using X, X ⋆ and Y , where X ⋆ is a column vector with i-th element that corresponds to the deviation value of i-th training instance, d ⋆ i . This means the constraint in Eq. (1b) for a data point x i is now upper bounded by a scaled and shifted version of other's mistake:</p><formula xml:id="formula_4">1 − y i [ w, x i + b] own mistake ≤ w ⋆ , (1 − y i [ ŵ, x ⋆ i +b])</formula><p>other's mistake</p><formula xml:id="formula_5">+ b ⋆ .</formula><p>The idea is that if it is difficult to classify the instance in the privileged domain X ⋆ , which is often assumed to have better quality instances <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28]</ref>, then it is going to be even more difficult in the original X domain. Non-slack based methods. In an ensemble approach, Chen et al. <ref type="bibr" target="#b3">[4]</ref> described an Adaboost algorithm that uses privileged information. The method proposed considers decision stumps as weak classifiers, which are trained on the union of X and X ⋆ at each step. In the context of Gaussian process classification, Hernández-Lobato et al. <ref type="bibr" target="#b13">[14]</ref> proposed a heteroscedastic Gaussian process classification model to address classification tasks with privileged information. Wang et al. <ref type="bibr" target="#b35">[36]</ref> proposed to solve a joint regularized risk functional over f and f ⋆ with an extra regularization term that couples the optimization problem in the form of</p><formula xml:id="formula_6">N i=1 (f (x i )−f ⋆ (x ⋆ i )) 2 .</formula><p>This extra term is similar to the squared difference term in the co-regularization based multi view semi-supervised learning approaches (e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>). The crucial difference is while co-regularization methods aim to improve the average performance of all single view classifiers, LUPI method is only interested in improving the performance of the classifier in the original space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Correspondence-free LUPI</head><p>We seek to relax the one-to-one correspondence assumption in the standard LUPI formulation. This is natural in the setting where we learn to recognize, for example, activities in images while the same activities are also available in videos as privileged information. We will of course not expect that there will be a one-to-one correspondence between images and videos. Nevertheless, learning about activities from videos could be informative about the action class and applicable to the same task with images. Another example is learning about interactions among people like dancing from real images and from abstract illustrations. Albeit the action does not appear the same way in abstract and real images, both representations are informative about the action and can learn from one another <ref type="bibr">[1]</ref>.</p><p>We argue that the LUPI framework can be generalized to such scenario by transferring the general class characteristic from the privileged to the original data via the distribution of the easy and hard samples. In the following we explain the idea of matching the distribution of slack variables as a model for the class-level information transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Distribution matching</head><p>We assume a linear form of classifier in the privileged and original spaces. Let p d ⋆ denote a distribution over deviation values d ⋆ (i.e. unthresholded slack variables but we might refer them simply as slack variables if the context is clear). The deviation values d ⋆ , as in the dSVM+ formulation, are obtained by first training a linear SVM on X ⋆ , Y ⋆ and then evaluating the slack variables using training data. The N ⋆ samples from this distribution are denoted as</p><formula xml:id="formula_7">D ⋆ = {d ⋆ i | d ⋆ i = 1 − y ⋆ i w ⋆ , x ⋆ i , i = 1, . . . , N ⋆ }.</formula><p>The shape of p d ⋆ reflects the distribution over easy and hard instances of the class in the privileged space. This distribution is class specific and can be seen as error distribution of the classifier in the privileged space. We assume that for a particular classification task like differentiating human actions, this error distribution stays similar across modalities. So that a distribution p d over deviation values in the original space X with N samples denoted as</p><formula xml:id="formula_8">D = {d i | d i = 1 − y i w, x i , i = 1, .</formula><p>. . , N }) and p d ⋆ coincide. Therefore, our main assumption is that the error distributions, p d ⋆ and p d , could be matched even if f and f ⋆ are learned from different modalities, images and videos, respectively.</p><p>Our main objective for the class-level information transfer is based on the regularized risk minimization framework with a divergence term characterizing the mismatch between the class error distributions in the privileged and original spaces acting as a loss term:</p><formula xml:id="formula_9">minimize w∈R d w 2 regularization + C KL(p d ⋆ ||p d )</formula><p>loss := divergence between own mistakes and others' mistakes <ref type="bibr" target="#b1">(2)</ref> where KL(p d ⋆ ||p d ) is the Kullback-Leibler divergence between distributions p d ⋆ and p d and C is the trade-off hyperparameter that controls the relative influence of the divergence (loss) component and the regularization. Note that the KL divergence is asymmetric, the choice in expressing the distribution distance measure as KL(p d ⋆ ||p d ) instead of KL(p d ||p d ⋆ ) is deliberate. This will simplify our learning algorithm as it will become clear below. Contrasting our proposed method with SVM+, we notice that in SVM+, Eq. (1), the training error is upper bounded per each in- For a binary problem of differentiating a kayaking action from others, we compare p d when training SVM on original data space X , p d with KL(p d ⋆ ||p d ) when training our proposed SVM MMD on data from X and X ⋆ , and p d ⋆ when training SVM on privileged data space X ⋆ . In this case, our proposed method successfully utilizes privileged information: the peak of train (2a: middle) and test (2b: middle) distributions p d with KL(p d ⋆ ||p d ) are shifted to the left comparing to p d . Train and test cases of p d ⋆ are the same as we use all available data in the other dataset as privileged information.</p><p>stance based on its privileged data (requires one-to-one correspondence). Instead in Eq. (2), we match the distribution of errors in privileged and original spaces (correspondencefree setting). Our intuition is that making errors on an instance basis is unavoidable, but we will make better decisions at a large scale by comparing error distributions.</p><p>To compute the KL divergence, we require a parametric assumption on the distribution p d ⋆ as well as p d . If we assume that X ⋆ is of much better quality than X as in for example <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28]</ref>, the distribution of the slack variables on privileged space d ⋆ will have a mean value in the negative region (for a correct prediction with a high confidence, the functional margin y ⋆ i w ⋆ , x ⋆ i will be large and therefore the slack variable will be negative). Whereas, the distribution of the slack variables in the original space, p d , will have a mean value around zero and tails that accounts for correctly (left tail) classified samples at negative region and incorrectly (right tail) classified samples at positive region. Please, refer to our visualization of the distribution over the deviation values in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>In the simplest case, we could model the p d distribution with the Gaussian exponential family,</p><formula xml:id="formula_10">p d = N (d|µ d , σ 2 d ).</formula><p>With this assumption, minimizing KL(p d ⋆ ||p d ) reduces to matching the first and second moments of the two distributions, which are the mean and the variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Maximum Mean Discrepancy</head><p>We can go beyond the Gaussian assumption and match skewness, kurtosis (third and fourth moments) or even higher order moments. In a more general case, to avoid a parametric assumption on the distance estimate between distributions, we propose to use the Maximum Mean Discrepancy (MMD) criterion <ref type="bibr" target="#b11">[12]</ref>, a non-parametric distance estimate. Denote by H a Reproducing Kernel Hilbert Space with kernel k defined on X . In this case one can show <ref type="bibr" target="#b11">[12]</ref> that whenever k is characteristic (or universal), the map</p><formula xml:id="formula_11">µ :p → µ[p] := E d∼p d [k(d, ·)] with associated distance MMD(p d ⋆ , p d ) := µ[p d ⋆ ] − µ[p d ] 2<label>(3)</label></formula><p>characterizes a distribution uniquely. Examples of characteristic kernels <ref type="bibr" target="#b30">[31]</ref> are Gaussian RBF, Laplacian and B 2n+1 -splines. With a this choice of kernel functions, the MMD criterion matches infinitely many moments in the Reproducing Kernel Hilbert Space (RKHS). The mean and variance matching described in the previous section is a special case when we use a polynomial kernel with degree 2. We use a biased estimate of MMD as follows:</p><formula xml:id="formula_12">MMD = 1 N 2 N i N i ′ k(d i , d i ′ ) − 2 N N ⋆ N i N ⋆ j k(d i , d ⋆ j )+ + 1 N ⋆,2 N ⋆ j N ⋆ j ′ k(d ⋆ j , d ⋆ j ′ ).<label>(4)</label></formula><p>The above quantity is then used as a plug-in estimator for non-parametric KL(p d ⋆ ||p d ) in <ref type="bibr" target="#b1">(2)</ref>. Please refer to Alg. 1 for the summary of our proposed method. Remark Using non universal kernels such as a polynomial kernel will only give necessary but not sufficient conditions for distribution matching. Hence we use RBF kernel Algorithm 1 Matching Error distributions on X ⋆ and X</p><formula xml:id="formula_13">Input original data (X, Y ), privileged data (X ⋆ , Y ⋆ ), assume f (x) := w, x and f ⋆ (x ⋆ ) := w ⋆ , x ⋆ w ⋆ ← solve ||w ⋆ || 2 + hingeloss(w ⋆ |X ⋆ , Y ⋆ ) D * = {1 − y ⋆ i w ⋆ , x ⋆ i } N ⋆ i=1 (errors on X ⋆ ) w ← solve ||w|| 2 + MMDloss (D ⋆ , D(w)|X, Y ) s.t. D(w) = {1 − y i w, x i } N i=1 (errors on X) Return w</formula><p>in the MMD criterion, while maintaining a linear classifier form in the proposed method. Computing MMD criterion in Eq. (4) costs O((N +N ⋆ ) 2 ) time <ref type="bibr" target="#b11">[12]</ref>, this is true for any kernel. We plan to explore advancements in fast two-sample test with cost that is linear in number of samples (e.g. <ref type="bibr" target="#b4">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We study the task of object as well as action recognition in images with three possible types of privileged information available at training time: clip art illustrations, videos, and 3D models. The classification task is the same in both modalities, so that the privileged data is informative about the objects/actions that we are primarily interested to recognize using the image modality.</p><p>Datasets. We use four publicly available datasets to test the performance of our cross-modal/dataset scenario: the INTERACT 1 dataset [1] with clip art illustrations collected in addition to images that capture the interaction between people, the UCF101 2 action recognition dataset of videos <ref type="bibr" target="#b29">[30]</ref>, and the CrossLink 3 dataset <ref type="bibr" target="#b15">[16]</ref> of 3DWarehouse 4 models accompanied by the action and object images from the ImageNet dataset 5 <ref type="bibr" target="#b25">[26]</ref>.</p><p>Methods. We compare our SVM MMD model (SVM MMD) with a standard object classification baseline such as SVM classifier trained on the image space X (SVM Images). To put our method into perspective of domain adaptation and provided that the feature dimensionality is the same across modalities X and X ⋆ , we compare the proposed SVM MMD with the instance-transfer approach that shares the data samples between the two modalities, i.e. SVM trained on union of image and privileged data (SVM Combined); and the model-transfer method that relies on parameter transfer from privileged (source) to image (target) space, such as adaptive SVM <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> (SVM Adaptive). For a given solution of the source task, w source , and training data of the target task, SVM Adaptive solves the following optimization problem: </p><formula xml:id="formula_14">minimize w w − w source 2 + C N N j=1 ξ j (5) s.t. 1 − y j w, x j ≤ ξ j , ξ j ≥ 0 for all 1 ≤ j ≤ N.</formula><p>To train a classifier on image data, we solve (5) using as w source the weight vector obtained from training using the privileged data. From the perspective of domain adaptation, SVM Adaptive transfers the information by introducing the bias into the regularization term of SVM, whereas the proposed MMD model introduces the bias into the loss term of the SVM. We also provide a reference comparison with the SVM+ baseline (SVM+) <ref type="bibr" target="#b34">[35]</ref> that relies on the one-toone correspondence between samples in the original and privileged spaces if applicable (Section 5.1). Model selection. We perform a cross-validation model selection approach for choosing the regularization tradeoff parameter(s) for each of the methods. In all our experiments, we select C over 5 hyper-parameter values {10 0 , 10 1 . . . , 10 4 } using 5 × 3 fold cross-validation. We set C ⋆ to be 100 everywhere except in SVM+. We use a Gaussian RBF kernel for the MMD term with a fixed kernel width of 10.0. From what we observed, the SVM+ baseline requires a broad range to infer its two hyper-parameters, C and C ⋆ , so we perform 5 × 3 fold joint cross-validation over the range {10 −4 , 10 −3 , . . . , 10 4 }.</p><p>Evaluation metric. To evaluate the performance of the methods, we use the classification accuracy. We repeat each experiment 20 times using different random splits of the data into train and test sets and report mean and standard error across repeats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Learning from the mistakes in abstract images</head><p>The INTERACT dataset contains 60 fine-grained classes that capture a variety of interactions between a pair of people, e.g., running after, running to, arguing with. Each of the interaction is represented as a set of real images and a set of clip art illustrations (on average, 50 images and 50 illustrations per class). The dataset has two settings: category-level, in which images and illustrations are collected independently given the category class, and instancelevel where 2-3 illustrations are collected for a given image. Here, we detail the experimental results of the categorylevel setting, and the supplementary material contains the full table of results of the instance-level setting.</p><p>For each interaction class, we train a binary classifier to distinguish this interaction (positive class) against the remaining 59 interactions (negative class). To train a classifier, we randomly sample 25 positive vs 25 negative images, and for testing we use the remaining positive images balanced with the negative samples. For those methods that use privileged data (for training only), we take 50 clip art illustrations as positive samples (all available per class) and balance them with the clip art images from the negative   <ref type="bibr" target="#b5">[6]</ref>. Average rank of the methods (x axis) is computed based on accuracy (the higher the better). A critical distance measures significant differences between methods based on their ranks. We link two methods with a solid line if they are not statistically different (p-value &gt; 5%).</p><p>classes. In this experiment, to train SVM+, we randomly pair images and illustrations of the same class label to define the constraints in Eq. (1b). In the instance-level setting (in the supplementary), we use one corresponding illustration per image. We noticed that within an action category, the variability of clip art illustrations is moderate, and SVM+ performs similarly in category and instance-level settings.</p><p>In this dataset, real images and clip art illustrations are represented using 765 dimensional features that capture human poses, expressions, relation and appearance and are provided with the dataset. We use the features computed for Person A, who is performing the action with respect to Person B. In this case the privileged modality and the real images are expressed using the same feature representation, which makes it a perfect testbed to compare our proposed model with all baselines.</p><p>Results. The full result of this experiment is presented in <ref type="table" target="#tab_2">Table 1</ref> and the summary in terms of a pairwise comparison between the proposed SVM MMD and the standard SVM is in <ref type="figure" target="#fig_1">Figure 3a</ref>. We analyzed our experimental results on the INTERACT dataset using the multiple dataset statistical comparison method of <ref type="bibr" target="#b5">[6]</ref> in <ref type="figure" target="#fig_2">Figure 4a</ref>. There is statistical evidence that SVM MMD performs best among the five methods. SVM+ performs better than SVM and SVM Combined in terms of ranking, however there is not enough evidence to support that the differences are significant. This conclusion holds true also for the instance-level setting (summarized in <ref type="figure" target="#fig_3">Figure 5</ref>). Advantageous performance of SVM MMD shows that learning from the mistakes of clip art classifiers help. We credit the significant improvement of SVM MMD over other methods to the principled idea of making the right decision at a larger scale by comparing error distributions rather than focusing on error in example-by-example basis. From the average rank perspective, SVM Combined and SVM Adaptive do not lead to an improved performance w.r.t. SVM Images. Looking closer at <ref type="table" target="#tab_2">Table 1</ref>, there are cases when SVM Combined improves significantly (sitting with) and SVM Adaptive improves by a large margin (elbowing) but they are followed by large performance dips in cases such as looking away from (SVM Combined) and talking with (SVM Adaptive). In contrast, the drop in SVM MMD is only moderate at worst (action waving at).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Learning from the mistakes in videos</head><p>In this experiment, we cross video data from the UCF101 dataset with image data from the ImageNet dataset to address the task of action recognition in images. Both datasets intersect at sport activities, so we focus on the following 15 actions: archery, basketball, biking, bowling, cricket shot, golf swing, horse riding, kayaking, pole vault, rafting, rowing, skateboard, skiing, surfing and tennis swing. We collect images from the most relevant synsets in the ImageNet  <ref type="table">Table 2</ref>: Learning image classifiers with the mistakes of video classifiers. Video data contain complementary information from still frames (spatial) and motion between frames (motion). The best result, for each of the video information (spatial, motion, and spatial+motion), is highlighted in boldface and an extra blue for our SVM MMD.</p><p>dataset to these action classes to form the image data modality. On average, each action class has 1000 images and 100 videos to train/test the models. Similarly to our previous experiment, we form 15 one-vs-rest binary classification tasks by randomly sampling 28 vs 28 images for training and 1000 vs 1000 images to test the methods (or as much as the class size allows). For those methods that use privileged data (for training only), we take all videos from the positive class and balance them with the same amount randomly sampled from the negative classes. As our image representation, we use 4096 dimensional features extracted from the fc7 activation layer in CaffeNet <ref type="bibr" target="#b16">[17]</ref> fine-tuned on ImageNet VOC2012 <ref type="bibr" target="#b25">[26]</ref>. As our video representation, we extract spatial and temporal representations from the Caffe models fine-tuned on ImageNet VOC2012 (the same as image data) and on optical flow of the UCF101 dataset as provided in <ref type="bibr" target="#b10">[11]</ref>. This video representation allows us to study the effects of three types of privileged data in this scenario: the spatial signal alone (4096 dimensional), the motion signal alone (4096 dimensional), and the spatial+motion signals combined (8192 dimensional). The first two types, as they are in the same dimension as the image space, can be used by the two domain adaptation baselines (SVM Adaptive and SVM Combined), whereas the spatial+motion information can not be used unless projection matrices are learned. Our SVM MMD does not depend on the dimensionality of the privileged data space.</p><p>Results. The results of this experiment are presented in <ref type="table">Table 2</ref>, the summary in terms of a pairwise comparison between the proposed SVM MMD and the standard SVM is depicted in <ref type="figure" target="#fig_1">Figure 3c</ref>-3e, and statistical comparison of all methods is reported in <ref type="figure" target="#fig_2">Figure 4c</ref>. Overall, SVM MMD clearly improves over SVM Images in all settings of video information: spatial, motion, and spatial+motion. Specifically, in all cases but one, bowling, we can see positive improvements when using video modality as privileged information. The largest improvement appears when SVM MMD learns only from the mistakes of video classifiers with motion features. This can be credited to the complementary view of motion features w.r.t. the original image space and a good motion feature representation (deep features fine-tuned on optical flow of the UCF101 dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Learning from the mistakes in 3D models</head><p>In contrast to our main task of object/action recognition in images, the CrossLink dataset was primarily designed to improve the performance of the 3D retrieval by leveraging images from the Bing search. We explore the setting where 3D models from the 3DWarehouse collection are used as privileged data to the images from the more complex Im-ageNet dataset. We collect 3D models by crawling the 3DWarehouse as described in <ref type="bibr" target="#b15">[16]</ref>, and manually checked all the models. Each 3D model is retrieved as a collection of 36 views of the object taken against no background. We use ImageNet synsets as our main image data. We focus on the following 9 classes: airplane, backpack, bicycle, boat, car, chair, couch, helicopter, laptop. Each object class has 1300 images on average and 90 3D models, ranging from 15 to 153 instances per class. As our image representation, we use 4096 dimensional deep features from the fc7 activation layer in CaffeNet fine-tuned on ImageNet VOC2012. As 3D model representation, we extract the same 4096 dimensional deep features from each of the views, and consider them as 36 data samples in the privileged space. For each pair of the 9 classes (36 in total) we train a one-vs-one binary classifier using 50 images (class balanced) for training and 2000 images (class balanced) for testing the models. For those methods that use privileged data, we balance 25 vs 25 instances of 3D models randomly sampled from the positive and negative classes.</p><p>Results. The full result of this experiment is presented in <ref type="table" target="#tab_2">Table 1</ref> of the supplementary material and the summary in terms of a pairwise comparison between the proposed SVM MMD and the standard SVM is in <ref type="figure" target="#fig_1">Figure 3b</ref>. Overall, SVM MMD improves over SVM Images (also supported by statistical summary in <ref type="figure" target="#fig_2">Figure 4b</ref>), but the actual improvement is minor. We credit this to the fact, that this recognition problem is rather simple, and the SVM Images baseline alone has achieved an average accuracy of 95.78%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>A fool learns from his mistakes, but a truly wise man learns from the mistakes of others.</p><p>Otto von Bismarck</p><p>Learning with privileged information (LUPI) aims to exploit extra information that is available for each instance at training time. A typical assumption made is that these extra data are useful to discriminate between easy and difficult instances. We generalize this idea by describing a model that uses a divergence between distribution of our own errors and of others' errors as the loss function. Our approach can handle setting with no strict one-to-one correspondence between privileged and original data. We have shown the usefulness of this correspondence-free LUPI in the setting of cross-dataset learning of image classifiers. Our results reveal that learning image classifiers with the mistakes of clip art classifiers, or 3D classifiers, or video classifiers can be more accurate than learning using images only. We seek to generalize our findings on correspondencefree LUPI for regression and multiple privileged information settings. We also aim to unify the LUPI setting and the setting where the extra attributes are available at test time but not at training time <ref type="bibr" target="#b17">[18]</ref> under our framework of divergence minimization between the classifier errors in the privileged and original spaces. Finally, in the direction of deep model compression or distillation, we will assess the benefits of error matching as an alternative to matching the output class-probabilities commonly used in the literature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of the error distributions of three classifiers in the experiment with Image+Video dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Learning image classifiers from the mistakes of classifiers trained on other datasets: clip arts (3a), 3D models (3b), and videos (3c-3e) datasets. Pairwise comparison of the proposed SVM MMD and the baseline method (SVM trained on images). The height of the bar corresponds to the relative accuracy improvement over the baselines for each of the 60 one-versus-rest (Image + Clip art), 36 one-versusone (Image + 3D model), and 15 one-versus-rest (Image + Video) problems. The full accuracy results are presented in Tables 1, 2, and in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Statistical summary of results based on Demšar</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Learning image classifiers with the mistakes of clip art classifiers (instance-level setting). The full results are in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>1 https://computing.ece.vt.edu/˜santol/projects/zsl_</figDesc><table>via_visual_abstraction/interact/index.html 
2 http://crcv.ucf.edu/data/UCF101.php 
3 http://geometry.cs.ucl.ac.uk/projects/2015/crosslink 
4 https://3dwarehouse.sketchup.com/?hl=en 
5 http://www.image-net.org 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Learning image classifiers with the mistakes of clip art classifiers (category-level setting). For instance-level setting, please refer to the supplementary material. The best result is highlighted in boldface with an extra blue for our SVM MMD.</figDesc><table>SVM 

SVM 
SVM [39] 
SVM+ [35] 
SVM (ours) 
Images 
Combined 
Adaptive 
MMD 
carrying 
97.21 ± 0.33 
94.50 ± 0.55 
96.36 ± 0.46 
97.64 ± 0.39 
97.43 ± 0.46 
catching 
83.75 ± 1.15 
84.20 ± 1.15 
83.41 ± 1.18 
84.20 ± 1.10 
85.11 ± 1.10 
pushing 
80.08 ± 1.14 
82.74 ± 0.90 
81.37 ± 1.17 
80.89 ± 0.93 
80.24 ± 1.15 
pulling 
63.79 ± 0.89 
67.18 ± 1.55 
64.60 ± 1.42 
62.82 ± 1.11 
63.79 ± 1.30 
reaching for 
66.42 ± 0.86 
68.58 ± 1.02 
67.17 ± 1.18 
66.50 ± 1.36 
68.50 ± 1.04 
jumping over 
90.10 ± 0.95 
93.17 ± 0.55 
93.46 ± 0.84 
90.77 ± 0.88 
89.81 ± 0.86 
hitting 
83.89 ± 1.39 
83.70 ± 1.28 
84.07 ± 1.24 
83.43 ± 1.29 
84.26 ± 1.11 
kicking 
89.67 ± 1.05 
92.17 ± 0.68 
91.08 ± 0.81 
90.75 ± 0.91 
90.75 ± 0.79 
elbowing 
82.39 ± 1.00 
84.43 ± 1.19 
86.82 ± 0.87 
83.86 ± 1.15 
83.86 ± 0.95 
tripping 
86.36 ± 0.79 
84.39 ± 0.96 
85.23 ± 1.05 
86.82 ± 0.61 
87.88 ± 0.74 
waving at 
71.20 ± 1.29 
67.72 ± 1.50 
68.04 ± 1.43 
70.33 ± 1.05 
69.67 ± 1.11 
pointing at 
77.33 ± 1.34 
74.22 ± 1.47 
73.79 ± 1.62 
77.16 ± 1.38 
78.79 ± 1.23 
point. aw. fr. 
66.50 ± 1.67 
66.00 ± 1.74 
63.62 ± 1.76 
67.00 ± 1.39 
67.88 ± 1.72 
looking at 
67.42 ± 1.37 
68.95 ± 1.75 
66.13 ± 1.28 
67.50 ± 1.71 
66.69 ± 1.49 
looking aw. fr. 
73.91 ± 1.61 
67.66 ± 0.91 
70.55 ± 1.25 
73.12 ± 1.22 
75.23 ± 1.29 
laughing at 
72.34 ± 0.98 
74.22 ± 1.28 
71.95 ± 1.05 
73.20 ± 0.95 
74.14 ± 1.16 
laughing with 
82.29 ± 0.90 
81.88 ± 1.10 
79.90 ± 1.32 
80.21 ± 1.12 
80.83 ± 1.05 
hugging 
87.89 ± 0.96 
87.66 ± 1.16 
88.36 ± 0.96 
88.44 ± 0.75 
87.42 ± 0.91 
wrestling with 
88.75 ± 0.87 
85.80 ± 0.98 
87.39 ± 0.81 
89.66 ± 0.87 
90.11 ± 0.95 
dancing with 
84.34 ± 0.82 
81.10 ± 1.32 
82.72 ± 1.09 
83.82 ± 1.04 
84.56 ± 0.82 
hold. hands w. 
85.48 ± 0.75 
83.87 ± 0.99 
84.60 ± 0.79 
86.13 ± 0.71 
86.05 ± 0.71 
shak. hands w. 
94.74 ± 0.84 
91.12 ± 0.69 
92.76 ± 0.69 
95.69 ± 0.51 
94.91 ± 0.67 
talking with 
82.06 ± 0.94 
79.49 ± 1.39 
76.54 ± 1.06 
81.62 ± 1.40 
82.21 ± 1.17 
arguing with 
83.62 ± 1.02 
83.97 ± 0.80 
85.17 ± 0.88 
83.88 ± 1.13 
83.28 ± 0.85 
walking with 
91.93 ± 0.63 
90.00 ± 0.89 
89.43 ± 1.06 
93.30 ± 0.57 
93.41 ± 0.58 
running with 
89.67 ± 0.89 
87.67 ± 1.22 
89.58 ± 1.00 
89.67 ± 0.88 
89.33 ± 0.97 
crawling with 
79.76 ± 1.47 
82.38 ± 0.85 
80.12 ± 1.23 
81.19 ± 1.46 
82.02 ± 1.24 
jumping with 
82.88 ± 0.96 
83.46 ± 1.61 
81.73 ± 1.56 
81.92 ± 1.12 
83.08 ± 0.83 
walking to 
78.75 ± 1.26 
79.73 ± 0.81 
77.68 ± 1.15 
79.64 ± 1.04 
79.20 ± 1.16 
running to 
78.12 ± 1.10 
77.81 ± 0.79 
77.42 ± 0.97 
77.81 ± 1.14 
78.05 ± 1.20 

SVM 
SVM 
SVM [39] 
SVM+ [35] 
SVM (ours) 
Images 
Combined 
Adaptive 
MMD 
crawling to spatial+motion 
spatial 
motion 
SVM 
SVM (ours) 
SVM 
SVM [39] 
SVM (ours) 
SVM 
SVM [39] 
SVM (ours) 
Images 
MMD 
Combined 
Adaptive 
MMD 
Combined 
Adaptive 
MMD 
Archery 
83.87 ± 0.50 
85.44 ± 0.30 
83.48 ± 0.24 
82.81 ± 0.36 85.69 ± 0.27 79.75 ± 0.70 73.47 ± 0.61 85.67 ± 0.28 
Basketball 
91.95 ± 0.33 
91.89 ± 0.27 
90.04 ± 0.35 
87.60 ± 0.48 92.16 ± 0.26 89.82 ± 0.42 82.25 ± 0.49 92.30 ± 0.30 
Biking 
90.71 ± 0.24 
91.26 ± 0.32 
89.93 ± 0.33 
88.49 ± 0.31 91.44 ± 0.20 86.87 ± 0.52 79.54 ± 0.54 91.45 ± 0.21 
Bowling 
94.66 ± 0.29 
94.18 ± 0.38 
93.24 ± 0.32 
90.09 ± 0.47 
94.27 ± 0.34 
90.61 ± 0.59 85.72 ± 0.50 
94.59 ± 0.34 
CricketShot 
84.96 ± 0.28 
85.29 ± 0.66 
83.77 ± 0.24 
82.92 ± 0.33 85.90 ± 0.22 82.44 ± 0.43 78.97 ± 0.54 85.84 ± 0.22 
GolfSwing 
82.17 ± 0.49 
83.06 ± 0.31 
81.13 ± 0.41 
80.03 ± 0.32 83.00 ± 0.30 80.51 ± 0.34 72.76 ± 0.53 83.23 ± 0.29 
HorseRiding 
90.39 ± 0.15 
90.60 ± 0.18 
90.18 ± 0.24 
89.05 ± 0.32 90.63 ± 0.20 87.37 ± 0.55 79.56 ± 0.43 90.49 ± 0.25 
Kayaking 
83.14 ± 0.45 
85.45 ± 0.19 
82.27 ± 0.37 
81.97 ± 0.46 85.35 ± 0.23 81.16 ± 0.47 75.28 ± 0.50 85.37 ± 0.26 
PoleVault 
87.86 ± 0.44 
88.20 ± 0.41 
84.41 ± 0.33 
83.97 ± 0.37 88.11 ± 0.37 83.53 ± 0.66 77.34 ± 0.35 88.54 ± 0.39 
Rafting 
87.55 ± 0.29 
87.97 ± 0.27 
87.54 ± 0.27 
86.35 ± 0.28 88.03 ± 0.16 85.40 ± 0.39 82.29 ± 0.38 88.33 ± 0.21 
Rowing 
87.80 ± 0.46 
87.71 ± 0.36 
89.49 ± 0.18 88.32 ± 0.23 
87.84 ± 0.35 
85.05 ± 0.52 80.13 ± 0.36 87.91 ± 0.31 
SkateBoarding 
81.13 ± 0.54 
82.39 ± 0.42 
81.46 ± 0.43 
79.83 ± 0.48 82.37 ± 0.36 77.48 ± 0.67 72.32 ± 0.50 82.70 ± 0.40 
Skiing 
90.24 ± 0.45 
90.61 ± 0.45 
90.24 ± 0.24 
88.49 ± 0.38 90.59 ± 0.31 86.93 ± 0.52 79.62 ± 0.39 91.18 ± 0.25 
Surfing 
88.52 ± 0.39 
88.85 ± 0.28 
88.64 ± 0.26 
87.76 ± 0.32 88.65 ± 0.35 86.34 ± 0.26 82.87 ± 0.46 89.15 ± 0.21 
TennisSwing 
83.35 ± 0.47 
83.59 ± 0.42 
82.78 ± 0.32 
81.62 ± 0.31 83.69 ± 0.36 79.49 ± 0.64 72.58 ± 0.67 83.82 ± 0.41 
avg. acc. 
87.22 
87.77 
86.57 
85.29 
87.85 
84.18 
78.31 
88.04 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We gratefully acknowledge NVIDIA for GPU donation and Amazon for AWS Cloud Credits.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zero-shot learning via visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boosting with side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast two-sample testing with analytic representations of probability measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chwialkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating privileged genetic info. for fundus image based glaucoma detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Privileged information for data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feyereisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Aickelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="23" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object localization based on structural svm using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feyereisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fouad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raychaudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
		<title level="m">corporating privileged information through metric learning. T-NNLS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-prob</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mind the nuisance: Gaussian process classification using privileged noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Distilling the knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crosslink: Joint understanding of image and 3D model collections through shape and camera pose variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hueting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CoConut: Co-classification with output space regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Personalized handwriting recognition via biased regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chellapilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning using privileged information: SVM+ and weighted SVM. Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="95" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. T-PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1134" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast optimization algorithms for solving SVM+</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pechyony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stat. Learning and Data Science</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distribution matching for transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ambiguity helps: Classification with disagreements in crowdsourced annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to rank using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An rkhs for multi-view learning and manifold co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universality, characteristic kernels and RKHS embedding of measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2389" to="2410" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A testbed for cross-dataset analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning using privileged information: Similarity control and knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2023" to="2049" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new learning paradigm: Learning using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vashist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with hidden information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting privileged information from web data for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">X</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Privileged information-based conditional regression forest for facial feature detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep transfer network: Unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00591</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
