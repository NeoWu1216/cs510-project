<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Summary Transfer: Exemplar-based Subset Selection for Video Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
							<email>zhang.ke@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
							<email>weilunc@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
							<email>feisha@cs.ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
							<email>grauman@cs.utexas.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78701</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Summary Transfer: Exemplar-based Subset Selection for Video Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video summarization has unprecedented importance to help us digest, browse, and search today's ever-growing video collections. We propose a novel subset selection technique that leverages supervision in the form of humancreated summaries to perform automatic keyframe-based video summarization. The main idea is to nonparametrically transfer summary structures from annotated videos to unseen test videos. We show how to extend our method to exploit semantic side information about the video's category/genre to guide the transfer process by those training videos semantically consistent with the test input. We also show how to generalize our method to subshot-based summarization, which not only reduces computational costs but also provides more flexible ways of defining visual similarity across subshots spanning several frames. We conduct extensive evaluation on several benchmarks and demonstrate promising results, outperforming existing methods in several settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The amount of video data has been explosively increasing due to the proliferation of video recording devices such as mobile phones, wearable and ego-centric cameras, surveillance equipment, and others. According to YouTube statistics, about 300 hours of video are uploaded every minute <ref type="bibr">[2]</ref>. To cope with this video data deluge, automatic video summarization has emerged as a promising tool to assist in curating video contents for fast browsing, retrieval, and understanding <ref type="bibr" target="#b11">[14,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b44">47]</ref>, without losing important information.</p><p>Video can be summarized at several levels of abstraction: keyframes <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b45">48]</ref>, segment or shotbased skims <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b33">36]</ref>, story-boards <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b6">9]</ref>, montages <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b38">41]</ref> or video synopses <ref type="bibr" target="#b36">[39]</ref>. In this paper, we focus on developing learning algorithms for selecting * Equal contributions keyframes or subshots from a video sequence. Namely, the input is a video and its subshots and the output is an (ordered) subset of the frames or subshots in the video.</p><p>Inherently, summarization is a structured prediction problem where the decisions on whether to include or exclude certain frames or subshots into the subset are interdependent. This is in sharp contrast to typical classification and recognition tasks where the output is a single label.</p><p>The structured nature of subset selection presents a major challenge. Current approaches rely heavily on several heuristics to decide the desirability of each frame: representativeness <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b33">36]</ref>, diversity or uniformity <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b45">48]</ref>, interestingness and relevance <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b33">36]</ref>. However, combining those frame-based properties to output an optimal subset remains an open and understudied problem. In particular, researchers are hampered by the lack of knowledge on the "global" criteria human annotators presumably optimize when manually creating a summary.</p><p>Recently, initial steps investigating supervised learning for video summarization have been made. They demonstrate promising results <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b9">12]</ref>, often exceeding the conventional unsupervised clustering of frames. The main idea is to use a training set of videos and human-created summaries as targets to adapt the parameters of a subset selection model to optimize the quality of the summarization. If successful, a strong form of supervised learning would extract high-level semantics and cues from human-created summaries to guide summarization.</p><p>Supervised learning for structured prediction is a challenging problem in itself. Existing parametric techniques typically require a complex model with sufficient annotated data to represent highly complicated decision regions in a combinatorially large output space. In this paper, we explore a nonparametric supervised learning approach to summarization. Our method is motivated by the observation that similar videos share similar summary structures. For instance, suppose we have a collection of videos of wedding ceremonies inside churches. It is quite likely good summaries for those videos would all contain frames portraying brides proceeding to the altar, standing of the grooms  <ref type="figure">Figure 1</ref>. The conceptual diagram of our approach, which leverages the intuition that similar videos share similar summary structures. The main idea is nonparametric structure transfer, ie, transferring the subset structures in the human-created summaries (blue frames) of the training videos to a new video. Concretely, for each new video, we first compute frame-level similarity between training and test videos (i.e., sim(·, ·), cf. eq. (4)). Then, we encode the summary structures in the training videos with kernel matrices made of binarized pairwise similarity among their frames. We combine those structures, factoring the pairwise similarity between the training and the test videos, into a kernel matrix that encodes the summary structure for the test video, cf. eq. (7). Finally, the summary is decoded by inputting the kernel matrix to a probabilistic model called the determinantal point process (DPP) to extract a globally optimal subset of frames.</p><p>and their best men, the priests' preaching, the exchange of rings, etc. Thus, if one such video is annotated with human-created summaries, a clever algorithm could essentially "copy and paste" the relative positions of the extracted frames in the annotated video sequence and apply them to an unannotated one to extract relevant frames. Note that this type of transferring summary structures across videos need not assume a precise matching of visual appearance in corresponding frames -there is no need to have the same priest as long as the frames of the priests in each video are sufficiently different from other frames to be "singled out" as possible candidates.</p><p>The main idea of our approach centers around this intuition, that is, non-parametric learning from exemplar videos to transfer summary structures to novel input videos. In recent years, non-parametric methods in the vision literature have shown great promise in letting the data "speak for itself", though thus far primarily for traditional categorization or regression tasks (e.g., label transfer for image recognition <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b39">42]</ref> or scene completion <ref type="bibr" target="#b10">[13]</ref>).</p><p>How can summarization be treated non-parametrically? A naive application of non-parametric learning to video summarization would treat keyframe selection as a binary classification problem-matching each frame in the unannotated test video to the nearest human-selected keyframes in some training video, and deciding independently per frame whether it should be included in or excluded from the summary. Such an approach, however, conceptually fails on two fronts. First, it fails to account for the relatedness between a summary's keyframes. Second, it limits the system to inputs having very similar frame-level matches in the annotated database, creating a data efficiency problem. Therefore, rather than transfer simple relevance labels, our key insight is to transfer the structures implied by subset selection. We show how kernel-based representations of a video's frames (subshots) can be used to detect and align the meta-cues present in selected subsets. In this way, we com-pose novel summaries by borrowing recurring structures in exemplars for which we have seen both the source video and its human-created summary. A conceptual diagram of our approach is shown in <ref type="figure">Fig. 1</ref>.</p><p>In short, our main contributions are an original modeling idea that leverages non-parametric learning for structured objects (namely, selecting subsets from video sequences), a summarization method that advances the frontier of supervised learning for video summarization, and an extensive empirical study validating the proposed method and attaining far better summarization results than competing methods on several benchmark datasets.</p><p>The rest of the paper is organized as follows. In section 3, we describe our approach of nonparametric structure transfer. We report and analyze experimental results in section 4 and conclude in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A variety of video summarization techniques have been developed in the literature <ref type="bibr" target="#b30">[33,</ref><ref type="bibr" target="#b40">43]</ref>. Broadly speaking, most methods first compute visual features at the frame level, then apply some selection criteria to prioritize frames for inclusion in the output summary.</p><p>Keyframe-based methods select a subset of frames to form a summary, and typically use low-level features like optical flow <ref type="bibr" target="#b41">[44]</ref> or image differences <ref type="bibr" target="#b45">[48]</ref>. Recent work also injects high-level information such as object tracks <ref type="bibr" target="#b24">[27]</ref> or "important" objects <ref type="bibr" target="#b21">[24]</ref>, or takes user input to generate a storyboard <ref type="bibr" target="#b6">[9]</ref>. In contrast, video skimming techniques first segment the input into subshots using shot boundary detection. The summary then consists of a selected set of representative subshots <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b33">36]</ref>.</p><p>Selection criteria for summaries often aim to retain diverse and representative frames <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b45">48</ref>]. Another strategy is to predict object and event saliency <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b33">36]</ref>, or to pose summarization as an anomaly detection problem <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b46">49]</ref>. When the camera is known to be stationary, background subtraction and object tracking offer valuable cues about the salient entities in the video <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b36">39]</ref>.</p><p>Whatever the above choices, existing methods are almost entirely unsupervised. For example, they employ clustering to identify groups of related frames, and/or manually define selection criteria based on intuition for the problem. Some prior work includes supervised learning components (e.g., to generate regions with learned saliency metrics <ref type="bibr" target="#b21">[24]</ref>, train classifiers for canonical viewpoints <ref type="bibr" target="#b14">[17]</ref>, or recognize fragments of a particular event category <ref type="bibr" target="#b35">[38]</ref>), but they do not learn the subset selection procedure itself.</p><p>Departing from unsupervised methods, limited recent work formulates video summarization as a subset selection problem <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b9">12,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b43">46]</ref>. This enables supervised learning, exploiting knowledge encoded in human-created summaries. In <ref type="bibr" target="#b9">[12]</ref>, a submodular function optimizes a global objective function of the desirability of selected frames, while <ref type="bibr" target="#b7">[10]</ref> uses a probabilistic model that maximizes the probability of the ground-truth subsets.</p><p>The novelty of our approach is to learn nonparametrically from exemplar training videos to transfer summary structures to test videos. In contrast to previous parametric models <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b9">12]</ref>, non-parametric learning generalizes to new videos by directly exploiting patterns in the training data. This has the advantage of generalizing locally within highly nonsmooth regions: as long as a test video's "neighborhood" contains an annotated training video, the summary structure of that training video will be transferred. In contrast, parametric techniques typically require a complex model with sufficient annotated data to parametrically represent those regions. Our non-parametric approach also puts design power into flexible kernel functions, as opposed to relying strictly on combinations of hand-crafted criteria (e.g., frame interestingness, diversity, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We cast the process of extracting a summary from a video as selecting a subset of items (i.e., video frames) from a ground set (i.e., the whole video). Given a corpus of videos and their human-created summaries, our learning algorithm learns the optimal criteria for subset selection and applies them to unseen videos to extract summaries.</p><p>The first step is to decide on a subset selection model that can output a structure (i.e., an ordered subset). For such structured prediction problems, we focus on the determinantal point process (DPP) <ref type="bibr" target="#b19">[22]</ref> which has the benefits of being more computationally tractable than many probabilistic graphical models <ref type="bibr" target="#b17">[20]</ref>. Empirically, DPP has been successfully applied to documentation summarization <ref type="bibr" target="#b18">[21]</ref>, image retrieval <ref type="bibr" target="#b4">[7]</ref> and more recently, to video summarization <ref type="bibr" target="#b0">[3,</ref><ref type="bibr" target="#b7">10]</ref>.</p><p>We will describe first DPP and how it can be used for video summarization. We then describe our main approach in detail, as well as its several extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>Let Y = {1, 2, · · · , N} denote a (ground) set of N items, such as video frames. The ground set has 2 N subsets. The DPP over the N items assigns a probability to each of those subsets. Let y ⊆ Y denote a subset and the probability of selecting it is given by</p><formula xml:id="formula_0">P (y; L) = det(L y ) det(L + I) ,<label>(1)</label></formula><p>where L is a symmetric, positive semidefinite matrix and I is an identity matrix of the same size of L. L y is the principal minor (sub-matrix) with rows and columns from L indexed by the integers in y. DPP can be seen conceptually as a fully connected Nnode Markov network where the nodes correspond to the items. This network's node-potentials are given by the diagonal elements of L and the edge potentials are given by the off-diagonal elements in L. Note that those "potentials" cannot be arbitrarily assigned -to ensure they form a valid probabilistic model, the matrix L needs to be positive semidefinite. Due to this constraint, L is often referred to as a kernel matrix whose elements can be interpreted as measuring the pairwise compatibility.</p><p>Besides computational tractability which facilitates parameter estimation, DPP has an important modeling advantage over standard Markov networks. Due to the celebrated Hammersley-Clifford Theorem, Markov networks cannot model distributions where there are zero-probability events. On the other hand, DPP is capable of assigning zero probability to absolutely impossible (or inadmissible) instantiations of random variables.</p><p>To see its use for video summarization, suppose there are two frames that are identical. For keyframe-based summarization, any subset containing such identical frames should be ruled out by being assigned zero probability. This is impossible in Markov networks -no matter how small, Markov networks will assign strictly positive probabilities to an exponentially large number of subsets containing identical frames. For DPP, since the two items are identical, they lead to two identical columns/rows in the matrix L, resulting a determinant zero (thus zero probability) for those subsets. Thus, DPP naturally encourages selected items in the subset to be diverse, an important objective for summarization and information retrieval <ref type="bibr" target="#b19">[22]</ref>.</p><p>The mode of the distribution is the most probable subset y * = arg max y P (y; L) = arg max y det(L y ).</p><p>(</p><p>This is an NP-hard combinatorial optimization problem, and there are several approaches to obtaining approximate solutions <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b19">22]</ref>.</p><p>The most crucial component in a DPP is its kernel matrix L. To apply DPP to video summarization, we define the ground set as the frames in a video and identify the most desired summarization as the MAP inference result of eq. (2). We compute L with a bivariate function over two frameswe dub it the summarization kernel:</p><formula xml:id="formula_2">L ij = φ(v i ) T φ(v j )<label>(3)</label></formula><p>where φ(·) is a function of the features v i (or v j ) computed on the i-th (or the j-th ) frames. There are several choices. For instance, φ(·) could be an identity function, a nonlinear mapping implied by a Gaussian RBF kernel, or the output of a neural network <ref type="bibr" target="#b7">[10]</ref>.</p><p>As each different video needs to have a different kernel, φ(·) needs to be identified from a sufficiently rich function space so it generalizes from modeling the training videos to new ones. If the videos are substantially different, this generalization can be challenging, especially when there are not enough annotated videos with human-created summaries. Our approach overcomes this challenge by directly using the training videos' kernel matrices, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Non-parametric video summarization</head><p>Our approach differs significantly from existing summary methods, including those based on DPPs. Rather than learn a single function φ(·) and discard the training dataset, we construct L for every unannotated video by comparing it to the annotated training videos. This construction exploits two sources of information: 1) how similar the new video is to annotated ones, and 2) how the training videos are summarized. The former can be inferred directly by comparing visual features at each frame, while the latter can be "read off" from the human-created training summaries.</p><p>We motivate our approach with an idealized example that provides useful insight. Let us assume we are given a training set of videos and their summaries D = {(Y r , y r )} R r=1 and a new video Y to be summarized. Suppose this new video is very similar -we define similarity more precisely later -to a particular Y r in D. Then we can reasonably assume that the summary y r might work well for the new video. As a concrete example, consider the case where both Y and Y r are videos for wedding ceremonies inside churches. We anticipate seeing similar events across both videos: brides proceeding to the altars, priests delivering speeches, exchanging rings etc. Moreover, similarity in their summaries exists on the higher-order structural level: the relative positions of the summary frames of y r in the sequence Y are an informative prior on where the frames of the summary y should be in the new video Y. Specifically, as long as we can link the test video to the training video by identifying similar frames, 1 we can "copy down"-transfer-the positions of y r and lift the corresponding frames in Y to generate its summary y.</p><p>While this intuition is conceptually analogous to the familiar paradigm of nearest-neighbor classification, our approach is significantly different. The foremost is that, as discussed in section 1, we cannot select frames independently (by nonparametrically learning its similarity to those in the training videos). We need to transfer summary structures which encode interdependencies of selecting frames. Therefore, a naive solution of representing videos with fixlength descriptors in Euclidean space and literally pretending their summaries are "labels" that can be transferred to new data is flawed.</p><p>The main steps of our approach are outlined in <ref type="figure">Fig. 1</ref>. We describe them in detail in the following.</p><p>Step 1: Frame-based visual similarity To infer similarity across videos, we experiment with common ones in the computer vision literature for calculating frame-based similarity from visual features v i and v k extracted from the corresponding frames:</p><formula xml:id="formula_3">sim 1 (i, k) = v T i v k sim 2 (i, k) = exp{− v i − v k 2 2 /σ} sim 3 (i, k) = exp{−(v i − v k ) T Ω(v i − v k )},<label>(4)</label></formula><p>where σ and Ω are adjustable parameters (constrained to be positive or positive definite, respectively). These forms of similarity measures are often used in vision tasks and are quite flexible, e.g., one can learn the kernel parameters for sim 3 . However, they are not the focus of our approach -we expect more sophisticated ones will only benefit our learning algorithm. We also expect high-level features (such as interestingness, objectness, etc.) could also be beneficial. In section 3.4 we discuss a generalization to replace framelevel similarity with subshot-level similarity.</p><p>Step 2: Summarization kernels for training videos The summarization kernels {L r } R r=1 are not given to us in the training data. However, note that the crucial function of those kernels is to ensure that when used to perform the MAP inference in eq. (2) to identify the summary on the training video Y r , it will lead to the correct summarization y r (which is in the training set). This prompts us to define the following idealized summarization kernels</p><formula xml:id="formula_4">L r = α r       δ(1 ∈ y r ) 0 · · · 0 0 δ(2 ∈ y r ) . . . . . . . . . . . . . . . 0 0 · · · 0 δ(N r ∈ y r )      <label>(5)</label></formula><p>rich literature on image matching and recognition work, including efficient search strategies. or more compactly,</p><formula xml:id="formula_5">L r = α r diag({δ(n ∈ y r )} Nr n=1 ),<label>(6)</label></formula><p>where diag turns a vector into a diagonal matrix, N r is the number of frames in Y r and α r &gt; 1 is an adjustable parameter. The structure of L r is intuitive: if a frame is in the summary y r , then its corresponding diagonal element is α r , otherwise 0. It is easy to verify that L r indeed gives rise to the correct summarization. Note that α r &gt; 1 is required. If α r = 1, any subset of y r is a solution to the MAP inference problem (and we will be getting a shorter summarization). If α r &lt; 1, the empty set would be the summary (as the determinant of an empty matrix is 1, by convention).</p><p>Step 3: Transfer summary structure Our aim is now to transfer the structures encoded by the idealized summarization kernels from the training videos to a new (test) video Y. To this end, we synthesize L for new video Y out of {L r }. Let i and j index the video frames in Y, with k and l for a specific training video Y r . Specifically, the "contribution" from Y r to L is given by</p><formula xml:id="formula_6">r ij = k l sim r (i, k)sim r (j, l)L r,kl<label>(7)</label></formula><p>where L r,kl is the element in L r , and sim r (·, ·) measures frame-based (visual) similarity between frames of Y and Y r . <ref type="figure">Fig. 1</ref> illustrates graphically how frame-based similarity enables transfer of structures in training summaries. We gain further insights by examining the case when the framebased similarity sim r (·, ·) is sharply peaked -namely, there are very good matches between specific pairs of frames (an assumption likely satisfied in the running example of summarizing wedding ceremony videos)</p><formula xml:id="formula_7">sim r (i, m) ≫ sim r (i, k), ∀ k = m sim r (j, n) ≫ sim r (j, l), ∀ l = n.<label>(8)</label></formula><p>Under these conditions,</p><formula xml:id="formula_8">r ij ≈ sim r (i, m)sim r (j, n)L r,mn .<label>(9)</label></formula><p>Intuitively, if Y and Y r are precisely the same video (and the video frames in Y r are sufficiently visually different), then the matrix L would be very similar to L r . Consequently, the summarization y r , computed from L r , would be a good summary for Y.</p><p>To include all the information in the training data, we sum up the contributions from all Y r and arrive at</p><formula xml:id="formula_9">L ij = r r ij .<label>(10)</label></formula><p>We introduce a few shorthand notions. Let S r be a N × N r matrix whose elements are sim r (i, k), the frame-based similarity between N frames in Y and N r frames in Y r . The kernel matrix L is thus</p><formula xml:id="formula_10">L = r S r L r S T r .<label>(11)</label></formula><p>Note that, L is for the test video with N frames -there is no need for all the videos have the same length.</p><p>Step 4: Extracting summary Once we have computed L for the new video, we use the MAP inference eq. (2) to extract the summary as the most probable subset of frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning</head><p>Our approach requires adjusting parameters such as α = {α 1 , α 2 , · · · , α R } for the ideal summarization kernels and/or Ω for computing frame-based visual similarity eq. (4). We use maximum likelihood estimation to estimate those parameters. Specifically, for each video in the training dataset, we pretend it is a new video and formulate a kernel matrixL</p><formula xml:id="formula_11">q = r S q r L r S q r T , ∀, q = 1, 2, · · · , R.<label>(12)</label></formula><p>We optimize the parameters such that the ground-truth summarization y q attains the highest probability underL q ,</p><formula xml:id="formula_12">α * = arg max α R q=1</formula><p>log P (y q ;L q ).</p><p>We can formulate a similar criterion to learn the Ω parameter for sim 3 (·, ·). We carry out the optimization by gradient descent. In our experiments, we set σ for sim 2 to be 1, with features normalized to have unit norm. Additional details are in the Suppl. and omitted here for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Extensions</head><p>Category-specific summary transfer Video datasets labeled with semantically consistent categories have been emerging <ref type="bibr" target="#b35">[38,</ref><ref type="bibr" target="#b37">40]</ref>. We view categories as valuable prior information that can be exploited by our nonparametric learning algorithm. Intuitively, videos from the same category (activity type, genre, etc.) are likely to be similar in part, not only in visual appearance but also in high-level semantic cues (such as how key events are temporally organized), resulting in a similar summary structures. We extend our method to take advantage such optional side information in two ways:</p><p>• hard transfer. We compare the new video from a category c only to the training videos from the same category c. Mathematically, for each video category, we learn a category-specific set of α (c) = {α</p><formula xml:id="formula_14">(c) 1 , α (c) 2 , · · · , α (c) R } such that α (c) r</formula><p>&gt; 0 only when the training video r belongs to category c.</p><p>• soft transfer. We relax the requirement in hard transfer such that α (c) r &gt; 0 even if the rth training video is not from the category c. Note that while we utilize structural information from all training videos, the way we use them still depends on the test video's category.</p><p>Subshot-based summary transfer Videos can also be summarized at the level of subshots. As opposed to selecting keyframes, subshots contain short but contiguous frames, giving a glimpse of a key event. We next extend our subset selection algorithm to select a subset of subshots.</p><p>To this end, in our conceptual diagram as in <ref type="figure">Fig. 1</ref>, we replace computing frame-level similarity with subshot-level similarity, where we compare subshots between the training videos and the new video. We explore two possible ways to compute the frame-set to frame-set similarity:</p><p>• Similarity between averaged features. We represent the subshots using the averaged frame-level feature vectors within each subshot. We then compute the similarity using the previously defined sim(·, ·).</p><p>• Maximum similarity. We compute pairwise similarity between frames within the subshots and select the maximum value as the similarity between the subshots.</p><p>Both of these two approaches reduce the reliance on framebased similarity defined in the global frame-based descriptors of visual appearance, loosening the required visual alignment for discovering a good match-especially with the latter max operator, in principle, since it can ignore many unmatchable frames in favor of a single strong link within the subshots. Moreover, the first approach can significantly reduce the computational cost of nonparametric learning as the amount of pairwise-similarity computation now depends on the number of subshots, which is substantially smaller than the number of frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation and computation cost</head><p>Computing S r in eq. (11) is an O(N × r N r ) operation. For long videos, several approaches will reduce the cost significantly. First, it is a standard procedure to downsample the video (by a factor of 5-30) to reduce the number of frames for keyframe-based summarization. Our subshotbased summarization can also reduce the computation cost, cf. section 3.4. Generic techniques should also helpsim(·, ·) computes various forms of distances among visual feature vectors. Thus, many fast search techniques apply, such as locality sensitive hashing or tree structures for nearest neighbor searches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We validate our approach on five benchmark datasets. It outperforms competing methods in many settings. We also analyze its strengths and weaknesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Data For keyframe-based summarization, we experiment on three video datasets: the Open Video Project (OVP) <ref type="bibr">[1,</ref><ref type="bibr" target="#b1">4]</ref>, the YouTube dataset <ref type="bibr" target="#b1">[4]</ref>, and the Kodak consumer video dataset <ref type="bibr" target="#b28">[31]</ref>. All the 3 datasets were used in <ref type="bibr" target="#b7">[10]</ref> and we follow the procedure described there to preprocess the data, and to generate training ground-truths from multiple human-created summaries. For the YouTube dataset, in the following, we report results on 31 videos after discarding 8 videos that are neither "Sports" nor "News" such that we can investigate category-specific video summarization (cf. sec. 3.4). In Suppl., we report results on the original dataset.</p><p>For subshot-based summarization (cf. sec. 3.4), we experiment on three video datasets: the portion of MED with 160 annotated summaries <ref type="bibr" target="#b35">[38]</ref>, SumMe <ref type="bibr" target="#b8">[11]</ref> and YouTube. Videos in MED are pre-segmented into subshots with the Kernel Temporal Segmentation (KTS) method <ref type="bibr" target="#b35">[38]</ref> and we observe those subshots. For SumMe and YouTube, we apply KTS to generate our own sets of subshots. MED has 10 well-defined video categories allowing us to experiment with category-specific video summarization on it too. SumMe does not have semantic category meta-data. Instead, its video contents have a large variation in visual appearance and can be classified according to shooting style: still camera, egocentric, or moving. <ref type="table" target="#tab_1">Table 1</ref> summarizes key characteristics of those datasets with details in the Suppl.</p><p>Features For OVP/YouTube/Kodak/SumMe, we encode each frame with an ℓ 2 -normalized 8192-dimensional Fisher vector <ref type="bibr" target="#b34">[37]</ref>, computed from SIFT features. For OVP/YouTube/Kodak, we also use color histograms. We also experimented with features from a pre-trained convolutional nets (CNN), details in the Suppl. For MED, we use the provided 16512-dimensional Fisher vectors.</p><p>Evaluation metrics As in <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b8">11,</ref><ref type="bibr" target="#b9">12]</ref> and other prior work, we evaluate automatic summarization results (A) by comparing them to the human-created summaries (B) and reporting the standard metrics of F-score (F), precision (P), and recall (R) -their definitions are given in the Suppl.</p><p>For OVP/YouTube/Kodak, we follow <ref type="bibr" target="#b7">[10]</ref> and utilize the VSUMM package <ref type="bibr" target="#b1">[4]</ref> for finding matching pairs of frames. For SumMe, we follow the procedure in <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b9">12]</ref>. More details are in the Suppl.</p><p>Implementation details For each dataset, we randomly choose 80% of the videos for training and use the remain- ing 20% for testing, repeating for 5 or 100 rounds so that we can calculate averaged performance and standard errors.</p><p>To report existing methods' results, we use prior published numbers when possible. We also implement the VSUMM approach <ref type="bibr" target="#b1">[4]</ref> and obtained code from the authors for se-qDPP <ref type="bibr" target="#b7">[10]</ref> in order to apply them to several datasets. We follow the practices in <ref type="bibr" target="#b7">[10]</ref> so that we can summarize videos sequentially. For MED, we implement KVS <ref type="bibr" target="#b35">[38]</ref> ourselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>We summarize our key findings in this section. For more details, please refer to the Suppl.</p><p>In <ref type="table" target="#tab_3">Table 2</ref>, we compare our approach to both supervised and unsupervised methods for video summarization. We report published results in the table as well as results from our own implementation of some methods. Only the best variants of all methods are quoted and presented; others are deferred to the Suppl.</p><p>On all but one of the five datasets (OVP), our nonparametric learning method achieves the best results. In general, the supervised methods achieve better results than the unsupervised ones. Note that even for datasets with a variety of videos that are not closely visually similar (such as SumMe), our approach attains the best result-it indicates our method of transferring summary structures is effective, able to build on top of even crude frame similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detailed analysis</head><p>Advantage of nonparametric learning Nonparametric learning enjoys the property of generalizing locally. That is, as long as a test video has enough correctly annotated training videos in its neighborhood, the summary structures of those videos will transfer. A parametric learning method, on the other end, needs to learn both the locations of those local regions and how to generalize within local regions. If there are not enough annotated training videos covering the whole range of data space, it could be difficult for a parametric learning method to learn well.</p><p>We design a simple example to illustrate this point. As it is difficult to assess "similarity" to derive nearest neighbors for video, we use a video's category to delineate those "semantically near". Specifically, we split YouTube's 31 videos into two piles, according to their categories "Sports" or "News". We then construct seqDPP, a parametric learning model <ref type="bibr" target="#b7">[10]</ref>, using all the 31 videos, as well as "Sports" or "News" videos only to summarize test videos from either category. We then contrast to our method in the same setting. <ref type="table" target="#tab_2">Table 3</ref> displays the results. The results on the "News" category convincingly suggest that the nonparametric approach like ours can leverage the semantically-close videos to outperform the parametric approach with the same amount of annotated data-or even more data. (Note that, the difference on the "Sports" category is nearly identical.)</p><p>Advantage of exploiting category prior <ref type="table" target="#tab_2">Table 3</ref> already alludes to the fact that exploiting category side-information can improve summarization (cf. contrasting the column of "same as test" to "all"). Now we investigate this advantage in more detail. <ref type="table">Table 4</ref> shows how our nonparametric summary transfer can exploit video category information, using the method explained in section 3.4.</p><p>Particularly interesting are our results on the SumMe dataset, which itself does not provide semantically meaningful categories. Instead, we generate two "fake" categories for it. We first collapse the 10 video categories in the dataset TVSum 2 <ref type="bibr" target="#b37">[40]</ref> into two super-categories (details in Suppl.) -these two super-categories are semantically similar within each other, though they do not have obvious visual similarity to videos in the SumMe.</p><p>We then build a binary classifier trained on TVSum videos but classify the videos in SumMe as "super-category I" and "super-category II" and then proceed as if they are ground-truth categories, as in MED and YouTube. Despite this independently developed dichotomy, the results on SumMe improve over using all video data together. Subshot-based summarization In section 3.4, we discuss an extension to summarize video at the level of selecting subshots. This extension not only reduces computational cost (as the number of subshots is significantly smaller than that of frames), but also provides additional means of measuring similarity across videos beyond framelevel visual similarity inferred from global frame-based descriptors. Next we examine how such flexibility can ultimately improve keyframe-based summarization. Concretely, we first perform subshot summarization, then pick the middle frame in each selected subshot as the output keyframes. This allows us to directly compare to keyframebased summarization using the same F-score metric. <ref type="table">Table 5</ref> shows the results. Subshot-based summarization clearly improves frame-based -this is very likely due to the more robust similarity measures now computed at the subshot-level. The improvement is more pronounced when a category prior is not used. One possible explanation is that measuring similarity on videos from the same categories is easier and more robust, whereas across categories it is noisier. Thus, when a category prior is not present, the subshotbased similarity measure benefits summarization more.</p><p>Other detailed analysis in Suppl. We summarize other analysis results as follows. We show how to improve framelevel similarity by learning better metrics. We also show deep features, powerful for visual category recognition, is not particularly advantageous comparing to shallow features. We also show how category prior can still be exploited even we do not know the true category of test videos. <ref type="figure" target="#fig_0">Fig. 2</ref> shows a failure case by our method. Here the test video depicts a natural scene, while its closest training video depicts beach activities. There is a visual similarity (e.g., in the swath of sky). However, semantically, these two videos do not seem to be relevant and it is likely difficult for the transfer to occur. In particular, our results miss the last two frames where there are a lot of grass. This suggests one weakness in our approach: the formulation of our summarization kernel for the test video does not directly consider the relationship between its own frames -instead, they interact through training videos. Thus, one possible direction to avoid unreliable neighbors in the training videos is to rely on the visual property of the test video itself. This suggests future work on a hybrid approach with parametric and nonparametric aspects that complement each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative analysis</head><p>Please see the Suppl. for more qualitative analysis and example output summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel supervised learning technique for video summarization. The main idea is to learn nonparametrically to transfer summary structures from training videos to test ones. We also show how to exploit side (semantic) information such as video categories and propose an extension for subshot-based summarization. Our method achieves promising results on several benchmark datasets, compared to an array of nine existing techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>A failure case by our approach. Our summary misses the last two frames from the ground-truth (red-boxed) as the test video (nature scene) transfers from the nearest video with a semantically different category (beach activity). See text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Key characteristics of datasets used in our empirical studies. Most videos in these datasets have a duration from 1 to 5 minutes.</figDesc><table>Dataset 

# of 
video 

# of 
category 

# of Training 
videos 

# of Test 
video 

Type of 
summarization 

Evaluation metrics 
F-score in matching 
Kodak 
18 
-
14 
4 
keyframe 
selected frames 
OVP 
50 
-
40 
10 
keyframe 
Youtube 
31 
2 
31 
8 
keyframe; subshot selected frames; frames in selected subshots 
SumMe 
25 
-
20 
5 
subshot 
frames in selected subshots 
MED 
160 
10 
128 
32 
subshot 
matching selected subshots 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Advantage of nonparametric summary transfer</figDesc><table>Type of 
seqDPP 
Ours 
test video 
all 
same as test 
all 
same as test 
Sports 
52.8 
54.5 
53.5 
54.4 
News 
67.9 
67.7 
66.9 
69.1 

Table 4. Video category information helps summarization 
Setting 
YouTube MED SumMe 
w/o category 
60.0 
28.9 
39.2 
category-specific hard 
61.5 
30.4 
40.9 
category-specific soft 
60.6 
30.7 
40.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Performance (F-score) of various video summarization methods. Numbers followed by citations are from published results. Others are from our own implementation. Dashes denote unavailable/inapplicable dataset-method combinations.</figDesc><table>Unsupervised 
Supervised 

VSUMM1 
VSUMM2 

DT 
STIMO KVS Video MMR SumMe Submodular seqDPP Ours 
[4] 
[4] 
[34] 
[6] 
[38] 
[25] 
[11] 
[12] 
[10] 
Kodak 
69.5 
67.6 
-
-
-
-
-
-
78.9 
82.3 
OVP 
70.3 
69.5 
57.6 
63.4 
-
-
-
-
77.7 
76.5 
YouTube 
58.7 
59.9 
-
-
-
-
-
-
60.8 
61.8 
MED 
28.9 
28.8 
-
-
20.6 
-
-
-
-
30.7 
SumMe 
32.8 
33.7 
-
-
-
26.6 
39.3 
39.7 
-
40.9 

Table 5. Subshot-based summarization on YouTube 
Category Frame-
Subshot-based 
-specific 
based 
Mean-Feature Max-similarity 
No 
60.0 
60.7 
60.9 
Yes 
61.5 
61.6 
61.8 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This task is itself not trivial, of course, but it does have the benefit of a</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We choose this one as it has raw videos for us to extract features and have a larger number of labeled videos for us to build a category classifier</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-margin determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E F</forename><surname>De Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P B</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Da Luz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Albuquerque Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="68" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online content-aware video condensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stimo: Still and moving video storyboard for the web scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Furini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Geraci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montangero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pellegrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="69" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discovering diverse and salient threads in document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Near-optimal MAP inference for determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Schematic storyboarding for video visualization and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="862" to="871" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diverse sequential subset selection for supervised video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Creating summaries from user videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video summarization by learning submodular mixtures of objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scene completion using millions of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Event driven summarization for web videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMM Workshop</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Space-time video montage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video summarization using web-image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint summarization of large-scale collections of web images and videos for storyline reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a spacetime mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Determinantal point processes for machine learning. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<idno>2012. 3</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video summarization from spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laganière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hocevar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Païs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TRECVid Video Summarization Workshop</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-video summarization based on videommr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WIAMIS Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonparametric scene parsing via label transfer. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2368" to="2382" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A hierarchical visual model for video object summarization. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2178" to="2190" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimization algorithms for the selection of key frame sequences of variable length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coherent parametric contours for interactive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards extracting semantically meaningful key frames from personal video clips: from humans to computers. Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Costello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="301" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A user attention model for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video summarisation: A conceptual framework and survey of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Money</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="143" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Keyframe-based video summarization using delaunay clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mundur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yesha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="232" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Event-driven video abstraction and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="55" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic video summarization by graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Categoryspecific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Webcam synopsis: Peeking around the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tvsum: Summarizing web videos using titles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Salient montages from unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video abstraction: A systematic review and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Key frame selection by motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Harnessing object and scene semantics for large-scale video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gaze-enabled egocentric video summarization via constrained submodular maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-occlusions and disocclusions in causal video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An integrated system for content-based video retrieval and browsing. Pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="643" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Quasi real-time summarization for consumer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
