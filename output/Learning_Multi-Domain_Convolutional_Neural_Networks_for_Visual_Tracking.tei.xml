<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Multi-Domain Convolutional Neural Networks for Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<email>bhhan@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Multi-Domain Convolutional Neural Networks for Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking groundtruths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify target in each domain. We train each domain in the network iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance in existing tracking benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) have recently been applied to various computer vision tasks such as image classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, semantic segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, object detection <ref type="bibr" target="#b11">[12]</ref>, and many others <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. Such great success of CNNs is mostly attributed to their outstanding performance in representing visual data. Visual tracking, however, has been less affected by these popular trends since it is difficult to collect a large amount of training data for video processing applications and training algorithms specialized for visual tracking are not available yet, while the approaches based on low-level handcraft features still work well in practice <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b46">47]</ref>. Several recent tracking algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b43">44]</ref> have addressed the data deficiency issue by transferring pretrained CNNs on a large-scale classification dataset such as ImageNet <ref type="bibr" target="#b35">[36]</ref>. Although these methods may be sufficient to obtain generic feature representations, its effectiveness in terms of tracking is limited due to the fundamental inconsistency between classification and tracking problems, i.e., predicting object class labels versus locating targets of arbitrary classes.</p><p>To fully exploit the representation power of CNNs in visual tracking, it is desirable to train them on large-scale data specialized for visual tracking, which cover a wide range of variations in the combination of target and background. However, it is truly challenging to learn a unified representation based on the video sequences that have completely different characteristics. Note that individual sequences involve different types of targets whose class labels, moving patterns, and appearances are different, and tracking algorithms suffer from sequence-specific challenges including occlusion, deformation, lighting condition change, motion blur, etc. Training CNNs is even more difficult since the same kind of objects can be considered as a target in a sequence and as a background object in another. Due to such variations and inconsistencies across sequences, we believe that the ordinary learning methods based on the standard classification task are not appropriate, and another approach to capture sequence-independent information should be incorporated for better representation.</p><p>Motivated by this fact, we propose a novel CNN architecture, referred to as Multi-Domain Network (MDNet), to learn the shared representation of targets from multiple annotated video sequences for tracking, where each video is regarded as a separate domain. The proposed network has separate branches of domain-specific layers for binary classification at the end of the network, and shares the common information captured from all sequences in the preceding layers for generic representation learning. Each domain in MDNet is trained separately and iteratively while the shared layers are updated in every iteration. By employing this strategy, we separate domain-independent information from domain-specific one and learn generic feature representations for visual tracking. Another interesting aspect of our architecture is that we design the CNN with a small number of layers compared to the networks for classification tasks such as AlexNet <ref type="bibr" target="#b27">[28]</ref> and VGG nets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>We also propose an effective online tracking framework based on the representations learned by MDNet. When a test sequence is given, all the existing branches of binary classification layers, which were used in the training phase, are removed and a new single branch is constructed to compute target scores in the test sequence. The new classification layer and the fully connected layers within the shared layers are then fine-tuned online during tracking to adapt to the new domain. The online update is conducted to model long-term and short-term appearance variations of a target for robustness and adaptiveness, respectively, and an effective and efficient hard negative mining technique is incorporated in the learning procedure.</p><p>Our algorithm consists of multi-domain representation learning and online visual tracking. The main contributions of our work are summarized below:</p><p>• We propose a multi-domain learning framework based on CNNs, which separates domain-independent information from domain-specific one, to capture shared representations effectively.</p><p>• Our framework is successfully applied to visual tracking, where the CNN pretrained by multi-domain learning is updated online in the context of a new sequence to learn domain-specific information adaptively.</p><p>• Our extensive experiment demonstrates the outstanding performance of our tracking algorithm compared to the state-of-the-art techniques in two public benchmarks: Object Tracking Benchmark <ref type="bibr" target="#b44">[45]</ref> and VOT2014 <ref type="bibr" target="#b25">[26]</ref>.</p><p>The rest of the paper is organized as follows. We first review related work in Section 2, and discuss our multidomain learning approach for visual tracking in Section 3. Section 4 describes our online learning and tracking algorithm, and Section 5 demonstrates the experimental results in two tracking benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual Tracking Algorithms</head><p>Visual tracking is one of the fundamental problems in computer vision and has been actively studied for decades. Most tracking algorithms fall into either generative or discriminative approaches. Generative methods describe the target appearances using generative models and search for the target regions that fit the models best. Various generative target appearance modeling algorithms have been proposed including sparse representation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b47">48]</ref>, density estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>, and incremental subspace learning <ref type="bibr" target="#b34">[35]</ref>. In contrast, discriminate methods aim to build a model that distinguishes the target object from the background. These tracking algorithms typically learn classifiers based on multiple instance learning <ref type="bibr" target="#b0">[1]</ref>, P-N learning <ref type="bibr" target="#b23">[24]</ref>, online boosting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref>, structured output SVMs <ref type="bibr" target="#b15">[16]</ref>, etc.</p><p>In recent years, correlation filters have gained attention in the area of visual tracking due to their computational efficiency and competitive performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref>. Bolme et al. <ref type="bibr" target="#b1">[2]</ref> proposed a fast correlation tracker with a minimum output sum of squared error (MOSSE) filter, which runs in hundreds of frames per second. Henriques et al. <ref type="bibr" target="#b16">[17]</ref> formulated kernelized correlation filters (KCF) using circulant matrices, and efficiently incorporated multi-channel features in a Fourier domain. Several variations of KCF tracker have been subsequently investigated to improve tracking performance. For example, DSST <ref type="bibr" target="#b4">[5]</ref> learns separate filters for translation and scaling, and MUSTer [21] employs short-term and long-term memory stores inspired by a psychological memory model. Although these approaches are satisfactory in constrained environments, they have an inherent limitation that they resort to low-level hand-crafted features, which are vulnerable in dynamic situations including illumination changes, occlusion, deformations, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Convolutional Neural Networks</head><p>CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. Krizhevsky et al. <ref type="bibr" target="#b27">[28]</ref> brought significant performance improvement in image classification by training a deep CNN with a largescale dataset and an efficient GPU implementation. R-CNN <ref type="bibr" target="#b11">[12]</ref> applies a CNN to an object detection task, where the training data are scarce, by pretraining on a large auxiliary dataset and fine-tuning on the target dataset.</p><p>Despite such huge success of CNNs, only a limited number of tracking algorithms using the representations from CNNs have been proposed so far <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44</ref>]. An early tracking algorithm based on a CNN can handle only predefined target object classes, e.g., human, since the CNN is trained offline before tracking and fixed afterwards <ref type="bibr" target="#b8">[9]</ref>. Although <ref type="bibr" target="#b28">[29]</ref> proposes an online learning method based on a pool of CNNs, it suffers from lack of training data to train deep networks and its accuracy is not particularly good compared to the methods based on hand-craft features. A few recent approaches <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b19">20]</ref> transfer CNNs pretrained on a large-scale dataset constructed for image classification, but the representation may not be very effective due to the fundamental difference between classification and tracking tasks. Contrary to the existing approaches, our algorithm takes advantage of large-scale visual tracking data for pretraining a CNN and obtain effective representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-Domain Learning</head><p>Our approach to pretrain deep CNNs belongs to multidomain learning, which refers to a learning method in which the training data are originated from multiple domains and the domain information is incorporated in learning procedure. Multi-domain learning is popular in natural lan- guage processing (e.g., sentiment classification with multiple products and spam filtering with multiple users), and various approaches have been proposed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>. In computer vision community, multi-domain learning is discussed in only a few domain adaptation approaches. For example, Duan et al. <ref type="bibr" target="#b7">[8]</ref> introduced a domain-weighted combination of SVMs for video concept detection, and Hoffman et al. <ref type="bibr" target="#b17">[18]</ref> presented a mixture-transform model for object classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Domain Network (MDNet)</head><p>This section describes our CNN architecture and multidomain learning approach to obtain domain-independent representations for visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>The architecture of our network is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. It receives a 107×107 RGB input 1 , and has five hidden layers including three convolutional layers (conv1-3) and two fully connected layers (fc4-5). Additionally, the network has K branches for the last fully connected layers (fc6 1 -fc6 K ) corresponding to K domains, in other words, training sequences. The convolutional layers are identical to the corresponding parts of VGG-M network <ref type="bibr" target="#b3">[4]</ref> except that the feature map sizes are adjusted by our input size. The next two fully connected layers have 512 output units and are combined with ReLUs and dropouts. Each of the K branches contains a binary classification layer with softmax crossentropy loss, which is responsible for distinguishing target and background in each domain. Note that we refer to fc6 1 -fc6 K as domain-specific layers and all the preceding layers as shared layers.</p><p>Our network architecture is substantially smaller than the ones commonly used in typical recognition tasks such as AlexNet <ref type="bibr" target="#b27">[28]</ref> and VGG-Nets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>. We believe that such a simple architecture is more appropriate for visual tracking due to the following reasons. First, visual tracking aims to distinguish only two classes, target and background, which requires much less model complexity than general visual recognition problems such as ImageNet classification with 1000 classes. Second, a deep CNN is less effective for precise target localization since the spatial information tends to be diluted as a network goes deeper <ref type="bibr" target="#b19">[20]</ref>. Third, since targets in visual tracking are typically small, it is desirable to make input size small, which reduces the depth of the network naturally. Finally, a smaller network is obviously more efficient in visual tracking problem, where training and testing are performed online. When we tested larger networks, the algorithm is less accurate and becomes slower significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Algorithm</head><p>The goal of our learning algorithm is to train a multidomain CNN disambiguating target and background in an arbitrary domain, which is not straightforward since the training data from different domains have different notions of target and background. However, there still exist some common properties that are desirable for target representations in all domains, such as robustness to illumination changes, motion blur, scale variations, etc. To extract useful features satisfying these common properties, we separate domain-independent information from domain-specific one by incorporating a multi-domain learning framework.</p><p>Our CNN is trained by the Stochastic Gradient Descent (SGD) method, where each domain is handled exclusively in each iteration. In the k th iteration, the network is updated based on a minibatch that consists of the training samples from the (k mod K) th sequence, where only a single branch fc6 (k mod K) is enabled. It is repeated until the network is converged or the predefined number of iterations is reached. Through this learning procedure, domain-independent information is modeled in the shared layers from which useful generic feature representations are obtained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Online Tracking using MDNet</head><p>Once we complete the multi-domain learning described in Section 3.2, the multiple branches of domain-specific layers (fc6 1 -fc6 K ) are replaced with a single branch (fc6) for a new test sequence. Then we fine-tune the new domainspecific layer and the fully connected layers in the shared network online at the same time. The detailed tracking procedure is discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tracking Control and Network Update</head><p>We consider two complementary aspects in visual tracking, robustness and adaptiveness, by long-term and shortterm updates. Long-term updates are performed in regular intervals using the positive samples collected for a long period while short-term updates are conducted whenever potential tracking failures are detected-when the positive score of the estimated target is less than 0.5-using the positive samples in a short-term period. In both cases we use the negative samples observed in the short-term since old negative examples are often redundant or irrelevant to the current frame. Note that we maintain a single network during tracking, where these two kinds of updates are performed depending on how fast the target appearance changes.</p><p>To estimate the target state in each frame, N target candidates x 1 , . . . , x N sampled around the previous target state are evaluated using the network, and we obtain their positive scores f + (x i ) and negative scores f − (x i ) from the network. The optimal target state x * is given by finding the example with the maximum positive score as</p><formula xml:id="formula_0">x * = argmax x i f + (x i ).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hard Minibatch Mining</head><p>The majority of negative examples are typically trivial or redundant in tracking-by-detection approaches, while only a few distracting negative samples are effective to training a classifier. Hence, the ordinary SGD method, where the training samples evenly contribute to learning, easily suffers from a drift problem since the distractors are considered insufficiently. A popular solution in object detection for this issue is hard negative mining <ref type="bibr" target="#b38">[39]</ref>, where training and testing procedures are alternated to identify the hard negative examples, typically false positives, and we adopt this idea for our online learning procedure.</p><p>We integrate hard negative mining step into minibatch selection. In each iteration of our learning procedure, a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Bounding Box Regression</head><p>Due to the high-level abstraction of CNN-based features and our data augmentation strategy which samples multiple positive examples around the target (which will be described in more detail in the next subsection), our network sometimes fails to find tight bounding boxes enclosing the target. We apply the bounding box regression technique, which is popular in object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>, to improve target localization accuracy. Given the first frame of a test sequence, we train a simple linear regression model to predict the precise target location using conv3 features of the samples near the target location. In the subsequent frames, we adjust the target locations estimated from Eq. (1) using the regression model if the estimated targets are reliable (i.e. f + (x * ) &gt; 0.5). The bounding box regressor is trained only in the first frame since it is time consuming for online update and incremental learning of the regression model may not be very helpful considering its risk. Refer to <ref type="bibr" target="#b11">[12]</ref> for details as we use the same formulation and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Details</head><p>The overall procedure of our tracking algorithm is presented in Algorithm 1. The filter weights in the j th layer of CNN are denoted by w j , where w 1:5 are pretrained by mutli-domain learning and w 6 is initialized randomly for a new sequence. Only the weights in the fully connected layers w 4:6 are updated online whereas the ones in the convolutional layers w 1:3 are fixed throughout tracking; this strategy is beneficial to not only computational efficiency but also avoiding overfitting by preserving domain-independent information. T s and T l are frame index sets in short-term (τ s = 20) and long-term (τ l = 100) periods, respectively. Find the optimal target state x * t by Eq. (1).</p><p>9:</p><p>if f + (x * t ) &gt; 0.5 then 10:</p><p>Draw training samples S + t and S − t . <ref type="bibr" target="#b10">11</ref>:</p><formula xml:id="formula_1">T s ← T s ∪ {t}, T l ← T l ∪ {t}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>if |T s | &gt; τ s then T s ← T s \ {min v∈Ts v}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>if |T l | &gt; τ l then T l ← T l \ {min v∈T l v}.</p><p>14:</p><p>Adjust x * t using bounding box regression. <ref type="bibr">15:</ref> if f + (x * t ) &lt; 0.5 then <ref type="bibr">16:</ref> Update {w 4 , w 5 , w 6 } using S + v∈Ts and S − v∈Ts .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>else if t mod 10 = 0 then 18:</p><p>Update {w 4 , w 5 , w 6 } using S + v∈T l and S − v∈Ts . <ref type="bibr" target="#b18">19</ref>: until end of sequence</p><p>The further implementation details are described below.</p><p>Target candidate generation To generate target candidates in each frame, we draw N (= 256) samples in translation and scale dimension, x i t = (x i t , y i t , s i t ), i = 1, . . . , N , from a Gaussian distribution whose mean is the previous target state x * t−1 and covariance is a diagonal matrix diag(0.09r 2 , 0.09r 2 , 0.25), where r is the mean of the width and height of the target in the previous frame. The scale of each candidate bounding box is computed by multiplying 1.05 si to the initial target scale.</p><p>Training data For offline multi-domain learning, we collect 50 positive and 200 negative samples from every frame, where positive and negative examples have ≥ 0.7 and ≤ 0.5 IoU overlap ratios with ground-truth bounding boxes, respectively. Similarly, for online learning, we collect S + t (= 50) positive and S − t (= 200) negative samples with ≥ 0.7 and ≤ 0.3 IoU overlap ratios with the estimated target bounding boxes, respectively, except that S + 1 = 500 and S − 1 = 5000. For bounding-box regression, we use 1000 training examples with the same parameters as <ref type="bibr" target="#b11">[12]</ref>.</p><p>Network learning For multi-domain learning with K training sequences, we train the network for 100K iterations with learning rates 0.0001 for convolutional layers 2 and 0.001 for fully connected layers. At the initial frame <ref type="bibr" target="#b1">2</ref> The convolutional layers are initialized by VGG-M network, which is pretrained on ImageNet.   <ref type="figure">Figure 3</ref>: Precision and success plots on OTB50 <ref type="bibr" target="#b45">[46]</ref> and OTB100 <ref type="bibr" target="#b44">[45]</ref>. The numbers in the legend indicate the representative precision at 20 pixels for precision plots, and the average area-under-curve scores for success plots.</p><p>of a test sequence, we train the fully connected layers for 30 iterations with learning rate 0.0001 for fc4-5 and 0.001 for fc6. For online update, we train the fully connected layers for 10 iterations with the learning rate three times larger than that in the initial frame for fast adaptation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>We evaluated MDNet on two datasets, Object Tracking Benchmark (OTB) <ref type="bibr" target="#b44">[45]</ref> and VOT2014 <ref type="bibr" target="#b25">[26]</ref>. Our algorithm is implemented in MATLAB using MatConvNet toolbox <ref type="bibr" target="#b41">[42]</ref>, and runs at around 1 fps with eight cores of 2.20GHz Intel Xeon E5-2660 and an NVIDIA Tesla K20m GPU. The source code of MDNet is available at http: //cvlab.postech.ac.kr/research/mdnet/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation on OTB</head><p>OTB <ref type="bibr" target="#b44">[45]</ref> is a popular tracking benchmark that contains 100 fully annotated videos with substantial variations. The evaluation is based on two metrics: center location error and bounding box overlap ratio. The one-pass evaluation (OPE) is employed to compare our algorithm with the six state-ofthe-art trackers including MUSTer <ref type="bibr" target="#b20">[21]</ref>, CNN-SVM <ref type="bibr" target="#b19">[20]</ref>, MEEM <ref type="bibr" target="#b46">[47]</ref>, TGPR <ref type="bibr" target="#b10">[11]</ref>, DSST <ref type="bibr" target="#b4">[5]</ref> and KCF <ref type="bibr" target="#b16">[17]</ref>, as well as the top 2 trackers included in the benchmark-SCM <ref type="bibr" target="#b48">[49]</ref>   and Struck <ref type="bibr" target="#b15">[16]</ref>. Note that CNN-SVM is another tracking algorithm based on the representations from CNN, which provides a baseline for tracking algorithms that adopt deep learning. In addition to the results on the entire 100 sequences in <ref type="bibr" target="#b44">[45]</ref> (OTB100), we also present the results on its earlier version containing 50 sequences <ref type="bibr" target="#b45">[46]</ref> (OTB50). For offline training of MDNet, we use 58 training sequences collected from VOT2013 <ref type="bibr" target="#b26">[27]</ref>, VOT2014 <ref type="bibr" target="#b25">[26]</ref> and VOT2015 <ref type="bibr" target="#b24">[25]</ref>, excluding the videos included in OTB100. <ref type="figure">Figure 3</ref> illustrates the precision and success plots based on center location error and bounding box overlap ratio, respectively. It clearly illustrates that our algorithm, denoted by MDNet, outperforms the state-of-the-art trackers significantly in both measures. The exceptional scores at mild thresholds means our tracker hardly misses targets while the competitive scores at strict thresholds implies that our algorithm also finds tight bounding boxes to targets. For detailed performance analysis, we also report the results on various challenge attributes in OTB100, such as occlusion, rotation, motion blur, etc. <ref type="figure" target="#fig_6">Figure 4</ref> demonstrates that our tracker effectively handles all kinds of challenging situations that often require high-level semantic understanding. In particular, our tracker successfully track targets in low resolution while all the trackers based on low-level features are not successful in the challenge.</p><p>To verify the contribution of each component in our algorithm, we implement and evaluate several variations of our approach. The effectiveness of our multi-domain pretraining technique is tested by comparison with the singledomain learning method (SDNet), where the network is trained with a single branch using the data from multiple sequences. We also investigate two additional versions of our tracking algorithm-MDNet without bounding box regres-  <ref type="figure" target="#fig_7">Figure 5</ref>. <ref type="figure" target="#fig_8">Figure 6</ref> presents the superiority of our algorithm qualitatively compared to the state-of-the-art trackers. <ref type="figure" target="#fig_9">Figure 7</ref> shows a few failure cases of our algorithm; it has drift problem due to slow target appearance changes in Coupon sequence, and a sudden appearance change makes our tracker miss the target completely in Jump sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation on VOT2014 Dataset</head><p>For completeness, we also present the evaluation results on VOT2014 dataset <ref type="bibr" target="#b25">[26]</ref>, which contains 25 sequences with substantial variations. In VOT challenge protocol, target is re-initialized whenever tracking fails and the evaluation module reports both accuracy and robustness, which correspond to the bounding box overlap ratio and the number of failures, respectively. There are two types of exper-    <ref type="bibr" target="#b25">[26]</ref>. The first and second best scores are highlighted in red and blue colors, respectively. iment settings; trackers are initialized with either groundtruth bounding boxes (baseline) or randomly perturbed ones (region noise). The VOT evaluation also provides a rank-ing analysis based on both statistical and practical significance of the performance gap between trackers. Please refer to <ref type="bibr" target="#b25">[26]</ref> for more details. We compare our algorithm  with the top 5 trackers in VOT2014 challenge-DSST <ref type="bibr" target="#b4">[5]</ref>, SAMF <ref type="bibr" target="#b29">[30]</ref>, KCF <ref type="bibr" target="#b16">[17]</ref>, DGT <ref type="bibr" target="#b2">[3]</ref> and PLT 14 <ref type="bibr" target="#b25">[26]</ref>-and additional two state-of-the-art trackers MUSTer <ref type="bibr" target="#b20">[21]</ref> and MEEM <ref type="bibr" target="#b46">[47]</ref>. Our network is pretrained using 89 sequences from OTB100, which do not include the common sequences with the VOT2014 dataset.</p><p>As illustrated in <ref type="table" target="#tab_3">Table 1</ref> and <ref type="figure" target="#fig_10">Figure 8</ref>, MDNet is ranked top overall-the first place in accuracy and the first or second place in robustness; it demonstrates much better accuracy than all other methods, even with fewer reinitializations. Furthermore, MDNet works well with imprecise re-initializations as shown in the region noise experiment results, which implies that it can be effectively combined with a re-detection module and achieve long-term tracking. We also report the results with respect to several visual attributes in <ref type="figure">Figure 9</ref>, which shows that our tracker is stable in various challenging situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a novel tracking algorithm based on CNN trained in a multi-domain learning framework, which is re- Ranking plot for label empty Robustness rank Accuracy rank <ref type="figure">Figure 9</ref>: The robustness-accuracy ranking plots for five visual attributes (camera motion, illumination change, motion change, occlusion and size change) and an empty attribute. ferred to as MDNet. Our tracking algorithm learns domainindependent representations from pretraining, and captures domain-specific information through online learning during tracking. The proposed network has a simple architecture compared to the one designed for image classification tasks. The entire network is pretrained offline, and the fully connected layers including a single domain-specific layer are fine-tuned online. We achieved outstanding performance in two large public tracking benchmarks, OTB and VOT2014, compared to the state-of-the-art tracking algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of our Multi-Domain Network, which consists of shared layers and K branches of domain-specific layers. Yellow and blue bounding boxes denote the positive and negative samples in each domain, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Identified training examples through our hard negative mining in Bolt2 (top) and Doll (bottom) sequences. Red and blue bounding boxes denote positive and negative samples in each minibatch, respectively. The negative samples becomes hard to classify as training proceeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>minibatch consists of M + positives and M − h hard negatives. The hard negative examples are identified by testing M − (≫ M − h ) negative samples and selecting the ones with top M − h scores. As the learning proceeds and the network becomes more discriminative, the classification in a minibatch becomes more challenging as illustrated in Figure 2. This approach examines a predefined number of samples and identifies critical negative examples effectively without explicitly running a detector to extract false positives as in the standard hard negative mining techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Online tracking algorithm Input : Pretrained CNN filters {w 1 , . . . , w 5 } Initial target state x 1 Output: Estimated target states x * t 1: Randomly initialize the last layer w 6 . 2: Train a bounding box regression model. 3: Draw positive samples S + 1 and negative samples S − 1 . 4: Update {w 4 , w 5 , w 6 } using S + 1 and S − 1 ; 5: T s ← {1} and T l ← {1}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The momentum and weight decay are always set to 0.9 and 0.0005, respectively. Each mini-batch consists of M + (= 32) positives and M − h (= 96) hard negatives selected out of M − (= 1024) negative examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>The success plots for eight challenge attributes: fast motion, background clutter, illumination variation, in-plain rotation, low resolution, occlusion, out of view, and scale variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Precision and success plots on OTB100 for the internal comparisons. sion (MDNet-BB) and MDNet without bounding box regression and hard negative mining (MDNet-BB-HM). The performances of all the variations are not as good as our full algorithm (MDNet) and each component in our tracking algorithm is helpful to improve performance. The detailed results are illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results of the proposed method on some challenging sequences (Bolt2, Diving, Freeman4, Human5, Ironman, Matrix and Skating2-1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Failure cases of our method (Coupon and Jump). Green and red bounding boxes denote the ground-truths and our tracking results, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>The robustness-accuracy ranking plots of tested algorithms in VOT2014 dataset. The better trackers are located at the upper-right corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc>The average scores and ranks of accuracy and robustness on the two experiments in VOT2014</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This input size is designed to obtain 3×3 feature maps in conv3: 107 = 75 (receptive field) + 2 × 16 (stride).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partly supported by IITP grant (B0101-16-0307; Machine Learning Center, B0101-16-0552; Deep-View) funded by the Korean government (MSIP).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust object tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1619" to="1632" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured visual tracking with dynamic graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-domain learning by confidence-weighted parameter combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources via auxiliary classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human tracking using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1610" to="1623" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transfer learning based visual tracking with gaussian processes regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time tracking via on-line boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised on-line boosting for robust tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequential kernel density approximation and its application to real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1186" to="1197" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering latent domains for multisource domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MUlti-Store Tracker (MUSTer): a cognitive psychology inspired approach to object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust online appearance models for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>El-Maraghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1296" to="1311" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multidomain learning: when do domains matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tracking-learningdetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2014 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojíř</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-CVW</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2013 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DeepTrack: Learning discriminative feature representations by convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust visual tracking using ℓ1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Imagenet large scale visual recognition challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tracking-bysegmentation with online gradient boosting decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Example-based learning for viewbased human face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Transferring rich feature hierarchies for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04587</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MEEM: Robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust visual tracking via multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust object tracking via sparsity-based collaborative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
