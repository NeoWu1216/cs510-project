<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-Stream Bi-Directional Recurrent Neural Network for Fine-Grained Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
							<email>bharat@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubish Electric Research Labs (MERL)</orgName>
								<orgName type="institution" key="instit1">U. of Maryland</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
							<email>tmarks@merl.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubish Electric Research Labs (MERL)</orgName>
								<orgName type="institution" key="instit1">U. of Maryland</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
							<email>mjones@merl.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubish Electric Research Labs (MERL)</orgName>
								<orgName type="institution" key="instit1">U. of Maryland</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubish Electric Research Labs (MERL)</orgName>
								<orgName type="institution" key="instit1">U. of Maryland</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
							<email>mingshao@ccs.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubish Electric Research Labs (MERL)</orgName>
								<orgName type="institution" key="instit1">U. of Maryland</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Multi-Stream Bi-Directional Recurrent Neural Network for Fine-Grained Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a multi-stream bi-directional recurrent neural network for fine-grained action detection. Recently, twostream convolutional neural networks (CNNs) trained on stacked optical flow and image frames have been successful for action recognition in videos. Our system uses a tracking algorithm to locate a bounding box around the person, which provides a frame of reference for appearance and motion and also suppresses background noise that is not within the bounding box. We train two additional streams on motion and appearance cropped to the tracked bounding box, along with full-frame streams. Our motion streams use pixel trajectories of a frame as raw features, in which the displacement values corresponding to a moving scene point are at the same spatial position across several frames. To model long-term temporal dynamics within and between actions, the multi-stream CNN is followed by a bi-directional Long Short-Term Memory (LSTM) layer. We show that our bi-directional LSTM network utilizes about 8 seconds of the video sequence to predict an action label. We test on two action detection datasets: the MPII Cooking 2 Dataset, and a new MERL Shopping Dataset that we introduce and make available to the community with this paper. The results demonstrate that our method significantly outperforms state-of-the-art action detection methods on both datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we present an approach for detecting actions in videos. Action detection refers to the problem of localizing temporally and spatially every occurrence of each action from a known set of action classes in a long video sequence. This is in contrast to most of the previous work in video activity analysis, which has focused on the problem of action recognition (also called action classification). In action recognition, a temporally segmented clip of a video is given as input, and the task is to classify it as one of N known actions. For action recognition, temporal localiza-tion is not required, as each video clip is trimmed to contain precisely the full duration (from start to finish) of one action. Furthermore, action recognition algorithms do not need to consider the case that a presented clip might not contain any of the known actions. In general, action detection is more difficult than action recognition. However, it is worth overcoming that difficulty because action detection is also much more relevant to real-world applications.</p><p>In this work, we focus on fine-grained action detection. We use the term fine-grained in the same sense as <ref type="bibr" target="#b19">[20]</ref> to indicate that the differences among the classes of actions to be detected are small. For example, in a cooking scenario, detecting similar actions such as chopping, grating, and peeling constitutes fine-grained action detection.</p><p>We propose a method for fine-grained action detection in long video sequences, based on a Multi-Stream Bi-Directional Recurrent Neural Network (MSB-RNN). We call our neural network multi-stream because it begins with a convolutional neural network (CNN) that has four streams: two different streams of information (motion and appearance) for each of two different spatial frames (fullframe and person-centric). The video that is input to the network is split into a sequence of brief (6-frame-long) chunks. The multi-stream network output is a sequence of highlevel representations of these chunks. These are input to bi-directional long short-term memory (LSTM) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref> units to analyze long-term temporal dynamics.</p><p>Previous deep learning approaches use features that are computed over the full spatial extent of the video frame. We show the importance of using a tracked bounding box around the person to compute features relative to the location of the person, in addition to full-frame features, to provide both location-independent and location-dependent information. Unlike some previous work that represents motion information using a sequence of flow fields <ref type="bibr" target="#b21">[22]</ref>, we instead use a sequence of corresponding pixel displacements that we call pixel trajectories, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The advantage of pixel trajectories is that the displacements for a moving point in the scene are represented at the same pixel location across several frames. We analyze the relative im- <ref type="figure">Figure 1</ref>. Framework for our approach. Short chunks of a video are given to a multi-stream network (MSN) to create a representation for each chunk. The sequence of these representations is then given to a bi-directional LSTM, which is used to predict the action label, Ai. Details of the multi-stream network are shown in <ref type="figure" target="#fig_0">Fig. 2.</ref> portance of each of these components using two different datasets. The first is the MPII Cooking 2 Dataset <ref type="bibr" target="#b20">[21]</ref>, and the second is a new dataset we introduce containing overhead videos of people shopping from grocery-store shelves. Our results on the MPII Cooking 2 Dataset represent a significant improvement over the previous state of the art.</p><p>Our work includes the following novel contributions:</p><p>• We demonstrate the effectiveness of a bi-directional LSTM for the action detection task. It should be noted that although LSTMs have been used before for action recognition and sentence generation, we are the first to analyze the importance of LSTMs for action detection. Furthermore, since our LSTM layer is trained on full-length videos containing multiple actions (not just trimmed clips of individual actions), it can learn interactions among temporally neighboring actions. • We train a multi-stream convolutional network that consists of two 2-stream networks, demonstrating the importance of using both full-frame and person-centric cropped video. We use pixel trajectories rather than stacked optical flow as input to the motion streams, leading to a significant improvement in results. • We introduce a new action detection dataset, which we release to the community with this publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early work that can be considered action detection includes methods that detect walking people by analyzing simple appearance and motion patterns <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2]</ref>. Several algorithms have been proposed since then for detecting actions using space time interest points <ref type="bibr" target="#b32">[33]</ref>, multiple instance learning <ref type="bibr" target="#b8">[9]</ref>, or part-based models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10]</ref>. By adding another dimension (time) to object proposals, action proposals have also been used for detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Until recently, the standard pipeline for most video analysis tasks such as action recognition, event detection, and video retrieval was to compute hand-crafted features such as Histogram of Oriented Gradients (HOG), Motion Boundary Histogram (MBH), and Histogram of Optical Flow (HOF) along improved dense trajectories <ref type="bibr" target="#b27">[28]</ref>, create a Fisher vector for each video clip, then perform classification using support vector machines. In fact, shallow architectures using Fisher vectors still give state-of-the-art results for action/activity recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b20">21]</ref>. Wang et al. <ref type="bibr" target="#b28">[29]</ref> showed that results improved when hand-crafted features were replaced by deep features that were computed by convolutional neural networks whose inputs were images and stacked optical flow along trajectories. In <ref type="bibr" target="#b21">[22]</ref>, a two-stream network was proposed in which video frames and stacked optical flow fields (computed over a few frames) were fed to a deep neural network for action recognition. A similar architecture was used for spatial localization of actions <ref type="bibr" target="#b4">[5]</ref> in short video clips. However, these networks did not learn long-term sequence information from videos.</p><p>Since recurrent neural networks can learn long-term sequence information in a data-driven fashion, they have recently gained traction in the action recognition community <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, a 3D convolutional neural network followed by an LSTM classifier was successful at classifying simple actions. LSTMs have shown improved performance over a two-stream network for action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. Recently, bi-directional LSTMs were also successful in skeletal action recognition <ref type="bibr" target="#b3">[4]</ref>. However, even after using LSTMs, deep learning methods perform only slightly better than fisher vectors built on hand-crafted features for many action recognition tasks <ref type="bibr" target="#b14">[15]</ref>.</p><p>Although substantial progress has been made in action recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b20">21]</ref>, not as much work has been done in action detection (spatio-temporal localization of actions in longer videos). The major focus in action detection has been on using high level semantic information to improve performance, rather than making use of bottom up cues. Using annotations for the objects being interacted with <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref> or enforcing the grammar of the high level activity being performed <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> is generally helpful, though these approaches may require learning extra detectors for objects and having prior knowledge about high-level activities. Sun et al. <ref type="bibr" target="#b23">[24]</ref> used LSTMs for action detection although their focus was on leveraging web images to help with video action detection. Their paper did not analyze the importance of LSTMs as we do here.</p><p>For fine-grained action detection, extracting trajectories from spatio-temporal regions of interest or using handtrajectories has shown significantly improved performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>. In recent work in generating sentences from images, LSTM networks with attention models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14]</ref> learn to focus on salient regions in an image to generate captions for the image. Since motion and actor location are important clues for knowing where an action is happening, we were inspired by these methods to add our network's two person-centric streams, which capture information from regions of video that are salient due to actor motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our framework is shown in Figs. 1 and 2. First, we train four independent convolutional neural networks, each based on the VGG architecture <ref type="bibr" target="#b22">[23]</ref>, to perform the task of action classification when given as input a single small chunk (6 consecutive frames) of video. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, two of the networks (one each for images and motion) are trained on chunks of full-frame video, so that the spatial context of the action being performed is preserved. The other two networks (one each for images and motion) are trained on frames that have been cropped to a tracked bounding box. These cropped frames provide actions with a reference frame, which helps in classifying them. After these four networks have been trained, we learn a fully-connected projection layer on top of all four fc7 layer outputs, to create a joint representation for these independent streams. This multi-stream network (MSN) is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. As il-lustrated in <ref type="figure">Fig. 1</ref>, the multi-stream network is provided with full-length video (arranged as a temporal sequence of 6-frame chunks), and the corresponding temporal sequence of outputs of the projection layer is then fed into an LSTM network running in two directions. We use a fully connected layer on top of each directional LSTM's hidden states, followed by a softmax layer, to obtain an intermediate score corresponding to each action. Finally, the scores for the two LSTMs are averaged to get action-specific scores.</p><p>There are multiple components in an action detection pipeline that are critical for achieving good performance. In this task, we need a model that captures both spatial and long-term temporal information that are present in a video. Person tracks (bounding boxes) provide a reference frame that make many actions easier to learn by removing location variation from the input representation. Some actions, however, are location dependent. For scenes shot using a static camera, as in our test datasets, these actions always occur at the same image location. For example, washing/rinsing are almost always done near the sink, and opening a door would most likely be performed near a refrigerator or a cupboard. For these reasons, we train two separate deep networks each on pixel trajectories and image appearance. The first network is trained on the entire frame to preserve the global spatial context. The second network is trained on cropped boxes from the tracker to reduce background noise and to provide a person-centric reference frame for trajectories and image regions. To capture short-term temporal information, we use pixel trajectories, in which each moving scene point is in positional correspondence with itself across several frames. This alignment enables pixel trajectories to capture richer motion information than stacked optical flow fields. Since actions can be of any duration, our method uses LSTMs to learn the duration and longterm temporal context of actions in a data-driven fashion. Our results demonstrate that LSTMs are quite effective in learning long-term temporal context for fine-grained action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tracking for Fine-Grained Action Detection</head><p>To provide a bounding box around the person for the location-independent appearance and motion streams, any good person-tracking algorithm could be used. In this paper, we use a simple state-based tracker to spatially localize actions in a video with a single actor. Keeping the size (chosen manually) of the tracked bounding box fixed, we update its position so that the magnitude of flow inside the box is maximized. If the magnitude is below a threshold, the location is not updated (when the person is not moving, the bounding box is stationary). Initially, if no actor is present, the bounding box is arbitrarily placed. The location of the bounding box is updated only after a video chunk (6 frames) is processed and flow/appearance features are computed relative to it, to ensure that the bounding box is stationary over the 6 frames of a chunk. Our simple tracking method can be effectively applied when the camera is stationary and we have a reasonable estimate about the size of the actor. This is a practical assumption for many videos taken at retail stores, individual homes, or in a surveillance setting where fine-grained action detection is likely to be used. For more difficult tracking situations, a more sophisticated tracker would be needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training of Flow Networks</head><p>Stacking optical flow as an input to the deep network has been a standard practice in the literature to train motionbased networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. However, in stacked optical flow, the motion vectors corresponding to a particular moving point in the scene (e.g., the tip of a finger) change their pixel location from one frame to the next. Thus, the convolutional neural network needs to learn the spatial movement of optical flow for classifying an action. The complete motion information could be learned by the network at a higher layer, but that would require more parameters and data to learn. An alternate representation for motion in a sequence of frames is to compute flow from a central frame, t, to each of the K previous and K subsequent frames (we use K = 3). This representation, which we call pixel trajectories, is illustrated and compared with stacked optical flow in <ref type="figure" target="#fig_1">Figure 3</ref>. In all 2K frames of a pixel trajectory, the flow values from each point to the corresponding point in frame t are all located at the point's location in frame t. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, in pixel trajectories, only the intensity of the optical flow image changes (its location is fixed). Thus, the network can learn a temporal filter for each pixel more easily than from stacked flow fields. Now, for each pixel in frame t, we have the complete motion information in a short window of time. To learn motion patterns for each pixel, a 1× 2K convolutional kernel can produce a feature map for the movement of each pixel. In contrast, a network layer that inputs stacked optical flow (using, e.g., a 3 × 3 × 2K kernel on stacked optical flow) will not be able to learn motion patterns using the first convolutional layer for pixels that have a displacement of more than 3 pixels over 2K frames. A similar method to pixel trajectories was mentioned in <ref type="bibr" target="#b21">[22]</ref>, but there it yielded slightly worse performance than stacked optical flow, likely because it was applied on moving camera videos where trajectories are less reliable. For fine-grained action detection with a stationary camera, however, we demonstrate that pixel trajectories perform better than stacked flow for both datasets (see <ref type="table">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training on Long Sequences using a Bi-Directional LSTM Network</head><p>We now provide a brief background of Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) cells <ref type="bibr" target="#b7">[8]</ref>. Given an input sequence, x = (x 1 , . . . , x T ) , an RNN uses a hidden state representation h = (h 1 , . . . , h T ) so that it can map the input x to the output sequence y = (y 1 , . . . , y T ). To compute this representation, it iterates through the following recurrence equations:</p><formula xml:id="formula_0">h t = g(W xh x t + W hh h t−1 + b h ), y t = g(W hy h t + b z ),</formula><p>where g is an activation function, W xh is the weight matrix which maps the input to the hidden state, W hh is the transition matrix between hidden states at two adjacent time steps, W hy is a matrix which maps the hidden state h to the output y, and b h and b z are bias terms.</p><p>The weight update equations for an LSTM cell are as <ref type="figure">Figure 5</ref>. The bottom row shows the output of a method that produces contiguous segments. The top row shows another method, which generates the same proportion of non-contiguous segments.</p><p>follows:</p><formula xml:id="formula_1">i t = σ(W xi x t + W hi h t−1 + b i ) f t = σ(W xf x t + W hf h t−1 + b f ) o t = σ(W xo x t + W ho h t−1 + b o ) g t = tanh(W xc x t + W hc h t−1 + b c ) c t = f t c t−1 + i t g t h t = o t tanh(c t )</formula><p>where σ is a sigmoid function, tanh is the hyperbolic tangent function, and i t , f t , o t , and c t are the input gate, forget gate, output gate, and memory cell activation vectors, respectively. The forget gate f t decides when (and which) information should be cleared from the memory cell c t . The input gate i t decides when (and which) new information should be incorporated into the memory. The tanh layer g t generates a candidate set of values which will be added to the memory cell if the input gate allows it. Based on the output of the forget gate f t , input gate i t , and the new candidate values g t , the memory cell c t is updated. The output gate o t controls which information in the memory cell should be used as a representation for the hidden state. Finally, the hidden state is represented as a product between a function of the memory cell state and the output gate.</p><p>In action recognition datatsets (e.g., UCF 101), video clips are temporally trimmed to start and end at the start and end times of each action, and are generally short in length (e.g., from 2-20 seconds). Hence, in the action recognition task, there is not enough long-term context to be learned in a data-driven fashion. This long-term context could include properties such as the expected duration of an action, which action follows or precedes an action, or long-term motion patterns that extend beyond action boundaries. In an action recognition setting, an LSTM network has little access to the longer-term temporal context. However, in fine-grained action detection, videos are typically on the order of minutes or hours. Thus, LSTM networks are more suited to this task as they are designed to model long-term temporal dynamics in a sequence.</p><p>Action recognition involves assigning a single label to a video sequence, while in action detection we need to assign a label per frame. Consider a video sequence consisting of 100 frames where each frame has the same label. Even if a 2-stream network predicts correct labels only for 50 frames, it is likely that it would assign the correct label to the complete video sequence in the task of action recognition. For action detection, however, if the 50 correctly predicted frames are not contiguous, it would generate many action segments, and all but one of them would be assigned as false positives. A bi-directional LSTM (B-LSTM) on top of a 2stream network would be more likely to produce contiguous action segments, and would thus have fewer false positives for action detection when compared to a 2-stream network (see <ref type="figure">Figure 5</ref>). However, in such cases, B-LSTM would not show any improvement over a 2-stream network for action recognition, because recognition performance does not change even if the predicted labels are fragmented.</p><p>Bi-directional LSTM networks <ref type="bibr" target="#b5">[6]</ref>, illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>, integrate information from the future as well as the past to make a prediction for each chunk in the video sequence. Therefore, they are expected to be better at predicting the temporal boundaries of an action as compared to a unidirectional LSTM. In this work, the forward and backward LSTM networks each give softmax scores for each action class, and we average the softmax predictions of the two LSTM networks to obtain the score for each action. While training these networks on long sequences, backpropagation through time can only be done up to a fixed number of steps, using a short sequence of chunks. To preserve long-term context, we retain the hidden state of the last element in the previous sequence when training on the subsequent sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our method on two datasets: the MPII Cooking 2 Dataset <ref type="bibr" target="#b20">[21]</ref>, and the new MERL Shopping Dataset that we collected and are releasing to the community. The MPII Cooking 2 Dataset consists of 273 video sequences that vary in length from 40 seconds to 40 minutes, with a total of 2.8 million frames. The videos are labeled with the start and end times of fine-grained actions from 67 action classes (6 of them are not part of the test set). Actions such as "smell," "screw open," and "take out" may be as brief as one-half of a second, while actions such as "grate," "peel," and "stir" can last as long as a few minutes. There is also significant intra-class variation in the duration of an action.</p><p>Our MERL Shopping Dataset consists of 96 two-minute videos, shot by a static overhead HD camera, of people shopping from grocery-store shelving units that we set up in a lab space. There are 32 subjects, each of whom is in 3 videos collected on different days. Videos are labeled with the start and end times of fine-grained actions from 5 different action classes: "Reach to Shelf," "Retract from Shelf," "Hand in Shelf," "Inspect Product," and "Inspect Shelf." We divide this dataset into three partitions: 60 training videos, 9 validation videos, and 27 test videos. For each subject, all <ref type="figure">Figure 6</ref>. Images for various actions in the MERL Shopping Dataset. We show images corresponding to different actions such as "retract from shelf," "inspect product," "hand in shelf," and "inspect shelf." three videos of that subject are in only one of the three partitions. Although the number of videos in this dataset is less than in MPII Cooking 2, there are many action instances per video, so the number of frames per action class is high (∼ 30,000). In this dataset, the duration of an action ranges from one-half of a second to on the order of a minute. We show examples of frames from this dataset in <ref type="figure">Figure 6</ref>. The version we make public will have more videos, as well as revised labels for existing videos. We will release our results for the larger version along with the dataset.</p><p>Although our algorithm performs both temporal and spatial localization of actions, only temporal accuracy is evaluated in order to be consistent with the MPII Cooking 2 evaluation protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We sample each video at 15 frames per second. We then extract optical flow between each frame (sampled every 6 frames) and its 6 neighboring frames (K = 3 each to the left and right). This provides pixel trajectories for each pixel. Epic flow is used to compute optical flow <ref type="bibr" target="#b18">[19]</ref>, as it gives reliable flow even for large movements. We then use our tracker to obtain bounding boxes for each video. Finally, all full-size image frames and cropped frames are resized to 256×256. Pixel trajectories are resized to 224×224. For training of frame-based networks (the appearance stream), we fine-tune VGG net <ref type="bibr" target="#b22">[23]</ref> using Caffe <ref type="bibr" target="#b11">[12]</ref>. From each 6-frame chunk of video, we use a single image frame for the appearance streams. We encode two sets of 6 optical flow fields (one stack each for xand y-direction) as pixel trajectories for the motion stream. While training motion networks, we change the conv 1 filter of VGG to a 1 × 2K kernel, which only performs convolution in time. We project the four fc7 layers of the multi-stream network using a fully connected layer to a 200-dimensional vector. This 200-dimensional vector is given to two LSTM layers (one forward and one backward in time) with 60 hidden units each. Finally, a softmax classifier is trained on each LSTM's hidden units, and the softmax predictions of both LSTM networks are averaged to get the action scores for each class. While training LSTMs for detection, we use the entire video sequence, so this also includes a background class. We use the same architecture for both datasets. Since the four networks that make up our multi-stream network cannot all fit in GPU (Tesla K40) memory at once, we train each network independently. To train the LSTM networks, we use the implementation provided in <ref type="bibr" target="#b2">[3]</ref>.</p><p>Since mean Average Precision (mAP) is the standard measure used to evaluate action detection in past work, we need to produce a ranked list of action clips, along with a start frame, an end frame, and a score associated with each clip. Midpoint hit criterion is used to evaluate detection as done in <ref type="bibr" target="#b20">[21]</ref>. This means that the midpoint of the detected interval should lie within the ground-truth interval in the test video. If a second detection fires within the same ground-truth interval, that second detection is considered a false positive. We use the evaluation code used in <ref type="bibr" target="#b19">[20]</ref>.</p><p>To obtain segments for each action class, we start with an initial threshold. We apply this threshold to the output score (average of the two LSTM softmax outputs) that was assigned to each 6-frame chunk of video by our MSB-RNN network. We group the above-threshold chunks into connected components, each of which represents one detection, which we refer to as a clip (defined by its start and end time). The initial threshold will give us some number of detections. If the number of detections is less than m for a class, we lower the threshold further until we get m unique clips. To get the next set of clips, we lower the threshold until we get 2m unique clips. If a new action clip intersects with any clip in the previous set, we discard the new clip. We keep on doubling the size of the next set until we obtain 2500 unique clips. In our experiments, we set m = 5. Each clip consists of some number of consecutive 6-frame chunks of video, each of which is assigned an output score (average of the two LSTM softmax outputs) by our MSB-RNN system. We assign a score to each clip by max-pooling the output scores of all of the chunks in the clip. Since the validation set in the MPII Cooking 2 Dataset does not contain every action class in the dataset, we adopt this method because it enables us to obtain a ranked list of detections without requiring us to select detection thresholds for each action class. We use the same process on the MERL Shop-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mAP</head><p>Hand-cSIFT <ref type="bibr" target="#b20">[21]</ref> 10.5% Hand-trajectories <ref type="bibr" target="#b20">[21]</ref> 21.3% Hand-cSIFT+Hand-trajectories <ref type="bibr" target="#b20">[21]</ref> 26.0%</p><p>Dense Trajectories <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21]</ref> 29.5% Two-Stream Network <ref type="bibr" target="#b21">[22]</ref> 30.18% DT+Hand-trajectories+cSIFT <ref type="bibr" target="#b20">[21]</ref> 34.5% MSB-RNN 41.2% ping Dataset. We replicate the output labels for each chunk 6 times to get per-frame labels for evaluation. The above process is similar to non-maximal suppression, as we rank confident detections at the top and do not include less confident detections in the ranked list whose subsets have already been detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments</head><p>In <ref type="table" target="#tab_0">Table 1</ref> we show that our MSB-RNN obtains an mAP of 41.2% on the MPII Cooking 2 Dataset, outperforming the previous state of the art's mAP of 34.5%. Note that the 34.5% reported in <ref type="bibr" target="#b20">[21]</ref> is a very strong baseline. Dense trajectories are still known to give state-of-the-art performance in fine-grained action recognition and detection, and <ref type="bibr" target="#b20">[21]</ref> uses a combination of dense trajectories along with the additional hand-centric color-SIFT and Hand-trajectories features. Our implementation of the two-stream network <ref type="bibr" target="#b21">[22]</ref> (just our two full-frame streams, without our person-centric streams, and without the LSTMs) yields an mAP of 30.18% on this dataset, which is only slightly better than the performance of using improved dense trajectories alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel Trajectories</head><p>In <ref type="table">Table 2</ref>, we compare the effectiveness of variations of the person-centric (cropped) appearance stream ("Frame"), and the person-centric motion stream using either pixel trajectories ("Trajectories") or stacked optical flow ("Stacked OF"). We evaluate mAP for two versions of each network: when the stream is followed by a unidirectional LSTM layer, and when the LSTM is omitted and replaced by a softmax layer. Using pixel trajectories instead of stacked flow improves performance both with and without LSTM, on both the MPII Cooking 2 (MPII 2) and MERL Shopping (Shop) datasets, making pixel trajectories a clear winner over stacked flow for action detection. For all three types of streams on both datasets, the LSTM layer produces a large improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Stream Network</head><p>In <ref type="table">Table 3</ref> we compare the performance of our multistream network (using both full-frame and person-centric bounding boxes) with that of a two-stream network (full-  <ref type="table">Table 3</ref>. Performance comparison of multi-stream vs. two-stream network. Performance when multi-stream network is followed by each unidirectional LSTM or by their bi-directional combination (MSB-RNN). mAP is reported.  <ref type="figure">Figure 7</ref>. Mean Average Precision (mAP) for pixel trajectory network (left) and frame network (right) on the Shopping Dataset, when followed by a unidirectional LSTM layer with restricted memory duration. LSTM memory duration (x-axis) is expressed as a number of 6-frame chunks. Note that the LSTM network can effectively remember as far as 10 chunks (4 seconds) into the past. frame only). Including a person-centric reference frame improves performance on both datasets. If we use B-LSTM only on full-frame features (Two-Stream + BLSTM, not shown in result tables), the performance drops by 3.4% on the MERL Shopping Dataset and 1.8% on the MPII Cooking 2 dataset compared to MSB-RNN. We report results for individual actions of the Shopping dataset in <ref type="table" target="#tab_2">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM</head><p>Tables 3 and 4 also compare the performance of our multi-stream network when followed by a forward unidirectional LSTM, a backward unidirectional LSTM, or the bi-directional LSTM (which is our complete MSB-RNN system). Each unidirectional LSTM provides a significant boost, and including bi-directional LSTMs (MSB-RNN) yields an even larger improvement, because it provides more temporal context than a unidirectional LSTM. The results in these tables and <ref type="table">Table 2</ref> clearly show that the LSTM layer is the most important factor contributing to our system's improved performance over previous methods. These observations led us to explore in more detail why using an LSTM layer improves performance by such a large margin. We conducted two experiments to analyze the contributions of an LSTM layer to our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How long does the LSTM remember?</head><p>In the first experiment, we use the trained model and analyze the effective temporal duration of the LSTM layer's memory. For this experiment, we clear the memory of the LSTM layer at different time steps using a continuation sequence indicator. A continuation sequence indicator is 0 at the beginning of a sequence and 1 otherwise. Thus, we can set every kth indicator to 0 for clearing the memory, if we are interested in an LSTM which remembers history from the past k chunks in the sequence. However, this would abruptly reduce the memory of the element at the beginning of the sequence to zero. To avoid this problem, we generate k different continuation indicator sequences, shifted by one element, to limit the memory of the LSTM layer to k time steps. Thus, when a prediction is made for an element, we choose the output from the sequence whose continuation indicator was set to zero k time steps before. In <ref type="figure">Figure 7</ref>, we plot the mAP when an LSTM is used on top of frame or pixel trajectory features on the Shopping Dataset. We observe that performance improves as we increase the memory duration for LSTM. It is quite encouraging that the unidirectional LSTM layer can make effective use of as many as 10 preceding chunks in a sequence. Thus, a bidirectional LSTM would use a context of 20 chunks in a sequence while making a prediction. In the context of a video, where each chunk comprises 6 frames of a video (sampled at 15 frames per second), this sequence length would correspond to 8 seconds. Thus, the bi-directional LSTM improves action detection performance by a large margin, by incorporating information from about 8 seconds of temporal context. Many actions last less than 8 seconds, and actions that last longer than that are likely to have a recurring pattern that can be captured in 8 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning transitions between actions</head><p>The first experiment (above) demonstrates that an LSTM can remember long-term temporal information. In the second experiment, we explore whether the LSTM can also learn information from the transitions between different actions in a video sequence. Recent works train an LSTM network on trimmed video sequences <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. Thus, they cannot learn long-term context that extends beyond the start or end of an action. Therefore, we conducted our second experiment, in which the continuation indicators are set to 0 (while training only) whenever an action starts or ends. This simulates training on trimmed video sequences, instead of a continuous video sequence that includes many actions. We observe that training on trimmed clips drops the performance from 77.24% to 75.51% on the Shopping Dataset and from 38.03% to 36.22% on the MPII Cooking 2 dataset (using a unidirectional LSTM). This confirms our hypothesis that training networks on long video sequences is beneficial as compared to training on temporally clipped videos of individual actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we showed that using a multi-stream network that augments full-frame image features with features from a bounding box surrounding the actor is useful in finegrained action detection. We showed that for this task, pixel trajectories give better results than stacked optical flow due to their location correspondence. We showed that to capture long-term temporal dynamics within and between actions, a bi-directional LSTM is highly effective. We also provided an analysis of how long an LSTM network can remember information in the action detection scenario. Finally, our results represent a significant step forward in accuracy on a difficult publicly available dataset (MPII Cooking 2), as well as on a new MERL Shopping Dataset that we are releasing with the publication of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure depicting our multi-stream network (MSN). The multi-stream network uses two different streams of information (motion and appearance) for each of two different spatial croppings (full-frame and person-centric) to analyze short chunks of video. One network (CNN-T) computes features on pixel trajectories (motion), while the other (CNN) computes features on RGB channels (appearance).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The middle row of squares represent a sequence of frames, and the arrows indicate the pairs of frames between which optical flow is computed for both pixel trajectories (arrows above the frames) and stacked optical flow (arrows below the frames). The top and bottom rows show the y-component of optical flow computed for pixel trajectories (top row) and stacked flow (bottom row). In pixel trajectories, note that only the intensity changes, while the spatial layout of the image stays the same. Thus, only a single convolution layer in time is sufficient for learning motion features for a pixel. In stacked optical flow, however, the spatial correspondence between pixels is lost. For example, the back of the head (lowest point of the silhouette) moves up and to the left in subsequent images of stacked optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Connections depicting architecture of a bi-directional LSTM<ref type="bibr" target="#b6">[7]</ref>. The circular nodes represent LSTM cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Comparison of performance of our MSB-RNN system 
with previous action detection methods on the MPII Cooking 2 
dataset. Mean Average Precision (mAP) is reported. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Results for each action class in the Shopping Dataset using various network configurations.</figDesc><table>Action 

Two Stream 
MSN 
LSTM ← LSTM → MSB-RNN 
Reach To Shelf 
75.95% 
80.8% 
84.39% 
84.86% 
89.74% 
Retract From Shelf 
74.53% 
77.71% 
81.45% 
84.61% 
90.47% 
Hand In Shelf 
52.48% 
54.13% 
59.69% 
68.73% 
65.56% 
Inspect Product 
67.56% 
75.68% 
79.29% 
78.6% 
82.7% 
Inspect Shelf 
55.52% 
57.09% 
70.57% 
70.31% 
73.09% 
Mean 
65.21% 
69.08% 
75.08% 
77.42% 
80.31% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors acknowledge the University of Maryland supercomputing resources http://www.it.umd.edu/ hpcc made available for conducting the research reported in this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Behavior Understanding</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust real-time periodic motion detection, analysis, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="781" to="796" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks: Formal Models and Their Applications (ICANN)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action detection in complex scenes with spatial and temporal ambiguities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representing videos using mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2571" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
		<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple granularity analysis for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Paramathayalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="756" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4506</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing fine-grained and composite activities using hand-centric features and script data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Conference on Multimedia</title>
		<meeting>the 23rd ACM Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable part models for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2642" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 9th International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="734" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1302" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative subvolume search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2442" to="2449" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
