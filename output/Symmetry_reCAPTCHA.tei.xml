<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symmetry reCAPTCHA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Funk</surname></persName>
							<email>funk@cse.psu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The Pennsylvania State University University Park</orgName>
								<address>
									<region>PA. 16802</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The Pennsylvania State University University Park</orgName>
								<address>
									<region>PA. 16802</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Symmetry reCAPTCHA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is a reaction to the poor performance of symmetry detection algorithms on real-world images, benchmarked since CVPR 2011. Our systematic study reveals significant difference between human labeled (reflection and rotation) symmetries on photos and the output of computer vision algorithms on the same photo set. We exploit this human-machine symmetry perception gap by proposing a novel symmetry-based Turing test. By leveraging a comprehensive user interface, we collected more than 78,000 symmetry labels from 400 Amazon Mechanical Turk raters on 1,200 photos from the Microsoft COCO dataset. Using a set of ground-truth symmetries automatically generated from noisy human labels, the effectiveness of our work is evidenced by a separate test where over 96% success rate is achieved. We demonstrate statistically significant outcomes for using symmetry perception as a powerful, alternative, image-based reCAPTCHA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With increasing malicious online attacks against companies, individuals and governments alike, CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref> becomes a necessary tool in many web applications to prevent automatic login, blog post, email, and DoS (denial-of-service) attacks. CAPTCHAs are constructed by taking advantage of the behavioral discrepancies between humans and machines. For example, since humans are in general superior at recognizing distorted text than computers, text-based CAPTCHAs such as <ref type="bibr" target="#b41">[42]</ref> have been widely used. reCAPTCHAs, where one word is used for testing and the other for dataset expansion, can also be implemented and deployed easily. Meanwhile, successful breakings of existing CAPTCHAs (e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>) have been extensively reported as well, including a recent self-broken reCAPTCHA by Google <ref type="bibr" target="#b16">[17]</ref>.</p><p>We are witnessing a healthy interleaving of more advanced CAPTCHAs that provide better software security and breakings of CAPTCHAs which advance machine intelligence. In recent years, automatic text recognition is closing the gap to human performance <ref type="bibr" target="#b16">[17]</ref>, thus more image-based CAPTCHAs have emerged as an alternative <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41]</ref>. Most current image-based CAPTCHAs focus on recognition of common objects, such as cats, dogs, human faces or relating objects (with the help of words) semantically. The primary challenges for computers come from image distortions and object semantics. Little work has explored and taken advantage of human ability in visual abstraction, for example, the perception of symmetry from photos (e.g. <ref type="figure" target="#fig_0">Figure 1</ref>). Strong evidence of inherent symmetry detection capability in humans has been shown at both behavioral and neural levels <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref>, while the outcome from the 2011/13 CVPR symmetry detection competitions <ref type="bibr" target="#b22">[23]</ref> is less encouraging. This performance gap between humans and machines presents an opportunity for a symmetryperception-based CAPTCHA. To the best of our knowledge, no systematic elicitations of human understanding of reflection and rotation symmetries on a large set of realworld photos has been reported, nor has the gap of symmetry perception between humans and computers been welldefined or quantified. This leads to our specific contributions in this work. We provide:</p><p>• via crowdsourcing, a systematic collection and evaluation of human symmetry labels from 401 online raters on 1,200 photos; • a well-defined set of symmetry metrics and algorithms to determine human perceived Ground Truth symme-tries from noisy human labels; • a systematic quantification of the gap between the human and machine symmetry perception in terms of precision and recall rates; • a prototype image-based symmetry reCAPTCHA; and • a validation of the proposed reCAPTCHA on 118 human online-users with a &gt; 96% success rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>reCAPTCHA <ref type="bibr" target="#b41">[42]</ref> is one of the most well-known CAPTCHAs and is widely used on the Internet today. It combines labeling new ground truth while simultaneously acting as a CAPTCHA. reCAPTCHA uses text recognition as the challenge and provides each rater two words. One word where the transcription is known is used as the CAPTCHA; the other word is unknown. The unknown words are found by two separate OCR algorithms transcribing books. A consensus has to be reached before the word is added into the new challenge word set. The reCAPTCHA system can detect humans with a success rate of 96.1%, but with the steady advancement of OCR <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref> in recent years, it has become less successful at rejecting computers.</p><p>Researchers have investigated various forms of CAPTCHAs based on natural image understanding. These can be divided into (1) object recognition-based and (2) 3D model-based (e.g. orientation judgment).</p><p>Simple image recognition CAPTCHAs <ref type="bibr" target="#b2">[3]</ref> present raters with undistorted generic images to be labeled using the provided word lists. ESP-PIX asks raters to recognize what object is common in a set of images <ref type="bibr" target="#b13">[14]</ref>. ARTiFACIAL exploits the face detection gap between human and algorithms. It provides synthesized images with a distorted face embedded in a cluttered background and raters are requested to first find the face and then click on four eye corners and two mouth corners on the face <ref type="bibr" target="#b31">[32]</ref>. Avatar CAPTCHA uses a similar task where the raters are required to find a synthetic faces among a table of faces, although computers have outperformed humans at the task <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>. Asirra is based on cat/dog classification; it presents images of 12 cats and dogs and asks raters to identify the cats among them <ref type="bibr" target="#b12">[13]</ref>. Golle <ref type="bibr" target="#b15">[16]</ref> shows that Asirra is vulnerable to machine learning attacks since it is possible to train a classifier that can identify cats and dogs with high accuracy. PI-CATCHA <ref type="bibr" target="#b29">[30]</ref> asks the rater to identify a type of object, such as books, balls, or buildings, from 8 listed images. Datta et al. <ref type="bibr" target="#b7">[8]</ref> explored further the use of systematic image distortion in designing CAPTCHAs and found that combining multiple atomic distortions can significantly reduce machine recognizability <ref type="bibr" target="#b38">[39]</ref>. Mitra et al. <ref type="bibr" target="#b25">[26]</ref> propose "emergence images" rendered with noise and clutter from 3D models as a potential source of CAPTCHA.</p><p>The second type of image-based CAPTCHAs relies on an understanding of orientations of 3D models. Gossweiler et al. <ref type="bibr" target="#b17">[18]</ref> introduced the idea of correcting image orientation for designing the "What's Up" CAPTCHA; it uses images drawn from popular web searches as a potential database and asks the humans to correct the image's orientation. Ross et al. <ref type="bibr" target="#b30">[31]</ref> presented the Sketcha CAPTCHA that requires raters to determine the upright orientation for a selection of 3D objects rendered as line drawings. Sketcha provides the rater with only four orientation options compared with a continuous rotation from "What's Up".</p><p>Many crowdsourcing image annotation tools have been developed for object segmentation and image labeling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref>, while little is available for labeling symmetry directly on photos via crowdsourcing, which is a new tool we develop in this work. Furthermore, our work differs from previous image-based CAPTCHAs in utilizing symmetry as a cue for CAPTCHA, which is objectclass, scale, shape, orientation and color independent. Our experiments demonstrate that a symmetry-concept-based CAPTCHA can be understood and utilized by human subjects across culture, age, gender, and education levels.</p><p>Symmetry detection has been a lasting research topic in computer vision and computer graphics <ref type="bibr" target="#b23">[24]</ref>. From the benchmarked outcome of 2011/2013 CVPR symmetry detection from real-world images competitions <ref type="bibr" target="#b22">[23]</ref>, the algorithm developed by Loy and Eklundh <ref type="bibr" target="#b24">[25]</ref> has been shown to yield consistently the best performance on both reflection and rotation symmetry detection. The algorithm is fast, requires no image-segmentation, and recognizes reflection and rotation symmetries respectively from extracted SIFT keys via an effective voting scheme. During these competitions, <ref type="bibr" target="#b24">[25]</ref> has been compared against dozens of algorithms. Thus far, we have not found newer work that surpasses <ref type="bibr" target="#b24">[25]</ref> consistently. Thus <ref type="bibr" target="#b24">[25]</ref> is chosen to be the representative for computer symmetry detection algorithms in our evaluation of machine perception of real world symmetries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Data Initialization</head><p>Our dataset consists of 1,200 images from the Microsoft COCO database <ref type="bibr" target="#b21">[22]</ref>. To be included in our initial image data set, each image must satisfy two conditions (through visual inspection by undergraduate/graduate students): (1) the image presents some kind of visual symmetry and (2) the symmetry detection algorithm <ref type="bibr" target="#b24">[25]</ref> fails on detecting the most prominent symmetry in the image. Before presenting the images to human raters, each image is scaled to 400 pixels (largest dimension) to permit two images to fit on most computer screens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mechanical Turk Labeling Tool</head><p>We have designed and implemented a graphical interface for human raters to enter their choice of perceived realworld symmetries on an image <ref type="figure" target="#fig_0">(Figures 1 and 2)</ref>. After a short introduction on the general concept of symmetry with visual examples, each Amazon Mechanical Turk rater must pass a training session that consists of labeling four images ( <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>The user interface ( <ref type="figure" target="#fig_1">Figure 2</ref>) guides the raters to label a rotation or a reflection symmetry. Once a type of symmetry is selected, they can identify a rotation center by one click or a reflection axis by clicking the two end points of a line segment. The rater has the choice of either identifying at least one perceived symmetry or skipping either image or both images. The task requires the rater to label 100 images total and can skip at most 100 images during the experiment. The ability to skip an image enables the rater to not be forced to label a symmetry unless he or she perceives one. The raters are asked to label the symmetries within the image according to perceived prominence. Each rater is given at least 90 unique images and 10 repeats. The repeats are used to determine the rater's reliability. We have collected 78,310 rater labeled symmetries from 401 raters. Statistical information on data collection is shown in <ref type="table">Table 1</ref>. We define a Skip-Label Score (SL-Score) to reflect the number of times each image is skipped due to a lack of perceived symmetries and is labeled due to a perceived symmetry. See <ref type="figure" target="#fig_2">Figure 3</ref> for a distribution of images in the SL-score space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Symmetry Distance Metrics</head><p>Our data is a collection of human inputs in the form of labeled symmetries on an image, either for rotation symmetry centers (one point) or for reflection symmetry axes (two points). An intriguing research question as well as an engineering necessity is HOW to group such labels into their intended semantic meanings: symmetry X is at this position of this image. A key theoretical basis for such a grouping task is the distance metric(s) that can measure the similarities among different human-perceived symmetry labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Definitions of Symmetry Distance</head><p>We define the rotation symmetry distance D • as the Euclidean distance between the two labeled rotation symmetry centers. Given two reflection axes (L 1 and L 2 ), the reflection symmetry distance between them is a measure comparing two line segments with potentially different lengths, positions and orientations. We propose four different reflection symmetry measures for a pair of reflection symmetry axes: d a (end points), d b (mid points), d c (mid-point to line), and d d (angle) distances as defined and illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>Previous work has observed that the closeness of two line segments is proportional to their lengths, i.e., two 10foot lines 1-inch apart may be perceived as closer than two 1-inch long line segments 1-inch apart. <ref type="bibr" target="#b22">[23]</ref>. We thus weigh our reflection symmetry distance measures d a , d b , d c with the lengths of the corresponding reflection axes by defining:</p><formula xml:id="formula_0">R = |L 1 | + |L 2 | 2 (1) D a = d a R , D b = d b R , D c = d c R , D d = d d .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Distance Distribution of Nearest Symmetries</head><p>Given a pair of human labels, we need to find out: are they labeling the same symmetry? In addition to a proper set of symmetry distance measures (Section 3.3.1), we need a membership algorithm to determine whether two labels belong to the same group. We build statistical distributions of nearest neighbors of all labeled symmetries for each symmetry distance ( <ref type="figure" target="#fig_5">Figure 5</ref>). To determine the membership threshold τ automatically, we algorithmically discover the   'knee-of-the-curve' for the distribution curves ( <ref type="figure" target="#fig_5">Figure 5)</ref>, where the 'knee' is the point at which a function has the maximum curvature and where the curve starts to grow exponentially. We use the Kneedle algorithm by Satopää et al. <ref type="bibr" target="#b35">[36]</ref> since it is a general knee-of-the-curve finding algorithm and meets our needs of adaptive thresholding based on human perceptual input. The neighbors with distance beyond τ can be considered as symmetry labels for a different symmetry. In the case of reflection symmetry, the four different thresholds have to work jointly to determine the membership (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ground Truth Extraction</head><p>The ground truths (GTs) rotation or reflection symmetry in an image is computed from a consensus of human labels based on each rater's perception of a real-world symmetry in that image. To obtain a set of symmetry GTs objectively and computationally, we adapt DBSCAN <ref type="bibr" target="#b14">[15]</ref>, a method for Density-Based Spatial Clustering of Applications with Noise (the winner of the test-of-time award in 2014), with special considerations for perceptual symmetries by humans. Different from K-means, DBSCAN does not need the number of clusters as input. DBSCAN is a localneighborhood distribution-based method whose inputs are the minimum distance ǫ of a neighbor for being a member of the cluster and the minimum number of neighbors minPts to be a dense region or cluster. In our case, we have automatically obtained symmetry distance threshold τ for nearestneighbors under different symmetry distance metrics (Section 3.3.2, <ref type="figure" target="#fig_5">Figure 5)</ref>, and for each labeled symmetry to be considered as a GT symmetry we expect that at least two unique raters have labeled it independently. Here we have to enforce the independence condition for each cluster to containing a unique set of raters since the same rater may label a symmetry twice when given the same image as a repeat to test rater consistency (Section 3.2). Therefore, we have ǫ = τ and minP ts = 2 as the input parameters for  <ref type="figure" target="#fig_4">Figure 4</ref>) sorted from low to high (left to right). The 'knee point' τ is automatically determined using the Kneedle algorithm by Satopää et al. <ref type="bibr" target="#b35">[36]</ref>. The 'knee' is the point at which a function has the maximum curvature and where the curve starts to grow exponentially. The automatically found threshold for rotation symmetry labels is 5 (pixels); the four thresholds for reflection symmetry labels are: 0.18 for Da, 0.14 for D b , 0.09 for Dc and 7 (degrees) for D d . Note: Da, D b , Dc are normalized by the average lengths of the pair of reflection axes under consideration <ref type="figure" target="#fig_0">(Equations 1, 2</ref>). DBSCAN. <ref type="figure">Figure 6</ref> demonstrates a set of sample GTs extracted automatically from rater labels. To illustrate the human perceived level of prominence of a labeled symmetry, the radii of the rotation symmetry centers and the thickness of the reflection axes shown in <ref type="figure">Figure 6</ref> are proportional to the number of raters who have labeled that particular symmetry. The circular contour around each point (rotation center, end points of a reflection axis) indicates the location uncertainty of the labels. <ref type="table">Table 2</ref> shows the statistics of the automatically extracted GTs. Using this set of automatically computed reflection and rotation symmetry GTs, we re-evaluate the admissible image set to ensure that the algorithm <ref type="bibr" target="#b24">[25]</ref> can not pass as human. This step reduces the initial dataset size from 1200 images to 961 images <ref type="table">(Table  1)</ref>.</p><p>Total # of <ref type="bibr">GT 8,</ref><ref type="bibr">146</ref> Total # of Rotation GT 2,704</p><p>Total # of Reflection <ref type="bibr">GT 5,</ref><ref type="bibr">442</ref> Total # of Images with Rotation GT 836</p><p>Total # of Images with Reflection GT 961</p><p>Average # of Reflection GT/Image 6 ± 3</p><p>Average # of Rotation GT/Image 3 ± 3 <ref type="table">Table 2</ref>. Statistics of the automatically extracted Ground Truth Symmetries from the raters' labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Quantitative Evaluation and Comparison</head><p>After the construction of both a user interface for capturing human perceived symmetries and a computational method for extracting GT symmetries, we validate that our method is indeed achieving the intended goals: (1) On the data from 400+ anonymous online raters, we quantify the human and machine performance on the same image set (961 images) in terms of their precision and recall rate under the variations of three parameters that are, respectively; rotation-symmetry threshold, reflectionsymmetry threshold and the minimum number of symmetry labels required for a symmetry GT.</p><p>(2) On a new set of human raters, excluding those who participated in (1), we perform a validation test of "are you human or machine?" using a subset of the 961 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance of Human and Machine</head><p>True Positive: Given a labeled symmetry A (either a rotation or a reflection), a GT symmetry, a symmetry distance D m , its corresponding threshold τ m (Section 3.3.2), and B to represent all more prominent symmetry labels from the same rater on the same image where prominence is defined by the rater's ordering: </p><p>We vary three parameters to create the Precision-Recall (PR) curves. They are: rotation center distance threshold, reflection axis distance thresholds and the minimum number of labeled symmetries for defining a GT symmetry. We use the τ values automatically discovered in Section 3.3.2 ( <ref type="figure" target="#fig_5">Figure 5</ref>) as a basis, and increase the threshold values up to 5 times the base value. A comparison of PR-surfaces and the differences between the human and the machine et al. <ref type="bibr" target="#b24">[25]</ref> are shown in Figures 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Symmetry reCAPTCHA Validation</head><p>To evaluate the effectiveness of the symmetry re-CAPTCHA mechanism, we automatically select a subset    of test images which are most discriminative between human and machine perception of symmetries. Each image is selected based on human/machine performance evaluation (Section 4.1) according to the following criteria: (1) the computer algorithm fails to find any GT symmetries;</p><p>(2) the total number of raters who successfully labeled at least one symmetry on the image (the higher the total, the more prominent the symmetry appears to humans); (3) the success rate ( total TPs / total GTs ) of human raters on the image; and (4) the number of human raters who failed to label the correct GT symmetry on the image: TP=0 while FP = 0. Based on these conditions, a total of 40 test images are chosen. By pairing each test image with a non-test image, we constructed a symmetry reCAPTCHA. In practice, the system would have a limitless supply of photos from the internet, labeled by users during reCAPTCHA, creating an ever moving target of symmetry image set.</p><p>A new set of human raters (exclusive from Section 4.1) are presented with our prototype symmetry reCAPTCHA, whose labels are used to determine whether the labeler is a human and compute the overall success rate <ref type="table" target="#tab_3">(Table 3)</ref>. For 118 new human raters, our symmetry reCAPTCHA prototype achieves a success rate of 96.31%, 95.05% and 95.81% using rotation symmetries alone, reflection symmetries alone and jointly, respectively. Using a one-sample t-test, with the null hypothesis that the human test result comes from a normal distribution with zero mean (or the same as machine performance), leads to p-value = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary and Discussion</head><p>We have explored taking advantage of the discrepancy of symmetry perception between humans and machines to build a symmetry-based reCAPTCHA. We have carefully designed, implemented, and tested a user interface to solicit input of human-perceived imperfect symmetries on realworld photos from the MS COCO database. Our methods objectively and systematically extract user labeled symmetry ground truth as statistical consensus or viable clusters in the proper symmetry distance spaces. A 2-stage quantitative evaluation has shown that human performance on visual symmetry selection from real-world photos is indeed significantly superior to the best proven computer algorithm <ref type="bibr" target="#b24">[25]</ref> thus far. A prototype symmetry reCAPTCHA generates promising initial results. In their extensive survey of image-based CAPTCHAs, Zhu et al. <ref type="bibr" target="#b42">[43]</ref> proposed three guidelines for image-based CAPTCHAs. The first is to rely on the unambiguous semantics of the images. Symmetry reCAPTCHA relies on symmetry detection which is innate for humans across different cultures, and our experimental results support the observation that human symmetry perception is well-clustered and computable. The second criteria is allowing large variations: symmetry is a phenomenon independent of object class, shape, size and color. The last desirable criteria is the ease of making the image perceptions harder. Given the types and variances of possible symmetries (simple to complex, global to local, small to large ...), real-world symmetry detection from noisy data can be continuously challenged. Even without any intentional manipulations of the images, <ref type="bibr" target="#b24">[25]</ref> fails to find any ground truth symmetries from human labels of the 961 photos from COCO database selected in this work. When comparing performance of humans versus machines on symmetry detection, we observe that human subjects are less distracted by clutters around real world symmetry objects (e.g. face, half of a moon, eyes of a dog); while the machine tends to have issues with near regular textures in the background or recurring patterns in the foreground. Since our focus is on rotation or reflection symmetries (excluding translation symmetry), the computer vision algorithm often fails to capture the real symmetry in a crowded image. Another common failure of machine is when the plane of symmetry is not parallel with the camera plane. These issues are illustrated in <ref type="figure">Figure 6</ref>.</p><p>Real-world symmetries have presented computational challenges to computer vision for decades. We hope to motivate more research in this direction such that better algorithms can break symmetry reCAPTCHA in the near future. Meanwhile, we continue our effort on learning how humans perceive noisy symmetries. A better understanding of human perception at both the behavioral and neural levels will help our design of more robust algorithms for symmetry detection. We will release the full dataset of images to help others test and improve their symmetry algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>This work is supported in part by an NSF CREATIV grant (IIS-1248076).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Human labeled symmetries on real photos. Red: rotation center. Green: reflection axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The symmetry labeling interface (Section 3.2 ). Best viewed online.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The average number of labelers per image is 33 and the average number of skippers per image is 3. Images labeled 1, 2, and 3 (left column) represent those images without any skips by any rater and images labeled 4, 5, and 6 (right column) represent images around the peak of the distributions of the SL-images). The different colors of the image numbers are for ease of viewing. Each image pair shows the input image and the image with all labeled symmetries. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The four different reflection symmetry distance measures between two reflection symmetry axes L1 and L2; (a) da measures the minimum mean distance between the corresponding end points of L1 and L2; (b) d b measures the distance between the midpoints; (c) dc measures the average shortest distance between their midpoints and the other line; and (d) d d measures the angle difference between the two line segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The nearest symmetry distance distributions (Do for rotation symmetry, Da, D b , Dc, D d for four reflection symmetry distance measures defined in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>∃ GT : (∀ m : D m (GT, A) &lt; τ m ) ¬∃ B : (∀ m : D m (GT, B) &lt; τ ¬∃ GT : (∀ m : D m (GT, A) &gt; τ m ) 0 otherwise (4) False Negatives: FN = 1 if ¬∃ A : (∀ m : D m (GT, A) &lt; τ m ) 0 otherwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Seven randomly selected sample images where we index each image with its SL-score (skip-label score fromFigure 3) and show the original image, all human labels on the image, the symmetry GTs automatically extracted, followed by the output of<ref type="bibr" target="#b24">[25]</ref> showing the detected rotation and reflection symmetries where the detected symmetries are ranked from bright (top choice) to dark in intensities. This figure is best viewed on the computer.Human Precision Human Recall Machine Precision Machine RecallThe performance PR-surfaces of all human raters (401) in comparison to those of a computer algorithm<ref type="bibr" target="#b24">[25]</ref> on all 961 images. The three varying parameters are: (1) rotation symmetry center distance threshold; (2) reflection symmetry distance thresholds (four); and (3) the minimum number of symmetries required for a symmetry GT to be defined. With the increasing of minimum labeled symmetries required for a symmetry GT, one can observe a tradeoff between the human PR-curves due to down of symmetry GTs and up of FPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Table of statistics on the performance of the prototype symmetry reCAPTCHA.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CAPTCHA: Using Hard AI Problems for Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="294" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chellapilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Recognition CAPTCHAs. Information Security: 7th International Conference</title>
		<meeting><address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09-27" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="268" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making CAPTCHAs Clickable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Golle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakobsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Mobile Computing Systems and Applications, HotMobile &apos;08</title>
		<meeting>the 9th Workshop on Mobile Computing Systems and Applications, HotMobile &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Perceptual Similarity: A Texture Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Halley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Chantler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Developing a quantitative model of human preattentive vision. Systems, Man and Cybernetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Conners</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1384" to="1407" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">IMAGINATION: A Robust Image-based CAPTCHA Generation System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual ACM International Conference on Multimedia, MULTIME-DIA &apos;05</title>
		<meeting>the 13th Annual ACM International Conference on Multimedia, MULTIME-DIA &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="331" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting the Human-Machine Gap in Image Recognition for Designing CAPTCHAs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="504" to="518" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Construction and analysis of a large scale image ontology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Sciences Society</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">186</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable Multi-label Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;14</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3099" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Avatar CAPTCHA: Telling Computers and Humans Apart via Face Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Polina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Yampolskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electro/Information Technology (EIT)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Asirra: A CAPTCHA that Exploits Interest-aligned Manual Image Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Douceur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM Conference on Computer and Communications Security</title>
		<meeting>the 14th ACM Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esp-Pix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esp-Pix</surname></persName>
		</author>
		<ptr target="http://www.captcha.net/" />
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Golle</surname></persName>
		</author>
		<title level="m">Machine Learning Attacks Against the Asirra CAPTCHA. Proceedings of the 15th ACM Conference on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Learning Representations, (ICLR2014)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What&apos;s Up CAPTCHA?: A CAPTCHA Based on Image Orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gossweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web, WWW &apos;09</title>
		<meeting>the 18th International Conference on World Wide Web, WWW &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="841" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representation of Maximally Regular Textures in Human Visual Cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yakovleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Norcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="729" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning visual features for the avatar captcha recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Korayem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Yampolskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Applications (ICMLA), 2012 11th International Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="584" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leyton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Symmetry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Causality</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massachusetts</forename><surname>Cambridge</surname></persName>
		</author>
		<idno>book. 1</idno>
		<imprint>
			<date type="published" when="1992" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common Objects in Context. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Symmetry Detection from RealWorld Images Competition 2013: Summary and Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Slota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rauschert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computational Symmetry in Computer Vision and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hel-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="195" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting Symmetry and Symmetric Constellations of Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2006: 9th European Conference on Computer Vision</title>
		<meeting><address><addrLine>Graz, Austria; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="508" to="521" />
		</imprint>
	</monogr>
	<note type="report_type">Proceedings</note>
	<note>Part II</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yeshurun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno>163:1-163:8</idno>
	</analytic>
	<monogr>
		<title level="j">Emerging Images. ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>I-134 -I-141. IEEE</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR 2003. Proceedings. 2003 IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tag-Captcha: annotating images with CAPTCHAs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Human Computation</title>
		<meeting>the ACM SIGKDD Workshop on Human Computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="44" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distortion estimation techniques in solving visual CAPTCHAs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harkless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Potter</surname></persName>
		</author>
		<idno>II-23-II-28. IEEE</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<meeting>the 2004 IEEE Computer Society Conference on</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picatcha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picatcha</surname></persName>
		</author>
		<ptr target="http://picatcha.com/captcha/.1,2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sketcha: A Captcha Based on Line Drawings of 3D Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Halderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web, WWW &apos;10</title>
		<meeting>the 19th International Conference on World Wide Web, WWW &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ARTiFACIAL: Automated Reverse Turing Test Using FACIAL Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Multimedia, MULTIMEDIA &apos;03</title>
		<meeting>the Eleventh ACM International Conference on Multimedia, MULTIMEDIA &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="295" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Im-ageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LabelMe: A Database and Web-Based Tool for Image Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Symmetry activates extrastriate visual cortex in human and nonhuman primates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vanduffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Knutsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tootell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="3159" to="3163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Finding a &quot;kneedle&quot; in a haystack: Detecting knee points in system behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Satopaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed Computing Systems Workshops (ICDCSW), 2011 31st International Conference on</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Breaking Google CAPTCHAs for some extra cash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New York Times</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Building Hierarchies of Concepts via Crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</title>
		<meeting>the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="844" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Human Symmetry Perception and its Computational Analysis. VSP, Utrecht, The Netherlands</title>
		<editor>C. Tyler</editor>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Telling Humans and Computers Apart Automatically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="60" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">reCAPTCHA: Human-Based Character Recognition via Web Security Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcmillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="issue">5895</biblScope>
			<biblScope unit="page" from="1465" to="1468" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attacks and Design of Image Recognition CAPTCHAs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Computer and Communications Security, CCS &apos;10</title>
		<meeting>the 17th ACM Conference on Computer and Communications Security, CCS &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="187" to="200" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
