<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Canny Text Detector: Fast and Robust Scene Text Localization Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojin</forename><surname>Cho</surname></persName>
							<email>hojin.cho@stradvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stradvision, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungchul</forename><surname>Sung</surname></persName>
							<email>myungchul.sung@stradvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stradvision, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongjin</forename><surname>Jun</surname></persName>
							<email>bongjin.jun@stradvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stradvision, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Canny Text Detector: Fast and Robust Scene Text Localization Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel scene text detection algorithm, Canny Text Detector, which takes advantage of the similarity between image edge and text for effective text localization with improved recall rate. As closely related edge pixels construct the structural information of an object, we observe that cohesive characters compose a meaningful word/sentence sharing similar properties such as spatial location, size, color, and stroke width regardless of language. However, prevalent scene text detection approaches have not fully utilized such similarity, but mostly rely on the characters classified with high confidence, leading to low recall rate. By exploiting the similarity, our approach can quickly and robustly localize a variety of texts. Inspired by the original Canny edge detector, our algorithm makes use of double threshold and hysteresis tracking to detect texts of low confidence. Experimental results on public datasets demonstrate that our algorithm outperforms the state-ofthe-art scene text detection methods in terms of detection rate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text in scene images usually conveys valuable information, hence detecting and recognizing scene text has been considered important for a variety of advanced computer vision applications such as image and video retrieval, multilingual translation, and automotive assistance. Especially, as most text recognition applications require texts in images to be localized in advance, there is a significant demand for text detection algorithms that can robustly localize texts from a given scene image.</p><p>Previous works for scene text detection have utilized the sliding window method <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref> and connected component analysis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>. The sliding window based methods detect texts of a given scene image by shifting a window onto all locations in multiple scales. This is an exhaustive search, so these methods can achieve high recall rates. However, heavy computations are (a) Input (b) MSERs <ref type="bibr" target="#b19">[20]</ref> (c) ERs after NMS (d) Our localization result <ref type="figure">Figure 1</ref>. Canny text detector. Compared to the character candidates of MSERs or ERs, our method localizes characters more robustly with less false positives. unavoidable due to the thorough scanning of windows and a large number of candidates can result in a great deal of false positives.</p><p>On the other hand, connected component based methods first extract character candidates from an input image, and then refine the candidates to suppress non-text candidates. Stroke width transform (SWT) and maximally stable extremal region (MSER) are two representative techniques for connected component analysis, and these methods have achieved outstanding performance in scene text detection. But, common constraints used for refining candidates are considered somewhat restrictive to preserve various true characters, leading to low recall rate in practice.</p><p>In this paper, we propose a novel scene text detection algorithm, Canny Text Detector, which takes advantage of the similarity between image edge and text to provide significantly improved detection rate. As edge pixels construct the structural information (i.e., contour) of an object, we observe that cohesive characters compose a word or sentence sharing similar properties such as spatial location, size, color, and stroke width regardless of language. In the original Canny edge detector <ref type="bibr" target="#b3">[4]</ref>, each edge pixel is first classified as strong edge, weak edge, or non-edge. Then the algorithm employs edge tracking by hysteresis to find con-nected edges, based on the fact that usually the weak edge pixel coming from true edges are connected to strong edge pixels. Similar to the Canny procedure, we classify texts using double threshold and track them by hysteresis to make the best use of plausible text candidates, even if they have low confidence.</p><p>Specifically, the proposed Canny text detector is a multistage algorithm. We first extract character candidates using a variant of MSER. Then, each candidate is evaluated using an AdaBoost classifier trained with a sort of local binary patterns. The classification step utilizes double threshold to determine strong and weak candidates, and after applying tracking by hysteresis, credible characters are finally selected. The surviving characters are grouped into words or sentences. Experimental results on public datasets demonstrate that our algorithm outperforms the state-of-the-art scene text detection methods in terms of detection rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There are a variety of text localization techniques in the literature. The most common approach involves three key components <ref type="bibr" target="#b35">[36]</ref>: character candidate extraction, character classification, and text grouping. Grouping text as a set of words or sentences depends on the objective of the algorithm and may involve text line estimation and validation. Existing scene text detection algorithms can be divided into two types based on their character candidate extraction method: (1) sliding window based methods that exhaustively scan windows at all possible locations and scales, and (2) connected component based methods that utilizes character candidates extracted with particular constraints, e.g., consistent stroke width or extremal region.</p><p>The sliding window based methods detect text of a given scene image by shifting a window onto all locations in multiple scales <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>. Then for each window, whether the location contains text or not is determined by a classifier that is usually trained with low level features such as image gradients, intensity histogram, and variants of Wavelet coefficients. Although these methods can detect text effectively with high recall rate, their classification can be sensitive to false positives due to the large number of candidates. To suppress false positives, more advanced text/non-text classifiers such as support vector machine and random forest <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b25">26]</ref> and convolutional neural networks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> have been also proposed. But due to the heavy computations for intensive window scanning and advanced classification, these approaches are unsatisfactory to real-time applications.</p><p>Recent works on scene text detection tend to utilize connected component analysis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref>. In these works, character candidates are first extracted from an input image, where each candidate is a set of pixels sharing similar text properties. The candidates are then refined to suppress non-texts and grouped into final text. Popular techniques for connected component analysis are stroke width transform (SWT) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> and maximally stable extremal region (MSER) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>, and such methods provide a basis to achieve the notable performance in scene text detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Recently, Yin et al. <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref> proposed several techniques to refine MSERs and improve the robustness for oriented text. Shi et al. <ref type="bibr" target="#b27">[28]</ref> utilized geometric information of MSERs for text refinement and grouping. Neumann and Matas <ref type="bibr" target="#b20">[21]</ref> used pruning techniques on MSERs to exhaustively search the space of all character sequences. They later included text recognition for end-to-end text reading <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Despite the success of connected component analysis methods, we observe that constraints commonly used in previous approaches are not enough to preserve various true characters, leading to low recall rate in practice. Thus, this paper aims to address such limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Canny Text Detector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Criteria for text detection</head><p>Given that the prevalent scene text detection procedure is insufficient to achieve high recall rate, we first identify the general criteria that should be considered in text detection, as listed below:</p><p>Recall Text detection should localize as many text regions as possible. Precision The detected results should not contain non-text regions if possible. Uniqueness Each character detected from the operator should only be marked once. Compactness The detected region should accurately localize its corresponding character without extra margin.</p><p>Similar to the original Canny edge detector <ref type="bibr" target="#b3">[4]</ref>, we develop a multi-stage algorithm that incorporates the above criteria for effective scene text detection. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the overall process of our text detection algorithm, which is capable of fast and robust localization of scene text. To extract character candidates with better recall rate, we utilize extremal regions (ERs) that are extracted with relatively weak constraints compared to those of the original MSER <ref type="bibr" target="#b19">[20]</ref>. Overlapped candidates are reduced to a unique candidate by non-maximum suppression. We then classify the candidates with double threshold as one of strong text, weak text, and non-text. Strong text candidates are included in the final result, and weak text candidates that are connected to the strong texts are only selected </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Process overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Algorithm Details</head><p>In this section, we describe each algorithmic component of <ref type="figure" target="#fig_0">Fig. 2</ref> with specific examples as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Character Candidate Extraction</head><p>Many of the previous approaches have adopted MSER <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref> to extract character candidates and achieved remarkable performance <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15]</ref>. However, the constraint for maximum stability is often too strong to embrace various kinds of scene text in practice <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>. So we mitigate the maximum stability constraint and employ only extremal regions (ERs) for better recall to satisfy the Recall criterion.</p><p>An ER is a set of connected pixels in an image whose intensity values are higher than its outer boundary pixels. Mathematically it is defined as</p><formula xml:id="formula_0">R t = {x|I(x) &gt; I(y) ∀x ∈ R t , ∀y ∈ B(R t )},<label>(1)</label></formula><p>where x and y are pixel indices of a given single channel image I, t is a threshold value used for extracting the region, and B(R t ) is the set of boundary pixels of R t . We can easily obtain ERs of an image by thresholding it and building an ER tree using an inclusion relationship between the extracted ERs as described in <ref type="bibr" target="#b6">[7]</ref>. The resulting tree is a rooted and directional graph where each node corresponds to one connected component, i.e., extremal region R t . <ref type="figure" target="#fig_2">Fig. 4</ref> shows an example of an ER tree extracted using the intensity channel of the image shown in <ref type="figure" target="#fig_1">Fig. 3a</ref>. In this paper, we used six color channels separately to extract ERs, i.e., YCrCb color channels and their inverted channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Non-maximum Suppression</head><p>It is well known that MSERs have a large number of repeating components <ref type="bibr" target="#b39">[40]</ref>. Since ER is a superset of MSER,  the initial ERs also suffer from the same problem. To guarantee the Uniqueness criterion, we suppress the repeating ERs and allow only one ER that has the maximum stability. Note that this process is similar to the subpath partitioning and pruning of Sung et al. <ref type="bibr" target="#b30">[31]</ref>, but we first find overlapping ERs and then suppress non-maximum ERs with a slightly different stability measure. We observe that the repeating component problem mainly occurs because some ERs (i.e., character components) have high contrast and thus are extracted over multiple threshold values (see <ref type="figure" target="#fig_2">Fig. 4</ref>). To identify the repeating ERs, we use the following measure that estimates overlap between ERs based on the hierarchy of the ER tree:</p><formula xml:id="formula_1">O(R t−k , R t ) = |R t | |R t−k | ,<label>(2)</label></formula><p>where R t−k is the parent of R t in the ER tree, and |R| denotes the bounding box area of R. Note that we do not use R t+k because the ER tree can have multiple children and computing R t+k would be ambiguous. For each node R t , we count the number of overlaps, n o , with R t−k for all k such that O(R t−k , R t ) &gt; 0.7. Among the overlapping ERs, we remove ERs such that n o &lt; 3 and select the one with the highest stability where the stability is defined as</p><formula xml:id="formula_2">S(R t ) = (|R t−t ′ | − |R t |) |R t | .<label>(3)</label></formula><p>We used t ′ = 2 in our implementation. If there exist two or more ERs with the same stability, we choose the one having the smallest area. To further reduce the number of nontexts, we intuitively remove candidates that have too large or too small aspect ratio. After this step, we have character candidates which comply with both Uniqueness and Compactness criteria. The selected characters through nonmaximum suppression are shown in <ref type="figure" target="#fig_1">Fig. 3b.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Double Threshold Classification</head><p>The surviving character candidates are classified into three classes: strong text, weak text, and non-text. For the classification, we train our classifier using AdaBoost <ref type="bibr" target="#b9">[10]</ref> and multiple cascades <ref type="bibr" target="#b32">[33]</ref> to accelerate the classification speed. The overall structure consists of two blocks of cascaded classifiers, each with a threshold value that satisfies precision of 99.0% and 90.0% in the training set, respectively, corresponding to the high and low threshold values of the original Canny edge detector. Note that the relatively lower precision of 90.0% was intended for finding as many weak texts as possible (i.e., high recall).</p><p>Since selection of features has a crucial impact on the classification performance, we use the mean local binary pattern (MLBP) which is known to be robust to illumination and rotation variations <ref type="bibr" target="#b1">[2]</ref>. The MLBP is a variant of the local binary pattern <ref type="bibr" target="#b24">[25]</ref>. Given a pixel, the average intensity value of 8-connected neighbors in a 3 × 3 patch is first calculated, and then compared with the intensity value of each pixel excluding the center pixel. If the pixel value is larger then the average value, then the pixel gets '1', otherwise '0'. Then, starting from the left-top pixel and going clockwise, the values are encoded into an 8-bit number, as illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>For training the English and Chinese classifiers, we gathered about 53,000 and 20,000 positive samples, respectively, together with about 50,000 negative samples (i.e., non-text) for each cascade using a bootstrap process. These samples were normalized to a size of 24 × 24 in gray-scale.</p><p>In our double threshold classification, all candidates goes through the first cascade block, and are classified as strong text or non-strong text. Non-strong text candidates goes through the second cascade block, which in turn classifies them as weak text or non-text. Figs. 3c and d show the classification results with double threshold, i.e., strong texts and weak texts, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Text Tracking by Hysteresis</head><p>We include the strong text in the final result, as they are classified with high confidence. However, the weak text can be either true text or non-text (e.g., window, leaf, and fence). So they are included if and only if they have similar properties to strong text candidates.</p><p>To meet the Recall criterion with a high recall rate, we start from each strong text R s and track its neighborhood text candidates classified as weak text, R w . Whenever R w satisfies the similar text properties against R s , we change the status of R w to R s and investigate its neighbors recursively. The properties we used are as follows:</p><p>1. The spatial location of R s and R w is close enough to be considered as part of the same text. The distance between them is less than twice of the maximum of height and width of R s . 2. The size (i.e., width and height) of R s and R w is similar enough to be considered as part of the same text.</p><p>In each size dimension, the difference is less than the minimum value between R s and R w . 3. The color in the YCrCb color space of R s and R w is similar enough to be considered as part of the same text. The difference between them in each channel is less than 25. 4. The ratio between large and small stroke widths of R s and R w is less than 1.5.</p><p>In our experiments, a variety of text is well tracked in diverse scenes. However, some characters may overlap because of the candidate extraction from different color channels and partial detection (e.g., detecting "l" from "T") that are not filtered by non-maximum suppression. To address this, we merge overlapping characters after text tracking if their intersection-over-union measure is greater than 0.5. <ref type="figure" target="#fig_1">Fig. 3e</ref> shows the tracked texts via hysteresis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Text Grouping</head><p>With double threshold classification and text tracking by hysteresis, we can robustly obtain credible characters. However, some applications require word-or sentence-level localization results in practice. For instance, the robust reading competition (RRC) of the international conference on document analysis and recognition (ICDAR) takes such grouped localization results for evaluation since words can provide more valuable information than individual characters in text reading.</p><p>Fortunately, the main advantage of our method is easy grouping. First of all, our method has extracted as many characters as possible, even if they have low confidence. So there is less chance to miss characters in a word, compared to other approaches. Second, as we have tracked character candidates by hysteresis, we can apply almost the same rules for grouping. Specifically, we compare two candidates on spatial location, size, color and aspect ratio using the same threshold values in Sec. 4.4. If they satisfy the properties, then we group them into the same word.</p><p>To provide compact bounding boxes as output, we compute the minimum-area encasing rectangle <ref type="bibr" target="#b8">[9]</ref>. Unlike the previous approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39]</ref>, we do not estimate the bottom or center line of characters. Instead, we estimate the smallest rectangle that encloses grouped characters in the 2D image space using the 2D coordinates of character pixels. The final grouping results of the proposed method are shown in <ref type="figure" target="#fig_1">Fig. 3f</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>We implemented our method using C/C++. Our testing environment is a PC running MS Windows 7 64bit version with Intel Core i7 CPU of 4.00GHz. In this section, we quantitatively evaluate the proposed algorithm in terms of character-level recall rate and text-level localization performance on the most widely used public datasets: ICDAR 2011 RRC <ref type="bibr" target="#b26">[27]</ref>, ICDAR 2013 RRC <ref type="bibr" target="#b14">[15]</ref>, and a multilingual dataset <ref type="bibr" target="#b25">[26]</ref> that contains both English and Chinese. Particularly, we use the images of "Challenge 2: Reading Text in Scene Images" in the ICDAR RRC. <ref type="table">Table 1</ref> shows a quantitative comparison of characterlevel recall on the ICDAR 2011 dataset <ref type="bibr" target="#b26">[27]</ref> with the stateof-the-art candidate extraction method proposed by Sung et al. <ref type="bibr" target="#b30">[31]</ref>. We obtained the ground truth data from the author that contains manually specified character-level bounding boxes for each image. The total number of images and characters in the test set are 255 and 6,309, respectively. Given ground truth bounding boxes, we determine the localized result as a correct detection if the intersection-over-union measure between a detected region and the ground truth region is over 0.5. Our method quickly reduces the number of candidates using non-maximum suppression in the ER tree <ref type="figure">Figure 6</ref>. Sample results on scene text detection. We take the input images from publicly available datasets: the ICDAR 2013 RRC, the multilingual dataset proposed by <ref type="bibr" target="#b25">[26]</ref>, MSRA-TD500, and HUST-TR400. Our results are marked in green bounding boxes. and results in almost one third of the initial ERs compared to <ref type="bibr" target="#b30">[31]</ref>. It is worth mentioning that our final localization results have reduced more than 90% of irrelevant candidates while preserving a comparable recall rate to the refined ERs of Sung et al. <ref type="bibr" target="#b30">[31]</ref> that still require further processing such as classification.</p><p>We also estimated the running time of our method with the ICDAR 2011 test set. The average image size of the dataset is about 1,145 by 886 pixels. On average, our method took 0.13 seconds to process one image (i.e., character candidate extraction, non-maximum suppression, double threshold classification, and text tracking by hysteresis).</p><p>We also evaluated our method on the ICDAR 2013 dataset <ref type="bibr" target="#b14">[15]</ref>. <ref type="table">Table 2</ref> shows the quantitative results provided by the online competition website. The winning algorithm of the ICDAR 2013 RRC (Challenge 2), proposed by Yin et al. <ref type="bibr" target="#b39">[40]</ref>, achieved a harmonic mean of 75.89% while our approach obtains 82.17%. The increased recall of ours is mainly due to the double threshold classification and text tracking by hysteresis.</p><p>To validate our method on another language, we use the multilingual dataset proposed by Pan et al. <ref type="bibr" target="#b25">[26]</ref>. The train- ground-truth text region rectangle gt are defined as</p><formula xml:id="formula_3">p(dt) = max gt∈GT [m(dt, gt)],<label>(4)</label></formula><formula xml:id="formula_4">r(gt) = max dt∈DT [m(dt, gt)],<label>(5)</label></formula><p>where m(dt, gt) is the intersection-over-union measure between dt and gt <ref type="bibr" target="#b25">[26]</ref>. The final precision and recall rates are the average of p and r for all dt and gt, respectively. As shown in <ref type="table">Table 3</ref>, the proposed algorithm has dramatically improved recall, precision, and their harmonic mean. The average running time of our method was 0.08 seconds on our PC. Although previous works have provided timing results for this dataset, computing environments are all different so fair comparison is not possible. We provide our running time for future reference. <ref type="figure">Fig. 6</ref> shows several sample results taken from MSRA-TD500 and HUST-TR400 public datasets as well as the datasets used in this paper. Regardless of the language, the Canny text detector works robustly for localizing a wide range of texts, even if there exist noise, blur, and disturbing textures such as windows, tree leaves, and so on. One of the merits of our algorithm is the fast speed and this is demonstrated on our demo website 1 . More text detection examples are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Future Work</head><p>Our Canny text detector delivers a fast and robust algorithm for scene text detection. The proposed approach is intuitive and easy to implement since we do not involve complex operations such as image optimization. Instead, the overall process is similar to the famous Canny operator that has been proven to be effective in the edge detection literature. Despite the simplicity of our algorithm, experiments on widely used datasets demonstrate that the proposed method can effectively localize texts in practice. The key to Canny text detector is double threshold classification and text tracking by hysteresis. We expect such effective detection framework can be adopted for other detection applications. In the following, we discuss some issues related to our approach.</p><p>Effect of dataset When compared to the state-of-the-art text detection methods, our method performs well in terms of recall score. This is not because we simply used more training images than others. To clarify this, we examined existing approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b30">31]</ref> with the same classifier we trained with our training images. However, whatever candidate extraction method is used, using character candidates classified with only a single threshold performed poorly (i.e., low recall if a high threshold value is used, and low precision if a low one is used). We do believe that our new framework can improve the detection rate as well as interoperate with existing methods. For follow-up researchers, we will provide our training dataset upon request by email.</p><p>Interoperability with existing localization methods As prevalent text detection approaches already use character classification, the essence of the proposed method (i.e., double threshold classification and text tracking) can be incorporated for interoperability to other methods without much difficulty.</p><p>Future work The fast speed and accurate localization of the Canny text detector lowers the barrier to develop a realtime end-to-end text reading system. Although there exist a bunch of text recognition algorithms available in practice <ref type="bibr" target="#b35">[36]</ref>, recent techniques employs a large amount of convolution operations for accuracy. Thus, we first need to optimize the running speed or develop an efficient recognition algorithm. In future, we will also explore along this direction to develop video algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overall process of Canny text detector. by hysteresis. The surviving text candidates are grouped to compose sentence(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Intermediate results of the Canny text detection process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Parts of an ER tree. The input image is shown in the left ofFig. 3a. For each node, the left half shows the cropped image of an ER, and the right half shows its corresponding binary mask of which pixels of the ER are marked as white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Mean local binary pattern (MLBP)<ref type="bibr" target="#b1">[2]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 2. Evaluation on the ICDAR 2013 competition on robust reading test set.ing set contains 248 images and the testing set contains 239 images. Given the ground-truth text region set GT and the detected text region set DT , the precision rate p of each detected region rectangle dt and the recall rate r of eachTable 3. Evaluation on the multilingual test set.</figDesc><table>Method 
Recall Precision Hmean 

Shi et al. [29] 
62.85 
84.70 
72.16 

Bai et al. [1] 
68.24 
78.89 
73.18 

Yin et al. [39] 
65.11 
83.98 
73.35 

Neumann and Matas [22] 64.84 
87.51 
74.49 

Yin et al. [40] 
66.45 
88.47 
75.89 

Zamberletti et al. [41] 
70.-
86.-
77.-

Tian et al. [32] 
75.89 
85.15 
80.25 

Sung et al. [31] 
74.23 
88.65 
80.80 

Our method 
78.45 
86.26 
82.17 

Method 

Recall Precision Hmean 

Pan et al. [26] 
65.9 
64.5 
65.5 

Baseline 
67.2 
78.6 
72.4 

Yin et al. [40] 
68.5 
82.6 
74.6 

Tian et al. [32] 
78.4 
84.7 
81.4 

Our method 
93.5 
93.1 
93.3 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://stradvision.com/demo.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene text localization using gradient local correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2013</title>
		<meeting>ICDAR 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1380" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical face recognition method based on local binary pattern</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Congress on Image and Signal Processing (CISP)</title>
		<meeting>Congress on Image and Signal essing (CISP)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="610" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pho-toOCR: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV 2013</title>
		<meeting>ICCV 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust text detection in natural images with edge-enhanced maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP 2011</title>
		<meeting>ICIP 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2609" to="2612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="366" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient maximally stable extremal region (MSER) tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR 2010</title>
		<meeting>CVPR 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determining the minimum-area encasing rectangle for an arbitrary closed curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shapira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="409" to="413" />
			<date type="published" when="1975-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text detection and localization in complex scene images using constrained adaboost algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hanif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2009</title>
		<meeting>ICDAR 2009</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text localization in natural images using stroke feature transform and text covariance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR 2013</title>
		<meeting>CVPR 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1241" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced MSER trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ICDAR 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2013</title>
		<meeting>ICDAR 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Texture-based approach for text detection in images using support vector machines and continuously adaptive mean shift algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1631" to="1639" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scene text detection via connected component clustering and nontext filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2296" to="2305" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaboost for text detection in natural scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2011</title>
		<meeting>ICDAR 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="429" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC 2002</title>
		<meeting>BMVC 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="384" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Robust widebaseline stereo from maximally stable extremal regions. Image and vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text localization in real-world images using efficiently pruned exhaustive search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2011</title>
		<meeting>ICDAR 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="687" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR 2012</title>
		<meeting>CVPR 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On combining multiple segmentations in scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2013</title>
		<meeting>ICDAR 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="523" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linear time maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A hybrid approach to detect and localize texts in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="800" to="813" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ICDAR 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2011</title>
		<meeting>ICDAR 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scene text detection using graph model built upon maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scene text recognition using part-based tree-structured character detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR 2013</title>
		<meeting>CVPR 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2961" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A robust approach for text detection from natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2906" to="2920" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scene text detection with robust character candidate extraction method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2015</title>
		<meeting>ICDAR 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="426" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV 2015</title>
		<meeting>ICCV 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV 2011</title>
		<meeting>ICCV 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR 2012</title>
		<meeting>CVPR 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Localizing text in scene images by boundary clustering, stroke segmentation, and string fragment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4256" to="4268" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Text extraction from scene images by character appearance and structure modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="182" to="194" />
		</imprint>
	</monogr>
	<note>Computer Vision and Image Understanding</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-orientation scene text detection with adaptive clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1930" to="1937" />
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="970" to="983" />
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Text localization based on fast feature pyramids and multi-resolution maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamberletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop on Robust Reading</title>
		<meeting>Int&apos;l Workshop on Robust Reading</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="91" to="105" />
		</imprint>
	</monogr>
	<note>ACCV 2014)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
