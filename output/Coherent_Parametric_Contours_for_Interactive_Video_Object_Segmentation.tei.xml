<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coherent Parametric Contours for Interactive Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
							<email>luyao@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Bai</surname></persName>
							<email>xubai@adobe.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Adobe</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
							<email>shapiro@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
							<email>juewang@adobe.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Adobe</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coherent Parametric Contours for Interactive Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interactive video segmentation systems aim at producing sub-pixel-level object boundaries for visual effect applications. Recent approaches mainly focus on using sparse user input (i.e. scribbles) for efficient segmentation; however, the quality of the final object boundaries is not satisfactory for the following reasons: (1) the boundary on each frame is often not accurate; (2) boundaries across adjacent frames wiggle around inconsistently, causing temporal flickering; and <ref type="formula">(3)</ref> there is a lack of direct user control for fine tuning.</p><p>We propose Coherent Parametric Contours, a novel video segmentation propagation framework that addresses all the above issues. Our approach directly models the object boundary using a set of parametric curves, providing direct user controls for manual adjustment. A spatiotemporal optimization algorithm is employed to produce object boundaries that are spatially accurate and temporally stable. We show that existing evaluation datasets are limited and demonstrate a new set to cover the common cases in professional rotoscoping. A new metric for evaluating temporal consistency is proposed. Results show that our approach generates higher quality, more coherent segmentation results than previous methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Interactive, or supervised video object segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref> is an essential step in professional video production, enabling numerous post-processing possibilities such as background replacement. The standard industrial approach for this task is rotoscoping, where boundaries of the foreground objects are first annotated manually at sparse keyframes, using parametric and controllable shapes such as Bézier curves. These curves are then smoothly interpolated for the in-between frames. Given the high precision and controllability of parametric curves, rotoscoping can achieve highly accurate and temporally stable results by artists; however it is an extremely labor-intensive process and requires professional expertise.</p><p>Recently, interactive video object segmentation based on sparse user input (i.e. foreground and background scribbles) has gained considerable attention given its ability to quickly generate reasonable segmentation results with a small amount of user input <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28]</ref>. While these scribble-based methods greatly improve the segmentation efficiency, they often suffer from inaccurate and/or inconsistent segmentation boundaries that prevent them from real production. Most of these approaches generate the segmentation results from pixels in single frames through a global optimization method such as graph cuts, which is easily affected by background clutter, image noise and edge pixelation. Furthermore, the factors affecting the global optimization often change across frames; thus even for a rigid object, there is no guarantee of the temporal shape consistency. Resulting boundaries often wiggle around across frames, causing temporal boundary jitter as shown in <ref type="figure">Figure 1</ref>. Manipulating the pixel-wise boundaries frame-by-frame in such non-parametric systems is practically not possible.</p><p>In this project, we propose a new method called Coherent Parametric Contours (CPC), which explicitly models the object boundary as a set of evolving Bézier curves for interactive video object segmentation. These curves are initialized by the user on the first frame, and automatically propagated to the following frames through a spatio-temporal optimization algorithm that seeks both spatial accuracy and temporal shape consistency. Since object boundaries are represented as parametric curves, users have the full access to local boundary shapes; manipulating the curves is therefore straightforward.</p><p>Previously, the evaluation datasets proposed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> do not provide ground-truth labeling with professional-level accuracy; they are not suitable for evaluating parametric algorithms as well, due to the ambiguity in parameterizing shapes with complex topology. Besides, there is also a lack of evaluation metric that focuses on temporal boundary consistency. We analyze the requirements for professional <ref type="bibr">Figure 1</ref>. Temporal shape inconsistency is a common problem in scribblebased video object segmentation systems. Left: for a simple example with a rigid object, the user marks several scribbles on the first frame to indicate the foreground (red) and background (yellow). Right: overlaying boundaries (in yellow) of multiple frames reveals the temporal boundary inconsistency. The contours wiggle around despite the rigidity of the object. The video result for this example is shown at http://yao.lu/CPC.html video object segmentation and construct a dataset containing various types of videos that commonly occur in real production. We provide ground-truth parametric boundaries carefully labeled by professionals using an industrial software package. A new metric is proposed to measure the temporal shape consistency for video object segmentation. Experimental results show that our approach outperforms state-of-the-art scribble-based segmentation methods, as well as rigid shape tracking algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multiple approaches exist for interactive video object segmentation. We discuss different types of systems below categorized by the input and the boundary propagation algorithm. (1) Rotoscoping tools such as Mocha <ref type="bibr" target="#b1">[2]</ref> leverage simple shape interpolation methods to propagate parametric curves from keyframes to in-between frames, without looking at the underlying image content. Rotoscoping meets the quality requirement for production; consistent results are obtained for unlimited types of video objects (rigid, non-rigid, occluded, etc.). However rotoscoping is labor-intensive and requires much professional expertise for the users. To improve the efficiency, Agarwala et al. <ref type="bibr" target="#b2">[3]</ref> encode image features through a non-linear optimization framework to interpolate for intermediate object shapes between user-annotated keyframes. (2) Scribblebased image and video segmentation systems take sparse user input and efficiently generate non-parametric segmentation results. Grabcut <ref type="bibr" target="#b19">[20]</ref> segments the foreground object within input bounding boxes. SnapCut <ref type="bibr" target="#b3">[4]</ref> improves foreground models using local classifiers. Most scribble-based frameworks utilize graph cuts to determine the boundary, hence potential solutions to enhance the fine boundary include using a prior to guide the graph cuts <ref type="bibr" target="#b24">[25]</ref>, and adjusting the affinity between image regions <ref type="bibr" target="#b12">[13]</ref>. However, due to the nature of non-parametric curves, it is still tedious to perform pixel-or subpixel-wise annotation for fine bound-ary manipulation. Besides, such manipulation is needed for every single frame, since results in these systems usually contain temporal jitter. (3) Keypoint-based contour tracking approaches extract local image descriptors and compute the homography between neighboring frames; the object boundary is then propagated to later frames using the homography. These systems provide efficient propagation of parametric contours. However they are usually limited to one planar surface; artifacts exist for objects with multiple parts or non-rigid motions. The Rigid Mask Tracker in Adobe After Effects <ref type="bibr" target="#b0">[1]</ref> is a state-of-the-art implementation of this approach.</p><p>To benefit from existing approaches as well as to avoid their limitations, the following designs are made in our proposed system. (1) We mimic the rotoscoping artists to produce high quality boundaries for unlimited types of objects. However unlike rotoscoping, the users are only required to annotate the first frame. Our system automatically propagates the shapes to subsequent frames, which greatly reduces the workload. (2) We leverage parametric curves as input to our system, since non-parametric object boundaries produced by scribble-based methods are difficult for fine adjustment. (3) Locally instead of globally rigid motion is assumed to overcome the difficulty for handling non-rigid objects in keypoint-based systems.</p><p>Our proposed parametric boundary propagation framework is built upon active contours, a classic model for non-parametric image segmentation. Several variations exist such as snakes <ref type="bibr" target="#b10">[11]</ref>, intelligent scissors <ref type="bibr" target="#b15">[16]</ref>, and level sets <ref type="bibr" target="#b5">[6]</ref>. Active contours has also been applied in several video object segmentation and tracking frameworks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. Unlike these approaches, our method models the input in a Bézier curve representation, and we perform spatio-temporal optimization upon these parametric boundaries to obtain accurate and smooth results.</p><p>Several datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref> exist to evaluate the effectiveness of interactive video object segmentation. Zhong et al. <ref type="bibr" target="#b27">[28]</ref> provide a video set with binary masks for the foreground objects. However the groundtruth boundaries are not manually annotated and contain noticeable visual artifacts. These non-parametric datasets are not suitable for evaluating parametric algorithms due to the ambiguity in representing complex topology. Further, no cross-frame boundary correspondence is provided; evaluating the temporal consistency is indirect in these datasets.</p><p>In this paper, we demonstrate a parametric video dataset annotated by professional rotoscoping artists, with the goal of evaluating both spatial accuracy and temporal consistency for interactive video object segmentation. Automatic key frame selection methods are proposed in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24]</ref>; however our annotation is on equally sampled frames so as to balance the difficulty between videos. For evaluation metrics, <ref type="bibr" target="#b11">[12]</ref> to our best knowledge is the only work to eval-uate the temporal consistency for soft object masks produced by video matting algorithms. Their metric is featuredependent; label difference versus feature difference is calculated as the consistency scores. Since our dataset provides groundtruth annotated by professional artists, there is no need to further involve image features in the evaluation. Hence, we propose a novel temporal consistency metric for parametric curves in this paper. Our metric is defined purely on boundary shapes, allowing efficient and precise measurement without looking into image regions that sometimes become unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Coherent Parametric Contours (CPC)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CPC in Single Images</head><p>Let I : [0, a] × [0, b] → R + be an intensity image, and let C(q) : [0, 1] → R 2 be a parametric curve. Recall that the energy for the active contour <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6]</ref> is defined as:</p><formula xml:id="formula_0">E(C) = α 1 0 |C ′ (q)| 2 dq + β 1 0 |C ′′ (q)| 2 dq −λ 1 0 |∇I(C(q))|dq.</formula><p>(1)</p><p>The first two terms penalize the first and second order discontinuities to ensure a smooth and continuous output, and the last term fits the curve to image gradient ∇I. CPC on single images has a similar energy formation. In particular, the curve C(q) is represented using a set of connected Bézier curves:</p><formula xml:id="formula_1">C(q) = {B(p i )}, i = 1, ..., m,<label>(2)</label></formula><p>where B(p i ) depicts the i-th cubic Bézier curve with param-</p><formula xml:id="formula_2">eter p i = {p i 0 , p i 1 , p i 2 , p i 3 }, and m is the number of segments. For simplicity we denote B(p i ) = b i (s) : s ∈ [0, 1] → [p i 0 , p i 3 ].</formula><p>Here b i (0) = p i 0 and b i (1) = p i 3 are the two terminal control points of Bézier curve B(p i ), and p i 1 , p i 2 are the two intermediate control points. Note that the Bézier curves are connected, hence b i (1) = b i+1 (0). We rewrite the fitness term in Equation 1 as:</p><formula xml:id="formula_3">1 0 |∇I(C(q))|dq = m i=1 1 0 |∇I(b i (s))|ds.<label>(3)</label></formula><p>Bézier curves are always continuous and smooth inside. To produce a reasonable CPC, the only requirement is the smoothness near the joints where two adjacent Bézier curves meet. Then the energy of CPC on a single frame can be written as:</p><formula xml:id="formula_4">E(B) = m i=1 |b i (0) ′′ | 2 − λ m i=1 1 0 |∇I(b i (s))|ds. (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CPC in Video</head><p>Given a video sequence V = (I 1 , ..., I n ), to achieve spatio-temporal accuracy as well as consistency, we optimize the total energy of CPCs on the video sequence:</p><formula xml:id="formula_5">min Bt∈Bt,∀t n t=1 E(B t ) + γ n−1 t=1 E(B t , B t+1 ),<label>(5)</label></formula><p>where the first term ensures the quality of CPC on each individual frame. B t is the candidate boundary set for frame t, from which we select an optimal boundary to construct the global solution. We discuss constructing the candidate set in Section §3.2.1.</p><formula xml:id="formula_6">E(B t , B t+1 )</formula><p>is the temporal consistency cost that measures the pairwise consistency between CPCs in neighboring frames. It is define as:</p><formula xml:id="formula_7">E(B t , B t+1 ) = dist(B t ⊕ f , B t+1 ),<label>(6)</label></formula><p>where f is the locally rigid motion vector, and ⊕ is a boundary warping operation. We compare the warped boundary B t ⊕ f in frame t with the candidate boundary B t+1 in frame t + 1; their pixel-wise distance is calculated and minimized, so that the resulting CPCs are consistent and deform progressively across frames according to the locally rigid motion. Note that f is only locally rigid; it can be non-rigid globally. Details about estimating the locally rigid motion and warping the contours are provided in Section §3.2.2.</p><p>A well established criteria <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref> is applied to measure the distance between the boundary m and n in two frames. It can be formulated as the percentage of pixels on m that have correspondences to n: dist(m, n) = 1 |m| 1 p∈m,∃q∈n,s.t.||p−q||≤th <ref type="formula">(7)</ref> where th is a tolerance threshold indicating the distance between corresponding boundary pixels.</p><p>To optimize Equation 5, we apply dynamic programming; the problem can then be solved within O(mk 2 ) time, where k is the number of candidate boundaries for each frame. To speed up the computation, distances between candidate boundaries are calculated in parallel. Meanwhile, a standard multi-scale approach is applied to narrow the search space for k; we generate CPCs on a rough scale and then refine them on a fine scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Generating Boundary Hypotheses</head><p>Under the Bézier curve representation, generating a boundary hypothesis is equivalent to locating a set of control points. A two-stage approach is utilized to propose the candidate boundary set B t for frame t. We first generate the terminal points; then the intermediate control points can be determined by solving a least-squares fitting problem. as the two terminal control points, project each nearby pixel (in the green box) to p 0 p 3 for parameterization. Right: we use the image gradient values as weights and perform least-squares Bézier fitting to estimate optimal intermediate control points p 1 and p 2 . Two box constraints are used to stabilize the fitting.</p><p>Proposing Candidate Terminal Points. The criteria to generate candidate terminal points are two-fold. First, their displacement across frames should be consistent with the estimated local object motion. Second, they should snap to strong image edges. Hence, the candidates {b t i (0)} m i=1 on frame t should minimize the following energy:</p><formula xml:id="formula_8">min {b t i (0)} m i=1 |b t i (0) − b t−1 i (0) − f t−1 i | − α|∇I t (b t i (0))|, (8) where f t−1 i</formula><p>is the locally rigid motion of point i's neighborhood within the object from frame t − 1 to t, which will be described in Section §3.2.2.</p><p>To solve Equation 8, computing and sorting the energies on a permutation set is a potential solution. However, moving candidate terminal points one or two pixels around yields a large permutation set with low variety; the computational complexity for Equation 5 is therefore high. We thus leverage a random sampling approach to obtain a small candidate set with large variety, similar in spirit to other sampling approaches applied in different applications such as matting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>The possibility of a pixel x being a terminal point in frame t can be formulated as:</p><formula xml:id="formula_9">P t i (x) ∝ ∇I t (x) · N (b t−1 i (0) + f t−1 i , σ)<label>(9)</label></formula><p>where N (, ) is a 2D normal distribution centered at the terminal point projected using the locally rigidy motion. Therefore, terminal points for different segments of the Bézier curves are generated independently and randomly according to</p><formula xml:id="formula_10">P t i (x), so that P (B t ) = m i=1 P t i (x)</formula><p>. We obtain k sets of terminal points for each frame. In our experiments k is typically chosen to be 300, and σ is set to be 5 pixels.</p><p>Least-Squares Bézier Fitting. Given the two terminal points p 0 and p 3 belonging to Bézier curve b, we perform constrained weighted least-squares fitting to infer the opti- </p><formula xml:id="formula_11">min p1,p2 y∈C ∇I(y) · [y − b(l(y))] 2 , s.t. 0 ≤ l(y) ≤ 1, r 1 lu ≤ p 1 ≤ r 1 rb , r 2 lu ≤ p 2 ≤ r 2 rb .<label>(10)</label></formula><p>where b(s) is the cubic Bézier curve: <ref type="figure" target="#fig_0">Figure 2</ref> demonstrates an example of the Bézier fitting. Let C be the set of possible pixel locations for Bézier curve b. In practise, so as to reduce the search space, C is set to be the pixels within the bounding box to contain p 0 and p 3 plus a constant margin (the green box in <ref type="figure" target="#fig_0">Figure 2)</ref>. l(y) = ||yp−p0|| ||p3−p0|| parameterizes an arbitrary point y, and y p is its projection on p 0 p 3 . The image gradient ∇I is used to weight the Bézier curves, so that they fit to strong edges. We apply additional constraints on the intermediate control points. Two box constraints r 1 = [r 1 lu , r 1 rb ] and r 2 = [r 2 lu , r 2 rb ] are used; their positions are bilinearly interpolated and projected from the previous frame using the locally rigid motion vector. We force the intermediate control points to locate within the box constraints so as to stabilize the fitting result. Equation 10 is solved using the Levenberg-Marquardt method <ref type="bibr" target="#b14">[15]</ref> with RANSAC <ref type="bibr" target="#b7">[8]</ref>; a subset of pixels in C is selected in a sample-and-test manner to best fit the Bézier curve.</p><formula xml:id="formula_12">b(s) = (1 − s) 3 · p 0 + 3(1 − s) 2 s · p 1 + 3(1 − s)s 2 · p 2 + s 3 · p 3 , s ∈ [0, 1].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Locally Rigid Motion Estimation</head><p>Previous keypoint tracking methods treat the object as a single plane and calculate a global homography between frames. This is sub-optimal for segmenting non-rigid, or rigid but non-planar video objects. In this work, we instead emphasize the local rigidity of objects and leverage local affinities to estimate the motion for the terminal points. <ref type="figure" target="#fig_1">Figure 3</ref> is an example of the estimation process.</p><p>The local homography H x for point x on the object boundary is calculated using the keypoints within a radius r centered at x; keypoints outside of the object boundary are not considered. RANSAC <ref type="bibr" target="#b7">[8]</ref> is applied in calculating the homography to eliminate outliers. Hence the locally rigid motion vector is denoted as</p><formula xml:id="formula_13">f = H · x − x.</formula><p>For terminal points without enough neighboring keypoints, their motion vectors are propagated from nearby terminal points to keep shape rigidity, so that</p><formula xml:id="formula_14">f = (d − · f + + d + · f − )/(d + + d − ), in which f + ( f − )</formula><p>is the motion of the next (previous) terminal point on the boundary (assuming the terminal points are annotated clockwise), and d + (d − ) is the distance on the object boundary to that point.</p><p>Constructing warped contours. Given the parametric Bézier curve B and the locally rigid motion vector f , computing the warped contour B ⊕ f is therefore straightforward. For terminal control points, f is applied directly; for intermediate control points, we apply motion vectors that are bilinear interpolated from the two neighboring terminal control points. Discrete pixels on the boundary are generated using these Bézier parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation.</head><p>Currently, datasets used to evaluate scribble-based frameworks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref> emphasize the overall correctness of video segmentation, while the sub-pixellevel boundary quality is weighted less. Furthermore, the existing datasets are not designed for evaluating parametric methods due to the ambiguity in the parametric contour representation. Temporal consistency cannot be measured either, since no cross-frame correspondence is provided. In this paper, we propose a video set for evaluating parametric video object segmentation algorithms with emphasis on both spatial and temporal boundary qualities. We consider the following issues in the dataset construction.</p><p>1. Complex topology. Modeling objects with complex and changing topology using a single parametric curve is an ill-defined problem, not feasible even for professional rotoscoping artists. The standard solution in video production is to divide complex objects into overlapping parts with simple shapes, while each part can still be deformable, and rough boundaries are put between parts. <ref type="figure">Figure 4a</ref> shows an example of this process. To mostly follow the production practise, videos with partial boundary (PB) and occlusion (OC) should be included in our dataset <ref type="figure">(Figure 4b</ref>).</p><p>2. Furry boundaries. For objects with furry boundaries, rotoscoping artists will first generate consistent outlines for the whole object, and then apply soft matting locally to the furry part. For the binary segmentation step, we also categorize this case as partial boundary (PB).</p><p>3. Occlusion. Occluded objects pose extra challenges for (a) (b) <ref type="figure">Figure 4</ref>. (a) Production practise for high quality interactive video object segmentation. Complex topology is decomposed into parts for segmentation. (b) A dataset is proposed to cover several common cases in production. In this case non-rigid motion (NR), partial boundary (PB) and motion blur (MB) exist. videos containing dynamic scenes. The segment boundaries should be consistent with the occlusion boundaries (OC). 4. Motion blur (MB) is a common artifact for videos containing intensive motions. It causes blurry object boundaries that are difficult for segmentation. Further, when there is severe motion blur, even humans cannot see the boundaries clearly. To handle such cases in production, a standard practice is to estimate temporally smooth boundaries and sacrifice spatial accuracy. 5. Rigidity. Rigid (R) and non-rigid (NR) objects are equally important for video segmentation, while previous systems overlook the boundary stability of rigid objects.</p><p>Dataset construction. We construct a dataset with 9 video sequences ranging from 60 to 128 frames in length <ref type="bibr">(15-100fps)</ref>, where each video contains a single shot for the rotoscoping process. We ask a professional rotoscoping artist to carefully label the boundary of a basic unit for each video, using the Bézier Pen tool in Mocha <ref type="bibr" target="#b1">[2]</ref>. The annotation is later verified and refined by another rotoscoping artist. To reduce the cost for annotation and to balance the difficulties of different videos, we sample 28 to 40 frames from each video sequence with fixed intervals. Finally precise groundtruth in parametric curves is obtained. We demonstrate the dataset in <ref type="figure">Figure 4b</ref> and <ref type="figure" target="#fig_2">Figure 5</ref>, and <ref type="table">Table 1</ref> summarizes the videos properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>Evaluating the spatial accuracy. We follow the edge comparison strategy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref>  evaluate the spatial accuracy for video object segmentation. Given the segmentation boundary sb and groundtruth boundary gt in discrete pixel format, the segmentation accuracy for a single frame is dist(gt, sb).</p><p>Evaluating the temporal consistency. We propose a novel metric to measure the temporal consistency for parametric video object segmentation. Given the segmentation boundaries sb 1 and sb 2 , and groundtruth boundaries gt 1 and gt 2 for a pair of consecutive frames, let p (q) be the closest pixels on sb 1 (sb 2 ) to the groundtruth gt 1 (gt 2 ), the consistency is therefore defined as:</p><formula xml:id="formula_15">consist(sb 1 , sb 2 ) = 1 |gt| · i 1 ||(gt i 1 −p i )−(gt i 2 −q i )||≤th (11)</formula><p>The basic idea is to calculate percentage of pixels on gt 1 and gt 2 that have coherent correspondences to sb 1 and sb 2 . At this point, pixels on gt 1 and gt 2 should be registered so as to perform the per-pixel matching. Since our groundtruth boundaries are generated from Bézier parameters, we know the exact correspondence between gt 1 and gt 2 . Hence calculating Equation 11 is straightforward; we demonstrate the process in <ref type="figure">Figure 6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluations</head><p>We compare our proposed approach with several existing techniques as well as one variant of our framework.</p><p>(1) Global plane tracking (GP) is a standard technique for video object segmentation. We compare with the Rigid Mask Tracker within Adobe After Effects <ref type="bibr" target="#b0">[1]</ref>, which is a state-of-the-art implementation of boundary tracking based on keypoints. We choose a perspective transform for the tracker. (2) We generate warped contours without the spatio-temporal optimization to see the effectiveness of the locally rigid motion (LR). (3) Finally, we compare with the state-of-the-art scribble-based (SB) approach SnapCut <ref type="bibr" target="#b3">[4]</ref>.</p><p>Since the videos are different in length, we divide each of them into multiple overlapping clips with a 5-frame offset between the clips. For each video clip, we evaluate the performances for len = 2, 6, 11 and th = 1, 2, 4 upon the <ref type="figure">Figure 6</ref>. Demonstration of our temporal consistency metric. We calculate signed distances for each pair of corresponding pixles on the groundtruth boundaries (in blue) to the result boundaries (in green). The percentage of pixels in consensus between the two distances (in white cells) under a tolerance threshold th is calculated as the consistency score. Note that in this example we only demonstrate sparse correspondences. annotated frames. Note that we find the results of previous tools often deteriorate quickly (e.g. across less than 10 frames); if the result is already not acceptable after propagating 11 frames, propagating more is not a meaningful comparison. The same first-frame annotations are fed to different methods as initialization; we then calculate accuracy per frame and consistency per consecutive two frames. Finally the average performances are reported on all the clips in each video.</p><p>We report the quantitative evaluations in <ref type="table">Table 2</ref> for the accuracy and consistency with th = 1. Under this tight setting the resulting boundaries should be within one pixel from the groundtruth. We have several observations from this comparison. (1) Global plane tracking (GP) works well on objects with rigid motion (R) in terms of both accuracy and consistency. It handles partially annotated boundaries (PB) but cannot cope with occlusion (OC). In contrast our approach achieves comparable performances on rigid tracking and finds the correct occlusion boundary in a better way.</p><p>(2) As expected, global plane tracking does not work on objects with non-rigid (NR) motion. In contrast our framework handles non-rigid motion well, since we assume locally instead of globally rigid motion. (3) The scribblebased method (SB) is not comparable to our approach; the output boundaries are rough and temporally inconsistent. SB is good at detecting occlusion boundaries but does not  <ref type="table">Table 2</ref>. Quantitative evaluation and comparison with a tight tolerance threshold th = 1 and different video length len = 2, 6, 11. SA: spatial accuracy.</p><p>TC: temporal consistency. We compare our method with a scribble-based method (SB) <ref type="bibr" target="#b3">[4]</ref>, global plane tracking (GP) <ref type="bibr" target="#b0">[1]</ref>, and locally rigid motions (LR). Note that for len = 2 the accuracy and consistency are the same. Our approach outperforms state-of-the-art methods in both accuracy and consistency.  handle partial boundaries (PB). (4) Tracking with locally rigid motion (LR) works reasonably well, but its performance is significantly worse than the full system, indicating the importance of the spatio-temporal optimization in our system. (5) Looking at the results for individual video sequences, we notice that (i) GP requires adequate texture for tracking; it fails in textureless regions (ToyHorse); (ii) SB works poorly on objects with long skinny structures (Tower); (iii) motion blur (MB) is a common obstacle to video object segmentation; our spatio-temporal smoothing scheme still generates consistent object boundaries. We illustrate the overall performance with different thresholds th in <ref type="figure" target="#fig_5">Figure 7</ref>. Average performances over all video sequences and clips are reported. Our proposed framework behaves similarly well and is the best in both accuracy and consistency under different tolerance settings.</p><p>Discussion on ∇I. In the case of strong shadow or motion blur, the object boundary could become vague, or even disappear completely, as shown in <ref type="figure" target="#fig_6">Figure 8</ref>. We have tried (a,b) Object boundaries can be easily affected by shadow, motion blur or background cluttering. (c) Probabilistic foreground mask G obtained using SnapCut <ref type="bibr" target="#b3">[4]</ref>. In these hard cases, G and ∇G provide no meaningful improvement. (d) Our proposed framework keeps the local shape rigidity to mimic rotoscoping artists. Please refer to <ref type="figure" target="#fig_1">Figure 3</ref> for the explanation.</p><p>using ∇G, the gradient of the probabilistic foreground mask produced by SnapCut <ref type="bibr" target="#b3">[4]</ref>, to replace the image gradient ∇I, but there is no meaningful improvement as the foreground mask itself is often erroneous in such cases <ref type="figure" target="#fig_6">(Figure 8c</ref>). In our system, weak edges and control points are regularized by their local as well as neighboring affinities. Once a control point or a Bézier curve is incorrectly snapped to a strong background edge, the resulting boundary shape will be constrained by Equation <ref type="bibr" target="#b5">6</ref>.</p><p>Qualitative Evaluation. <ref type="figure">Figure 9</ref> shows visual comparisons of segmented boundaries in four video sequences. For GP, we can easily notice the boundary errors when there is occlusion (Sunset), or the object is deforming (Minion). SB produces zigzag and temporal inconsistency boundaries (ToyHorse and Car). Our method finds false object bound-  aries in some cases (Minion), but overall achieves higher quality and temporally consistent results. Further video results and comparisons are shown in the project website. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluating the User Interactions</head><p>We show in <ref type="figure">Figure 10</ref> that our approach is convenient in precise boundary manipulation. The advantages are twofold: (1) Spatial adjustment. Users can directly move the control points of Bézier curves to adjust the segmentation boundary. In contrast, scribble-based approaches require several rounds of interaction. Multiple scribbles need to be added near the true object boundary for further refinement.</p><p>(2) Temporal propagation. Since scribble-based systems do not pose strong constraints on temporal shape stability, the same refinement is needed on multiple adjacent frames. On the contrary, our proposed framework produces more reliable results; once a refinement is done for one frame, the modified object shape can stay much longer. <ref type="table" target="#tab_3">Table 3</ref> demonstrates a user study showing the efficiency of different video segmentation systems. We ask three users to segment two video clips, each with 20 frames in length 1 http://yao.lu/CPC.html <ref type="figure">Figure 10</ref>. Our system allows direct boundary editing for segmentation error correction (left). In contrast, scribble-based systems require scribbles drawn near the object boundary (middle), however the result may still be unsatisfactory (right). and initial contours not given. The users have the experience for more than ten hours in SnapCut and Mocha. We show them the groundtruth and ask them to achieve both accuracy and consistency. Results indicate that: (1) for Snap-Cut <ref type="bibr" target="#b3">[4]</ref>, although a rough foreground mask can be drawn efficiently, users spend most of the time refining the temporal boundary consistency, and (2) for segmenting the Minion clip with non-rigid motion, annotation is needed frequently in Mocha. Although our system requires more user input on the first frame, it produces better boundary curves with a greater degree of temporal stability, thus requiring less user intervention in the propagation process. Our method clearly outperforms SnapCut and Mocha in usability and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We describe Coherent Parametric Contours, a boundary propagation framework for interactive video object segmentation, aiming to produce high quality object boundaries suitable for real video production. Compared with traditional scribble-based methods, it generates accurate and temporal coherent boundaries and supports direct and natural boundary editing. We also provide a new dataset and a new metric to measure the temporal boundary consistency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of our Bézier fitting scheme. Left: given p 0 and p 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Estimating locally rigid motion for control points. Left: frame 1 with parametric annotation B in pink. Terminal points are shown in rectangles. Right: frame 2 with warped contour B ⊕ f . Small circles represent the keypoints, and short lines indicate their displacement from frame 1. For estimating the motion of the terminal point pointed to by the yellow arrow, we look into a local region within a radius r. Keypoints nearby are used to calculate the homography. Terminal points pointed to by red arrows do not have sufficient nearby keypoints, thus their motions are propagated from neighboring terminal points to keep shape rigidity. mal locations for p 1 and p 2 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Different types of videos in our dataset. First row: frame 1. Second row: frame 25. The images are cropped for better visualization. The first three video sequences contain objects with non-rigid motion, and the rest contain objects with rigid motion. Groundtruth boundaries are shown in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Quantitative evaluation for different video length (x-axis) and tolerance threshold (th = 1, 2, 4). First row: accuracy. Second row: consistency. Our method performs the best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Demonstration of three hard cases with vague or missing boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>MB: motion blur. OC: occlusion. PB: partially annotated boundaries.</figDesc><table>Sequence 
Init size 
Len 
Anno 
Motion 
MB 
OC 
PB 
Boy 
190x300 
60 
30 
NR 
√ 

Drop 
80x100 
84 
28 
NR 
Minion 
420x570 
102 
34 
NR 
√ 
√ 

Car 
80x60 
111 
37 
R 
√ 

ToyMonkey 
210x190 
120 
40 
R 
√ 

ToyHorse 
320x290 
120 
40 
R 
√ 

Plane 
800x810 
93 
31 
R 
Sunset 
310x90 
128 
32 
R 
√ 

Tower 
220x600 
60 
30 
R 

Table 1. Overview of our proposed dataset. Len: original length of 

frames. Anno: number of annotated keyframes. Init size: rough object 
size (in pixel) on the first frame. R: rigid motion. NR: non-rigid motion. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Figure 9. Qualitative evaluation and comparison. First and third row: our results. Second row: Rigid Mask Tracker<ref type="bibr" target="#b0">[1]</ref>. Last row: SnapCut<ref type="bibr" target="#b3">[4]</ref>. All methods start from the same annotation on the first frame. Images are cropped to suitable regions for visualization. In comparison, global shape tracking cannot handle non-rigid deformation (Minion) and occlusion (Sunset). Video SnapCut has lower quality for boundary smoothness and temporal consistency. Our method produces more accurate and consistent results.</figDesc><table>Frame 1 

Frame 3 
Frame 6 
Frame 9 
Frame 1 
Frame 3 
Frame 6 
Frame 9 

Seq 1 
Minion 
(sec) 
Seq 2 
Tower 
(sec) 
SnapCut 
Mocha 
Ours 
SnapCut 
Mocha 
Ours 
User 1 
147 
1357 
115 
201 
184 
75 
User 2 
172 
1223 
135 
326 
311 
84 
User 3 
255 
1729 
144 
177 
197 
75 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>User study on the efficiency of different video segmentation systems. Our system clearly outperforms the SnapCut<ref type="bibr" target="#b3">[4]</ref> and the Mocha<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>After Effects</surname></persName>
		</author>
		<ptr target="http://www.adobe.com/products/aftereffects.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mocha</forename><surname>Software</surname></persName>
		</author>
		<ptr target="http://www.imagineersystems.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Keyframe-based tracking for rotoscoping and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>TOG</publisher>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="584" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video snapcut: robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Edge detector evaluation using empirical roc curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kranenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geodesic active contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="79" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="117" to="119" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video object segmentation using eulerian region-based active contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jehan-Besson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Snakes: Active contour models. IJCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporally coherent video matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="25" to="33" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salient object detection using concavity context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The levenberg-marquardt algorithm: implementation and theory. Numerical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intelligent scissors for image composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer graphics and interactive techniques</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">B-spline active contour with handling of topology changes for fast video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. on Applied Signal Processing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Livecut: Learningbased interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time tracking using level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Karl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactive video cutout. TOG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="585" to="594" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Key frame selection by motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Segmentation given partial grouping constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="183" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Summary transfer: Exemplar-based subset selection for video summarizatio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discontinuityaware video object cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
