<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hole Filling Approach based on Background Reconstruction for View Synthesis in 3D Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guibo</forename><surname>Luo</surname></persName>
							<email>luoguibo@sz.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Commun. Inf. Secur. Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesheng</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Commun. Inf. Secur. Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaotian</forename><surname>Li</surname></persName>
							<email>lizhaotian@sz.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Commun. Inf. Secur. Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhang</surname></persName>
							<email>lmzhang@umac.mo</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Science and Technology</orgName>
								<orgName type="institution">University of Macau</orgName>
								<address>
									<settlement>Macao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hole Filling Approach based on Background Reconstruction for View Synthesis in 3D Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The depth image based rendering (DIBR) plays a key role in 3D video synthesis, by which other virtual views can be generated from a 2D video and its depth map. However, in the synthesis process, the background occluded by the foreground objects might be exposed in the new view, resulting in some holes in the synthetized video. In this paper, a hole filling approach based on background reconstruction is proposed, in which the temporal correlation information in both the 2D video and its corresponding depth map are exploited to construct a background video. To construct a clean background video, the foreground objects are detected and removed. Also motion compensation is applied to make the background reconstruction model suitable for moving camera scenario. Each frame is projected to the current plane where a modified Gaussian mixture model is performed. The constructed background video is used to eliminate the holes in the synthetized video. Our experimental results have indicated that the proposed approach has better quality of the synthetized 3D video compared with the other methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As 3D TV and 3D movie become increasing popular recently, the technology of the production and communication for 3D video is a hot topic. How to acquire and transmit these 3D video data is a challenge. Depth image based rendering (DIBR) <ref type="bibr" target="#b8">[9]</ref> technique is a practical way to generate multi-view video by using a reference 2D video and its corresponding depth map, which can reduce the storage and save much bandwidth. However, in the DIBR technique, the regions of background occluded by the foreground objects in the original views might become visible in the virtual views, resulting in some holes in the synthetized video. This is also known as "disocclusion".</p><p>Generally, there are two types of methods to fill the holes. One is to preprocess the depth map by a low-pass filter so that the hole regions are reduced. The symmetric Gaussian low-pass filter <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref> or asymmetric filter <ref type="bibr" target="#b13">[14]</ref> is employed to smooth the whole depth map, which would lead to some geometrical distortions in the virtual view. These methods smooth not only the horizontal edge areas but also the areas that do not cause holes. To alleviate this problem, an edge-dependent Gaussian filter <ref type="bibr" target="#b3">[4]</ref> is used to smooth the horizontal edge only and keep the non-hole areas unchanged, or an adaptive edge-oriented smoothing process <ref type="bibr" target="#b19">[20]</ref> is utilized to preprocess the depth map with two types of smoothing filters. This type of methods would reduce 3D effect as the depth map is smoothed. Moreover, they are not suitable for the situation that the virtual camera is far away from the reference camera. The other type of methods is to use the spatial or temporal correlation of the video to fill the holes. In the spatial domain, the view blending approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22]</ref> can fill most of the holes by using multiple views, but they require more capturing devices and transmission bandwidth. Therefore, single view approaches are more practical and draw more attention. The hierarchical hole filling method <ref type="bibr" target="#b24">[25]</ref> down-samples and then progressively up-samples the virtual view, in which no geometrical distortions are produced but blurry regions around the large holes may be introduced. The exemplar-based inpainting method is a popular solution to fill the large holes without introducing blur artifacts. Criminisi et al. <ref type="bibr" target="#b6">[7]</ref> fill the holes by propagating both texture and structure simultaneously from non-hole regions, so blurry effect is not produced, but the foreground textures might be sampled to fill the holes. In order to alleviate this problem, some improved methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1]</ref> employ the depth information to exclude the foreground textures in the filling process. Daribo and Saito <ref type="bibr" target="#b7">[8]</ref> add the depth values to compute the priority and patch distance. Gautier et al. <ref type="bibr" target="#b10">[11]</ref> apply 3D structure tensor of Di Zenzo matrix to compute the priority and add depth information for patch matching. But they <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref> both use the depth map of the virtual view, which is not practical. To overcome this problem, Ahn and Kim <ref type="bibr" target="#b0">[1]</ref> compute the depth values of the virtual view in the filling process, but the artifacts would be produced in the hole regions when the depth values are incorrect.</p><p>Spatial filling methods try to fill the disoccluded areas with visually plausible backgrounds according to some spatial correlation assumptions, but may not reflect the groundtruth textures occluded by the foreground objects. Temporal filling method is able to reveal the ground-truth textures in the disoccluded areas by using more frames.</p><p>In temporal domain, the occluded background in current frame might become visible in other frames when the foreground objects move away. Background reconstruction can exploit the temporal correlation information in both the 2D video and its corresponding depth map to generate a background video, which can be used to eliminate the holes in the synthetized video. Therefore, some background models are employed to recover the occluded background. The average background model <ref type="bibr" target="#b5">[6]</ref> extracts the background from the scene and updates the background dynamically, which can get a stable background, but does not work for fast moving scene. The temporal background model <ref type="bibr" target="#b14">[15]</ref> generates the uncovered background layer by median filtering its neighboring frames, so the background information is limited in the neighboring frames. The sprite update method <ref type="bibr" target="#b17">[18]</ref> separates the background and foregrounds by depth values, and updates the background of 2D video and depth map respectively. The Switchable Gaussian Model (SGM) <ref type="bibr" target="#b26">[27]</ref> builds an online background method, also reduces the computational complexity and increases the scene adaptability. The Gaussian Mixture Model (GMM) and Foreground Depth Correlation (FDC) <ref type="bibr" target="#b30">[31]</ref> construct a stable background offline from several consecutive video frames and depth map. If the foreground object moves slowly or rotates, the GMM may regard this foreground object as part of the stable background since the real background is occluded by the foreground object in most of frames. If the depth map is imperfect, FDC may yield to some artifacts of the foreground textures.</p><p>Most of the background models based methods may bring some foreground textures in the constructed background or not suitable for moving camera scenario. In this paper, a hole filling approach based on background reconstruction is proposed, in which the foreground objects are removed, and then motion compensation is applied for moving camera scenario, finally a clean background video is generated by modified GMM. Our approach is suitable for motion scene, and can prevent blurry effect, or bringing artifacts from the foreground textures as the foreground objects are removed.</p><p>The rest of this paper is organized as follows. The proposed hole filling approach is presented in Section II. The foreground extraction is described in Section III, the new background models is described in Section IV, and the new  disocclusions filling is given in Section V, and in Section VI, the experimental results are presented. The conclusions are given in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed hole filling approach</head><p>Due to the inaccuracy of depth map and the round-off error caused by 3D warping, there are some distortions in non-hole regions of the virtual view. So in our proposed approach, the background is reconstructed in the reference view.</p><p>The proposed approach consists of three parts, including the foreground extraction in <ref type="figure" target="#fig_0">Figure 1</ref> In <ref type="figure" target="#fig_0">Figure 1</ref>(a), the depth map is preprocessed by a crossbilateral filter and morphological operations, and then the Canny's edge detection method is used to extract the initial seeds for the random walker, finally the foreground and background are separated in the depth map by the random walker segmentation.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>(b), the foreground objects in the 2D video and depth map are removed, motion compensation is applied for moving camera scenario, and the modified GMM is applied to obtain the BG video and BG depth.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>(c), the 2D video and its depth map, and the BG video and the BG depth map are warped by the 3D warping module to get the rendered video and rendered BG video, respectively; the holes of the rendered BG video are filled by inpainting-based method; and the rendered BG video are used to fill the holes in the rendered video.</p><p>The main processing modules will be described in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Foreground extraction</head><p>In order to generate a clean background video, the foreground objects need to be removed from the 2D video and its depth map in the reference view. How to automatically extract the foreground in the video is still a challenging issue. Image segmentation methods in literature <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b11">[12]</ref> can separate the foreground from background well, but they need to interact with users. In the literatures <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>, GMM is employed to build a background model, and foreground objects can be extracted by background subtraction, but only the moving objects can be detected. In the literature <ref type="bibr" target="#b4">[5]</ref>, the foreground and background are separated in the depth map by the random walker segmentation, but it is usually used in the virtual views, in which the initial labels can be gotten by applying the Laplacian operator to the depth map.</p><p>In our proposed method, the foreground objects are automatically extracted in the reference view by random walker segmentation algorithm. One of the most important steps for random walks is to extract the initial seeds automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Initial seeds</head><p>The initial seeds of the random walker are extracted by applying some characteristics of the depth map in the reference view. First, the edges of foreground objects and background need to be detected. Since there are some irregular depth discontinuities in the same object, these edges are not supposed to be extracted, so it is desirable to smooth them away. They can be smoothed away by using a low-pass filter, but the real edge regions will also be smoothed. In order to eliminate these unreal edges and preserve the real edges, a cross-bilateral filter <ref type="bibr" target="#b1">[2]</ref> is applied as follows: <ref type="formula">(1)</ref> where D is the filtering window with size W × W , the weight c and s both are Gaussian functions, and is given, respectively as:</p><formula xml:id="formula_0">h(x) = k −1 (x) D Z(ξ)c(ξ, x)s(f (ξ), f (x))dξ</formula><formula xml:id="formula_1">c (ξ, x) = exp − 1 2 |ξ − x| σ d 2 (2) s (f (ξ) , f (x)) = exp − 1 2 f (ξ) − f (x) σ r 2 (3) Z(x) is the depth value at pixel x, f (x)</formula><p>is the color value at pixel x, σ d is the variance of Euclidean distance, σ r is the variance of color space, k is the normalization factor, and is given by:</p><formula xml:id="formula_2">k(x) = D c(ξ, x)s(f (ξ), f (x))dξ<label>(4)</label></formula><p>By introducing weight function s, large distance in color space would have small weight, which preserves real edge regions from smoothing.</p><p>Since the edges are the boundaries of foreground objects and background, they might locate in the background or foreground. So grayscale morphological erosion operation is conducted to the depth map to ensure the edges locate in the foreground objects, and grayscale morphological dilation operation is conducted to the depth map to ensure the edges locate in the background, their corresponding results are denoted as Z i and Z o , respectively.</p><formula xml:id="formula_3">Z i = Z ⊙ B (5) Z o = Z ⊕ B<label>(6)</label></formula><p>where Z is the depth map of reference view, ⊙ is the operation of morphological erosion, ⊕ is the operation of morphological dilation, B is the structuring element with size L × L. After the preprocessing, Canny's edge detection method <ref type="bibr" target="#b29">[30]</ref> is conducted to both Z i and Z o to extract the inner boundaries of foreground objects (IBFO) and the outer boundaries of foreground objects (OBFO) as shown in <ref type="figure" target="#fig_2">Figure 2(b)</ref> and <ref type="figure" target="#fig_2">Figure 2(c)</ref>, respectively.</p><p>Based on the result of IBFO and OBFO, initial seeds can be automatically assigned to the random walker algorithm. Let us define a label set S = {s1, s2}, where labels s1 and s2 correspond to the foreground and background, respectively. Note that the points of IBFO (red line) are in the foreground, and the points of OBFO (green line) are in the background. So the points of IBFO are served as foreground label (s1), and the points of OBFO are served as the background label (s2) as shown in <ref type="figure" target="#fig_2">Figure 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Random Walker Segmentation</head><p>To formulate the separation of foreground and background as a labeling problem, an undirected graph G = (V, E) is constructed for random walks <ref type="bibr" target="#b11">[12]</ref> formulation, where V is the set of all the points in the depth map, and E is the set of weighted edges. Let us define v i represents the i th point in the depth map , v ∈ V .</p><p>In order to solve this labeling problem, the weights between nodes are defined as typical Gaussian weighting function.</p><formula xml:id="formula_4">w ij = exp −β(g i − g j ) 2 + ε<label>(7)</label></formula><p>where g i indicates the depth value at pixel i , ε is a small constant (e.g., 10 −6 ), β is a weighting factor to balance the sensitivity of the depth similarity cost (e.g., β = 90 in our experiment), depth values are normalized prior to applying <ref type="bibr" target="#b6">(7)</ref>, so that 0 ≤ |g i − g j | ≤ 1.</p><p>From the weight defined in <ref type="formula" target="#formula_4">(7)</ref>, the combinatorial Laplacian matrix L is given as</p><formula xml:id="formula_5">L ij =    d i if i = j −w ij</formula><p>if v i and v j are adjacent nodes 0 otherwise</p><p>where L ij is indexed by points v i and v j , and d i = w (e ij ) is the degree of node i for all edges e ij incident on point v i .</p><p>The vertices are partitioned into two sets, seeded nodes V M and unseeded nodes V U , and L can be decomposed as</p><formula xml:id="formula_7">L = L M B B T L U<label>(9)</label></formula><p>where L M is the weights of seeded nodes and L U is the weight of unseeded nodes.</p><p>Solving the unknown probabilities for the label is equivalent to solving the matrix equation of</p><formula xml:id="formula_8">L U x U = −B T x M<label>(10)</label></formula><p>where x M and x U correspond to the probabilities of the seeded and unseeded nodes respectively. Additionally, let us define the probability at node v i for each label s by x s i . Define the set of labels for the seeded points as a function Q (v j ) = s, ∀v j ∈ V M , where s ∈ S , S = {s1, s2}. Define the |V M | × 1 vector for each label s at node v j ∈ V M as</p><formula xml:id="formula_9">m s j = 1 if Q(v j ) = s 0 if Q(v j ) = s<label>(11)</label></formula><p>Therefore for label s, the solution to the combinatorial Dirichlet problem can be found by solving</p><formula xml:id="formula_10">L U x s = −B T m s<label>(12)</label></formula><p>With the initial seeds, random walker's probability map of the foreground label and the background label can be obtained by solving <ref type="formula" target="#formula_10">(12)</ref>, whose results are shown in <ref type="figure" target="#fig_4">Figure 3</ref>(a) and <ref type="figure" target="#fig_4">Figure 3(b)</ref>, respectively. The higher intensity value corresponds to higher probability. The label of each unseeded point is obtained from the label with the highest probability, then the segmented regions associated with the foreground label and the background label can be decided, as shown in <ref type="figure" target="#fig_4">Figure 3</ref>(c) and <ref type="figure" target="#fig_4">Figure 3</ref>(d), respectively. Notice that the still foreground object (the pillar) also can be extracted in the proposed method, as shown in <ref type="figure" target="#fig_4">Figure 3</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dynamically background reconstruction</head><p>After the foreground objects are removed, the remaining part of the video can be used to reconstruct a clean background. Considering traditional background reconstruction methods are not suitable for moving camera scenario, the proposed dynamically background reconstruction is processed by two modules: motion compensation and modified GMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motion compensation</head><p>In the case of non-stationary camera, the model learned until time t − 1 cannot be used directly in time t. All the parameters of the model need to be warped to the new positions by using motion compensation.</p><p>In the motion compensation process, SURF <ref type="bibr" target="#b2">[3]</ref> is used to detect and describe the feature points in the reference frame and the current frame. To be more robust, the RANSAC algorithm <ref type="bibr" target="#b9">[10]</ref> is utilized for optimally matched feature pointpair. The homography matrix H t:t−1 can be gotten once the optimized feature point-pair are obtained. Then all the parameters of the model in time t − 1 are warped to time t through a perspective transformation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Modified GMM</head><p>The Gaussian Mixture Model is usually used for detecting the moving objects <ref type="bibr" target="#b25">[26]</ref>, as it can be applied to model the stable background. GMM is performed at pixel level, where each pixel is modeled independently by a mixture of K Gaussian distributions (a common setting is K = 3) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>. In our proposed method, the foreground pixels are not used to build the models, and motion compensation is used for non-stationary scene. The modified Gaussian mixture distribution with K components can be written as:</p><formula xml:id="formula_11">p (I x,t ) = B (x t ) · K i=1 w x,i,t · η I x,t , µ x,i,t , σ 2 x,i,t<label>(13)</label></formula><p>where p (I x,t ) indicates the probability density of pixel x at time t, η is the Gaussian function with I x,t representing the value of pixel x at time t, µ x,i,t and σ 2</p><p>x,i,t denote the mean and variance of pixel x at time t, respectively, and w x,i,t is the i th Gaussian distributions weight of pixel x at time</p><formula xml:id="formula_12">t, with K i=1 w x,i,t = 1 , B (x t )</formula><p>is the background mask of pixel x at time t, with B (x t ) = 0 when the models are empty, B (x t ) = 1 when the models are not empty.</p><p>The detailed process of the proposed background model is described as follows: (1) Firstly, an empty set of models is initialized at the time instant t 0 .</p><formula xml:id="formula_13">µ x,i,t0 = I x,t0 if i = 1 and F (x t0 ) = 0 0 others (14) σ x,i,t0 = σ 0 (15) w x,i,t0 = 1 if i = 1 0 others (16) B (x t0 ) = 1 if F (x t0 ) = 0 0 others (17)</formula><p>where σ 0 is a pre-defined large value, F (x t ) is the foreground mask of pixel x at time t, if pixel x t is detected as a foreground pixel, F (x t ) = 1; otherwise, F (x t ) = 0.</p><p>(2) For the next frame at the time instant t 1 , all background models in time t − 1 are warped to background models in time t through a perspective transformation. By using the homography matrix H t:t−1 , the coordinate x t in the plane at time t is warped to x ′ t−1 in the plane at time t − 1, correspondingly, the background models at time t of pixel x t are updated from the background models at time t − 1 of pixel</p><formula xml:id="formula_14">x ′ t−1 . µ x,i,t−1 = µ x ′ ,i,t−1 (18) σ 2 x,i,t−1 = σ 2 x ′ ,i,t−1 (19) w x,i,t−1 = w x ′ ,i,t−1 (20) B (x t−1 ) = B x ′ t−1<label>(21)</label></formula><p>Then the background models will update if current pixel is not a foreground pixel (F (x t ) = 0 ), the update process is described as the following role:</p><p>The current pixel is used to match with the K Gaussian models.</p><p>For each model i, if the condition |I x,t − µ x,i,t−1 | ≤ 2.5σ x,i,t−1 is satisfied, the matching process will be stopped. The parameters of the matched Gaussian model will be updated as:</p><formula xml:id="formula_15">µ x,i,t = (1 − ρ) µ x,i,t−1 + ρI x,t<label>(22)</label></formula><formula xml:id="formula_16">σ 2 x,i,t = (1 − ρ) σ 2 x,i,t−1 + ρ(I x,t − µ x,i,t ) 2 (23) w x,i,t = (1 − α) w x,i,t−1 + α<label>(24)</label></formula><p>And the parameters of the other Gaussian models will be updated as:</p><formula xml:id="formula_17">µ x,i,t = µ x,i,t−1 (25) σ 2 x,i,t = σ 2 x,i,t−1 (26) w x,i,t = (1 − α) w x,i,t−1<label>(27)</label></formula><p>where ρ = α · η I x,t , µ i,t , σ 2 i,t ,α is the learning rate. Whereas, if all of the Gaussian models fail to match the current pixel, then a new Gaussian model is introduced with µ x,t = I x,t , σ x,t = σ 0 , ω x,t = w 0 , where w 0 is a low weight value to evict the Gaussian model which has the smallest ω/σ value. The mean and variance value of the other Gaussian models remain unchanged, and the weight value of K Gaussian models are normalized to</p><formula xml:id="formula_18">K i=1 w x,i,t = 1 . (3)</formula><p>The remaining frames are processed by repeating the previous step <ref type="bibr" target="#b1">(2)</ref>. Finally the K Gaussian models are sorted based on ω/σ, and the value of the background pixel bp (x t ) at the time instant t is obtained as</p><formula xml:id="formula_19">bp (x t ) = µ x,1,t , if B (x t ) = 1<label>(28)</label></formula><p>The setting of parameter σ 0 , α and w 0 have been discussed in literature <ref type="bibr" target="#b15">[16]</ref>, their typical values σ 0 = 30 , α = 0.005 and w 0 = 0.001 are used here.</p><p>One example of the BG video and BG depth map reconstruction by the modified GMM method is shown in <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Disocclusions filling</head><p>After backgrounds are constructed, they are used to fill the holes in the virtual view.</p><p>In the virtual view, the rendered video and rendered BG video are generated by 3D warping module, which is shown in <ref type="figure" target="#fig_6">Figure 5</ref>(a) and (b), respectively. In the rendered BG video, some regions occluded by foreground objects cannot be reconstructed by background model, these regions are filled by Criminisi's inpainting method. This approach can prevent blurry effect and the penetration problem of the foreground textures since the foreground objects are removed. The BG video shown in <ref type="figure" target="#fig_6">Figure 5</ref>(c) is used to fill the holes in the rendered video, the result is shown in <ref type="figure" target="#fig_6">Figure 5</ref>  <ref type="bibr" target="#b32">[33]</ref>, and 'Dancer' <ref type="bibr" target="#b23">[24]</ref>) are used to evaluate the performance of the proposed approach in our experiment. They consist of stationary and moving camera scenario. The parameters of the test dataset are shown in <ref type="table">Table 1</ref>.</p><p>The performance of visual quality are compared among the proposed method, the Criminisi's exemplar-based inpainting method <ref type="bibr" target="#b6">[7]</ref>, the Daribo's disocclusion filling method <ref type="bibr" target="#b7">[8]</ref>, the Ahn's depth-based image completion method <ref type="bibr" target="#b0">[1]</ref>, the MPEG view synthesis reference software (VSRS, version 3.5) <ref type="bibr" target="#b28">[29]</ref> , and the Yao's GMM-based method <ref type="bibr" target="#b30">[31]</ref>. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> and the proposed method, the virtual view generation need only single Video-plus-Depth (SVD) of reference view, but in <ref type="bibr" target="#b7">[8]</ref>, the depth map of the virtual view is also needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Visual quality evaluation</head><p>In our experiment, PSNR is used to measure the squared intensity differences of synthesized and reference image pixels, and SSIM (structural similarity) <ref type="bibr" target="#b29">[30]</ref> is used to measure the structural similarity between synthesized and reference image. The average PSNR and SSIM values of proposed method and other methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> for the test sequences are shown in <ref type="table">Table 2</ref>, where 'Test Seq.' denotes the dataset and projection information, for example, the sequence warped from view 5 to view 4 of 'Ballet' is named as BA54. The best results are highlighted in boldface. The results show that the proposed method yields the best overall results. The measured results on each frame of 'BA54','BR54',and 'DA15' are shown in <ref type="figure" target="#fig_8">Figure 6</ref>. The proposed method shows almost the best in both PSNR and SSIM measures compared with the other methods.</p><p>The visual quality comparisons of disoccluded areas for 'BA54','BR54',and 'DA15' are shown in <ref type="figure" target="#fig_9">Figure 7</ref>, the proposed method has more plausible results and preserves sharp edges, while other methods contain some artifacts or blurry results. In the Criminisi's method <ref type="bibr" target="#b6">[7]</ref>, foreground textures are sampled to fill a large part of hole regions as shown in <ref type="figure" target="#fig_9">Figure 7</ref>(b); in the Daribo's method <ref type="bibr" target="#b7">[8]</ref>, some artifacts occur along the foreground boundaries as shown in <ref type="figure" target="#fig_9">Figure 7</ref>(c); in the Ahn's method <ref type="bibr" target="#b0">[1]</ref>, some artifacts occurred along the image boundaries as shown in <ref type="figure" target="#fig_9">Figure 7(</ref>  in the VSRS <ref type="bibr" target="#b28">[29]</ref> , some unrealistic regions or blurry artifacts are produced as shown in <ref type="figure" target="#fig_9">Figure 7</ref>(e); in the Yao's method <ref type="bibr" target="#b30">[31]</ref>, some artifacts of the foreground textures are produced as shown in <ref type="figure" target="#fig_9">Figure 7</ref>(f). The proposed method successfully preserves sharp edges along the foreground boundaries and shows realistic appearance in <ref type="figure" target="#fig_9">Figure 7</ref>(g).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In our hole filling approach, a constructed background video with modified GMM is used to eliminate the holes in the synthesized video. The foreground objects in 2D video and the depth map in the reference view are extracted and removed, and then motion compensation and modified GMM are applied to construct a stable background. Our investigation have indicated that a clean background without bringing the artifacts of foreground objects can be generated by using the proposed background model, so that the blurry effect or artifacts in the disoccluded regions can be eliminated and the sharp edges along the foreground boundaries with realistic appearance can be preserved as well. Criminisi's method <ref type="bibr" target="#b6">[7]</ref>. (c) Daribo's method <ref type="bibr" target="#b7">[8]</ref>. (d) Ahn's method <ref type="bibr" target="#b0">[1]</ref>. (e) VSRS <ref type="bibr" target="#b28">[29]</ref>. (f) GMM-based method <ref type="bibr" target="#b30">[31]</ref>. (g) Proposed. (h) Ground truth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Block diagram of the proposed algorithm. (a) Foreground extraction. (b) BG video and BG depth map reconstruction. (c) DIBR module with BG for disocclusions filling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a), the reconstruction of background (BG) video and BG depth map in Figure 1(b), and DIBR module with BG for disocclusions filling in Figure 1(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Initial seeds extraction for "Dancer". (a) Original depth map. (b) Zi and IBFO (red). (c) Zo and OBFO (green). (d) Initial seeds on the depth map, foreground label (red) and the background label (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Foreground extraction for "Dancer". (a) and (b) Random walker's probability map of the foreground label (x s1 ) and the background label (x s2 ), respectively. (c) and (d) Foreground regions and background regions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Background reconstruction for 'Dancer'. (a) BG reconstruction by modified GMM. (b) The occluded regions recovered by (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Disocclusions filling for 'Ballet'. (a) Rendered video. (b) Rendered BG video. (c) Inpainting result of (b) . (d) (c) is used to fill the holes in (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Objective quality measure results of the proposed and other methods. (a)(b)(c) PSNR of 'BA54','BR54' and 'DA15' respectively. (d)(e)(f) SSIM of 'BA54', 'BR54' and 'DA15'respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Results of the proposed method and competing algorithms in 'Ballet', 'BreakDancer' and 'Dancer'. (a) Hole regions. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>(d).</figDesc><table>Name 
Resolution 
Scene 
Baseline 
Ballet 
1024×768 Stationary 380mm 
Breakdancers 1024×768 Stationary 370mm 
Dancer 
1920×1088 Dynamic 2000mm 

Table 1. Parameters of test dataset 

6. Experimental results 

6.1. Experiment setup 

Three Multiview Video-plus-Depth (MVD) sequences 
('Ballet', 'Breakdancers' </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>d);Table 2. The objective evaluations of the proposed and other methods using PSNR and SSIM</figDesc><table>Test 

PSNR 
SSIM 

Seq. 
[7] 
[8] 
[1] 
[29] 
[31] 
Ours 
[7] 
[8] 
[1] 
[29] 
[31] 
Ours 

BA41 22.76 22.56 23.27 22.23 23.05 24.15 0.7391 0.7130 0.7478 0.7611 0.7496 0.7759 

BA43 25.08 27.63 28.15 25.93 25.61 28.86 0.8388 0.8351 0.8465 0.8514 0.8429 0.8570 

BA52 24.38 23.97 24.29 23.89 24.81 25.76 0.7422 0.7200 0.7385 0.7654 0.7563 0.7768 

BA54 26.56 29.60 30.54 27.60 27.53 32.00 0.8468 0.8448 0.8545 0.8584 0.8524 0.8665 

BR41 25.87 26.92 26.91 27.03 27.09 27.30 0.7639 0.7639 0.7737 0.7814 0.7694 0.7763 

BR43 29.74 30.20 30.40 29.61 30.53 30.60 0.8197 0.8151 0.8223 0.8126 0.8227 0.8248 

BR52 26.23 27.55 27.32 26.40 27.72 27.85 0.7660 0.7675 0.7737 0.7606 0.7712 0.7776 

BR54 30.24 30.86 30.27 30.25 31.50 31.73 0.8217 0.8177 0.8225 0.8133 0.8255 0.8275 

DA15 26.74 27.64 27.80 26.42 27.47 29.76 0.9412 0.9429 0.9446 0.9425 0.9408 0.9471 

DA59 26.69 27.56 27.32 26.22 27.76 29.53 0.9405 0.9419 0.9434 0.9412 0.9415 0.9463 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel depth-based virtual view synthesis method for free viewpoint video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Broadcasting</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="614" to="626" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A 2D to 3D video and image conversion technique based on a bilateral filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Angot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="75260" to="75260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision-ECCV 2006</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient depth image based rendering with edge dependent depth filter and interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1314" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Space-time hole filling with random walks in view extrapolation for 3D video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2429" to="2441" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient dense stereo with occlusions for new viewsynthesis by four-state dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="110" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel inpainting-based layered depth video for 3DTV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daribo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Broadcasting</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="533" to="541" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth-image-based rendering (dibr), compression, and transmission for a new approach on 3D-tv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Imaging</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth-based image completion for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gautier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DTV Conference: The True Vision-Capture, Transmission and Display of 3D Video (3DTV-CON)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved gaussian mixtures for robust object detection by adaptive multibackground generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Murshed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stereoscopic images generation with directional gaussian filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-R</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2010 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>2010 IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2650" to="2653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A layered method of visibility resolving in depth image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th IEEE International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An improved adaptive background mixture model for real-time tracking with shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kaewtrakulpong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Video-based surveillance systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depth map creation and imagebased rendering for advanced 3DTV services providing interoperability and scalability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kauff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Atzpadin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Schreer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="234" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporally consistent handling of disocclusions with texture synthesis for depth-image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ndjiki-Nya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doshkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merkle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1809" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective gaussian mixture learning for video background subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="827" to="832" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nongeometric distortion smoothing approach for depth map preprocessing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans.Multimedia Expo</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="254" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">View generation with 3D warping using depth information for FTV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yendo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tanimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="72" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">View synthesis for advanced 3D video systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merkle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kauff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Description of exploration experiments in 3d video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEC JTC1/SC29/WG11 MPEG2010 N, 11274</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical hole-filling for depthbased view synthesis in ftv and 3D video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Solh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="495" to="504" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on CVPR</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Novel temporal domain hole filling based on background modeling for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">C</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2721" to="2724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smoothing depth maps for improved steroscopic image quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Renaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optics East</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="162" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reference softwares for depth estimation and view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tanimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISO/IEC JTC1/SC29/WG11 MPEG</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15377</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Depth map driven hole filling algorithm exploiting temporal correlation information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Broadcasting</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="404" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Free-viewpoint depth image based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De With</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of visual communication and image representation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="533" to="541" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-quality video view interpolation using a layered representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="600" to="608" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
