<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Highway Vehicle Counting in Compressed Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Xi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Highway Vehicle Counting in Compressed Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a highway vehicle counting method in compressed domain, aiming at achieving acceptable estimation performance approaching the pixel-domain methods. Such a task essentially is challenging because the available information (e.g. motion vector) to describe vehicles in videos is quite limited and inaccurate, and the vehicle count in realistic traffic scenes always varies greatly. To tackle this issue, we first develop a batch of low-level features, which can be extracted from the encoding metadata of videos, to mitigate the informational insufficiency of compressed videos. Then we propose a Hierarchical Classification based Regression (HCR) model to estimate the vehicle count from features. HCR hierarchically divides the traffic scenes into different cases according to vehicle density, such that the broad-variation characteristics of traffic scenes can be better approximated. Finally, we evaluated the proposed method on the real highway surveillance videos. The results show that our method is very competitive to the pixeldomain methods, which can reach similar performance along with its lower complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the number of on-road vehicles is one of the important tasks in intelligent transportation system (ITS), which can be used to monitor the traffic status and then guide the traffic control and optimization, e.g. programing better driving routes through bypassing the congested roads <ref type="bibr" target="#b1">[2]</ref>. In this paper, we particularly consider the vehicle counting issue by the means of video analysis. Compared with employing specialized sensors (e.g. infrared or inductive loop detector), the visual counting system is more easy to deploy by reusing the roadside cameras, and thus pays lower cost <ref type="bibr" target="#b15">[16]</ref>.</p><p>In a typical traffic surveillance system, the central subsystem connects all terminal cameras through a private network and is expected to constantly receive the surveillance videos. In practice, however, only part of video streams can  To tackle the boundary cases produced by the first-layer classifier, i.e. samples around the boundary area of two categories are more likely to be misclassified, the second layer classifiers are introduced to distinguish the samples whether located in the boundary area (i.e. B1 or B2). Afterwards, regression models are applied to each category. be simultaneously accessed due to the limitation of network bandwidth <ref type="bibr" target="#b11">[12]</ref>. Such partial acquisition is rather tolerable for the purpose of monitoring with the help of free stream switch. But it is inapplicable to the video analysis that needs to process almost all of video streams for fully capturing the traffic status of a wide range. Thus it is naturally considered to conduct the analysis function in the terminal devices (e.g. surveillance workstations). These devices are usually equipped with lower configurations than the central servers considering the overall costs. Therefore, the video analysis algorithms are expected to have low computational complexity for fulfilling the real-time requirements.</p><p>The video analysis can be conducted in the pixel or compressed domain. Particularly, the pixel-domain methods are to first decode the surveillance videos into huge volume of frame pixels and then operate on the pixels to complete some specific target. Due to involving such decoding procedure and massiveness of processed pixels, the high computational complexity is usually possessed <ref type="bibr" target="#b12">[13]</ref>. On the contrary, the compressed-domain methods are to directly operate on the video data of compressed format, which is exactly the original form of storing and transmitting videos <ref type="bibr" target="#b0">[1]</ref>.</p><p>Hence it is considered that the compressed-domain methods may be more appropriate for large-scale video analysis system. In this paper, we particularly address the vehicle counting issue in compressed domain.</p><p>The video analysis methods in compressed domain mainly rely on the encoding metadata, which can be easily extracted from video bitstreams, e.g. the motion vector (MV), DCT coefficients, and macro-blocks (MB) partition modes <ref type="bibr" target="#b0">[1]</ref>. There are two non-trivial challenges for vehicle counting. First, the critical metadata in compressed videos (i.e. motion estimation and compensation vectors) are originally determined from the view of compression efficiency rather than video analysis <ref type="bibr" target="#b29">[30]</ref>. Consequently, the features extracted from video bitstreams are probably inaccurate and noisy in describing moving vehicles, besides less available information can be provided compared to the frame pixels. Second, the traffic scenes are usually fast-changing with containing various numbers of vehicles in a broad range, and vulnerable to the external factors (e.g. weather conditions, and illumination changes) <ref type="bibr" target="#b27">[28]</ref>. These challenges make it quite difficult to accurately model the realistic traffic scenes for vehicle counting.</p><p>In this paper, we propose a multi-regression method for highway vehicle counting in compressed domain, with aims of achieving acceptable prediction performance approaching the pixel-domain methods. To our best knowledge, this is the first attempt on this issue. Specifically, we first develop a batch of low-level features to capture the crucial information associated with vehicle count. These features can be easily computed from the provided MVs and block partition modes, and cover the size, shape, motion, and texture of traffic scenes. We believe that all of features together can rather mitigate the disadvantage of informational insufficiency. Then we propose a Hierarchical Classification based Regression (HCR) model for vehicle counting, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. HCR divides the traffic scenes into multiple cases according to the vehicle density (e.g. heavy, medium, and light here), and then adopts one well-performed regression model for each of them. Indeed, introducing such classification is for better approximating the broad-variation characteristics of traffic scenes, since it is observed in practice that some density-specific patterns are presented for the scenes involving different vehicle counts. Furthermore, we add one more layer of classifiers for handling the boundary cases produced by the first-layer classifiers. As a result, the large estimation deviation incurred by misclassification can be greatly alleviated.</p><p>We evaluated the proposed method on the real highway surveillance videos presenting various traffic scenes. The experimental results show that our method is very competitive compared to the pixel-domain methods, which results in the similar performance (the estimation error of 2 ∼ 3), while possesses lower computation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Object counting as one of the typical visual tasks targets to estimate the number of specific objects within a given image <ref type="bibr" target="#b14">[15]</ref>. According to the adopted strategy, existing approaches can be roughly divided into three categories: counting by detection, counting by clustering, and counting by regression <ref type="bibr" target="#b16">[17]</ref>. Usually, the methods based on detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> or clustering <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref> need to explicitly segment the objects or track the feature points, and thus may fail if the serious occlusions or scene clutters arise. Differently, the regression based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33]</ref> are to straightforwardly learn a mapping from the extracted image features to the desired density value. Such a way can alleviate the ubiquitous interferences and thus is usually outperforming in practice along with the advantage of simplicity. Hence the regression model is considered more applicable to counting objects from the realistic scenes <ref type="bibr" target="#b16">[17]</ref>.</p><p>In the previous literatures, most of the proposed regression models are primarily aimed to crowd counting in public <ref type="bibr" target="#b24">[25]</ref>. For example, Davies et al. <ref type="bibr" target="#b6">[7]</ref> first proposed a linear regression model mapping the holistic features into the people count. Chan et al. <ref type="bibr" target="#b2">[3]</ref> proposed a perspective normalization method to handle the diversity of camera perspectives and a bank of complementary features to improve the accuracy. In addition, some semi-supervised counting methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref> were also proposed, which were principally to utilize the continuity and consistency between unlabeled samples and their temporally neighboring samples. Comparatively, these methods can exploit more unlabeled data, e.g. via transfer learning <ref type="bibr" target="#b17">[18]</ref>, and thus perform better for the complicated crowd scenes difficult to label. Recently, convolutional neural network (CNN) was specially introduced to cross-scene crowd counting <ref type="bibr" target="#b32">[33]</ref>. Specifically, the model was pretrained using a given dataset and then finetuned for an unseen target scene by feeding the retrieved similar training samples.</p><p>However, existing regression methods for vehicle counting are very rare, which practically is rather difficult due to the visual diversity of vehicle appearance (e.g. truck vs. car). To our best knowledge, only a three-level cascaded regression model in pixel domain <ref type="bibr" target="#b15">[16]</ref> was proposed to classify the vehicle scenes, and no any compressed-domain method has been investigated yet. Actually, current works in compressed domain mostly focus on the detection and segmentation of moving objects <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14]</ref>. Other related works to vehicle counting are to estimate some traffic parameters, e.g. the congestion level, and vehicle speed. Specifically, Porikli et al. <ref type="bibr" target="#b21">[22]</ref> proposed a traffic congestion estimation method by analyzing the MPEG-encoded videos, where the DCT coefficients and MVs were exploited. Tusch et al. <ref type="bibr" target="#b28">[29]</ref> introduced four features associated with the vehicle density to estimate level of service (LOS). Yu et al. <ref type="bibr" target="#b31">[32]</ref>   <ref type="figure">Figure 2</ref>. The framework of our proposed highway vehicle counting system, which includes three key stages: (1) video preprocessing, (2) feature extraction, and (3) estimation of vehicle count. The preprocessing stage is to extract the video encoding information from input video bitstream, and then prepares the necessary data for extracting features. Then these data are translated into various features to represent the complex traffic scenes. Finally, the number of vehicles is estimated using the proposed HCR.</p><p>using MVs of MPEG-encoded videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>In this paper, we propose to solve the vehicle counting issue in compressed domain, aiming at approaching the performance achieved by the pixel-domain methods. The main obstacles to vehicle counting in compressed domain lie in the limited and inaccurate provision of available information and broad-changing of traffic scenes. In this work, we tackle these challenges by constructing rich features, which can effectively exploit the provided data for representing the traffic scenes, and proposing a novel counting model, i.e. Hierarchical Classification based Regression (HCR), which adaptively applies the suitable submodel for the given traffic scene according to its presenting characteristics. <ref type="figure">Figure 2</ref> shows the framework of the proposed counting method. It can be seen that the processing pipeline mainly comprises of three key stages: (1) video preprocessing, (2) feature extraction, and (3) estimation of vehicle count. Specifically, in the preprocessing stage, we first parse the input video bitstream to extract the video encoding information, and then prepare the necessary data for extracting features. Then we translate these data into various features, which are expected to be rich enough for accurately representing complex traffic scenes. Finally, we conduct the vehicle counting, i.e. estimate the number of vehicles, using the proposed HCR. In this paper, H.264 codec <ref type="bibr" target="#b29">[30]</ref> is particularly adopted due to its high encoding efficiency and wide application in the real video surveillance systems. For a given compressed video bitstream, we mainly extract the metadata of Motion Vector (MV) and Macro-block (MB) partition modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preprocessing</head><p>The preprocessing stage targets to produce the metadata of standardized format from the raw video bitstream, which mainly includes the motion vector normalization, macroblock weighting, foreground segmentation, and perspective normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Motion vector normalization</head><p>In the H.264 compressed format, MB is the basic unit of video encoding <ref type="bibr" target="#b29">[30]</ref>. The MBs can be encoded in various block partition modes, such as 16×16, 16×8, 8×16, 8×8, 8 × 4, 4 × 8, and 4 × 4, and each block corresponds to a MV. In addition, multiple reference frames can be used for one frame in order to improve the efficiency of inter-frame encoding. Thus the MVs in the same frame often have different temporal scales. In this work, we use the temporal interpolation to normalize the MVs to a uniform temporal scale. Let B ij denotes the MB at the location (i, j), where i and j denote the index of MB along the X-axis and Y-axis in the video frames, respectively. The MV of B ij at the time t is denoted by V ij (t). The corresponding normalized MV is defined as:Ṽ</p><formula xml:id="formula_0">ij (t) = V ij (t) t − r ,<label>(1)</label></formula><p>where r is the time of the reference MB. The mode of the smallest block in H.264 is 4 × 4. In order to obtain a uniform MV field, we split all the blocks into 4 × 4, e.g. one 8 × 16 would be partitioned into 8 pseudo-MBs with the size of 4 × 4. Particularly, the MVs of 4 × 4 pseudo-MBs are straightforwardly assigned using the MV of corresponding parent MB. Additionally, for the intra-coded blocks originally having no MVs, we adopt the Polar Vector Median (PVM) <ref type="bibr" target="#b12">[13]</ref> method to compute their MVs. Finally, we obtain a normalized MV fieldṼ . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Macro-block weighting</head><p>There are seven different partition sizes in H.264 with the application of deformable macro-block technology. Particularly, the areas around moving objects usually have small partition sizes in order to achieve higher compression efficiency <ref type="bibr" target="#b25">[26]</ref>. Therefore, the blocks with smaller partition sizes are more likely to represent the actual vehicle motion, and thus are expected to make more contributions to holistic features. We assign MBs different weights according to the MB partition modes. Let f m (B ij ) denotes the partition mode of B ij . Then the weights are determined by following the rules as:</p><formula xml:id="formula_1">W ij =            1 if f m (P B ij ) is 16 × 16 2 if f m (P B ij ) is 16 × 8 or 8 × 16 3 if f m (P B ij ) is 8 × 8 4 if f m (P B ij ) is 8 × 4 or 4 × 8 5 if f m (P B ij ) is 4 × 4 ,<label>(2)</label></formula><p>where P B ij denotes the parent block of the 4 × 4 pseudoblock B ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Foreground segmentation</head><p>This process is to separate the foreground regions from background in the normalized MV field. To this end, we first apply a binary region of interest (ROI) to the MV field. Then we adopt the thresholding strategy to produce the final foreground regions, i.e. the block B i inside the ROI is labeled as foreground if its MV is larger than the preset threshold T f . To capture the vehicle motion in all scenes, the threshold is set as T f = 1 in our implementation. <ref type="figure" target="#fig_2">Figure 3</ref> presents an exemplar of segmentation result. It can be observed that the moving vehicles can be roughly contained by the segmented foreground regions although part of backgrounds may also be involved. In particular, adopting the simple thresholding strategy here is due to its low complexity, and we believe that the performance would be further improved if some advanced methods are taken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Perspective normalization</head><p>In the video surveillance systems, the far vehicles appear smaller than those closer to the camera due to the perspective effects. Consequently, the features extracted from the same object with different scene depths would be diversified. To deal with such an issue, the perspective normalization is usually performed. Practically, each block is scaled with a weight, and larger weights are assigned to the further vehicle candidates. The perspective effect is almost fixed for a certain camera or workstation. Thus we only need to periodically sample the video frames and then update the perspective normalization map (denoted by S with each block one value). In this paper, we first adopt the method in <ref type="bibr" target="#b2">[3]</ref> to compute the perspective normalization map in the pixel domain, and then transform it into the desired map S in the MV filed by down-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature extraction</head><p>We elaborate on the feature extraction in this section. Ideally, the features should capture the significant information associated with vehicle count or density. To this end, we develop a batch of low-level features, which cover the size, shape, motion, and texture.</p><p>Size: The size features can capture the magnitude of holistic foreground segment. Here we particularly use two metrics, i.e. area, and perimeter length.</p><p>• Area: It is defined as the total number of blocks belonging to the segmented foreground. This feature denoted by f a is calculated from the perspective normalization map S and MB type weights W ij , i.e.</p><formula xml:id="formula_2">f a = Bij ∈F W ij · S ij ,<label>(3)</label></formula><p>where F represents the foreground area.</p><p>• Perimeter length: It is defined as the total number of blocks lying on the perimeter of foreground segment. Formally, this feature denoted by f l is weighted using MB type weights W ij and square root of perspective normalization map S as in <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_3">f l = Bij ∈P W ij · S ij ,<label>(4)</label></formula><p>where P denotes the set of perimeter blocks.</p><p>Shape: Aside from the Perimeter length, which captures the global properties of the segments, the orientation of perimeter blocks also carries significant shape information due to presenting some local and internal pattern. In this paper, therefore, we define the shape feature as an orientation histogram of perimeter blocks, where eight bins are used. For the block B ij , the orientation o ij and magnitude m ij are calculated as follows:</p><formula xml:id="formula_4">o ij = tan −1 {g y (Ṽ ij )/g x (Ṽ ij )} m ij = g x (Ṽ ij ) 2 + g y (Ṽ ij ) 2 ,<label>(5)</label></formula><p>where g x (Ṽ ij ) and g y (Ṽ ij ) denote the horizontal and vertical components ofṼ ij , respectively. In addition, the voting weight of B ij is adjusted by W and S when computing the histogram, and is (m ij · W ij · S ij ). HOMV: MVs reflect the motion orientation and magnitude of objects represented by MBs. In this paper, we compute a feature named Histogram of Oriented Motion Vector (HOMV) to present such information. The calculation of HOMV is similar to the shape feature, except for the M-B range and weights. To be specific, all foreground MBs are involved for HOMV, and the voting weight for B ij is (m ij · W ij · S ij ).</p><p>Texture: The texture features are strongly correlated to the vehicle density for traffic scenes. Compared with scenes of low density, scenes of high density tend to present finer patterns <ref type="bibr" target="#b19">[20]</ref>. So we extract the texture feature to capture such a clue. In object counting, two texture features are widely used, i.e. Gray-level co-occurrence matrix (GLCM) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3]</ref> and local binary pattern (LBP) <ref type="bibr" target="#b18">[19]</ref>. In this work, we particularly employ the LBP operator <ref type="bibr" target="#b20">[21]</ref> due to its simplicity and effectiveness. The LBP feature is constructed by comparing the MVs of eight-neighboring MBs to the target one. Formally, LBP of the target MB B ij is defined as:</p><formula xml:id="formula_5">LBP ij = B k ∈Nij s(d(Ṽ ij ,Ṽ k )) · 2 k ,<label>(6)</label></formula><p>where N ij represents the set of neighboring MBs of B ij , and B k is its element with k = 0, 1, · · · , 7. In addition, d(·) is a distance metric function to measure the similarity between two MVs, which is defined as:</p><formula xml:id="formula_6">d(Ṽ i ,Ṽ j ) = exp − Ṽ i −Ṽ j 2 Ṽ i 2 + Ṽ j 2 .<label>(7)</label></formula><p>And s(·) is a sign function:</p><formula xml:id="formula_7">s(x) = 1 x ≥ T s 0 x &lt; T s .<label>(8)</label></formula><p>Here T s is a threshold, which is set to 0.9 throughout our experiments.  The final texture feature employed in this paper is the histogram of the LBP outputs accumulated over all foreground MBs. All of proposed features are then concatenated together to form the feature vector of one frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Counting method</head><p>Now we investigate the vehicle counting method that is used to estimate the number of vehicles. In the previous works, the regression based methods have shown impressive performance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>. Thus we also adopt the regression as the base model. This work is the first attempt to the vehicle counting in compressed domain, while the previous works mainly focus on the crowd counting in pixel domain, e.g. on the UCSD pedestrian dataset <ref type="bibr" target="#b2">[3]</ref> or Mall dataset <ref type="bibr" target="#b5">[6]</ref>, in which the foreground areas vary nearly linear with the number of people. However, such a correlation has not been held yet for vehicle counting, especially in compressed domain.</p><p>To intuitively show the characteristics of traffic scenes, <ref type="figure" target="#fig_3">Figure 4</ref> provides some realistic highway images, and Figure 5 demonstrates an exemplar of the relationship between the vehicle count and foreground area. It can be seen that the correlation is quite complicated compared to the simple linearity. Theoretically, the major factors causing such complication include the broad variation of vehicle appearance, inaccurate information provision by compressed videos, and wide visual field of surveillance cameras.</p><p>To tackle these challenges, we propose a Hierarchical Classification based Regression (HCR) model in this work.</p><p>Here it is considered that the local linearity is approximately held if the vehicle density 1 only varies within a small range. That is, the traffic scenes containing different numbers of vehicles present the density-specific patterns. This assumption practically is reasonable according to the results in <ref type="figure" target="#fig_4">Figure 5</ref>, and exactly turns to be the core idea of HCR.</p><p>The HCR model is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. Specifically, we first apply a multi-class classifier to divide the traffic scenes into K categories representing different ranges of vehicle density (K = 3 is adopted in our experiments with heavy, medium, and light as used in <ref type="bibr" target="#b3">[4]</ref>). However, the estimation error may be rather bigger once the input traffic scene is misclassified. Then we also introduce the secondlayer classifiers to deal with the boundary cases, which together with the first-layer classifiers form a soft-segmented multiple classifiers. Here the (K − 1) binary classifiers are deployed in the second layer, each of which takes charge of one boundary area. When an new traffic scene arrives, it would be separately classified by both layers of classifiers, and two or three confidence scores are obtained. In particular, only the samples with the scores in the second layer more than a given threshold T c = 0.7 are identified to belong to the boundary area. As a result, the input scene is finally assigned to one of the (2K − 1) classes.</p><p>In our implementation, we adopt the SVM classifier with radial basis function (RBF) kernel <ref type="bibr" target="#b4">[5]</ref> to perform the classification, and Gaussian Process Regression (GPR) <ref type="bibr" target="#b23">[24]</ref>, which does not impose any prior assumptions, as the regression model to estimate the number of vehicles for each class. It is worth pointing out that by combining different covariance functions, e.g. linear, rational quadratic, and squared-exponential, GPR has the flexibility to encode different assumptions about the function we wish to learn. In this work, the following covariance function <ref type="bibr" target="#b2">[3]</ref> is employed:</p><formula xml:id="formula_8">K(x i , x j ) = a 0 +a 1 x T i x j +a 2 exp − |x i − x j | 2 2a 3 +δ ij a 4 .<label>(9)</label></formula><p>Here δ ij is a sign function with 1 if i = j and 0 otherwise, and θ = <ref type="figure" target="#fig_1">(a 0 , a 1 , a 2 , a 3 , a 4 )</ref> is the hyper-parameters, which defines the covariance function. The first two terms capture <ref type="bibr" target="#b0">1</ref> It is equivalent to the vehicle count for fixed camera vision. the linear trend, the third term captures local non-linearities, while the last term models the observation noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>There are three types of temporally interleaved frames in H.264 bitstream <ref type="bibr" target="#b29">[30]</ref>: I-frame, P-frame and B-frame. I-frame is absolutely intra-coded, P-frame is motion compensated in the forward direction from I-frame or other Pframe, and B-frame is motion compensated in both forward and backward directions. In our experiments, only P-frames and I-frames are used since the consecutive P-frames can provide continuous motion information. All videos are encoded using the H.264/AVC JM v.18.6 encoder 2 . We use the same frame features extracted in Section 3.3 for both classification and counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>Due to the lack of benchmark database for vehicle counting, we adopt the UCSD highway traffic dataset <ref type="bibr" target="#b3">[4]</ref> for evaluation. This dataset consists of 254 video sequences of daytime highway traffic in Seattle and Washington. Each video contains 42 to 52 frames recorded at 10 frames per second (fps) and the resolution is 320 × 240 pixels. <ref type="figure" target="#fig_3">Figure 4</ref> provides some exemplar frames. Such a dataset is challenging due to containing diverse traffic patterns, e.g. covering the light, medium and heavy congestion with various weather conditions (clear, overcast, and raining).</p><p>The UCSD dataset was originally built for classification. To perform the vehicle counting considered in this work, we constructed the corresponding counting dataset with the same traffic videos. Specifically, we first select a region of interest (ROI) in the traffic scene (see <ref type="figure" target="#fig_2">Figure 3</ref>(b)). Then we extract 8 samples from each video every 5 frames, i.e. the 5th, 10th, 15th, 20th, 25th, 30th, 35th, and 40th frames. Finally, we manually label these frames by marking the central points of vehicle bodies. As a result, a total of 42, 859 vehicles in the 2032 frames are labeled, which cover all representative traffic situations in the USCD dataset.</p><p>As for the classification, we define the dense categories by counting the labeled vehicles. Specifically, the samples containing vehicles less than 20 are categorized as light, the ones between 20 and 40 are as medium, and the rests are as heavy. Besides, we define the boundary area of light and medium as the range of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref>, and that of medium and heavy is [36, 44].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification results</head><p>In the experiment, the samples were randomly split into the training set containing 800 samples and the test set  holding the remaining 1232 samples to balance the different traffic patterns and weather conditions in the training and test set. The feature of any frame for classification is generated by averaging the features of five consecutive neighboring frames in order to improve the robustness. We increase the number of training samples from 200 to 800 with the interval of 100 to investigate its influence to the performance. For each training setting, we repeat the experiment five times, and report the mean classification accuracy. <ref type="figure" target="#fig_5">Figure 6</ref> gives the classification results under different numbers of training samples. It can be seen that the classification accuracy is around 90% and increases as more training samples are used.</p><p>In addition, we compare the proposed method to the pixel-domain baseline methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. For fair comparison, the same training/test samples and settings are used, and the average features on the whole video clips are adopted as video representations. On this dataset, we finally achieve the mean classification accuracy of 94.22%, which is very close to the best performance of 94.50% achieved by <ref type="bibr" target="#b3">[4]</ref> and 95.28% by <ref type="bibr" target="#b7">[8]</ref>. These results show that the features extracted from encoded videos are discriminative and robust enough for classification, i.e. the compressed-domain method is rather competitive to the pixel-domain ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Counting results</head><p>For vehicle counting, we adopt the mean-absolute-error (MAE) to measure the performance, which represents the difference between the predicted counts and the ground truth. Here we conduct multiple experiments with different combinations of features in order to demonstrate the effect of each feature. In addition, we also compare the proposed method against the pixel-domain regression methods in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>. For this type of experiments, two splitting strategies are adopted, and each experiemnt is repeated five times. For the first strategy, the samples are randomly split into the training set containing 800 samples and the test set containing 1232 samples as in the classification experiment. Consequently, the different traffic scenarios and weather conditions are roughly balanced for the training and test sets. The second strategy is to randomly select 80 of 254 videos and use the samples in the selected videos as the training set. By selecting videos rather than samples, we can remove the impact that the training and test samples may from the same video. <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref> report the resulting MAEs for both splitting strategies. It can be seen that the counting performance is consistently improved as more features are im- posed, which demonstrates the effectiveness of all proposed features for vehicle counting. Additionally, our method results in the similar performance to the pixel-domain methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>, although the second splitting strategy involves a slight performance decrease due to the inconsistence of the sample patterns for training and testing. Thus the proposed method is considered to be very competitive.</p><p>We further evaluate the proposed HCR by comparing with the single Gaussian model (GP), one-layer multiregression model (MGR), and the ideal version of HCR that adopts the classification ground truth to replace the predicted ones. <ref type="figure" target="#fig_6">Figure 7</ref> provides an intuitive comparison performance for different methods. Evidently, the multiple regressions always outperform the single regression, and the introduced second-layer classifiers in HCR put the counting performance towards the optimal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present a highway vehicle counting method in compressed domain. Our purpose is to achieve acceptable estimation performance approaching the pixeldomain methods. Specifically, we first developed a batch of low-level features by utilizing the codec metadata of compressed videos. Then we proposed a hierarchical classification based regression model (HCR) to estimate the number of vehicles. Finally, we verified the effectiveness of the proposed method through the experimental evaluation. This work shows that the compressed-domain method may be very applicable for the real-world video surveillance systems, due to the advantages of low complexity, convenient deployment, and competitive performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The Hierarchical Classification based Regression (HCR) model. The traffic scenes are firstly classified into three categories: H (heavy), M (medium), and L (light).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Foreground Segmentation: (a) Original frame, (b) ROI, (c) Foreground mask, and (d) Foreground image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Example frames from the UCSD highway traffic dataset. The sample frames present various vehicle densities: light (top row), medium (middle row) and heavy (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Correspondence between vehicle count and foreground size. The correlation is quite complicated compared to the simple linearity, while the local linearity is approximately held if the vehicle density varies within a small range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Classification accuracies for different training sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Comparison of MAE for different models. Here GP denotes the single GPR model. MGP represents the one-layer multiregression model, and HCR-ideal is the ideal version of HCR that adopts the ground truth as classification results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>proposed to estimate the mean vehicle speed</figDesc><table>H.264 
bitstream 

partition 
modes 

Normalized 
MV field 

MV 
Normalization 
Data parsing 

Foreground 
segmentation 

Feature 
Extraction 
Estimation 

Perspective 
Normalization 

Selected 
frames 
Reconstruction 

vehicle 
counts 

Preprocessing 

Decoder 

HCR 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1. Comparison of approaches and feature sets on UCSD dataset with the first splitting strategy.Table 2. Comparison of approaches and feature sets on UCSD dataset with the second splitting strategy.</figDesc><table>Number of training samples 

200 
300 
400 
500 
600 
700 
800 
size 
3.700 3.411 3.441 3.358 3.145 3.034 2.935 
size + shape 
3.641 3.360 3.338 3.315 3.009 2.909 2.808 
size + shape + HOMV 
3.582 3.324 3.288 3.281 2.899 2.824 2.702 
Ours (with all features) 
3.389 3.214 3.193 3.063 2.813 2.736 2.593 
[3] 
3.324 3.294 3.217 3.074 2.838 2.650 2.653 
[25] 
3.301 3.242 3.196 3.181 3.035 2.586 2.543 

Number of training samples 
200 
300 
400 
500 
600 
700 
800 
size 
3.981 3.747 3.719 3.665 3.577 3.352 3.332 
size + shape 
3.865 3.639 3.576 3.474 3.355 3.179 3.173 
size + shape + HOMV 
3.820 3.618 3.556 3.448 3.310 3.110 3.111 
Ours (with all features) 
3.639 3.483 3.420 3.332 3.142 2.956 2.938 
[3] 
4.140 3.581 3.439 3.515 3.229 3.020 2.970 
[25] 
3.866 3.443 3.432 3.268 3.170 2.923 2.917 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://iphome.hhi.de/suehring/tml/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported partially by the National Natural Science Foundation of China under Grant 61233003 and 61203256, Natural Science Foundation of Anhui Province (1408085MF112), and the Fundamental Research Funds for the Central Universities (WK3500000002 and WK3490000001).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey on compressed domain video analysis techniques. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wadekar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic vehicle counting from video for traffic flow analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Salman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IVS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification and retrieval of traffic video using auto-regressive stochastic processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IVS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Crowd monitoring using image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics &amp; Communication Engineering Journal</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classification of traffic video based on a spatiotemporal orientation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Textural features for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Dinstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<publisher>TSMC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic moving object extraction through a real-world variable-bandwidth network for traffic monitoring systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video object tracking in the compressed domain using spatio-temporal markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khatoonabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Bajić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crowd flow segmentation in compressed domain using crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Counting and classification of highway vehicles by regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tokuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TITS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Crowd counting and profiling: Methodology and evaluation. Modeling, Simulation and Visual Analysis of Crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From semi-supervised to transfer counting of crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Advanced local binary pattern descriptors for crowd estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACIIA</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimation of crowd density using image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lotufo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Colloquium Image Processing for Security Applications</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Traffic congestion estimation using hmm models without vehicle tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IVS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Counting crowded moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An evaluation of crowd counting methods, features and regression models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Moving object detection and tracking using a spatio-temporal graph in h. 264/avc bitstreams for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sabirin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised elastic net for pedestrian counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hierarchical and networked vehicle surveillance in its: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TITS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient level of service classification for traffic monitoring in the compressed video domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pletzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krätschmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Böszörmenyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mariacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Overview of the h. 264/avc video coding standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bjøntegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luthra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semisupervised pedestrian counting with temporal and spatial consistencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kruger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TITS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An algorithm to estimate mean vehicle speed from mpeg skycam video. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Counting vehicles from semantic regions. TITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
