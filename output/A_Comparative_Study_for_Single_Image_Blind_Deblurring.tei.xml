<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparative Study for Single Image Blind Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparative Study for Single Image Blind Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Numerous single image blind deblurring algorithms have been proposed to restore latent sharp images under camera motion. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real blurred images. It is thus unclear how these algorithms would perform on images acquired "in the wild" and how we could gauge the progress in the field. In this paper, we aim to bridge this gap. We present the first comprehensive perceptual study and analysis of single image blind deblurring using real-world blurred images. First, we collect a dataset of real blurred images and a dataset of synthetically blurred images. Using these datasets, we conduct a large-scale user study to quantify the performance of several representative state-of-the-art blind deblurring algorithms. Second, we systematically analyze subject preferences, including the level of agreement, significance tests of score differences, and rationales for preferring one method over another. Third, we study the correlation between human subjective scores and several full-reference and noreference image quality metrics. Our evaluation and analysis indicate the performance gap between synthetically blurred images and real blurred image and sheds light on future research in single image blind deblurring.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent years have witnessed significant progress in single image blind deblurring (or motion deblurring). The progress in this field can be attributed to the advancement of efficient inference algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>, various natural image priors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>, and more general motion blur models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43]</ref>. To quantify and compare the performance of competing algorithms, existing methods either use (1) synthetic datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref> with uniform blurred images generated by convolving a sharp image with a known blur kernel, or (2) a non-uniform blurred benchmark <ref type="bibr" target="#b12">[13]</ref> constructed by recording and playing back camera motion in a lab setting. However, existing datasets do not consider several crucial factors, e.g., scene depth variation, sensor saturation and nonlinear camera response func-Blurred input <ref type="bibr">image</ref> Cho &amp; Lee <ref type="bibr" target="#b1">[2]</ref> Krishnan et al. <ref type="bibr" target="#b14">[15]</ref> Whyte et al. <ref type="bibr" target="#b42">[43]</ref> Sun et al. <ref type="bibr" target="#b37">[38]</ref> Xu et al. <ref type="bibr" target="#b44">[45]</ref> Zhong et al. <ref type="bibr" target="#b48">[49]</ref> Pan et al. <ref type="bibr" target="#b28">[29]</ref> Perrone et al. <ref type="bibr" target="#b30">[31]</ref> Figure 1. A blurred image from the real dataset proposed in this work and a few deblurred results from state-of-the-art algorithms, where there is no clear winner. In this work, we aim to evaluate the performance of single image deblurring algorithms on the realworld blurred images by human perceptual studies.</p><p>tions in a camera pipeline. To understand the true progress of deblurring algorithms, it is important to evaluate the performance "in the wild". While several work has reported impressive results on real blurred images, the lack of a large benchmark dataset and perceptual comparison makes it impossible to evaluate the relative strengths of these algorithms in the literature. There are two main drawbacks in existing evaluation approaches. First, the synthetically generated blurred images often fail to capture the complexity and the characteristics of real motion blur degradation. For example, the camera motion has 6 degrees of freedom (3 translations and 3 rotations) while a convolution model considers only 2D translations parallel to the image plane <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>. Lens distortion, sensor saturation, nonlinear transform functions, noise, and compression in a camera pipeline are also not taken into account in these synthetically generated images. Furthermore, the constant scene depth assumption in a convolution model and the non-uniform blurred benchmark <ref type="bibr" target="#b12">[13]</ref> may not hold in many real scenes where the depth variation cannot be neglected. Evaluations on these datasets do not reflect the performance of single image blind deblurring algorithms on the <ref type="table">Table 1</ref>. Existing and the proposed datasets for performance evaluation of single image blind deblurring algorithms. The proposed dataset consists of both synthetic and real blurred images. Our real dataset contains real blurred images that cover a wide variety of scenarios. Our synthetic dataset includes both uniform and non-uniform blurred images. <ref type="bibr">Dataset</ref> Levin et al. <ref type="bibr" target="#b16">[17]</ref> Sun et al. <ref type="bibr" target="#b37">[38]</ref> Köhler et al. <ref type="bibr" target="#b12">[13]</ref> Ours <ref type="formula">(</ref>  <ref type="bibr" target="#b19">[20]</ref>. The lack of human perceptual studies makes it difficult to compare the performance of deblurring algorithms. While numerous full-reference <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> and no-reference image quality metrics <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref> have been proposed, it is unclear whether these metrics can be applied to measure the quality of deblurred images.</p><p>In this paper, we propose a comparative study of single image blind deblurring algorithms to address these issues. First, we construct two large datasets: (1) real blurred images and (2) synthetic motion-blurred images. We annotate these images with attributes including man-made, natural, people/face, saturated, and text. Second, we conduct a large-scale user study to compare state-of-the-art single image blind deblurring algorithms (see <ref type="figure">Figure 1</ref> for an example of deblurred results from several state-of-the-art algorithms). The statistical analysis of different image attributes helps understand the performance of algorithms in a systematic manner. Third, by comparing our user study results between the real and synthetic datasets, we gain insights that cannot be inferred from the original publications, including image priors, blur models, evaluated datasets and quality metrics (See Section 5).</p><p>The contributions of this work are threefold:</p><p>• Datasets. We construct two large datasets for evaluating image deblurring algorithms: (1) real blurred images and (2) synthetically blurred images. Our real dataset contains real blurred images captured under different scenarios. Our synthetic dataset includes both uniform and non-uniform motion-blurred images. • Human subject evaluation. We evaluate 13 state-ofthe-art single image motion deblurring algorithms, including uniform and non-uniform deblurring, with a large-scale perceptual user study. We show the performance gap when we evaluate these algorithms on synthetic and real images. • Quality metric comparison. We compare the performance of several full-reference and no-reference image quality metrics on both synthetic and real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Datasets and benchmarks. Several datasets have been used to measure and compare the performance of image deblurring algorithms. Levin et al. <ref type="bibr" target="#b16">[17]</ref> construct a dataset with four latent sharp images and eight uniform blur kernels, resulting in total 32 test images. However, the four gray-scale latent images with spatial resolution of 255×255 pixels are not sufficient to cover a wide variety of realworld scenarios. Sun et al. <ref type="bibr" target="#b37">[38]</ref> extend the dataset by using 80 high-resolution natural images of diverse scenes and synthetically blurring each one with the eight blur kernels from <ref type="bibr" target="#b16">[17]</ref>. To construct a non-uniform blur dataset, Köhler et al. <ref type="bibr" target="#b12">[13]</ref> record 6D camera trajectories over time, and play back the camera motion on a robotic platform to capture blurred images. This benchmark contains four latent images and 12 camera trajectories. However, similar to <ref type="bibr" target="#b16">[17]</ref>, the scene is assumed planar and at a fixed distance from the camera. While the PSNR and SSIM metrics are widely used for evaluating the performance of image deblurring algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38]</ref>, these measures do not match human perception very well <ref type="bibr" target="#b19">[20]</ref>. In contrast, our real dataset contains 100 real-world blurred images, and our synthetic dataset has 200 synthetic blurred images with both uniform and non-uniform blur. The latent images in our datasets cover various scenarios and represent the actual challenges and variations in the real world. <ref type="table">Table 1</ref> lists the comparison between the proposed and existing datasets. We describe more details in Section 3.1.</p><p>Human perceptual study. As many commonly used quantitative metrics do not reflect human perceptual preferences, large-scale user studies have been used to evaluate the performance based on visual perception in several computational photography problems. Examples include tone mapping <ref type="bibr" target="#b15">[16]</ref>, image retargeting <ref type="bibr" target="#b31">[32]</ref>, and single image super resolution <ref type="bibr" target="#b45">[46]</ref>. To obtain the ranking of the evaluated algorithms, a straightforward way is to show all deblurred results simultaneously for a subject to rank. However, such an approach is neither feasible nor accurate when there exist a large number of test images and algorithms to be evaluated. Thus, prior work often adopts paired comparison in  <ref type="formula" target="#formula_0">(14)</ref>, people/face <ref type="bibr" target="#b11">(12)</ref>, saturated <ref type="bibr" target="#b27">(28)</ref>, and text <ref type="bibr" target="#b16">(17)</ref>.</p><p>a user study, where subjects are asked to choose a preference between two results (i.e., partial order), instead of giving unreliable scores or rankings for comparing multiple results. We note that Liu et al. <ref type="bibr" target="#b19">[20]</ref> also conduct a user study to obtain subjective perceptual scores for image deblurring. Our work differs from <ref type="bibr" target="#b19">[20]</ref> in three major aspects. First, the goal of <ref type="bibr" target="#b19">[20]</ref> is to learn a no-reference metric from the collected perceptual scores while our objective is the performance comparison of state-of-the-art deblurring algorithms on the real-world images. Second, unlike <ref type="bibr" target="#b19">[20]</ref> where synthetic uniform blurred images are used, our real dataset better reflects the complexity and variations of blurred images in the real world. Third, we evaluate both uniform and non-uniform deblurring algorithms while the focus of <ref type="bibr" target="#b19">[20]</ref> is on algorithms addressing uniform blur only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Datasets</head><p>Real image dataset. In this paper, we aim to evaluate the performance of deblurring algorithms for real blurred images "in the wild". To this end, we construct a set of 100 real blurred images via multiple sources, e.g., representative images from previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, images from Flickr and Google Image Search, or pictures captured by ourselves. All these blurred images are captured in the real-world scenarios from different cameras (e.g., consumer cameras, DSLR, or cellphone cameras), different settings (e.g., exposure time, aperture size, ISO), and different users. We categorize images according to the following five attributes: man-made, natural, people/face, saturated, and text. <ref type="figure" target="#fig_0">Figure 2</ref> shows sample images along with these main attributes. To make these images computationally feasible for most deblurring algorithms, we resize each image such that the maximum dimension is less than 1200 pixels.</p><p>Synthetic dataset. To examine the performance consistency between real and synthetic images, we collect 25 sharp images from the Internet as ground truth and synthesize 100 non-uniform and 100 uniform blurred images. We label these images using the above-mentioned five attributes with each group having at least five images. To synthesize non-uniform blurred images, we record the 6D camera trajectories using a cellphone with inertial sensors (gyroscope and accelerometer), and construct a collection of spatially varying blur kernels by assuming constant depth for the scenes. We obtain the 100 non-uniform blurred images by applying four camera trajectories to 25 latent images and adding 1% Gaussian noise to simulate camera noise. As the sizes of local blur kernels are less than 25 × 25, we set the support size to 25 × 25 when deblurring those nonuniform blurred images. For uniform blur, the eight blur kernels provided by Levin et al. <ref type="bibr" target="#b16">[17]</ref> have been used in several datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>. However, the maximum blur kernel size of these eight blur kernels is 41 × 41, which is relatively small in the real-world cases. We thus use the algorithm in <ref type="bibr" target="#b33">[34]</ref> to synthesize blur kernels by sampling random 6D camera trajectories, generating four uniform blur kernels with size ranging from 51 × 51 to 101 × 101. We then use a convolution model with 1% Gaussian noise to synthesize the 100 uniform blurred images. We present our uniform and non-uniform blur kernels in the supplementary material. We create the synthetic saturated images in a way similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>. Specifically, we first stretch the intensity range of the latent image from [0, 1] to [−0.1, 1.8], and convolve the blur kernels with the images. We then clip the blurred images into the range of [0, 1]. The same process is adopted for generating non-uniform blurred images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluated Algorithms</head><p>We evaluate deblurring algorithms with publicly available source code or binary executable. We evaluate 13 representative state-of-the-art algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref> in our experiments 1 . We include the input blurred images in the evaluation as well. <ref type="table" target="#tab_1">Table 2</ref> shows the list of evaluated algorithms.</p><p>We fix the support size of the blur kernel for each evaluated algorithm when estimating a blur kernel from a test image. For fair comparisons, we use the default parameter values and apply the same non-blind deconvolution method <ref type="bibr" target="#b13">[14]</ref> with the estimated kernels to obtain the final deblurred results. However, the method <ref type="bibr" target="#b13">[14]</ref> fails to handle images with large saturated regions. In <ref type="figure">Figure 3</ref>(a), we show an example where the non-blind deconvolution  <ref type="figure">Figure 3</ref>. Deblurring saturated images with fast non-blind deconvolution <ref type="bibr" target="#b13">[14]</ref> and non-blind deconvolution with saturation handling <ref type="bibr" target="#b41">[42]</ref>.</p><p>method <ref type="bibr" target="#b13">[14]</ref> results in serious ringing artifacts. To address this issue, we adopt the non-blind deconvolution algorithm <ref type="bibr" target="#b41">[42]</ref> that explicitly handles saturated regions when reconstructing the test images with this attribute. <ref type="figure">Figure 3</ref>(b) shows the deblurred result by using <ref type="bibr" target="#b41">[42]</ref> for non-blind deconvolution. We note that the some methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> fail to estimate large blur kernels in about 0.5% of all test images. We exclude these failure cases from our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Human Subject Study</head><p>We conduct our experiments using Amazon Mechanical Turk for large-scale subject studies to evaluate the performance of single image blind deblurring algorithms. We adopt the paired comparison approach that requires each human subject to choose a preferred image from a pair of deblurred images. We design a website that allows each subject to flip between two deblurred images and easily examine the differences. We show a screenshot of the user interface in the supplementary material. There are 14 result images (13 deblurred images and 1 blurred input) for each test image in our datasets. The total number of pair comparisons is <ref type="bibr" target="#b13">14</ref> 2 × 100 = 9100. In a user study session, we ask each subject to compare 50 pairs of images. To remove careless comparisons by the subjects, we introduce sanity check by adding several image pairs where one image is considerably better than the other. For the real dataset, we manually select some pairs of well-deblurred images and images containing severe ringing artifacts or noise. For the synthetic dataset, we use the pairs of ground-truth latent and blurred images. Among the 50 pairs of images for each subject, we select 10 pairs for a sanity check. We discard the voting results by a subject if the subject fails the sanity check more than once. We collect the results of human sub-ject studies from 2,100 users. The average time to complete a survey is 15 minutes. We discard 1.45% of the votes (from subjects who fail to pass the sanity check).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation and Analysis</head><p>In this work, we aim to evaluate the performance of single image blind deblurring algorithms based on human visual perception. In addition to ranking the algorithms, we exploit the correlation between the real and synthetic datasets, as well as the performance of image quality metrics on predicting the quality of deblurred images. First, we analyze the degree of agreement <ref type="bibr" target="#b11">[12]</ref> among subjects to make sure that the votes are not random. Second, we fit paired comparison results to the Bradley-Terry model <ref type="bibr" target="#b0">[1]</ref> to obtain a global score and rank for each algorithm. We analyze the convergence of ranking to show that the number of votes and images in our human subject study are sufficient to draw solid conclusions. We also conduct a significance test <ref type="bibr" target="#b5">[6]</ref> to group the evaluated algorithms based on perceptual quality (that are statistically indistinguishable). Finally, we show the correlation between human perceptual scores and existing image quality metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Coefficient of Agreement</head><p>While we rule out noisy votes with the use of sanity check, the votes may appear random if the participants' preferences are dramatically different. We thus study the similarity of choices (i.e., the degree of agreement) among subjects. We apply the Kendall coefficient of agreement <ref type="bibr" target="#b11">[12]</ref> to quantify the level of agreement with u:</p><formula xml:id="formula_0">u = 2W S 2 M 2 − 1, W = ∑ i = j c i j 2 ,<label>(1)</label></formula><p>where c i j is the number of times that method i is chosen over method j, S denotes the number of subjects, and M represents the number of compared methods. If all subjects make the same choice for each comparison, then the Kendall coefficient of agreement u is equal to 1. In our evaluation, we randomly select pairs of deblurred images and ask the subjects' preference. As a result, we balance the subject votes by uniformly sampling the paired comparisons so that all the evaluated methods are compared at the same frequency. In <ref type="figure" target="#fig_2">Figure 4</ref>, we show the Kendall coefficients of the agreement under different attributes. The  coefficients for both the real and the synthetic non-uniform datasets are all around 0.12. On the other hand, we observe a higher level of agreement for the synthetic uniform dataset (u = 0.26). We note that many algorithms (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48]</ref>) favor no-blur explanations for the test images in the synthetic non-uniform dataset. In such cases, subjects have to choose between two poorly deblurred images, which leads to inconsistent votes. For real blurred images, many factors (e.g., depth variation, nonlinear camera response functions, and unknown camera noise) affect the performance of deblurring algorithms. Existing algorithms often have difficulty in handling real blurred images as they do not take these factors into considerations. In many test images, there is no clear winner. As a result, the degree of agreement in the real dataset is relatively low. We observe that the coefficients of agreement for text images on both the real and the synthetic datasets are higher than other attributes. One possible reason is that human subjects prefer sharp text images (as they are easier to read). The high contrast of the deblurred text images makes the comparisons less ambiguous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Global Ranking</head><p>To compute the global ranking from paired comparisons, we use the Bradley-Terry model (B-T model) <ref type="bibr" target="#b0">[1]</ref>: a probability model that predicts the outcome of the paired comparison. We denote s = [s 1 , s 2 , · · · , s M ] as M scores of the evaluated methods. The B-T model assumes that the probability of choosing method i over method j is:</p><formula xml:id="formula_1">p i j = e s i e s i + e s j .<label>(2)</label></formula><p>Since each pair of result (i, j) is compared by multiple subjects, the likelihood of i over j is defined as p c i j i j , where c i j is the number of times that method i is chosen over method j. The likelihood of all (i, j) pairs is:</p><formula xml:id="formula_2">P = M ∏ i=1 M ∏ j=1 j =i p c i j i j .<label>(3)</label></formula><p>We can estimate the score s i by minimizing the negative log likelihood of <ref type="formula" target="#formula_2">(3)</ref>:</p><formula xml:id="formula_3">L(s) = M ∑ i=1 M ∑ j=1 j =i c i j log(e s i + e s j ) − M ∑ i=1 M ∑ j=1 j =i c i j s i .<label>(4)</label></formula><p>We can easily solve the optimization problem by setting the derivative of (4) to zero. We note that there is an ambiguity in the computed scores s that cannot be determined by solving (4). That is, adding a constant δ to all s i does not change the objective value. Thus, we normalize the B-T scores by shifting the scores to zero mean after solving (4).</p><p>Ranking. We rank the evaluated methods by the average B-T scores, and plot the cumulative frequency of B-T scores in <ref type="figure">Figure 5</ref>. We show the complete ranking results on different attributes in the supplementary material. In <ref type="figure">Figure 5</ref>, the method with the rightmost curve has a better overall performance because it has more images with higher B-T scores. In addition to the performance evaluation, we analyze the correlation of B-T scores between each pair of datasets as shown in <ref type="figure">Figure 6</ref>. The performance of those methods located in the lower-right corner of the figure is over-estimated on the dataset shown on the x-axis. On the other hand, the performance of those methods located in the upper-left corner is under-estimated. Take <ref type="figure">Figure 6</ref>(a) as an example. The methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> outperform all the other algorithms on the synthetic uniform dataset. However, their performances on the real dataset are not as competitive, e.g., the results from <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b14">15]</ref> achieves higher scores than that of <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. In this case, performance of <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> is over-estimated and the performance of <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b14">15]</ref> is underestimated on the synthetic uniform dataset. Therefore, the evaluation results based on synthetic uniform images do not well reflect the performance on the real-world images. In <ref type="figure">Figure 6</ref>(b), we observe that the performance of deblurring algorithms are similar on the real and the synthetic nonuniform datasets.</p><p>Convergence analysis. We analyze the convergence of global ranking on (1) the number of subject votes and <ref type="formula" target="#formula_1">(2)</ref> the number of test images to ensure that the number of votes and images are sufficiently large for performance evaluation. We randomly sample k = 500, 1000, 2000, 3000, 4000 votes from a total of 23,478 voting results in the real dataset and compute the B-T scores for each evaluated algorithm. We repeat this process 1000 times with different sample of votes. <ref type="figure">Figure 7(a)</ref> shows the mean and standard deviation for each k. At the beginning (k = 500), the error bars of these methods overlap with each other. When k ≥ 2000, the standard deviations become sufficiently small, and the B-T scores of each method converge. We conduct a similar analysis regarding the number of images. We randomly sample k (k = 5, 10, 20, 30, 40) images out of a total of 100 images from our real dataset,  <ref type="figure">Figure 5</ref>. Cumulative frequency of B-T scores for each dataset. We normalize the frequency such that the maximum frequency is equal to 1. The method whose curve locates at right has more images with higher scores, which stands for better overall performance. Only the top 7 methods are presented for clarity. The complete plots of all evaluated methods are illustrated in the supplementary material.  and then plot the means and the standard deviations in <ref type="figure">Figure 7(b)</ref>. The error-bars become sufficiently small to separate each evaluated method after using 30 images. These two experiments demonstrate that the number of votes and the number of test images are indeed sufficient to obtain stable scores or ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Significance Test</head><p>An algorithm having a higher B-T score does not mean that it always outperforms others. To investigate whether the results of the evaluated methods are statistically distinguishable, we conduct the significance test <ref type="bibr" target="#b5">[6]</ref> for each dataset. Specifically, we group the evaluated methods if the difference of obtained votes between any two methods within a group is less than a threshold. We denote the difference of the number of votes within a group of methods as R. We aim to find a threshold R ′ such that the probability P[R ≥ R ′ ] ≤ α, where α is the significance level (α = 0.01 in this work). Since the distribution of R is asymptotically equivalent to the distribution of the variance-normalized range W t <ref type="bibr" target="#b6">[7]</ref>, we can use the following <ref type="table" target="#tab_1">Blur   Fergus-06  Cho-09  Xu-10  Krishnan-11  Levin-11  Whyte-12   Sun-13  Xu-13  Zhang-13  Zhong-13  Michaeli-14  Pan-14  Perrone-14   0  2  4  6  8  10  12</ref>  relationship to approximate P[R ≥ R ′ ]:</p><formula xml:id="formula_4">P[W M ≥ W M,α ] ≤ α where W M,α = 2R ′ − 0.5 √ MS .<label>(5)</label></formula><p>In W M,α , M is the number of evaluated methods and S is the number of subjects. The value of W M,α can be obtained from the table in <ref type="bibr" target="#b29">[30]</ref>, or one can draw M samples from a Gaussian distribution with variance 1 and then compute the upper (1 − α) × 100 percentage points as W M,α . Once we determine the value of W M,α , we can solve the value of R ′ using:</p><formula xml:id="formula_5">R ′ = 0.5W M,α √ MS + 0.25.<label>(6)</label></formula><p>A group is formed if and only if the score difference of any pair within a group is less or equal to R ′ . Otherwise, these two methods belong to different groups. <ref type="table" target="#tab_4">Table 3</ref> lists the total number of votes, the number of comparisons for each pair (i, j), and the value of R ′ used in each dataset. We show the ranking and the grouping results in <ref type="figure" target="#fig_5">Figure 8</ref>. The top three groups are different in the real and synthetic datasets. This suggests that the performance of a deblurring algorithm on synthetic images does not correlate well with the performance on real-world blurred images. In <ref type="figure" target="#fig_6">Figure 9</ref>, we also show the percentage of obtained votes. For avoiding clutter, we only show the results on images with the saturated and text attributes. We refer the readers to the supplementary material for the complete results for all attributes.</p><p>From the experimental results, we have a few interesting observations:</p><p>1. Krishnan-11 <ref type="bibr" target="#b14">[15]</ref> performs worse in the synthetic dataset but better than most of algorithms in the real dataset. We attribute this observation to the fact that the deblurred results of Krishnan-11 usually contain  <ref type="figure">Figure 10</ref>. Human subjects often favor slightly blurry results with less ringing artifacts or over-sharpened edges (e.g., <ref type="bibr" target="#b14">[15]</ref>). fewer artifacts, e.g., serious ringings or over-sharpened edges. <ref type="figure">Figure 1</ref> and <ref type="figure">Figure 10</ref> show the deblurred results of Krishnan-11 compared to recent work. 2. Since most uniform deblurring algorithms do not handle non-uniform blurred images well, the results from many of the deblurring algorithms are statistically indistinguishable. However, some uniform deblurring algorithms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref> are more robust than the non-uniform deblurring algorithm <ref type="bibr" target="#b42">[43]</ref> in the synthetic non-uniform dataset and real dataset. Our empirical observations agree with the findings in <ref type="bibr" target="#b12">[13]</ref>. 3. Although  is designed to deblur text images, it performs well on real blurred images, particularly for saturated ones. The L 0 intensity prior used in Pan-14 favors images which have more pixels with zero intensity, and thus can reduce light streaks and saturated regions. In addition, Xu-10 <ref type="bibr" target="#b43">[44]</ref> can deblur images with large motion blur in the presence of noise. The refinement phase in Xu-10 helps reduce noise in blur kernels and leads to robust deblurring results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis on Image Quality Metrics</head><p>In this section, we analyze the correlation between human subject scores and several image quality metrics. For full-reference metrics, we choose several widely used metrics, PSNR and SSIM <ref type="bibr" target="#b39">[40]</ref>, as well as WSNR <ref type="bibr" target="#b21">[22]</ref>, MS-SSIM <ref type="bibr" target="#b40">[41]</ref>, IFC <ref type="bibr" target="#b36">[37]</ref>, NQM <ref type="bibr" target="#b3">[4]</ref>, UIQI <ref type="bibr" target="#b38">[39]</ref>, VIF <ref type="bibr" target="#b35">[36]</ref>. For no-reference metrics, we choose seven state-of-the-art noreference metrics including BIQI <ref type="bibr" target="#b25">[26]</ref>, BLIINDS2 <ref type="bibr" target="#b32">[33]</ref>, BRISQUE <ref type="bibr" target="#b22">[23]</ref>, CORNIA <ref type="bibr" target="#b46">[47]</ref>, DIIVINE <ref type="bibr" target="#b26">[27]</ref>, NIQE <ref type="bibr" target="#b23">[24]</ref>, SSEQ <ref type="bibr" target="#b18">[19]</ref> 2 and the no-reference metric for motion deblurring <ref type="bibr" target="#b19">[20]</ref>. In total, there are eight full-reference metrics and eight no-reference metrics in our experiment. We com-  <ref type="bibr" target="#b46">[47]</ref> 0.0967 0.2630 0.0765 DIIVINE <ref type="bibr" target="#b26">[27]</ref> 0.0284 -0.0805 -0.0017 NIQE <ref type="bibr" target="#b23">[24]</ref> 0.0776 0.0110 -0.0308 SSEQ <ref type="bibr" target="#b18">[19]</ref> -0.0120 -0.0331 0.0212 Liu et al. <ref type="bibr" target="#b19">[20]</ref> 0.1667 0.4991 0.2928 pute the Spearman's rank correlation coefficient (denoted by ρ) <ref type="bibr" target="#b24">[25]</ref>, which measures the level of association between a pair of ranked variables. Note that Spearman's rank correlation coefficient is not affected by the range of variables. This is particularly suitable for our study as different metrics may have different ranges. <ref type="table" target="#tab_5">Table 4</ref> shows the value of ρ for all evaluated image quality metrics on both real and synthetic datasets. For fullreference metrics (the top eight rows in <ref type="table" target="#tab_5">Table 4</ref>), IFC and VIF have higher ρ values than PSNR and SSIM. We note that the IFC metric uses wavelet features with a focus on high-frequency details, and VIF puts more weight on image edges when extracting features. Thus, these two metrics have higher correlation on the synthetic dataset. However, the overall correlation of all full-reference metrics on the non-uniform dataset is lower than that on the uniform dataset. This can be attributed to the fact that most evaluated algorithms assume the uniform blur model, which is not effective for handling non-uniform blurred images. For noreference metrics (bottom eight of <ref type="table" target="#tab_5">Table 4</ref>), most of them have lower or even negative correlation to human subjective scores. We note that the metric for motion deblurring <ref type="bibr" target="#b19">[20]</ref> has high correlation on the synthetic dataset because this metric is trained specifically to evaluate the image quality of deblurred results. However, the correlation becomes significantly lower in the real dataset. As the metric <ref type="bibr" target="#b19">[20]</ref> is learned from a synthetic uniform dataset, it does not work well on real images which contain non-uniform blur.</p><p>In addition to these image quality metrics, we also compute the correlation between human subject scores and the error ratio <ref type="bibr" target="#b16">[17]</ref>, which is a commonly used metric for evaluating the quality of deblurred images. The Spearmans rank correlation coefficient of the error ratio is 0.5941, which is lower than IFC (0.6773) but better than PSNR (0.4135) and SSIM (0.5162). The result suggests that the error ratio is a fair metric to evaluate the quality of deblurred images. However, the error ratio can only be applied to synthetic uniform blurred images with known ground truth blur kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions and Conclusion</head><p>In this paper, we carry out large-scale experiments to evaluate the performance of state-of-the-art single image motion deblurring algorithms on both real and synthetic blurred datasets. From our evaluation and analysis, we present our observations and suggestions for future research with respect to the following issues. Image priors: Sparse gradient priors <ref type="bibr" target="#b14">[15]</ref> and intensity <ref type="bibr" target="#b28">[29]</ref> are more reliable and effective than explicit edge-based methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> for real images. We attribute this to the heuristic edge selection steps, in which the thresholding parameters are sensitive to image contents and less robust to noise in real images. We observe that human subjects tend to prefer slightly blurry results to sharper results but with noticeable artifacts. Blur models: Existing deblurring algorithms are less effective in dealing with saturated regions, non-Gaussian noise, and complicated scenes with large depth variation. We advocate putting more research attention on better model design to handle complex and non-uniform motion blur as well as image noise caused by outliers. Datasets: From the study, we found that existing methods already perform well on synthetic images but not on real images. Similar to other fields in computer vision (e.g., object detection and recognition), the cumulative advances make this community ready to focus on development and evaluation on real images. Quality metrics: IFC and VIF perform better than PSNR and SSIM when evaluating the quality of deblurred images on the synthetic dataset. For no-reference quality metrics, the motion deblurring metric <ref type="bibr" target="#b19">[20]</ref> has a high correlation with human perceptual scores on the synthetic dataset, but less so on the real dataset.</p><p>Our datasets, subject votes and code for the statistical analysis in this paper are available on our project website. We encourage community to develop robust algorithms to account for practical scenarios and design image quality metrics to measure the performance of deblurring algorithms on real-world blurred images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Sample images with the annotated attributes in our real dataset. The numbers of images belonging to each attribute in our real dataset are: man-made (66), natural</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Kendall coefficient of agreement under different attributes in our human subject study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Correlation of B-T scores between a pair of datasets. The performance of those methods located in the lower-right is overestimated on the dataset shown on the x-axis, while the performance of the methods located in the upper-left is under-estimated. Convergence analysis on B-T scores with respect to the number of votes and the number of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Grouping of algorithms by the significance test. Algorithms within the same circle have statistically indistinguishable scores, i.e., the performance is similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Percentage of obtained votes (y-axis) per attribute on each dataset. We show the full results of all five attributes in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>List of evaluated algorithms. We present a complete table with image priors, blur kernel priors and the execution time of all evaluated algorithms in the supplementary material.</figDesc><table>Algorithm 
Blur Model 
Algorithm 
Blur Model 

Fergus-06 [5] 
Uniform 
Xu-13 [45] 
Uniform 
Cho-09 [2] 
Uniform 
Zhang-13 [48] 
Uniform 
Xu-10 [44] 
Uniform 
Zhong-13 [49] 
Uniform 
Krishnan-11 [15] 
Uniform 
Michaeli-14 [21] 
Uniform 
Levin-11 [18] 
Uniform 
Pan-14 [29] 
Uniform 
Whyte-12 [43] 
Non-uniform 
Perrone-14 [31] 
Uniform 
Sun-13 [38] 
Uniform 

(a) kernel [29] + nonblind [14] 
(b) kernel [29] + nonblind [42] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Total number of votes, number of comparisons for each 
pair (i, j), and value of threshold R ′ used in the significance test, 
where the significance level α = 0.01. 

Real Dataset 
Synthetic Dataset 
Uniform 
Non-uniform 

votes 
23478 
23478 
22750 
comparisons 
258 
258 
250 
threshold R ′ 
163 
163 
156 

(a) Krishnan et al. [15] 
(b) Xu et al. [45] 
(c) Perrone et al. [31] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Spearman's rank correlation coefficient<ref type="bibr" target="#b24">[25]</ref> of fullreference metrics (top 8) and no-reference metrics (bottom 8).</figDesc><table>Real Dataset 
Synthetic Dataset 
Uniform 
Non-uniform 

PSNR 
-
0.4135 
0.0819 
WSNR [22] 
-
0.4669 
0.1662 
SSIM [40] 
-
0.5162 
0.1511 
MS-SSIM [41] 
-
0.6385 
0.2204 
IFC [37] 
-
0.6773 
0.3132 
NQM [4] 
-
0.5394 
0.1422 
UIQI [39] 
-
0.6282 
0.2127 
VIF [36] 
-
0.5779 
0.3366 
BIQI [26] 
0.0622 
-0.0528 
-0.1218 
BLIINDS2 [33] 
-0.0614 
-0.0461 
-0.1078 
BRISQUE [23] 
-0.0556 
-0.0857 
-0.1316 
CORNIA </table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While the method<ref type="bibr" target="#b47">[48]</ref> is designed for multi-image blind deconvolution, it can be applied to single images as well. As the non-uniform method<ref type="bibr" target="#b44">[45]</ref> on the project website does not work stably for large blur kernels, we use the uniform deblurring code of<ref type="bibr" target="#b44">[45]</ref> requested from the author.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the score used in BIQI, BLIINDS2, BRISQUE, CORNIA, DIIVINE, NIQE and SSEQ range from 0 (best) to 100 (worst), and we reverse the score to make the correlation consistent to other quality metrics.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported in part by the NSF CAREER Grant #1149783, NSF IIS Grant #1152576, a gift from Adobe, Office of Naval Research N00014-12-1-0259, and Air Force Office of Scientific Research AF FA8750-13-2-0008.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rank analysis of incomplete block designs the method of paired comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno>145:1-145:8</idno>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIG-GRAPH Asia)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Handling outliers in non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image quality assessment based on a degradation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Damera-Venkata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="636" to="650" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Statistical methods for research workers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1925" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Review: H. A. David, The method of paired comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1386" to="1387" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Single image deblurring using motion density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<idno>ECCV. 2010. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast removal of non-uniform camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deblurring low-light images with light streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint depth estimation and camera shake removal from single blurry image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the method of paired comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="324" to="345" />
			<date type="published" when="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast image deconvolution using hyperlaplacian priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blind deconvolution using a normalized sparsity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation of tone mapping operators using a high dynamic range display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seetzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="640" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding and evaluating blind deconvolution algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient marginal likelihood optimization in blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment based on spatial and spectral entropies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="856" to="863" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A no-reference metric for evaluating the quality of motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">175</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blind deblurring using internal patch recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluation of contrast sensitivity functions for the formulation of quality measures incorporated in halftoning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Varkur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making a completely blind image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introduction to the practice of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Mccabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A two-step framework for constructing blind image quality indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="516" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deblurring face images with exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno>ECCV. 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deblurring text images via l0-regularized intensity and gradient prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Biometrika tables for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Hartley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Total variation blind deconvolution: The devil is in the details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comparative study of image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<idno>160:1-160:10</idno>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the dct domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discriminative non-blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. De</forename><surname>Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2128" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Edge-based blur kernel estimation using patch priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A universal image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="81" to="84" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preceedings of Conference on Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deblurring shaken and partially saturated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="201" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="186" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unnatural L 0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single-image super-resolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-image blind deblurring using a coupled adaptive sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Handling noise in single image deblurring using directional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
