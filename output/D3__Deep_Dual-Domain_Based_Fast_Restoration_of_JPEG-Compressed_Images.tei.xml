<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D 3 : Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
							<email>dingliu2@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<email>chang87@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ling</surname></persName>
							<email>qingling@mail.ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">†Beckman Institute</orgName>
								<orgName type="department" key="dep2">‡Department of Automation</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">D 3 : Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we design a Deep Dual-Domain (D 3 ) based fast restoration model to remove artifacts of JPEG compressed images. It leverages the large learning capacity of deep networks, as well as the problem-specific expertise that was hardly incorporated in the past design of deep architectures. For the latter, we take into consideration both the prior knowledge of the JPEG compression scheme, and the successful practice of the sparsity-based dual-domain approach. We further design the One-Step Sparse Inference (1-SI) module, as an efficient and lightweighted feed-forward approximation of sparse coding. Extensive experiments verify the superiority of the proposed D 3 model over several state-of-the-art methods. Specifically, our best model is capable of outperforming the latest deep model for around 1 dB in PSNR, and is 30 times faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In visual communication and computing systems, the most common cause of image degradation is arguably compression. Lossy compression, such as JPEG <ref type="bibr" target="#b23">[25]</ref> and HEVC-MSP <ref type="bibr" target="#b2">[4]</ref>, is widely adopted in image and video codecs for saving both bandwidth and in-device storage. It exploits inexact approximations for representing the encoded content compactly. Inevitably, it will introduce undesired complex artifacts, such as blockiness, ringing effects, and blurs. They are usually caused by the discontinuities arising from batch-wise processing, the loss of highfrequency components by coarse quantization, and so on. These artifacts not only degrade perceptual visual quality, but also adversely affect various low-level image processing routines that take compressed images as input <ref type="bibr" target="#b9">[11]</ref>.</p><p>As practical image compression methods are not information theoretically optimal <ref type="bibr" target="#b22">[24]</ref>, the resulting compression code streams still possess residual redundancies, which makes the restoration of the original signals possible. Different from general image restoration problems, compression artifact restoration has problem-specific properties that can be utilized as powerful priors. For example, JPEG compression first divides an image into 8 × 8 pixel blocks, followed by discrete cosine transformation (DCT) on every block. Quantization is applied on the DCT coefficients of every block, with pre-known quantization levels <ref type="bibr" target="#b23">[25]</ref>. Moreover, the compression noises are more difficult to model than other common noise types. In contrast to the tradition of assuming noise to be white and signal independent <ref type="bibr" target="#b0">[2]</ref>, the non-linearity of quantization operations makes quantization noises non-stationary and signal-dependent.</p><p>Various approaches have been proposed to suppress compression artifacts. Early works <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b20">22]</ref> utilized filteringbased methods to remove simple artifacts. Data-driven methods were then considered to avoid inaccurate empirical modeling of compression degradations. Sparsitybased image restoration approaches have been discussed in <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b24">26]</ref> to produce sharpened images, but they are often accompanied with artifacts along edges, and unnatural smooth regions. In <ref type="bibr" target="#b22">[24]</ref>, Liu et.al. proposed a sparse coding process carried out jointly in the DCT and pixel domains, to simultaneously exploit residual redundancies of JPEG code streams and sparsity properties of latent images. More recently, Dong et. al. <ref type="bibr" target="#b9">[11]</ref> first introduced deep learning techniques <ref type="bibr" target="#b19">[21]</ref> into this problem, by specifically adapting their SR-CNN model in <ref type="bibr" target="#b10">[12]</ref>. However, it does not incorporate much problem-specific prior knowledge.</p><p>The time constraint is often stringent in image or video codec post-processing scenarios. Low-complexity or even real-time attenuation of compression artifacts is highly desirable <ref type="bibr" target="#b26">[28]</ref>. The inference process of traditional approaches, for example, sparse coding, usually involves iterative optimization algorithms, whose inherently sequential structure as well as the data-dependent complexity and latency often constitute a major bottleneck in the computational efficiency <ref type="bibr" target="#b12">[14]</ref>. Deep networks benefit from the feed-forward structure and enjoy much faster inference. However, to maintain their competitive performances, deep networks show demands for increased width (numbers of filters) and depth (number of layers), as well as smaller strides, all leading to growing computational costs <ref type="bibr" target="#b14">[16]</ref>.</p><p>In the paper, we focus on removing artifacts in JPEG compressed images. Our major innovation is to explicitly combine both the prior knowledge in the JPEG compression scheme and the successful practice of dual-domain sparse coding <ref type="bibr" target="#b22">[24]</ref>, for designing a task-specific deep architecture. Furthermore, we introduce a One-Step Sparse Inference (1-SI) module, that acts as a highly efficient and light-weighted approximation of the sparse coding inference <ref type="bibr" target="#b8">[10]</ref>. 1-SI also reveals important inner connections between sparse coding and deep learning. The proposed model, named Deep Dual-Domain (D 3 ) based fast restoration, proves to be more effective and interpretable than general deep models. It gains remarkable margins over several state-of-the-art methods, in terms of both restoration performance and time efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is inspired by the prior wisdom in <ref type="bibr" target="#b22">[24]</ref>. Most previous works restored compressed images in either the pixel domain <ref type="bibr" target="#b0">[2]</ref> or the DCT domain <ref type="bibr" target="#b23">[25]</ref> solely. However, an isolated quantization error of one single DCT coefficient is propagated to all pixels of the same block. An aggressively quantized DCT coefficient can further produce structured errors in the pixel-domain that correlate to the latent signal. On the other hand, the compression process sets most high frequency coefficients to zero, making it impossible to recover details from only the DCT domain. In view of their complementary characteristics, the dualdomain model was proposed in <ref type="bibr" target="#b22">[24]</ref>. While the spatial redundancies in the pixel domain were exploited by a learned dictionary <ref type="bibr" target="#b0">[2]</ref>, the residual redundancies in the DCT domain were also utilized to directly restore DCT coefficients. In this way, quantization noises were suppressed without propagating errors. The final objective (see Section 3.1) is a combination of DCT-and pixel-domain sparse representations, which could cross validate each other.</p><p>To date, deep learning <ref type="bibr" target="#b19">[21]</ref> has shown impressive results on both high-level and low-level vision problems <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b34">36]</ref>. The SR-CNN proposed by Dong et al. <ref type="bibr" target="#b10">[12]</ref> showed the great potential of end-to-end trained networks in image super resolution (SR). Their recent work <ref type="bibr" target="#b9">[11]</ref> proposed a four-layer convolutional network that was tuned based on SR-CNN, named Artifacts Reduction Convolutional Neural Networks (AR-CNN), which was effective in dealing with various compression artifacts.</p><p>In <ref type="bibr" target="#b12">[14]</ref>, the authors leveraged fast trainable regressors and constructed feed-forward network approximations of the learned sparse models. By turning sparse coding into deep networks, one may expect faster inference, larger learning capacity, and better scalability. Similar views were adopted in <ref type="bibr" target="#b27">[29]</ref> to develop a fixed-complexity algorithm for solving structured sparse and robust low rank models. The paper <ref type="bibr" target="#b15">[17]</ref> summarized the methodology of "deep unfolding". <ref type="bibr" target="#b33">[35]</ref> proposed deeply improved sparse coding for SR, which can be incarnated as an end-to-end neural network. Lately, <ref type="bibr" target="#b32">[34]</ref> proposed Deep ℓ 0 Encoders, to model ℓ 0 sparse approximation as feed-forward neural networks. <ref type="bibr" target="#b31">[33]</ref> further extended the same "task-specific" strategy to graphregularized ℓ 1 approximation. Our task-specific architecture shares similar spirits with these works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Dual-Domain (D 3 ) based Restoration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sparsity-based Dual-Domain Formulation</head><p>We first review the sparsity-based dual-domain restoration model established in <ref type="bibr" target="#b22">[24]</ref>. Considering a training set of uncompressed images, pixel-domain blocks {x i } ∈ R m (vectorized from a √ m × √ m patch; m = 64 for JPEG) are drawn for training, along with their quantized DCT coefficient blocks {y i }∈ R m . For each (JPEG-coded) input x t ∈ R m , two dictionaries Φ ∈ R m×pΦ and Ψ ∈ R m×pΨ (p Φ and p Ψ denote the dictionary sizes) are constructed from training data {y i } and {x i }, in the DCT and pixel domains, respectively, via locally adaptive feature selection and projection. The following optimization model is then solved during the testing stage:</p><formula xml:id="formula_0">min {α,β} ||y t − Φα|| 2 2 + λ 1 ||α|| 1 +λ 2 ||T −1 Φα − Ψβ|| 2 2 + λ 3 ||β|| 1 , s.t. q L Φα q U .</formula><p>(1)</p><p>where y t ∈ R m is the DCT coefficient block for x t . α ∈ R pΦ and β ∈ R pΨ are sparse codes in the DCT and pixel domains, respectively. T −1 denotes the inverse discrete cosine transform (IDCT) operator. λ 1 , λ 2 and λ 3 are positive scalars. One noteworthy point is the inequality constraint, where q L and q U represents the (pre-known) quantization intervals according to the JPEG quantization table <ref type="bibr" target="#b23">[25]</ref>. The constraint incorporates the important side information and further confines the solution space. Finally, Ψβ provides an estimate of the original uncompressed pixel blockx t .</p><p>Such a sparsity-based dual-domain model (1) exploits residual redundancies (e,g, inter-DCT-block correlations) in the DCT domain without spreading errors into the pixel domain, and at the same time recovers high-frequency information driven by a large training set. However, note that the inference process of (1) relies on iterative algorithms, and is computational expensive. Also in (1), the three parameters λ 1 , λ 2 and λ 3 have to be manually tuned. The authors of <ref type="bibr" target="#b22">[24]</ref> simply set them all equal, which may hamper the performance. In addition, the dictionaries Φ and Ψ have to be individually learned for each patch, which allows for extra flexibility but also brings in heavy computation load. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">D 3 : A Feed-Forward Network Formulation</head><p>In training, we have the compressed pixel-domain blocks {x i }, accompanied with the original uncompressed blocks {x i }. During testing, for an compressed input x t , our goal is to estimate the originalx t , using the redundancies in both DCT and pixel domains, as well as JPEG prior knowledge.</p><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the input x t is first transformed into its DCT coefficient block y t , by feeding through the constant 2-D DCT matrix layer T . The subsequent two layers aim to enforce DCT domain sparsity, where we refer to the concepts of analysis and synthesis dictionaries in sparse coding <ref type="bibr" target="#b13">[15]</ref>. The Sparse Coding (SC) Analysis Module 1 is implemented to solve the following type of sparse inference problem in the DCT domain (λ is a positive coefficient):</p><formula xml:id="formula_1">min α 1 2 ||y t − Φα|| 2 2 + λ||α|| 1 .<label>(2)</label></formula><p>The Sparse Coding (SC) Synthesis Module 1 outputs the DCT-domain sparsity-based reconstruction in (1), i.e., Φα.</p><p>The intermediate output Φα is further constrained by an auxiliary loss, which encodes the inequality constraint in (1): q L Φα q U . We design the following signaldependent, box-constrained <ref type="bibr" target="#b18">[20]</ref> loss:</p><formula xml:id="formula_2">L B (Φα, x) = ||[Φα − q U (x)] + || 2 2 + ||[q L (x) − Φα] + || 2 2 .<label>(3)</label></formula><p>Note it takes not only Φα, but also x as inputs, since the actual JPEG quantization interval [q L , q U ] depends on x. The operator [ ] + keeps the nonnegative elements unchanged while setting others to zero. Eqn. (3) will thus only penalize the coefficients falling out of the quantization interval.</p><p>After the constant IDCT matrix layer T −1 , the DCTdomain reconstruction Φα is transformed back to the pixel domain for one more sparse representation. The SC Analysis Module 2 solves (γ is a positive coefficient):</p><formula xml:id="formula_3">min β 1 2 ||T −1 Φα − Ψβ|| 2 2 + γ||β|| 1 ,<label>(4)</label></formula><p>while the SC Synthesis Module 2 produces the final pixeldomain reconstruction Ψβ. Finally, the L 2 loss between Ψβ andx i is enforced.</p><p>Note that in the above, we try to correspond the intermediate outputs of D 3 with the variables in (1), in order to help understand the close analytical relationship between the proposed deep architecture with the sparse coding-based model. That does not necessarily imply any exact numerical equivalence, since D 3 allows for end-to-end learning of all parameters (including λ in <ref type="formula" target="#formula_1">(2)</ref> and γ in <ref type="formula" target="#formula_3">(4)</ref>). However, we will see in experiments that such enforcement of the specific problem structure improves the network performance and efficiency remarkably. In addition, the above relationships remind us that the deep model could be well initialized from the sparse coding components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">One-Step Sparse Inference Module</head><p>The implementation of SC Analysis and Synthesis Modules appears to be the core of D 3 . While the synthesis process is naturally feed-forward by multiplying the dictionary, it is less straightforward to transform the sparse analysis (or inference) process into a feed-forward network.</p><p>We take (2) as an example, while the same solution applies to <ref type="bibr" target="#b2">(4)</ref>. Such a sparse inference problem could be solved by the iterative shrinkage and thresholding algorithm (ISTA) <ref type="bibr" target="#b3">[5]</ref>, each iteration of which updates as follows:</p><formula xml:id="formula_4">α k+1 = s λ (α k + Φ T (y t − Φα k )),<label>(5)</label></formula><p>where α k denotes the intermediate result of the k-th iteration, and where s λ is an element-wise shrinkage function (u is a vector and u i is its i-th element, i = 1, 2, ..., p):</p><formula xml:id="formula_5">[s λ (u)] i = sign(u i )[|u i | − λ i ] + .<label>(6)</label></formula><p>The learned ISTA (LISTA) <ref type="bibr" target="#b12">[14]</ref> parameterized encoder further proposed a natural network implementation of ISTA. The authors time-unfolded and truncated (5) into a fixed number of stages (more than 2), and then jointly tuned all parameters with training data, for a good feed-forward approximation of sparse inference. The similar unfolding methodology has been lately exploited in <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b28">[30]</ref>. In our work, we launch a more aggressive approximation, by only keeping one iteration of (5), leading to a One-</p><p>Step Sparse Inference (1-SI) Module. Our major motivation lies in the same observation as in <ref type="bibr" target="#b9">[11]</ref> that overly deep networks could adversely affect the performance in low-level vision tasks. Note that we have two SC Analysis modules where the original LISTA applies, and two more SC Synthesis modules (each with one learnable layer). Even only two iterations are kept as in <ref type="bibr" target="#b12">[14]</ref>, we end up with a six-layer network, that suffers from both difficulties in training <ref type="bibr" target="#b9">[11]</ref> and fragility in generalization <ref type="bibr" target="#b29">[31]</ref> for this task.</p><p>A 1-SI module takes the following simplest form:</p><formula xml:id="formula_6">α = s λ (Φy t ),<label>(7)</label></formula><p>which could be viewed as first passing through a fullyconnected layer (Φ), followed by neurons that take the form of s λ . We further rewrite (6) as <ref type="bibr" target="#b33">[35]</ref> did 1 :</p><formula xml:id="formula_7">[s λ (u)] i = λ i · sign(u i )(|u i |/λ i − 1) + = λ i s 1 (u i /λ i )<label>(8)</label></formula><p>Eqn. <ref type="bibr" target="#b6">(8)</ref> indicates that the original neuron with trainable thresholds can be decomposed into two linear scaling layers plus a unit-threshold neuron. The weights of the two scaling layers are diagonal matrices defined by θ and its elementwise reciprocal, respectively. The unit-threshold neuron s 1 could in essence be viewed as a double-sided and translated variant of ReLU <ref type="bibr" target="#b19">[21]</ref>. <ref type="figure">Figure 3</ref>. The illustration of SC Analysis and Synthesis Modules. The former is implemented by the proposed 1-SI module <ref type="bibr" target="#b5">(7)</ref>. Both DA and DS are fully-connected layers, while diag(θ) and diag(1/θ) denotes the two diagonal scaling layers.</p><formula xml:id="formula_8">D A diag(1/ɵ) diag(ɵ) D S s 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SC Analysis Module (1-SI) SC Synthesis Module</head><p>A related form to <ref type="bibr" target="#b5">(7)</ref> was obtained in [10] on a different case of non-negative sparse coding. The authors studied its connections with the soft-threshold feature for classification, but did not correlate it with network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Overview</head><p>By plugging in the 1-SI module <ref type="formula" target="#formula_6">(7)</ref>, we are ready to obtain the SC Analysis and Synthesis Modules, as in <ref type="figure">Fig.  3</ref>. By comparing <ref type="figure">Fig. 3</ref> with Eqn. (2) (or (4)), it is easy to notice the analytical relationships between D A and Φ T (or Ψ T ), D S and Φ (or Ψ), as well as θ and λ (or γ). In fact, those network hyperparamters could be well initialized from the sparse coding parameters, which could be obtained easily. The entire D 3 model, consisting of four learnable fully-connected weight layers (except for the diagonal layers), are then trained from end to end 2 . In <ref type="figure">Fig. 3</ref>, we intentionally do not combine θ into D A layer (also 1/θ into D S layer ), for the reason that we still wish to keep θ and 1/θ layers tied as element-wise reciprocal. That proves to have positive implications in our experiments. If we absorb the two diagonal layers into D A and D S , <ref type="figure">Fig. 3</ref> is reduced to two fully connected weight matrices, concatenated by one layer of hidden neurons (8). However, keeping the "decomposed" model architecture facilitates the incorporation of problem-specific structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Complexity Analysis</head><p>From the clear correspondences between the sparsitybased formulation and the D 3 model, we immediately derive the dimensions of weight layers, as in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Time Complexity</head><p>During training, deep learning with the aid of gradient descent scales linearly in time and space with the number of training samples. We are primarily concerned with the time complexity during testing (inference), which is more relevant to practical usages. Since all learnable layers in the D 3 model are fully-connected, the inference process of D 3 is nothing more than a series of matrix multiplications. The multiplication times are counted as:</p><formula xml:id="formula_9">p Φ m (D A in Stage I) + 2p Φ (two diagonal layers) + p Φ m (D S in Stage I) + p Ψ m (D A in Stage II) + 2p Ψ (two diagonal layers) + p Ψ m (D S</formula><p>in Stage II). The 2D DCT and IDCT each takes 1 2 m log(m) multiplications <ref type="bibr" target="#b23">[25]</ref> . Therefore, the total inference time complexity of D 3 is:</p><formula xml:id="formula_10">C D 3 = 2(p Φ + p Ψ )(m + 1) + m log(m) ≈ 2m(p Φ + p Ψ ).</formula><p>(9) The complexity could also be expressed as O(p Φ + p Ψ ).</p><p>It is obvious that the sparse coding inference <ref type="bibr" target="#b22">[24]</ref> has dramatically higher time complexity. We are also interested in the inference time complexity of other competitive deep models, especially AR-CNN <ref type="bibr" target="#b9">[11]</ref>. For their fully convolutional architecture, the total complexity <ref type="bibr" target="#b14">[16]</ref> is:</p><formula xml:id="formula_11">C conv = d l=1 n l−1 · s 2 l · n l · m 2 l ,<label>(10)</label></formula><p>where l is the layer index, d is the total depth, n l is the number of filters in the l-th layer, s l is the spatial size of the filter, and m l is the spatial size of the output feature map.</p><p>The theoretical time complexities in (9) and (10) do not represent the actual running time, as they depend on different configurations and can be sensitive to implementations and hardware. Yet, our actual running time scales nicely with those theoretical results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Parameter Complexity</head><p>The total number of free parameters in D 3 is:</p><formula xml:id="formula_12">N D 3 = 2p Φ m + p Φ + 2p Ψ m + p Ψ = 2(p Φ + p Ψ )(m + 1).</formula><p>(11) As a comparison, the AR-CNN model <ref type="bibr" target="#b9">[11]</ref> contains:</p><formula xml:id="formula_13">N conv = d l=1 n l−1 · n l · s 2 l .<label>(12)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation and Setting</head><p>We use the disjoint training set (200 images) and test set (200 images) of BSDS500 database <ref type="bibr" target="#b1">[3]</ref>, as our training set; its validation set (100 images) is used for validation, which follows <ref type="bibr" target="#b9">[11]</ref>. For training the D 3 model, we first divide each original image into overlapped 8 × 8 patches, and subtract the pixel values by 128 as in the JPEG mean shifting process. We then perform JPEG encoding on them by MATLAB JPEG encoder with a specific quality factor Q, to generate the corresponding compressed samples. Whereas JPEG works on non-overlapping patches, we emphasize that the training patches are overlapped and extracted from arbitrary positions. For a testing image, we sample 8 × 8 blocks with a stride of 4, and apply the D 3 model in a patch-wise manner. For a patch that misaligns with the original JPEG block boundaries, we find its most similar coding block from its 16 × 16 local neighborhood, whose quantization intervals are then applied to the misaligned patch. We find this practice effective and important for removing blocking artifacts and ensuring the neighborhood consistency. The final result is obtained via aggregating all patches, with the overlapping regions averaged.</p><p>The proposed networks are implemented using the cudaconvnet package <ref type="bibr" target="#b19">[21]</ref>. We apply a constant learning rate of 0.01, a batch size of 128, with no momentum. Experiments run on a workstation with 12 Intel Xeon 2.67GHz CPUs and 1 GTX680 GPU. The two losses, L B and L 2 , are equally weighted. For the parameters in <ref type="table" target="#tab_0">Table 1</ref>, m is fixed as 64. We try different values of p Φ and p Ψ in experiments.</p><p>Based on the solved Eqn. (1), one could initialize D A , D S , and θ from Φ, Φ T and λ in the DCT domain block of <ref type="figure" target="#fig_0">Fig. 1</ref>, and from Ψ, Ψ T and γ in the pixel domain block, respectively. In practice, we find that such an initialization strategy benefits the performances, and usually leads to faster convergence.</p><p>We test the quality factor Q = 5, 10, and 20. For each Q, we train a dedicated model. We further find the easy-hard transfer suggested by <ref type="bibr" target="#b9">[11]</ref> useful. As images of low Q values (heavily compressed) contain more complex artifacts, it is helpful to use the features learned from images of high Q values (lightly compressed) as a starting point. In practice, we first train the D 3 model on JPEG compressed images with Q = 20 (the highest quality). We then initialize the Q = 10 model with the Q = 20 model, and similarly, initialize Q = 5 model from the Q = 10 one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Restoration Performance Comparison</head><p>We include the following two relevant, state-of-the-art methods for comparison:</p><p>• Sparsity-based Dual-Domain Method (S-D 2 ) <ref type="bibr" target="#b22">[24]</ref> could be viewed as the "shallow" counterpart of D 3 . It has outperformed most traditional methods <ref type="bibr" target="#b22">[24]</ref>, such as BM3D <ref type="bibr" target="#b7">[9]</ref> and DicTV <ref type="bibr" target="#b5">[7]</ref>, with which we thus do not compare again. The algorithm has a few parameters to be manually tuned. Especially, their dictionary atoms are adaptively selected by a nearest-neighbour type algorithm; the number of selected atoms varies for every testing patch. Therefore, the parameter complexity of S-D 2 cannot be exactly computed.</p><p>• AR-CNN has been the latest deep model resolving the JPEG compression artifact removal problem. In <ref type="bibr" target="#b9">[11]</ref>, the authors show its advantage over SA-DCT <ref type="bibr" target="#b11">[13]</ref>, RTF <ref type="bibr" target="#b16">[18]</ref>, and SR-CNN <ref type="bibr" target="#b10">[12]</ref>. We adopt the default network configuration in <ref type="bibr" target="#b9">[11]</ref>: s 1 = 9, s 2 = 7, s 3 = 1, s 4 = 5; n 1 = 64, n 2 = 32, n 3 = 16, n 4 = 1. The authors adopted the easy-hard transfer in training.</p><p>For D 3 , we test p Φ = p Ψ = 128 and 256 3 . The resulting D 3 models are denoted as D 3 -128 and D 3 -256, respectively. In addition, to verify the superiority of our task-specific design, we construct a fully-connected Deep Baseline Model (D-Base), of the same complexity with D 3 -256, named D-Base-256. It consists of four weight matrices of the same dimensions as D 3 -256's four trainable layers 4 . D-Base-256 utilizes ReLU <ref type="bibr" target="#b19">[21]</ref> neurons and the dropout technique. We use the 29 images in the LIVE1 dataset <ref type="bibr" target="#b25">[27]</ref> (converted to the gray scale) to evaluate both the quantitative and qualitative performances. Three quality assessment criteria: PSNR, structural similarity (SSIM) <ref type="bibr" target="#b30">[32]</ref>, and PSNR-B <ref type="bibr" target="#b35">[37]</ref>, are evaluated, the last of which is designed specifically to assess blocky images. The averaged results on the LIVE1 dataset are list in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Compared to S-D 2 , both D 3 -128 and D 3 -256 gain remarkable advantages, thanks to the end-to-end training as deep architectures. As p Φ and p Ψ grow from 128 to 256, one observes clear improvements in PSNR/SSIM/PSNR-B. D 3 -256 has outperformed the state-of-the-art ARCNN, for around 1 dB in PSNR. Moreover, D 3 -256 also demonstrates a notable performance margin over D-Base-256, although they possess the same number of parameters. D 3 is thus verified to benefit from its task-specific architecture inspired by the sparse coding process (1), rather than just the large learning capacity of generic deep models. The parameter numbers of different models are compared in the last row of <ref type="table" target="#tab_1">Table 2</ref>. It is impressive to see that D 3 -256 also takes less parameters than AR-CNN.</p><p>We display three groups of visual results, on Bike, Monarch and Parrots images, when Q = 5, in Figs. 2, 4 and 5, respectively. AR-CNN tends to generate oversmoothness, such as in the edge regions of butterfly wings and parrot head. S-D 2 is capable of restoring sharper edges and textures. The D 3 models further reduce the unnatural artifacts occurring in S-D 2 results. Especially, while D 3 -128 results still suffer from a small amount of visible ringing artifacts, D 3 -256 not only shows superior in preserving details, but also suppresses artifacts well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analyzing the Impressive Results of D 3</head><p>We attribute our impressive recovery of clear fine details, to the combination of our specific pipeline, the initialization, and the box-constrained loss. Task-specific and interpretable pipeline The benefits of our specifically designed architecture were demonstrated by the comparison experiments to baseline encoders. Further, we provide intermediate outputs of the IDCT layer, i.e., the recovery after the DCT-domain reconstruction. We hope that it helps understand how each component, i.e., the DCTdomain reconstruction or the pixel-domain reconstruction, contributes to the final results. As shown in <ref type="figure" target="#fig_4">Fig. 6 (a)</ref> such intermediate reconstruction results contain both sharpened details (see the characters in (a), which become more recognizable), and unexpected noisy patterns (see (a) (b) (c) for the blockiness, and ringing-type noise along edges and textures). It implies that Stage I DCT-domain reconstruction has enhanced the high-frequency features, yet introducing artifacts simultaneously due to quantization noises. Afterwards, Stage II pixel-domain reconstruction performs extra noise suppression and global reconstruction, which leads to the artifact-free and more visually pleasing final results. Sparse coding-based initialization We conjecture that the reason why D 3 is more capable in restoring the text on Bike and other subtle textures hinges on our sparse coding-based initialization, as an important training detail in D 3 . To verify that, we re-train D 3 with random initialization, with the testing results in <ref type="figure" target="#fig_4">Fig. 6</ref> (d)-(f), which turn out to be visually smoother (closer to AR-CNN results). For example, the characters in (d) are now hardly recognizable. We notice that the S-D 2 results, as in original <ref type="figure" target="#fig_1">Fig. 2-5 (c)</ref>, also presented sharper and more recognizable texts and details than AR-CNN. These observations validate our conjecture. So the next question is, why sparse coding helps significantly here? The quantization process can be considered as as a low-pass filter that cuts off high-frequency information. The dictionary atoms are learned from offline highquality training images, which contain rich high-frequency information. The sparse linear combination of atoms is thus richer in high-frequency details, which might not necessarily be the case in generic regression (as in deep learning). Box-constrained loss The loss L B (3) acts as another effective regularization. We re-train D 3 without the loss, and obtain the results in <ref type="figure" target="#fig_4">Fig. 6</ref> (g)-(i). It is observed that the box-constrained loss helps generate details (e.g., comparing characters in (g) with those in <ref type="figure" target="#fig_1">Fig. 2 (f)</ref>), by bounding the DCT coefficients, and brings PSNR gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Running Time Comparison</head><p>The image or video codecs desire highly efficient compression artifact removal algorithms as the post-processing tool. Traditional TV and digital cinema business uses frame rate standards such as 24p (i.e., 24 frames per second), 25p, and 30p. Emerging standards require much higher rates. For example, high-end High-Definition (HD) TV systems adopt 50p or 60p; the Ultra-HD (UHD) TV standard advocates 100p/119.88p/120p; the HEVC format could reach the maximum frame rate of 300p [1]. To this end, higher time efficiency is as desirable as improved performances. We compare the averaged testing times of AR-CNN and the proposed D 3 models in <ref type="table" target="#tab_2">Table 3</ref>, on the LIVE29 dataset, using the same machine and software environment. All running time was collected from GPU tests. Our best model, D 3 -256, takes approximately 12 ms per image; that is more than 30 times faster than AR-CNN. The speed difference is NOT mainly caused by the different implementations. Both being completely feed-forward, AR-CNN relies on the time-consuming convolution operations while ours takes only a few matrix multiplications. That is in accordance with the theoretical time complexities computed from <ref type="formula">(9)</ref> and (10), too. As a result, D 3 -256 is able to process 80p image sequences (or even higher). To our best knowledge, D 3 is the fastest among all state-of-the-art algorithms, and proves to be a practical choice for HDTV industrial usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce the D 3 model, for the fast restoration of JPEG compressed images. The successful combination of both JPEG prior knowledge and sparse coding expertise has made D 3 highly effective and efficient. In the future, we aim to extend the methodology to more related applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The illustration of Deep Dual-Domain (D 3 ) based model (all subscripts are omitted for simplicity). The black solid lines denote the network inter-layer connections, while the black dash lines connect to the loss functions. The two red dash-line boxes depict the two stages that incorporate DCT and pixel domain sparsity priors, respectively. The two grey blocks denote constant DCT and IDCT layers, respectively. The notations within parentheses along the pipeline are to remind the corresponding variables in (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Visual comparison of various methods on Bike at Q = 5. The corresponding PSNR values (in dB) are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparison of various methods on Monarch at Q = 5. The corresponding PSNR values (in dB) are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparison of various methods on Parrots at Q = 5. The corresponding PSNR values are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Intermediate and comparison results, on Bike, Monarch, and Parrot, at Q = 5: (a) -(c) the intermediate recovery results after the DCT-domain reconstruction; (d) -(f) the results trained with random initialization; (g) -(i) the results trained without the box-constrained loss. PSNR values are reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Dimensions of all layers in the D 3 model</figDesc><table>Layer 
DA 
DS 
diag(θ) 
Stage I (DCT Domain) 
pΦ × m m × pΦ 
pΦ 
Stage II (Pixel Domain) pΨ × m m × pΨ 
pΨ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>The average results of PSNR (dB), SSIM, PSNR-B (dB) on the LIVE1 dataset.</figDesc><table>Compressed 
S-D 2 
AR-CNN D 3 -128 D 3 -256 D-Base-256 

Q = 5 

PSNR 
24.61 
25.83 
26.64 
26.26 
27.37 
25.83 
SSIM 
0.7020 
0.7170 
0.7274 
0.7203 
0.7303 
0.7186 
PSNR-B 
22.01 
25.64 
26.46 
25.86 
26.95 
25.51 

Q = 10 

PSNR 
27.77 
28.88 
29.03 
28.62 
29.96 
28.24 
SSIM 
0.7905 
0.8195 
0.8218 
0.8198 
0.8233 
0.8161 
PSNR-B 
25.33 
27.96 
28.76 
28.33 
29.45 
27.57 

Q = 20 

PSNR 
30.07 
31.62 
31.30 
31.20 
32.21 
31.27 
SSIM 
0.8683 
0.8830 
0.8871 
0.8829 
0.8903 
0.8868 
PSNR-B 
27.57 
29.73 
30.80 
30.56 
31.35 
29.25 
#Param 
\ 
NA 
106,448 
33, 280 66, 560 
66, 560 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Averaged running time comparison (ms) on LIVE1. AR-CNN D 3 -128 D 3 -256 D-Base-256</figDesc><table>Q = 5 
396.76 
7.62 
12.20 
9.85 
Q = 10 
400.34 
8.84 
12.79 
10.27 
Q = 20 
394.61 
8.42 
12.02 
9.97 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In<ref type="bibr" target="#b6">(8)</ref>, we slightly abuse notations, and set λ to be a vector of the same dimension as u, in order for extra element-wise flexibility.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">From the analytical perspective, D S is the transpose of D A , but we untie them during training for larger learning capability.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">from the common experiences of choosing dictionary sizes [2] 4 D-Base-256 is a four-layer neural network, performed on the pixel domain, without DCT/IDCT layers. The diagonal layers contain a very small portion of parameters and are ignored here.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSP</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Review of proposed high efficiency video coding (hevc) standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Ayele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Iterative thresholding for sparse approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="629" to="654" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A total variation-based jpeg decompression model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="366" to="393" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing artifacts in jpeg decompression via a learned dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A learningbased approach to reduce jpeg artifacts in image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2880" to="2887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Recklessly approximate sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1208.0959</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointwise shapeadaptive dct for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Projective dictionary pair learning for pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="793" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks at constrained time cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2574</idno>
		<title level="m">Deep unfolding: Model-based inspiration of novel deep architectures</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Loss-specific training of non-parametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="112" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image deblocking via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="663" to="677" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tackling box-constrained optimization via a new projected quasi-newton approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regression-based prediction for blocking artifact reduction in jpeg-compressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="48" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inter-block soft decoding of jpeg images with sparsity and graph-signal smoothness priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP. IEEE</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data-driven sparsity-based restoration of jpeg-compressed images in dual transform-pixel domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">JPEG: Still image data compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient regression priors for reducing image compression artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Live image quality assessment database release 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time compression artifact reduction via robust nonlinear filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="565" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning efficient sparse and low rank models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Supervised sparse analysis and synthesis operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Yakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="908" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a task-specific deep architecture for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SDM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep ℓ0 encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-tuned deep super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quality assessment of deblocked images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="98" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
