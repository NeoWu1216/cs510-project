<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Local Search: Tracking Objects Everywhere with Instance-Specific Proposals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Zhu</surname></persName>
							<email>gao.zhu@anu.edu.au*</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ARC Centre of Excellence for Robotic Vision 3</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
							<email>fatih.porikli@anu.edu.au*</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ARC Centre of Excellence for Robotic Vision 3</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<email>hongdong.li@anu.edu.au*</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ARC Centre of Excellence for Robotic Vision 3</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicta</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ARC Centre of Excellence for Robotic Vision 3</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Local Search: Tracking Objects Everywhere with Instance-Specific Proposals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most tracking-by-detection methods employ a local search window around the predicted object location in the current frame assuming the previous location is accurate, the trajectory is smooth, and the computational capacity permits a search radius that can accommodate the maximum speed yet small enough to reduce mismatches. These, however, may not be valid always, in particular for fast and irregularly moving objects. Here, we present an object tracker that is not limited to a local search window and has ability to probe efficiently the entire frame. Our method generates a small number of "high-quality" proposals by a novel instance-specific objectness measure and evaluates them against the object model that can be adopted from an existing tracking-by-detection approach as a core tracker. During the tracking process, we update the object model concentrating on hard false-positives supplied by the proposals, which help suppressing distractors caused by difficult background clutters, and learn how to re-rank proposals according to the object model. Since we reduce significantly the number of hypotheses the core tracker evaluates, we can use richer object descriptors and stronger detector. Our method outperforms most recent state-of-the-art trackers on popular tracking benchmarks, and provides improved robustness for fast moving objects as well as for ultra lowframe-rate videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Model-free object tracking, which aims to track arbitrary objects based on a single bounding-box annotation, has gained significant attention recently with numerous approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>  datasets <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref> released. Significant amount of effort has been devoted to either designing a better object representation, including subspace <ref type="bibr" target="#b29">[30]</ref>, sparsity <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">43]</ref>, and deep learned features <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>, or building complex classifiers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29]</ref> for better discrimination of the object from its background patches. Most of these methods, however, require a search window centered at the previous object location to select candidate patches, partly due to computational complexity. This is sometimes referred as the motion model <ref type="bibr" target="#b34">[35]</ref>, and it is implicitly assumed that the object is correctly tracked in the previous frames and the object motion is not large. Even though this simplification works in some situations, it also introduces serious difficulties especially when the object undergoes deformations and occlusions (which may cause drift), or when the object and camera motion puts the object beyond the search window radius. One important reason that the existing trackers avoid employing a wider search radius is the potential distractions from the background <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>. It is not a trivial task to update a discriminative classifier when the negative sample space grows greatly with the samples coming from the extended search radius. In <ref type="bibr" target="#b17">[18]</ref>, extended set of training data is obtained by implicitly including all shifted versions of the given samples within the circulant matrices. However, it is impractical to apply the same trick for the negative samples, especially for the ones far away from the object.</p><p>To overcome this, in this work we introduce a proposal generation procedure for handling the problem of sample selection, both for the object detection and the model update stages. Generally, the motion model limits the search radius and the applied sampling schemes disregard the contents presented on them. Instead of working within a limited search radius, we generate a small yet high-quality set of proposals efficiently in entire frame by using simple bottom-up, edge-based features <ref type="bibr" target="#b45">[46]</ref> as shown in <ref type="figure">Figure  1</ref>. Intuitively, edge information provides valuable guidance for object tracking since objects may often be identified by their silhouettes. In addition, concentrating on image regions where edge information is eminent allows efficient selection of more object-like proposals.</p><p>Our method can incorporate any existing object model including simpler template matching models, e.g. normalized cross correlation (NCC) and sophisticated classifiers, e.g., structured support vector machines (SSVM). Using the object model, we adapt the edge-based features used in proposal generation. In an online fashion, we learn how to re-rank the proposal by a linear support vector machine, trained on the current proposals, with a crafted feature vector. Our proposal scheme, thus, generates windows that suggest certain similarity to the tracked object. This allows taking advantage of objectness to regulate the proposal selection in a temporally coherent manner instead of treating objectness as yet another cue by (linearly) combining the original tracking response with some objectness score. Since we adapt the generic edge-based objectness measure to the specific object, this selection is superior to replacing the search window with simple objectness responses.</p><p>Furthermore, for the chosen object model, we explore the best combination of global proposals provided by instance specific edge-based features and local candidates sampled around the previous location for model update (e.g., for negative support vectors in case of SSVM). We also adapt the size and scale to obtain the best proposals.</p><p>The benefits of our proposal generation is threefold:</p><p>• Our method can execute global search over entire image. Thus, it can track objects without making any assumption on object motion. • The high-quality proposals increase the tracking accuracy since they allow including better hard negatives into training set, hence reduces drift. • It adapts the specific object, thus provides better object model update (than generic proposals).</p><p>We validate the above arguments with two object models (from NCC tracker and Struck) and show that the incorporation of instance-specific proposals has potential to improve most detection-by-tracking approaches. Our method is conceptually simple, easy to implement, and most importantly, provides the best results (at the time of submission) in comparison to all state-of-the-art trackers. Our method ranks as the top tracker on VOT2014 <ref type="bibr" target="#b25">[26]</ref> benchmark as well as on OTB <ref type="bibr" target="#b38">[39]</ref> and TB50 <ref type="bibr" target="#b37">[38]</ref> datasets in comparison to the latest state-of-the-art including MEEM <ref type="bibr" target="#b39">[40]</ref>, KCF <ref type="bibr" target="#b17">[18]</ref>, Struck <ref type="bibr" target="#b15">[16]</ref>, and over twenty other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Providing an inclusive overview of the object tracking literature is outside the scope and capacity of this paper. We refer readers to the excellent surveys on object tracking. Here, we only compare with some relevant algorithms. We briefly examine different search schemes and then summarize recent object proposal methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Schemes in Tracking</head><p>There is a wide-spectrum of styles to select which windows will be tested in a current frame to locate the target object and also update its model.</p><p>Single Window Search: Several trackers use the local window around the former object location to find the object in the current frame. Examples include the tracking on Lie groups <ref type="bibr" target="#b32">[33]</ref>, which applies iteratively a feature-motion regressor to estimate object window in the next frame, and the mean-shift tracker <ref type="bibr" target="#b10">[11]</ref>, which uses gradient-based local optimization to determine the mode of the underlying similarity distribution. Particle-based Search: In recent years, tracking algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b20">21]</ref> based on particle filtering has been extensively studied. Particle filters apply importance sampling on the previous particle states (e.g. candidate locations) within mostly a mixed number of candidates. On the negative side, the random sampling is blind to the underlying texture, edgeness, and other spatial information. Searching for the Hard Negatives: It is worthwhile to mention that tracking-by-detection, which allows an online trained classifier <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref> as an object model to distinguish the object from its surrounding background, has recently become particularly popular. Rather than explicitly coupling to the accurate estimation of object position, <ref type="bibr" target="#b3">[4]</ref> limits its focus on increasing the robustness to poorly labeled samples. <ref type="bibr" target="#b15">[16]</ref> proposes directly predicting the change in object location between frames by an online structured output SVM. Even though it produces comparably accurate tracking, it uniformly samples the state space to generate positive and negative support vectors. Such a brute force approach on a larger search window is computationally intractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objectness in Object Detection</head><p>As shown in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b45">46]</ref>, use of proposal has significantly improved the object detection benchmark along with the convolutional neural nets. Since, a subset of high-quality candidates are used for detection, object proposal methods improve not only the speed but also the accuracy by reducing false positives. The top performing detection methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36]</ref> for PASCAL VOC <ref type="bibr" target="#b13">[14]</ref> use detection proposals.</p><p>Edge Box: <ref type="bibr" target="#b45">[46]</ref> proposes object candidates based on the observation that the number of contours wholly enclosed by a bounding box is an indicator of the likelihood of the box containing an object. Edge Box is designed as a fast algorithm to balance between speed and proposal recall. Its 1-D feature generates remarkably accurate results.</p><p>BING: <ref type="bibr" target="#b9">[10]</ref> made a similar observation that generic objects with well-defined closed boundary can be discriminated by looking at the norm of gradients.They further designed a feature called binarized normed gradients (BING), which can be used for efficient objectness estimation and requires only a few atomic operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objectness as Supportive Cue for Tracking</head><p>A straightforward strategy, i.e., linear combination of the original tracking confidence and an adaptive objectness score based on BING <ref type="bibr" target="#b9">[10]</ref> is employed in <ref type="bibr" target="#b24">[25]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, a detection proposal scheme is applied as a post-processing step, mainly to improve the tracker's adaptability to scale and aspect ratio changes. These methods are substantially different from our work, where we adapt objectness to specific object using a separate classifier and generate highquality proposal to regulate the tracking process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Global Tracking with Proposals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pipeline</head><p>A typical tracking-by-detection framework is composed mainly of motion model, observation model and model updater <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. Motion model generates a set of candidates which might contain the target in the current frame based on the estimation from the previous frame. Observation model judges whether a candidate is the target based on the features extracted from it. Model updater online updates the observation model to adapt the change of the object appearance.</p><p>Suppose the object location is initialized manually at the first frame t = 1 and B t is its bounding box at frame t. Then, given an observation model, i.e., a classification function f t−1 trained on the previous frames, the current loca- (a) Input frame (ground truth is the green bounding box); (b) 10dimensional feature vector for ranking of the bounding boxes; (c) Top proposals using the proposed method; (d) Top proposals from <ref type="bibr" target="#b45">[46]</ref>. As shown, the instance specific proposals are far more precise.</p><p>tion of the object is estimated through:</p><formula xml:id="formula_0">B ⋆ t = arg max Bt∈Bt f t−1 (B t ),<label>(1)</label></formula><p>where B t is a set of samples generated by the motion model at the current frame. To select samples, traditional trackers use heuristic search windows around the previously estimated object location for computational and accuracy reasons. For example, a search radius of 30 pixels is used in <ref type="bibr" target="#b15">[16]</ref>. Each sample is labeled by a classifier that models the object. The update routine will then revises its model f t−1 → f t with the new location of the object to adapt possible appearance changes. It is not trivial to design a robust updating scheme <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>. As there is only one reliable example, the tracker must maintain a trade-off between adapting to new but possibly noisy examples collected during tracking and preventing the tracker from drifting to the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Our Method</head><p>The method proposed in this paper uses a similar framework as introduced in Section 3.1, yet we made two critical changes to the motion model. The first change is that we recognize not all candidate bounding boxes B t ∈ B t should be treated equally (as the traditional trackers often do) since those boxes possess different object-like appearance, i.e. objectness <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> characteristics, which should be taken into account. Secondly, we do not constrain the search radius to a small window that causes throwing so much available image information away.</p><p>To execute our changes, we take advantage of the sparse, simple, yet critical edge information. The current frame I t is processed into an edge map as shown in <ref type="figure" target="#fig_1">Figure 2a</ref>. Then, we employ an instance specific proposal method (explained in Section 3.3) build on top of the object proposal algorithm <ref type="bibr" target="#b45">[46]</ref> to produce a number of candidate bounding boxes <ref type="figure" target="#fig_1">(Figure 2b</ref> and 3c) denoted as B E t . Notice that, we impose a smooth size change constraint to the bounding boxes between consecutive frames.</p><p>Suppose the bounding box set generated by sampling only around the previous object location as B R t (as in traditional methods). Now we have two different sets of candidates, i.e., B E t and B R t . The first one possesses object regularity while the second one is with no discriminative information. As shown in the experimental section 5.2, the choice of using only the proposals B E t generates the best results, better than combining them together. This confirms our argument that object proposals not only reduce the candidate sample space but also reduce spurious false positive and improve tracking accuracy. Our tracker will not drift to a textureless region like other trackers due to the objectness constraint.</p><p>During the update stage, we also have different options for using B E t and B R t . As validated in the experimental part 5.2, the combination of using both of them to choose negative support vectors results in the best performance. This can be easily explained: B E t \B ⋆ t only represents other good object-like regions. By putting them as negative support vectors, we would only increase the discriminative power among objects-like candidates. However, the negative sample space contains a lot more other negative samples. Thus, the advantageous option is to augment B E t \B ⋆ t with B R t in order to achieve the best discriminative ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instance Specific Proposals</head><p>Objectness attempts to generate quickly as few as possible hypotheses yet cover all of the objects present in an image. Take EdgeBox <ref type="bibr" target="#b45">[46]</ref> for example -it generates a pool of bounding boxes {B t,i } uniformly sampled in a sliding window manner, then ranks and extracts the top H candidates with the highest objectness score E t,i , represented by:</p><formula xml:id="formula_1">B EB t = {B t,i |E t,i } H .<label>(2)</label></formula><p>E t,i is basically a weighted and normalized number of contours wholly enclosed by the bounding box B t,i . This feature can be calculated very efficiently in real-time. We refer <ref type="bibr" target="#b45">[46]</ref> for more details. Instead of directly applying the computed proposals B EB t for tracking, we argue that the object instance level properties should be taken into account. As such, there is a strong object prior in terms of its geometric structure of contours and size in contrast to object detection where the goal is to locate all instances of all object classes in the image. EdgeBox generates proposals that favors bounding boxes with many internal contour segments, thus it is likely to miss the target in a cluttered background as shown in <ref type="figure" target="#fig_2">Figure 3d</ref>.</p><p>To this end, we incorporated an online updated linear SVM <ref type="bibr" target="#b36">[37]</ref> classifier f R t−1 to re-rank proposals and determine the top H proposals based on their classification scores:</p><formula xml:id="formula_2">B E t = {B t,i |f R t−1 (B t,i )} H ,<label>(3)</label></formula><p>with a 10-dimensional feature vector {E 1 t,i , . . . , E 10 t,i } as shown in <ref type="figure" target="#fig_2">Figure 3b</ref>. This feature characterizes the spatial structure of edge information. It concatenates Edge-Box scores corresponding to Haar wavelet like partitioning of the bounding box B t,i . Notice that, only the bounding boxes whose initial objectness scores are above a threshold, i.e., B EB T t = {B t,i |E t,i &gt; e T } (in all experiments e T = 0.005) are accepted into the classifier for re-ranking to save computing time.</p><p>The re-ranking classifier is initialized using the top EdgeBox proposal (top 200 in all experiments) and then online updated at every 5 frames with the same number of proposals. The estimated position gives the positive sample and bounding boxes which overlap the estimation less than 0.5 are assigned as negative ones. We use the implementation and parameters as in <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Candidate Classification</head><p>We use the following decision function to estimate the new location of the object <ref type="figure" target="#fig_1">(Figure 2c)</ref>:</p><formula xml:id="formula_3">B ⋆ t = arg max Bt∈Bt f t−1 (B t ) + s(B t , B ⋆ t−1 ).<label>(4)</label></formula><p>s(B t , B ⋆ t−1 ) is a term representing the motion smoothness between the previous object location and the candidate box. This is important in our formulation as we are testing candidates all over the image, though not penalizing it too much. We use a simple function in this paper:</p><formula xml:id="formula_4">s(B t , B ⋆ t−1 ) = w s exp(− 1 2σ 2 c(B t ) − c(B ⋆ t−1 ) 2 ), where c(B t )</formula><p>is the center of bounding box B t , w s = 0.1 and σ is set as the diagonal length of the initialized bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Trackers</head><p>Two core object models are integrated in the proposal tracker. The first one (called as EBT to indicate its relation to EdgeBox) follows a popular structured support vector machine (SSVM) framework <ref type="bibr" target="#b15">[16]</ref>, which shows good performance on several benchmarks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b31">32]</ref>. We additionally incorporated a much simpler, normalized cross correlation (NCC) template matching, called as NCC EB , to investigate how much additional performance improvement our method is able to provide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">EBT Tracker</head><p>Suppose the support vector set maintained by the SSVM as V t−1 and the classification function can be written as a weighted sum of affinities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>:</p><formula xml:id="formula_5">f S t−1 (B t ) = B i t−1 ∈Vt−1 w i t−1 k(B i t−1 , B t ),<label>(5)</label></formula><p>where w i t−1 is a scalar weight associated with the support vector B i t−1 . Kernel function k(B i t−1 , B t ) calculates the affinity between two feature vectors extracted from B i t−1 and B t respectively. The classifier is updated in an online fashion using <ref type="bibr" target="#b5">[6]</ref> with a budget <ref type="bibr" target="#b36">[37]</ref>. Intersection kernel is used and other parameters are set same as <ref type="bibr" target="#b15">[16]</ref>.</p><p>To take advantage of the small set of proposals, we use histogram features obtained by concatenating 16-bin intensity histograms from a spatial pyramid of 5 levels and RGB channels separately. At each level L, the patch is divided into L × L cells, resulting in a 2640-D feature vector, comparing to the 480-D feature used in <ref type="bibr" target="#b15">[16]</ref>, while running at a similar speed. The performance gain of using the richer feature is demonstrated in the experimental section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">NCC EB Tracker</head><p>The classification function for the normalized cross correlation can be written as:</p><formula xml:id="formula_6">f N t−1 (B t ) = ρ(B t , B T emp ),<label>(6)</label></formula><p>where ρ calculates the normalized cross-correlation coefficient <ref type="bibr" target="#b6">[7]</ref> between the candidate patch and the object template. This procedure can be accelerated using the fast Fourier transform (FFT) trick. We compared the proposed NCC EB tracker with instance-specific proposals and fixed template with: (1) NCC, an implementation from <ref type="bibr" target="#b25">[26]</ref>, uses local exhaustive search, and has no update; and (2) IMP-NCC, an improved NCC version from <ref type="bibr" target="#b25">[26]</ref>, uses local exhaustive search, online update, and Kalman Filter <ref type="bibr" target="#b22">[23]</ref> for trajectory smoothness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In the first part, we compare our method with the stateof-the-art trackers on benchmark datasets for a general performance evaluation. We also test on fast-motion related categories to put it under the spotlight to understand how well our method can handle the challenging scenarios such as fast moving objects, randomly moving objects, and tracking under low-frame-rate. In the second part, we analyze different components of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Full Benchmark Evaluations</head><p>Our method is tested on three large datasets: OTB <ref type="bibr" target="#b38">[39]</ref>, TB50 <ref type="bibr" target="#b37">[38]</ref> and VOT2014 <ref type="bibr" target="#b25">[26]</ref>. The first two of     these datasets are composed of around 50 sequences each. They are annotated with ground truth bounding boxes and various visual attributes. TB50 is an upgraded version of OTB and contains much more challenging sequences. VOT2014 dataset selectively collects 25 sequences from various datasets and allows the tracker to re-initialize once the tracker drifts away from the object.</p><p>We compare against the existing algorithms on respective benchmarks and additionally two recent works: KCF <ref type="bibr" target="#b17">[18]</ref> and MEEM <ref type="bibr" target="#b39">[40]</ref>. Evaluation metrics and code are pro- vided by the respective benchmark. For OTB and TB50, we employ the one-pass evaluation (OPE) and use two metrics: precision plot and success plot. The former metric calculates the rate of frames whose center location is within a certain threshold distance with the ground truth. The latter one calculates a same ratio but based on bounding box overlap threshold.</p><p>Parameters For EdgeBox proposals, the sampling step of sliding window is set at α = 0.85 since we aim for a high accurate localization. The minimal and maximal areas are 0.5 and 2 of the area of the previous estimated bounding box respectively. Non-maximum suppression parameter is fixed at β = 0.8. The maximum number of proposal is 200 (more discussion in Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Benchmark Results</head><p>The results are summarized in <ref type="table" target="#tab_2">Table 1</ref>, 2 and <ref type="figure" target="#fig_5">Figure 4</ref>. Our EBT tracker ranks as the best tracker on VOT2014 as shown in <ref type="table" target="#tab_2">Table 1</ref>. We use the original VOT protocol. EBT achieves the best overall performance in all datasets 1 . It consistently outperforms the state-of-the-art trackers and improves the base Struck tracker by a large margin. A few examples can be found in <ref type="figure" target="#fig_6">Figure 5</ref>.</p><p>Even the proposed NCC EB tracker using only template matching manages to improve the simple NCC tracker significantly and outperforms several other trackers including  the IMPNCC tracker, which has incorporated sophisticated mechanisms in comparison to ours and NCC. This result is not surprising since the incorporation of objectness has proven to be a successful strategy in single image object detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14]</ref>. We believe that our method is a counterpart in the tracking domain as no existing tracking methods successfully adopted such objectness schemes before, to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Tracking Fast Objects</head><p>Since our method searches over the entire image, it is suitable for tracking fast moving objects, which could move outside of the search radius of the traditional trackers. As shown in <ref type="table" target="#tab_5">Table 3</ref>, our method outperforms other trackers in the fast-motion related categories as well.</p><p>We also tested our method on an extra category Moving Camera from ALOV300 <ref type="bibr" target="#b31">[32]</ref>. This category contains many sequences that depict camera shake, sudden object motion, and abrupt jumps. ALOV300 provides a high number of short sequences with 14 visual attributes. The main source of their data is real-life videos from YouTube.</p><p>Tracking under Ultra-Low-Frame-Rate We additionally created a dataset, called as VOT2014+ by temporally sampling sequences at every 20 frames on VOT2014, thus, it contains 20× faster moving objects. Our method is tested against with other top-ranked trackers, KCF and MEEM. Even though both MEEM and KCF rapidly failed, our tracker retained very high performance scores (see Table.4). As discussed in Section 3.2, we tested different combinations of the hypothesis proposals B E t and candidate bounding boxes B R t sampled around the previous object location within a radius. The results are shown in Table5. For combinations which use only B R t in the testing stage, we apply an exhaustive sampling within a 30-pixels radius to achieve a comparable result. For the others which use B R t , we only generate 80 samples uniformly within a 30-pixels radius. Our main discussion about these results can be found in Section 3.2. We observed the combination of using samples from the hypothesis proposals and local region in update stage and samples only from the proposed locations in the test stage performs the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Proposals</head><p>To quantitatively compare the proposed instance specific proposals and the one using Edge Box <ref type="bibr" target="#b45">[46]</ref>, we analyzed the upper bound performance with respect to varying number of proposals as shown in <ref type="figure">Figure  6</ref>. A variant denoted as EBTeb using EdgeBox proposals instead of ours is also tested and available in <ref type="figure" target="#fig_8">Figure 7</ref>. Both results show that the proposed re-ranking method outperforms the one directly applies EdgeBox. We also tested the variants using different number of proposals. EBT100 and EBT400 use 100 and 400 respectively, comparing to the proposed EBT that uses 200. Our observations are, using insufficient number of proposal leads to a bad coverage of the false positives as well as the object, while using a large number of proposals attracts spurious candidates.   <ref type="figure">Figure 6</ref>: The performance bounds for using EdgeBox proposals and the proposed instance-specific proposal method on TB50. The best candidate in each frame is used for calculating the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Richer Features and Motion Constraint</head><p>EBTfeature denotes the variant using a lower dimensional 480-D feature. This version has lower performance than the one uses 2640-D feature as expected. More details about the feature can be found in Section 4.1. EBTwm denotes the variant without using the smoothness term s(B t , B ⋆ t−1 ) in Function 4. The success rate dropped due to the fact that the motion in the tracking sequences is not completely random.</p><p>Proposals using BING We evaluated another popular object proposal method, BING <ref type="bibr" target="#b9">[10]</ref>, for proposals. Two ways of incorporation were tested. The first one (BING-VOC) uses the pretrained model on VOC dataset <ref type="bibr" target="#b13">[14]</ref>, while the second one (BING-Adapt) relearns the model using the first frame of each sequence. We tested these two variants on TB50. Results are in <ref type="table" target="#tab_8">Table 6</ref>. Both performances are    worse than the baseline Struck. This is expected. As shown <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b45">46]</ref>, BING results in a relatively low recall of the objects, which is one reason for its mediocre performance.</p><p>Computational Speed The computational speed of the proposed is comparable to the state-of-the-art trackers even though we can track over the entire image. The proposal part takes less than 100 milliseconds and the overall tracking speed is available in <ref type="table" target="#tab_5">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper presented a robust method that can locate objects that are moving randomly and very fast, as well as perform tracking under extremely low-frame rates. To the best of our knowledge, our tracker achieves the best results on all common benchmark datasets including OTB <ref type="bibr" target="#b38">[39]</ref>, TB50 <ref type="bibr" target="#b37">[38]</ref>, VOT2014 <ref type="bibr" target="#b25">[26]</ref> and ALOV300 <ref type="bibr" target="#b31">[32]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Figure 1 :</head><label>11</label><figDesc>proposed and several large benchmark * This work was supported under the Australian Research Councils Discovery Projects funding scheme (project DP150104645, DP120103896), Linkage Projects funding scheme (LP100100588), ARC Centre of Excellence on Robotic Vision (CE140100016) and NICTA (Data61).(a) Frame t (b) Frame t + Top row: Most existing tracking-by-detection methods examine hypothesis locations within a local and heuristically defined search window around the last detected location. Bottom row: Our tracker seeks high-quality hypotheses over the entire image using instance-specific edgebox locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Framework of the proposed method. First column: (a) Edge map extracted from the current frame (e); Second column: (b) Object proposals in blue bounding boxes (Section 3.3) and (f) corresponding heatmap of instance specific proposals; Third column: (c) Detection results on proposals (green is detected as object) and (g) detection heatmap (by the proposed EBT classifier); Fourth column: (d) EBT is updated using the proposals and (h) detection heatmap with updated EBT. Notice that spurious hypotheses (bright regions in (g)) are suppressed significantly by treating them as negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Instance specific proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Success plot and precison plot on two large benchmarks: OTB and TB50. Algorithms are ranked by the area under the curve and the precision score (20 pixels threshold). Our method achieves consistently superior performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparisons with the state-of-the art trackers on the DragonBaby, Skating2, and CarScale videos. Our method exhibits robustness in challenging scenarios such as fast motion, occlusion, and scale changing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Success plot of variants of the proposed method on TB50. Details can be found in Section 5.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Success plots of OTB (OPE)</figDesc><table>Proposed EBT [58.1] 
MEEM [56.4] 
KCF [51.7] 
SCM [49.8] 
Struck [47.2] 
TLD [43.4] 
ASLA [43.4] 

0 
10 
20 
30 
40 
50 
0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Location error threshold 

Precision 

Precision plots of OTB (OPE) 

Proposed EBT [84.8] 
MEEM [82.5] 
KCF [74.2] 
Struck [65.3] 
SCM [64.8] 
TLD [60.1] 
ASLA [52.9] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Success plots of OTB − Fast Motion(17)</figDesc><table>Proposed EBT [58.1] 
MEEM [54.3] 
KCF [46.8] 
Struck [45.7] 
TLD [40.7] 
SCM [29.4] 
ASLA [24.4] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Performance on VOT2014.</figDesc><table>Final Rank Acc. Rank Rob. Rank 
Proposed EBT 
13.03 
15.81 
10.24 
PLT14 [26] 
13.75 
16.66 
10.84 
PLT13 [26] 
14.26 
18.59 
9.92 
DGT [8] 
14.54 
15.48 
13.61 
DSST [12] 
15.25 
13.40 
17.09 
KCF [18] 
15.25 
12.20 
18.29 
SAMF [24] 
15.47 
12.79 
18.15 
MEEM [40] 
18.95 
21.15 
16.76 
Struck [16] 
22.83 
22.30 
23.36 
Proposed NCC EB 
27.27 
24.20 
30.35 
MIL [4] 
27.69 
31.24 
24.14 
FSDT [26] 
27.86 
25.97 
29.75 
IMPNCC [26] 
27.99 
26.05 
29.94 
CT [42] 
28.26 
29.14 
27.38 
FRT [1] 
28.64 
25.02 
32.26 
NCC [26] 
29.30 
22.32 
36.28 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Area Under Curve (AUC) of success plot and Precision Score (20 pixels threshold) reported on various datasets (AUC/PS) corresponding to the one-pass evaluation (OPE).</figDesc><table>Pro. EBT KCF [18] MEEM [40] Struck [16] SCM [44] ASLA [21] TLD [22] CXT [13] CSK [17] 
OTB 58.1/84.8 51.7/74.2 56.4/82.5 47.2/65.3 49.8/64.8 43.4/52.9 43.4/60.1 42.3/57.0 39.6/54.1 
TB50 49.6/73.9 40.2/61.1 47.9/72.3 36.3/49.9 35.5/47.8 35.8/46.2 32.1/45.0 32.1/43.2 31.4/43.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Performance on the low-fps dataset. VOT2014+ 43.7/58.5 28.4/34.1 37.5/47.75.2. Further RemarksCombination of B E t and B R</figDesc><table>Pro. EBT KCF [18] MEEM [40] 
VOT2014 
46.7/65.9 38.9/53.7 44.5/62.3 
t 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Area Under Curve (AUC) of success plot and Precision Score (20 pixels threshold) reported on various fast-motion related categories (AUC/PS). FM: fast motion, MB: motion blur, MC: moving camera. fps: frames per second. MC (22) (ALOV300) 60.9/68.4 56.4/62.9 57.2/65.1 44.9/44.8 35.7/37.9 38.6/38.8 56.1/67.9</figDesc><table>Attributes 
Pro. EBT KCF [18] MEEM [40] Struck [16] SCM [44] ASLA [21] TLD [22] 
FM (17) (OTB) 
58.1/77.8 46.8/61.0 54.3/71.4 45.7/59.6 29.4/32.9 24.4/24.6 40.7/53.2 
MB (12) 
58.3/77.1 50.8/66.0 53.0/68.0 42.6/54.0 29.5/33.3 25.1/26.8 39.0/49.0 
FM (25) (TB50) 
53.3/74.5 39.0/54.0 48.2/68.4 34.4/42.5 25.2/29.6 25.0/29.6 35.6/46.5 
MB (19) 
54.9/78.5 40.6/56.4 52.8/72.9 30.9/35.5 21.7/25.1 23.3/25.5 39.3/49.7 
fps 
4.4 
70.9 
7.1 
4.8 
0.3 
3.8 
8.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Results for different combinations of B E t and B R t .Number of candidates Precison score(20 pixels)    </figDesc><table>TB50 
(Test) B R 

t 

B E 

t 

B E 
t + B R 

t 

B R 
t (Update) 41.1/58.7 44.7/64.2 42.7/59.4 
B E 

t 

40.1/56.3 46.5/68.6 43.0/61.8 
B E 
t + B R 

t 

39.2/56.5 49.6/73.9 43.2/63.6 

0 
100 
200 
300 
400 
0 

0.2 

0.4 

0.6 

0.8 

1 

Our Method 
EdgeBox 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>Success plots of TB50 (OPE)</figDesc><table>Proposed EBT [49.6] 
EBT400 [48.0] 
EBTeb [47.3] 
EBT100 [46.8] 
EBTwm [46.2] 
EBTfeature [45.5] 
KCF [40.2] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Performance when BING is used instead of Edge Box.</figDesc><table>Struck [16] 
BING-VOC 
BING-Adapt 
TB50 
36.3/49.9 
30.8/47.6 
33.7/48.0 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As stated in FAQ of the official VOT website, the rankings would not be identical to theTable 1in the 2014 paper.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust fragmentsbased tracking using the integral histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Visual tracking with online multiple instance learning. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to localize objects with structured output regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Solving multiclass support vector machines with LaRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Template matching using fast normalized cross correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Briechle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">D</forename><surname>Hanebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE: Optical Pattern Recognition XII</title>
		<meeting>SPIE: Optical Pattern Recognition XII</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured visual tracking with dynamic graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">CPMC: Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TP-MAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BING: binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive color attributes for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context tracker: Exploring supporters and distracters in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Highspeed tracking with kernelized correlation filters. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">How good are detection proposals, really? In BMVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enable scale and aspect ratio adaptability in visual tracking with detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive structural local sparse appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">P-N learning: Bootstrapping binary classifiers by structural constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Kalman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ASME-Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adaptive objectness for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2014 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The template update problem. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust visual tracking using l1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">In defense of color-based model-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Incremental learning for robust visual tracking. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online multi-class LPBoost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Visual tracking: An experimental survey. TPMAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning on Lie groups for invariant detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Transferring rich feature hierarchies for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding and diagnosing visual tracking systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-class Pegasos on a budget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vucetic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MEEM: Robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Robust tracking via convolutional networks without learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real-time compressive tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structural sparse tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust object tracking via sparsity-based collaborative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Lie-Struck: Affine tracking on Lie groups using structured SVM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>WACV</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
