<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><forename type="middle">Chen</forename><surname>Xiaojuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Lequan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pheng</surname></persName>
							<email>pheng@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Heng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The morphology of glands has been used routinely by pathologists to assess the malignancy degree of adenocarcinomas. Accurate segmentation of glands from histology images is a crucial step to obtain reliable morphological statistics for quantitative diagnosis. In this paper, we proposed an efficient deep contour-aware network (DCAN) to solve this challenging problem under a unified multi-task learning framework. In the proposed network, multi-level contextual features from the hierarchical architecture are explored with auxiliary supervision for accurate gland segmentation. When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved. Moreover, our network can not only output accurate probability maps of glands, but also depict clear contours simultaneously for separating clustered objects, which further boosts the gland segmentation performance. This unified framework can be efficient when applied to large-scale histopathological data without resorting to additional steps to generate contours based on low-level cues for post-separating. Our method won the 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams, surpassing all the other methods by a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Normally, a typical gland is composed of a lumen area forming the interior tubular structure and epithelial cell nuclei surrounding the cytoplasm, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (top left). Malignant tumours arising from glandular epithelium, also known as adenocarcinomas, are the most prevalent form of cancer. In the routine of histopathological examination, the morphology of glands has been widely used for assessing the malignancy degree of several adenocarcinomas, including breast <ref type="bibr" target="#b13">[14]</ref>, prostate <ref type="bibr" target="#b18">[19]</ref>, and colon <ref type="bibr" target="#b16">[17]</ref>. Accurate segmentation of glands is one crucial pre-requisite step to obtain reliable morphological statistics that indicate the aggressiveness of tumors. Conventionally, this is per- formed by expert pathologists who evaluate the structure of glands in the biopsy samples. However, manual annotation suffers from issues such as limited reproducibility, considerable efforts, and time-consuming. With the advent of whole slide imaging, large-scale histopathological data need to be analyzed. Therefore, automatic segmentation methods are highly demanded in clinical practice to improve the efficiency as well as reliability and reduce the workload on pathologists.</p><p>Nevertheless, this task is quite challenging for several reasons. First, there is a huge variation of glandular morphology depending on the different histologic grades as well as from one disease to another. <ref type="figure" target="#fig_0">Figure 1</ref> (left column) shows the large difference of glandular structures between benign and malignant cases from colon tissues. Second, the existence of touching glands in tissue samples makes it quite hard for automated methods to separate objects individually. Third, in the malignant cases such as moderately and poorly differentiated adenocarcinomas, the glandular structures are seriously degenerated, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (bottom left). Therefore, methods utilizing the prior knowledge with glandular regularity are prone to fail in such cases <ref type="bibr" target="#b34">[35]</ref>. In addition, the variation of tissue preparation procedures such as sectioning and staining can cause deformation, artifacts and inconsistency of tissue appearance, which can impede the segmentation process as well.</p><p>In the last few years, many researchers have devoted their efforts to addressing this challenging problem and achieved a considerable progress. However, obvious performance gap is still observed between the results given by the algorithms and annotations from pathologists. Broadly speaking, previous studies in the literature can be categorized into two classes: (1) pixel based methods. For this kind of method, various hand-crafted features including texture, color, morphological cues and Haar-like features were utilized to detect the glandular structure from histology images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref>; (2) structure based methods. Most of approaches in this category take advantage of prior knowledge about the glandular structure, such as graph based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>, glandular boundary delineation with geodesic distance transform <ref type="bibr" target="#b15">[16]</ref>, polar space random field model <ref type="bibr" target="#b17">[18]</ref>, stochastic polygons model <ref type="bibr" target="#b34">[35]</ref>, etc. Although these methods achieved promising results in cases of adenoma and well differentiated (low grade) adenocarcinoma, they may fail to achieve satisfying performance in malignant subjects, where the glandular structures are seriously deformed. Recently, deep neural networks are driving advances in image recognition related tasks in computer vision <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b2">3]</ref> and medical image computing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>. The most relevant study to our work is the U-net that designed a U-shaped deep convolutional network for biomedical image segmentation and won several grand challenges recently <ref type="bibr" target="#b29">[30]</ref>.</p><p>In this paper, we propose a novel deep contour-aware network to solve this challenging problem. Our method tackles three critical issues for gland segmentation. First, our method harnesses multi-level contextual feature representations in an end-to-end way for effective gland segmentation. Leveraging the fully convolutional networks, it can take an image as input and output the probability map directly with one single forward propagation. Hence, it's very efficient when applied to large-scale histopathological image analysis. Second, because our method doesn't make an assumption about glandular structure, it can be easily generalized to biopsy samples with different histopathological grades including benign and malignant cases. Furthermore, instead of treating the segmentation task independently, our method investigates the complementary information, i.e., gland objects and contours, under a multi-task learning framework. Therefore, it can simultaneously segment the gland and separate the clustered objects into individual ones, especially in benign cases with existence of touching glands. Extensive experimental results on the benchmark dataset of 2015 MICCAI Gland Segmentation Challenge corroborated the effectiveness of our method, yielding much better performance than other advanced methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>In this section, we describe in detail the formulation of our proposed deep contour-aware network for accurate gland segmentation. We start by introducing the fully convolutional network (FCN) for end-to-end training. Furthermore, we propose to harness the multi-level contextual features with auxiliary supervision for generating good likelihood maps of glands. Then we elaborate the deep contouraware network drawn from FCN for effective gland segmentation by fusing the complementary information of objects and contours. In order to mitigate the challenge of insufficient training data, we employ the transfer learning approach by exploiting the knowledge learned from cross domains to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">FCN with multi-level contextual features</head><p>Fully convolutional networks achieved the state-of-theart performance on image segmentation related tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref>. Such great success is mostly attributed to the outstanding capability in feature representation for dense classification. The whole network can be trained in an end-to-end (image-to-image) way, which takes an image as input and outputs the probability map directly. The architecture basically contains two modules including downsampling path and upsampling path. The downsampling path contains convolutional and max-pooling layers, which are extensively used in the convolutional neural networks for image classification tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>. The upsampling path contains convolutional and deconvolutional layers (backwards strided convolution <ref type="bibr" target="#b26">[27]</ref>), which upsample the feature maps and output the score masks. The motivation behind this is that the downsampling path aims at extracting the high level abstraction information, while the upsampling path predicting the score masks in a pixel-wise way.</p><p>The classification scores from FCN are established based on the intensity information from the given receptive field. However, the network with single receptive field size can-  not handle the large variation of gland shape properly. For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, a small receptive field (e.g., 150 × 150) is suitable for normal glands in benign cases, while malignant cases usually need a large receptive field since the gland shape in adenocarcinomas are degenerated and elongated, hence enclosing larger contextual information can help to eliminate ambiguity, suppress the interior tubular structure, and improve the recognition performance. Therefore, based on the FCN, we push it further by harnessing multi-level contextual feature representations, which include different levels of contextual information, i.e., intensities appearing in various sizes of receptive field. The schematic illustration of FCN with multi-level contextual feature representations can be seen in <ref type="figure" target="#fig_1">Figure 2</ref>. Specifically, the architecture of neural network contains a number of convolutional layers, 5 max-pooling layers for downsampling and 3 deconvolutional layers for upsampling. With the network going deeper, the size of receptive field is becoming larger. Derived from this, the upsampling layers are designed deliberately by considering the requirement of different receptive field sizes. They upsample the feature maps and make predictions based on the contextual cues from given receptive field. Then these predictions are fused together with a summing operation and final segmentation results based on multi-level contextual features are generated after softmax classification.</p><p>Direct training a network with such a large depth may fall into a local minima. Inspired by previous studies on training neural networks with deep supervision <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5]</ref>, weighted auxiliary classifiers C1-C3 are added into the network to further strengthen the training process, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. This can alleviate the problem of vanishing gradients with auxiliary supervision for encouraging the back-propagation of gradient flow. Finally, the FCN with multi-level contextual features extracted from input I can be trained by minimizing the overall loss L, i.e., a combination of auxiliary loss L a (I; W ) with corresponding discount weights w a and data error loss L e (I; W ) between the predicted results and ground truth annotation, as shown following:</p><formula xml:id="formula_0">L(I; W ) = λψ(W ) + a w a L a (I; W ) + L e (I; W ) (1)</formula><p>where W denotes the parameters of neural network and ψ(W ) is the regularization term with hyperparameter λ for balancing the tradeoff with other terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep contour-aware network</head><p>By harnessing the multi-level contextual features with auxiliary supervision, the network can produce good probability maps of gland objects. However, it's still quite hard to separate the touching glands by leveraging only on the likelihood of gland objects due to the essential ambiguity in touching regions. This is rooted in the downsampling path causing spatial information loss along with feature abstraction. The boundary information formed by epithelial cell nuclei provides good complementary cues for splitting objects. To this end, we propose a deep contour-aware network to segment the glands and separate clustered objects into individual ones.</p><p>The overview of the proposed deep contour-aware network can be seen in <ref type="figure" target="#fig_3">Figure 3</ref>. Instead of treating the gland segmentation task as a single and independent problem, we formulate it as a multi-task learning framework by exploring the complementary information, which can infer the results of gland objects and contours simultaneously. Specifically, the feature maps are upsampled with two different branches (green and blue arrows shown in the figure) in order to output the segmentation masks of gland objects and contours, respectively. In each branch, the mask is predicted by FCN with multi-level contextual features as illustrated in Section 2.1. During the training process, the parameters of downsampling path W s are shared and updated for these two tasks jointly, while the parameters of upsampling layers for two individual branches (denoted as W o and W c ) are updated independently for inferring the probability of gland objects and contours, respectively. Therefore, the feature representations through the hierarchical structure can encode the information of segmented objects and contours at the meantime. Note that the network with multiple tasks is optimized together in an end-to-end way. This joint multi-task learning process has several advantages. First, it can increase the discriminative capability of intermediate feature representations with multiple regularizations on disentangling subtly correlated tasks <ref type="bibr" target="#b40">[41]</ref>, hence improve the robustness of segmentation performance. Second, in the application of gland segmentation, the multi-task learning framework can also provide the complementary contour information that serves well to separate the clustered objects. This can improve the objectlevel segmentation performance significantly, especially in benign histology images where touching gland objects often exist. When dealing with large-scale histopathological data, this unified framework can be quite efficient. With one forward propagation, it can generate the results of gland objects and contours simultaneously instead of resorting to additional post-separating steps by generating contours based on low-level cues <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>In the training process, the discount weights w a from auxiliary classifiers are decreased until marginal values with the number of iterations increasing, therefore we dropped these terms in the final loss for simplicity. Finally the training of network is formulated as a per-pixel classification problem regarding the ground truth segmentation masks including gland objects and contours, as shown following:</p><formula xml:id="formula_1">L total (x; θ) = λψ(θ) − x∈X log p o (x, ℓ o (x); W o , W s ) − x∈X log p c (x, ℓ c (x); W c , W s )<label>(2)</label></formula><p>where the first part is the L 2 regularization term and latter two are the data error loss terms. </p><formula xml:id="formula_2">m(x) = 1 if p o (x; W o , W s ) ≥ t o and p c (x; W c , W s ) &lt; t c 0 otherwise<label>(3)</label></formula><p>where t o and t c are the thresholds (set as 0.5 in our experiments empirically). Then, post-processing steps including smoothing with a disk filter (radius 3), filling holes and removing small areas are performed on the fused segmentation results. Finally, each connected component is labeled with a unique value for representing one segmented gland.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Transfer learning with rich feature hierarchies</head><p>There is a scarcity of medical training data along with accurate annotations in most situations due to the expensive cost and complicated acquisition procedures. Compared with the limited data in medical domain, much more training data can be obtained in the field of computer vision. Previous studies have evidenced that transfer learning in deep convolutional networks can alleviate the problem of insufficient training data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>. The learned parameters (convolutional filters) in the lower layers of network are general while those in higher layers are more specific to different tasks <ref type="bibr" target="#b39">[40]</ref>. Thus, transfer the rich feature hierarchies with embedded knowledge learned from plausibly related datasets could help to reduce overfitting on limited medical dataset and further boost the performance. Therefore, we utilized an off-the-shelf model from DeepLab <ref type="bibr" target="#b6">[7]</ref>, which was trained on the PASCAL VOC 2012 dataset <ref type="bibr" target="#b14">[15]</ref>. Compared to the small scale dataset (a few hundred images) in gland segmentation, the PASCAL VOC dataset contains more than ten thousand images with pixellevel annotations. Leveraging the effective generalization ability of transfer learning in deep neural networks, we initialized the layers in downsampling path with pre-trained weights from the DeepLab model while the rest layers randomly with Gaussian distribution. Then we fine tuned the whole network on our medical task in an end-to-end way with stochastic gradient descent. In our experiments, we observed the training process converged much faster (about four hours) by virtue of the prior knowledge learned from rich dataset than random initialization setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and pre-processing</head><p>We evaluated our method on the public benchmark dataset of Gland Segmentation Challenge Contest in MIC-CAI 2015 (also named as Warwick-QU dataset) <ref type="bibr" target="#b33">[34]</ref>. The images were acquired by a Zeiss MIRAX MIDI slide scanner from colorectal cancer tissues with a resolution of 0.62µm/pixel. They consist of a wide range of histologic grades from benign to malignant subjects. It's worth noting that poorly-differentiated cases are included to evaluate the performance of algorithms. The training dataset is composed of 85 images (benign/malignant=37/48) with ground truth annotations provided by expert pathologists. The testing data contains two sections: Part A (60 images) for of-fline evaluation and Part B (20 images) for on-site evaluation. For the on-site contest, participants must submit their results to the organizers within an hour after data release. The ground truths of testing data are held out by the challenge organizers for independent evaluation. The final ranking is based on the evaluation results from testing data Part A and Part B with an equal weight 1 . To increase the robustness and reduce overfitting, we utilized the strategy of data augmentation to enlarge the training dataset. The augmentation transformations include translation, rotation, and elastic distortion (e.g., pincushion and barrel distortions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation details</head><p>Our framework was implemented under the open-source deep learning library Caffe <ref type="bibr" target="#b23">[24]</ref>. The network randomly crops a 480 × 480 region from the original image as input and output the prediction masks of gland objects and contours. The score masks of whole testing image are produced with an overlap-tile strategy. For the label of contours, we extracted the boundaries of connected components based on the gland annotations from pathologists, then dilated them with a disk filter (radius 3). In the training phase, the learning rate was set as 0.001 initially and decreased by a factor of 10 when the loss stopped decreasing till 10 −7 . The discount weight w a was set as 1 initially and decreased by a factor of 10 every ten thousand iterations until a marginal value 10 −3 . In addition, dropout layers <ref type="bibr" target="#b21">[22]</ref> (dropout rate 0.5) were incorporated in the convolutional layers with kernel size 1 × 1 for preventing the co-adaption of intermediate features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Qualitative evaluation</head><p>In order to illustrate the efficacy of our method qualitatively, some segmentation results of testing data are shown in <ref type="figure" target="#fig_5">Figure 4</ref> (benign cases) and <ref type="figure" target="#fig_6">Figure 5</ref> (malignant cases), respectively. For diagnosing the role of complementary contour information (i.e., contour-aware component), we also performed an ablation study and compared the performance of network relying only on the prediction of gland objects. Qualitative results are shown in <ref type="figure" target="#fig_5">Figure 4</ref> and <ref type="figure" target="#fig_6">Figure 5</ref> (middle row). From the segmentation results we can see that the method leveraging the multi-level contextual features without contour-aware can accurately segment the gland objects in both benign and malignant cases. However, there are some touching gland objects that cannot be separated. The situation is deteriorated when the touching objects are clustered together, as the case shown in the first column of <ref type="figure" target="#fig_5">Figure 4</ref>. In comparison, the deep contouraware network is capable of separating these touching gland objects clearly. This highlights the superiority of deep contour-aware network by exploring the complementary information under a unified multi-task learning framework qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Quantitative evaluation and comparison</head><p>The evaluation criteria in the grand challenge includes F1 score, object-level Dice index and Hausdorff distance, which consider the performance of gland detection, segmentation and shape similarity, respectively. Due to limited submissions in this challenge, we only submitted two entries to probe the performance of our method quantitatively. They were generated from the deep contour-aware network illustrated in <ref type="figure" target="#fig_3">Figure 3</ref> without and with fusing the contour-aware results, denoted as CUMedVision1 and C-UMedVision2, respectively. Detection For the gland detection evaluation, the metric F1 score is utilized, which is the harmonic mean of precision P and recall R, defined as:</p><formula xml:id="formula_3">F 1 = 2P R P + R , P = N tp N tp + N f p , R = N tp N tp + N f n<label>(4)</label></formula><p>where N tp , N f p , and N f n denote the number of true positives, false positives, and false negatives, respectively. According to the challenge evaluation, the ground truth for each segmented object is the object in the manual annotation that has maximum overlap with that segmented object. A segmented gland object that intersects with at least 50% of its ground truth is considered as a true positive, otherwise it's considered as a false positive. A ground truth gland object that has no corresponding segmented object or has less than 50% of its area overlapped by its corresponding segmented object is considered as a false negative. The detection results of different methods in this challenge are shown in <ref type="table">Table 1</ref>. Note that all the top 5 entries utilized methods based on the deep convolutional neural networks. Specially, the method from Freiburg designed a very deep U-shaped network and achieved the best results in several grand challenges <ref type="bibr" target="#b29">[30]</ref>. This method also explored the multi-level feature representations by concatenating feature maps from hierarchical layers and weighted loss was utilized to separate the touching objects.</p><p>Our submitted entry CUMedVision1 without fusing the contour-aware results surpassed all the other methods by a significant margin on testing data Part B, highlighting the strength of FCN with multi-level contextual feature representations for image segmentation. Our second submitted entry CUMedVision2 with contour-aware component achieved the best results on testing data Part A and competitive performance on Part B, which demonstrated the effectiveness of deep contour-aware network on this challenging problem. From <ref type="table">Table 1</ref>, we noticed that all methods yielded relatively lower performance on testing data Part B than Part A. This mainly comes from the different data dis-  tributions. We observed that benign cases make up about 55% in Part A while most of Part B are malignant cases. C-UMedVision2 achieved inferior performance (but still com-petitive compared to other methods) than CUMedVision1 on Part B. This arises from the fact that irregular structures in malignant cases can make the gland segmentation more challenging. For example, the low-contrast between interior tubular structure and stroma as a result of tissue degeneration may make methods relying on epithelial boundary cues more likely fail in such cases. Nevertheless, our deep contour-aware network ranked first regarding the detection results on all testing data. Segmentation Given a set of pixels G annotated as a ground truth object and a set of pixels S segmented as a gland object, Dice index is often employed for segmentation evaluation D(G, S) = 2(|G ∩ S|)/(|G| + |S|). However, this is not suitable for segmentation evaluation on individual objects. Instead, an object-level Dice index is utilized and defined as:</p><formula xml:id="formula_4">D object (G, S) = 1 2   n S i=1 ω i D(G i , S i ) + n G j=1ω j D(G j ,S j )  <label>(5)</label></formula><p>where S i denotes the ith segmented object, G i denotes a ground truth object that maximally overlaps S i ,G j denotes the jth ground truth object,S j denotes a segmented object that maximally overlapsG j ,</p><formula xml:id="formula_5">ω i = |S i |/ n S m=1 |S m |,ω j = |G j |/ n G n=1</formula><p>|G n |, n S and n G are the total number of segmented objects and ground truth objects, respectively.</p><p>The segmentation results of different methods are shown in <ref type="table" target="#tab_2">Table 2</ref>. We can see that our results CUMedVision2 achieved the best performance on testing data Part A and C-UMedVision1 outperformed all the other advanced methods on Part B. Similarly, there is around 3% improvement in Part A and 2% decrement on Part B in terms of object-level Dice index comparing our method with and without fusing contour-aware results. By examining some malignant cases, we observed that some inaccurate contours in interior structures may cause the deformed glands fragmented. One failure example is shown in <ref type="figure" target="#fig_6">Figure 5</ref> (fourth column),  which indicates that contours may over-split the object in some seriously degenerated cases. In summary, the deep contour-aware network achieved the best segmentation results regarding the object-level Dice index on all testing data, which evidenced the efficacy of our method consistently. Shape similarity The shape similarity is measured by using the Hausdorff distance between the shape of segmented object and that of the ground truth object, defined as:</p><formula xml:id="formula_6">H(G, S) = max{sup x∈G inf y∈S x − y , sup y∈S inf x∈G x − y }<label>(6)</label></formula><p>Likewise, an object-level Hausdorff is employed:</p><formula xml:id="formula_7">H object (G, S) = 1 2   n S i=1 ω i H(G i , S i ) + n G j=1ω j H(G j ,S j )  <label>(7)</label></formula><p>The shape similarity results of different methods are shown in <ref type="table" target="#tab_5">Table 3</ref>. Our results CUMedVision2 from deep contour aware network achieved the smallest Hausdorff distance (the only one less than 50 pixels), outperforming other methods by a significant margin on testing data Part A. In addition, the results of CUMedVision1 is comparable to the best results from ExB1 regarding the shape similarity on Part B. Overall results For the overall results, each team is assigned three ranking numbers for each part of testing data based on the three criteria mentioned above, one ranking number per criterion, using a standard competition ranking <ref type="bibr" target="#b0">[1]</ref>. The sum score of these numbers is used for the final ranking, i.e., a smaller score stands for better overall segmentation results. The final ranking can be seen in <ref type="table">Ta</ref>   performance in terms of overall results out of 13 teams, outperforming all the other advanced methods by a significant margin. One straightforward way to refrain from the sideeffect is to classify the histopathological images into benign and malignant cases first, then segment the image with contour-aware component or not depending on the classification results. This may enlighten other researchers for more advanced fusion algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Computation cost</head><p>It took about four hours to train the deep contour-aware network on a workstation with 2.50 GHz Intel(R) Xeon(R) E5-1620 CPU and a NVIDIA GeForce GTX Titan X GPU. Leveraging the efficient inference of fully convolutional architecture, the average time for processing one testing image with size 755 × 522 was about 1.5 seconds, which was much faster than other methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20]</ref> in the literature. Considering large-scale histology images are demanded for prompt analysis with the advent of whole slide imaging, the fast speed implies the possibility of our method in clinical practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this paper, we have presented a deep contour-aware network that integrates multi-level contextual features to accurately segment glands from histology images. Instead of learning gland segmentation in isolation, we formulated it as a unified multi-task learning process by harnessing the complementary information, which helps to further separate the clustered gland objects efficiently. Extensive experimental results on the benchmark dataset with rich comparison results demonstrated the outstanding performance of our method. In the future work, we will optimize the method and investigate its capability on large-scale histopathological dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of gland segmentation in benign (top row) and malignant (bottom row) cases. From left to right columns show the original images (stained with hematoxylin and eosin) and annotations by pathologists (individual objects are denoted by different colors), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The schematic illustration of FCN with multilevel contextual features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The overview of the proposed deep contour-aware network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>x is the pixel position in image space X , p o (x, ℓ o (x); W o , W s ) denotes the predicted probability for true label ℓ o (x) (i.e., the index of 1 in one hot vector) of gland objects after softmax classification, and similarly p c (x, ℓ c (x); W c , W s ) is the predicted probability for true label ℓ c (x) of gland contours. The parameters θ = {W s , W o , W c } of network are optimized by minimizing the total loss function L total with standard back-propagation.With the predicted probability maps of gland object p o (x; W c , W s ) and contour p c (x; W c , W s ) from the deep contour-aware network, these complementary information are fused together to generate the final segmentation masks m(x), defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Segmentation results of benign cases (from top to bottom): original images, segmentation results without contouraware, and segmentation results with contour-aware (different colors denote individual gland objects).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Segmentation results of malignant cases (from top to bottom): original images, segmentation results without contour-aware, and segmentation results with contour-aware (different colors denote individual gland objects).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The segmentation results of different methods in 2015 MICCAI Gland Segmentation Challenge.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>ble 4 (only top 10 entries are shown). Although there is a side-effect with contour-aware component in some malignant cases, our deep contour-aware network yielded the best Method Ranking score Sum score Final ranking F1 A F1 B Dice A Dice B Hausdorff A Hausdorff B</figDesc><table>CUMedVision2 
1 
3 
1 
5 
1 
6 
17 
1 
ExB1 
4 
4 
4 
2 
6 
1 
21 
2 
ExB3 
2 
2 
2 
6 
5 
5 
22 
3 
Freiburg2 [30] 
5 
5 
5 
3 
3 
3 
24 
4 
CUMedVision1 
6 
1 
8 
1 
8 
4 
28 
5 
ExB2 
3 
6 
3 
7 
2 
8 
29 
6 
Freiburg1 [30] 
8 
8 
6 
4 
4 
2 
32 
7 
CVIP 
7 
7 
7 
8 
7 
10 
46 
8 
CVML 
10 
9 
11 
9 
11 
7 
57 
9 
LIB 
9 
16 
9 
12 
9 
9 
64 
10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 :</head><label>4</label><figDesc>The final ranking of different methods in 2015 MICCAI Gland Segmentation Challenge (A and B denote the part of testing data, only top 10 entries are shown here).</figDesc><table>Method 
Part A 
Part B 
Freiburg2 [30] 
57.0932 
148.4630 
Freiburg1 [30] 
57.1938 
146.6065 
CUMedVision2 45.4182 
160.3469 
ExB1 
57.4126 
145.5748 
ExB2 
54.7853 
187.4420 
ExB3 
57.3500 
159.8730 
CUMedVision1 74.5955 
153.6457 
CVIP Dundee 
58.3386 
209.0483 
LIB 
101.1668 190.4467 
CVML 
155.4326 176.2439 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The shape similarity results of different methods in 2015 MICCAI Gland Segmentation Challenge.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Please refer to the challenge website for more details: http://www2.warwick.ac.uk/fac/sci/dcs/research/ combi/research/bic/glascontest/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://en.wikipedia.org/wiki/Ranking" />
		<title level="m">Standard competition ranking</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Color graphs for automated cancer diagnosis and grading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Altunbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cigir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sokmensuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gunduz-Demir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="665" to="674" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-for-low and lowfor-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic fetal ultrasound standard plane detection using knowledge transferred recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep contextual networks for neuronal structure segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic localization and identification of vertebrae in spine CT via a joint learning model with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In MICCAI</title>
		<imprint>
			<biblScope unit="page" from="515" to="522" />
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning and structured prediction for the segmentation of mass in mammograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dhungel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="605" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The use of morphological characteristics and texture analysis in the identification of tissue composition in prostatic neoplasia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Montironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Pathology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1121" to="1131" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lequan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Defeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A boosting cascade for automated detection of prostate cancer from digitized histology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomaszeweski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pathological prognostic factors in breast cancer. i. the value of histological grade in breast cancer: experience from a large study with long-term followup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Elston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Histopathology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analyzing tubular tissue in histopathological thin sections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fakhrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sporndly-Nees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Holm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L L</forename><surname>Hendriks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing Techniques and Applications (DICTA), 2012 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Colorectal carcinoma: pathologic aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Tatishchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of gastrointestinal oncology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="173" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel polar space random field model for the detection of glandular structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ilyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="764" to="776" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Histologic grading of prostate cancer: a perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Gleason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human pathology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="279" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic segmentation of colon glands using object-graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gunduz-Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tosun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sokmensuer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arX- iv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gleason grading of prostate tumours with max-margin conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Panagiotaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno>arX- iv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Structure and context in prostatic gland segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic segmentation with object clique potential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2587" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="556" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated analysis of pin-4 stained prostate needle biopsies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sabata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prostate Cancer Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gland segmentation in colon histology images: The GlaS Challenge Contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sanchez</surname></persName>
		</author>
		<idno>arX- iv:1603.00275</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A stochastic polygons model for glandular structures in colon histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2366" to="2378" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A novel texture descriptor for detection of glandular structures in colon histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Medical Imaging, pages 94200S-94200S. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multifeature prostate cancer diagnosis and gleason grading of histological images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tabesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teverovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Verbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kotsianti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Saidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1366" to="1378" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segmentation of intestinal gland images with iterative region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harpaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Microscopy</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="190" to="204" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
