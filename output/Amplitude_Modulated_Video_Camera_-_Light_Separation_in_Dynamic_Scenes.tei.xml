<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Amplitude Modulated Video Camera -Light Separation in Dynamic Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kolaman</surname></persName>
							<email>kolaman@post.bgu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
								<address>
									<postBox>POB 653</postBox>
									<postCode>8410501</postCode>
									<settlement>Beer-Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Lvov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
								<address>
									<postBox>POB 653</postBox>
									<postCode>8410501</postCode>
									<settlement>Beer-Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Hagege</surname></persName>
							<email>hagege@ee.bgu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
								<address>
									<postBox>POB 653</postBox>
									<postCode>8410501</postCode>
									<settlement>Beer-Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Guterman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
								<address>
									<postBox>POB 653</postBox>
									<postCode>8410501</postCode>
									<settlement>Beer-Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Amplitude Modulated Video Camera -Light Separation in Dynamic Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Controlled light conditions improve considerably the performance of most computer vision algorithms. Dynamic light conditions create varying spatial changes in color and intensity across the scene. These condition, caused by a moving shadow for example, force developers to create algorithms which are robust to such variations. We suggest a computational camera which produces images that are not influenced by environmental variations in light conditions. The key insight is that many years ago, similar difficulties were already solved in radio communication; As a result each channel is immune to interference from other radio channels. Amplitude Modulated (AM) video camera separates the influence of a modulated light from other unknown light sources in the scene; Causing the AM video camera frame to appear the same -independent of the light conditions in which it was taken. We built a prototype of the AM video camera by using off the shelf hardware and tested it. AM video camera was used to demonstrate color constancy, shadow removal and contrast enhancement in real time. We show theoretically and empirically that: 1. the proposed system can produce images with similar noise levels as a standard camera. 2. The images created by such camera are almost completely immune to temporal, spatial and spectral changes in the background light.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many image and video analysis algorithms demonstrate their performance in a dark room with no background lights <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>. Added dynamic background lights, which vary in space, time, and spectra, force these algorithms to compensate for the presence of additional lights. A camera, which creates the same image ( <ref type="figure">Fig.1)</ref> anywhere, could solve this problem and promote the common use of many state of the art algorithms in consumer products independent of dynamic light conditions.</p><p>An example for such dynamic light condition is a casted shadow upon an object ( <ref type="figure">Fig.2(a)</ref>). This generates video frames with changing intensity and color across space and time. Removing these casted shadows is done by separating the effect a singlet light source has on the scene. This light  source separation clears casted shadows, corrects color, and enhances contrast ( <ref type="figure">Fig.2(b)</ref>). Thus creating the same image under any light condition is done by light source separation. Light source separation on video can be performed by one of the two approaches -passive solution or active solution. Passive solutions compensate for the effect of the unwanted background lights by assuming a pre-known behavior of light upon objects in the scene. Spatially varying light conditions, for example, raise the challenge bar on color constancy algorithms <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b3">[4]</ref>[5] <ref type="bibr" target="#b5">[6]</ref>. Active solutions use a controlled light source in order to clear the effect of the background lights in the scene. Flash no flash <ref type="bibr" target="#b6">[7]</ref>[8] <ref type="bibr" target="#b8">[9]</ref>, for example, is a common active solution, which generates results from two captured frames: first with flash on, second with flash off. It assumes a static background <ref type="bibr" target="#b9">[10]</ref> and precise synchronization between flash and camera. Video color correction <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b11">[12]</ref> and shadow removal <ref type="bibr" target="#b12">[13]</ref> are usually treated as separate problems in the literature. Therefor shadow removal and color correction combined could enhance the performance of many video analysis methods such as object tracking <ref type="bibr" target="#b13">[14]</ref> and face detection <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions:</head><p>Our active solution proposes the following contributions:</p><p>• Light source separation: capture a frame that records the contribution of a single light source, and clear out the contributions of other lights, which change in space, time, or spectra (subsection 2.1).</p><p>• Video shadow removal, contrast enhancement, and color correction: perform all of the above using a low complexity algorithm in real time video(subsection 2.3).</p><p>• Preserving the appearance of an object independent of the light conditions: how to maximize reconstruction accuracy (by minimizing reconstruction error) and minimize noise (subsection 2.2). <ref type="figure" target="#fig_1">Fig.3</ref> shows a typical video scene with dynamic lights, shadows, and objects. The background light conditions may change unexpectedly in time and space creating nonuniform color and intensity on the moving object. A nonsynchronized light is situated on top of the video camera. We would like to know the impact of the non-synchronized light on the scene in order to ease color correction and remove shadows created by the other lights <ref type="figure" target="#fig_2">(Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Problem formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Modulated Light Source Separation</head><p>Our suggested solution is inspired by Amplitude Modulation (AM) radio (left column of <ref type="figure" target="#fig_3">Fig.5)</ref>, where the voice -modulated by a radio wave -is received by the tuned radio receiver, which filters out all the other frequencies. In photography the scene is modulated by the light source and received by the camera. By tuning the camera to a specific frequency, it should be possible to filter out the background lights from a modulated light oscillating at that frequency (right column of <ref type="figure" target="#fig_3">Fig.5</ref>). This system has the same limitations as AM radio -where every station must transmit at a separate frequency -the modulated light must oscillate at a unique frequency, different from the background lights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mathematical Formulation</head><p>Modulated light source converts the objects in the scene into modulated signals in time. Consider a light path (figure 3) that begins at the light sources, reflects from a patch, and is measured by a camera pixel. Light sources divide into two groups: 1.Background sources L b (t) with an unknown behavior in time and space 2. Ideal modulated light (nonsynchronized) source L m (f 1 , t), which is modeled by the following:</p><formula xml:id="formula_0">L m (f 1 , t) = a 0 + a 1 cos(2πf 1 t)<label>(1)</label></formula><p>where t represents time, a 0 is the constant intensity over time, a 1 is the amplitude of the main harmonic oscillating at f 1 = 1 T1 . Total light in the scene is reflected by the object patch generates a radiance I(t) equal to:</p><formula xml:id="formula_1">I(t) = C + A 1 · cos(2πf 1 t) + I b (t)<label>(2)</label></formula><p>where C depends on the patch reflectance and constant part of all the lights (modulated and background), radiance coefficient A 1 depends on the patch reflectance and intensity amplitude a 1 from Eq. (1), and I b (t) are the dynamic background lights. Modulated radiance A 1 · cos(2πf 1 t) has two important properties: 1. The frequency f 1 of cos(2πf 1 t) is the same as the frequency of the modulated light source. 2. A 1 is constant in time. These properties help to separate the influence of the modulated light (A 1 ) from the influence of the dynamic background lights (I b (t)) and constant part (C).</p><p>Radiance I(t) is sampled by a camera pixel at discrete times n ∈ {0, 1, ..., N − 1}:</p><formula xml:id="formula_2">X[n] = C + I b [nT s ] + A 1 · cos[2πf 1 nT s + ϕ 1 ]<label>(3)</label></formula><p>where C is the measured radiance 1 of constant part (modulated and background), I b [nT s ] is the intensity of the dynamic background radiance, T s = 1 fs is the sample time of the camera (the sample frequency f s is also referred as Frames Per Second (FPS)), A 1 is the amplitude of the modulated radiance, cos[2πf 1 nT s + ϕ 1 ] is a discrete sample of cos(2πf 1 t), and ϕ 1 is the unknown phase difference between modulated light and camera. Note that this sample model is ideal without noise artifacts, which will be discussed in subsection 2.2</p><p>The aim of the AM video camera system is to reconstruct A 1 using pre-known information on the frequency of the modulated light f 1 . This can be done by various methods, one of which is the inner product using a Finite Impulse Response (FIR) filter:</p><formula xml:id="formula_3">A 1 = 2 N N −1 n=0 X[n]e −i2πf1Tsn)<label>(4)</label></formula><p>This filter will attenuate all the terms in Eq. (3) except for the amplitude of the oscillating part at the target frequency f 1 i.e. A 1 . The purpose of the absolute value is get rid of the phase term e iϕ1 which is unknown. In summary AM video camera creates a single processed frame (last row of <ref type="figure" target="#fig_4">Fig.6</ref>) using Eq. (4) from N captured frames (third row of <ref type="figure" target="#fig_4">Fig. 6</ref>), which are illuminated by both modulated light (first row of <ref type="figure" target="#fig_4">Fig. 6</ref>) and a random background light (second row of <ref type="figure" target="#fig_4">Fig.6</ref>). The processed frameÂ 1 (right side of the last row of <ref type="figure" target="#fig_4">Fig. (6)</ref>) is a reconstruction of the amplitude of the AM light source A 1 up to a scale (second image from the left of the first row of <ref type="figure" target="#fig_4">Fig. (6)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Performance Analysis</head><p>Performance of this reconstruction system is measured by two factors: 1.Reconstruction Error 2.Noise levels. The real modulated light source has unwanted harmonicswhich change Eq. <ref type="formula" target="#formula_0">(1)</ref> into</p><formula xml:id="formula_4">L m (f 1 , t) = a 0 + a 1 cos(2πf 1 t) + ∞ k=2 a k cos(2πf k t) (5) where {a k } M</formula><p>k=2 are amplitudes of the parasitic harmonics, and {f k } M k=2 are their frequencies. This changes light radiance Eq. (2) into:</p><formula xml:id="formula_5">I(t) = C + M k=1 A k cos (2πf k t + ϕ k ) ,<label>(6)</label></formula><p>where the term C is a constant term of the illumination, {A k } M k=1 are the amplitudes of different harmonics (including modulated light source and background illuminations).</p><p>A camera captures N frames of the scene at a frame rate f s . Denote by {t n } N −1 n=0 the acquisition time of frame number n ∈ {0, 1, ..., N − 1}. The time between consequent frame acquisitions is not constant due to noise, and can be described by:</p><formula xml:id="formula_6">t n = t n−1 + (1/f s )(1 + q n ),<label>(7)</label></formula><p>where {q n } N −1 n=0 is a zero mean white Gaussian noise with variance σ 2 q . Non ideal radiance Eq. (6) change the ideal sampled signal, presented in Eq. (3), into</p><formula xml:id="formula_7">X[n] = C + M k=1 A k cos (ω k T s (n + r n ) + ϕ k ) + Z n ,<label>(8)</label></formula><p>where A 1 is the radiance amplitude of the modulated light from Eq. (3), {A k } M k=2 are the radiance amplitudes of background lights and parasitic harmonics of the modulated light, with their frequencies</p><formula xml:id="formula_8">f k = ω k 2π , {r n } N −1 n=0 is a</formula><p>Gaussian random walk process defined by r n = n m=0 q m , {ϕ k } M k=2 are the random phases of the additional harmonics distributed uniformly on the interval [0, 2π] and independent of {r n } N −1 n=0 2 , and Z n is a zero mean additive noise. Reconstruction error is important in many applications such as spectral measurements and radiance evaluations, which gather information from a digital camera or a photometric sensor. This creates the need to evaluate the reconstruction error of our method in order to justify its use in precise measurement tools. The reconstruction error can be measured by:</p><formula xml:id="formula_9">M SE = E Â 1 − A 1 2 (9)</formula><p>where M SE is the Mean Square Error between the reconstructed signalÂ 1 , and the amplitude intensity A 1 .</p><p>Precise derivation of the MSE is difficult due to the non linearity of the reconstruction formula (caused by the absolute value operation). A simple bound, however, on the MSE can be derived:</p><formula xml:id="formula_10">E Â 1 − A 1 2 ≤ M SE C + M SE A1 + M SE A k + 2 N E Z 2 n<label>(10)</label></formula><p>where M SE C is due to the constant term C, M SE A1 is due to the modulated light harmonic</p><formula xml:id="formula_11">{A 1 , f 1 } , M SE A k due to all the other harmonics {A k , f k } M k=2 , and E[Z 2 n ]</formula><p>is the variance of the additive noise in Eq. <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_12">• M SE C = C 2 2 N · sin(πN f1/fs) sin(πf1/fs) 2 . • M SE A k = M k=2 A 2 k (I + k + I − k ) where I ± k = 1 N 2 N −1</formula><p>n,m=0 e i2π(n−m)(f k ±f1)Ts−2|n−m|(σqπf k Ts) 2 . It can be shown that I ± k decays as O(1/N ) if σ q &gt; 0. If σ q = 0 then I ± k simplifies to:</p><formula xml:id="formula_13">I ± k = 1 N · sin(πN (f1±f k )/fs) sin(π(f1±f k )/fs) 2 . • M SE A1 = A 2 1 (I 1 + I + 1 ) where I 1 = I − 1 + 1 − 2 N N −1 n=0 e −2n(πσqf1Ts) 2 is due to the unwanted phase noise. I 1 → 0 as (σ q f 1 T s ) 2 N → 0 and I 1 → 1 as (σ q f 1 T s ) 2 N → ∞.</formula><p>Reconstruction error Eq. (10) explains several visible phenomenons of its graph <ref type="figure" target="#fig_8">(Fig. 10</ref>): 1. Local minimums are generated by the variance of M SE c → 0 when N · f 1 /f s ∈ N and f 1 /f s is far from an integer. 2. MSE diminishes as N gets larger up to a limit.</p><p>The model can be generalized to contain additional random processes (such as white/colorful noise), not necessary periodic. If,for instance, a wide-sense-stationary noise {b[n]} with a power spectral density S b (θ) is added to X[n] then its contribution to the MSE is</p><formula xml:id="formula_14">M SE b = 1 2π [−π,π] S b (θ)K N (2πf 1 T s − θ)dθ,<label>(11)</label></formula><p>where K N (θ) = 1 N · sin(θN/2) sin(θ/2) 2 . If the noise {b[n]} does not contain high spectral power near the frequency θ = 2πf 1 T s then its contribution to the MSE will be small.</p><p>Noise Level is one of the important factors for measuring the quality of an output color image <ref type="bibr" target="#b15">[16]</ref>.</p><p>Amplitude of the modulated intensity A 1 should be bigger than the camera noise levels in order to have no apparent noise artifacts <ref type="bibr" target="#b16">[17]</ref>; meaning the radiance of the AM light should be in the same order of magnitude as the background light. In addition the noise level is inverse-proportional to the number of captured frames N . Apparent noise levels depend on intensity relation between AM light and background lights.</p><p>Z n is a zero mean additive noise. Its standard deviation (STD) depends on many factors, such as the temperature, exposure time and the average light intensity during the frame acquisition. Since only the light intensity changes from one frame to another, we can define a function g(µ) to be the conditional STD of Z n given that X[n] − Z[n] is equal to µ. Examples of the functions g(µ) for red, green and blue pixels are shown in <ref type="figure">Fig. 11</ref>. The conditional mean of Z n given that (X[n] − Z[n]) is equal to µ, is zero. The noise terms {Z n } N −1 n=0 are statistically independent of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Example Applications</head><p>Background lights generate shadows, which are captured by the camera. A light source situated on-top of the camera creates shadows that do not appear in the frame because this light path is almost aligned with the optical axis of the camera <ref type="figure" target="#fig_2">(Fig.4)</ref>. Thus AM video camera removes shadows by generating an image influenced only by the modulated light (middle images of <ref type="figure" target="#fig_6">Fig. 8(b) and 8(d)</ref>).</p><p>Background light removal causes the output frames of AM video camera to have a single light source. This single light source helps color constancy algorithms to perform better since most of them assume a dominant light in the scene <ref type="bibr" target="#b5">[6]</ref>. Am video frame fixate the light source in the scene -reducing the need for sophisticated color constancy/white balance algorithms -and may be replaced by a fixed color correction matrix. Color correction results are better in AM video frames compared to the standard camera (top two images in <ref type="figure" target="#fig_6">Fig. 8(d)</ref> and bottom image at <ref type="figure" target="#fig_6">Fig. 8(d)</ref>.</p><p>Uneven light across the scene may generate low contrast at some parts of the image even though the entire dynamic range of the camera is used (two button images of <ref type="figure" target="#fig_6">Fig. 8(c)</ref>). Uniform light conditions, generated by the removed background lights and shadows, improve local image contrast (two button images of <ref type="figure" target="#fig_6">Fig. 8(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Live Modulated Light Source Separation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">System Overview of the Prototype</head><p>An online video system was built using off the shelf products, and includes three parts -usb3 camera, laptop, and a modulated light source. The laptop controls the AM light, captures the frame from the camera, and performs the post-processing. The laptop sets the required frequency and amplitude of the AM light <ref type="figure" target="#fig_5">(Fig.7)</ref> by configuring a PWM sine generator, which is an input to a driver board of the 100W LED light. The system is capable of generating sine waves from 1 Hz to 600 Hz with up to 256 points per cycle and varying amplitude from 10% to 100% of the 100W LED light source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Constraints</head><p>AM light frequency should be unique enough to make sure that the reconstructed frame contains only the AM light source and no background lights. The required modulated light f 1 is set according to the available frequencies in the captured scene. The system finds the required frequency f 1 by capturing a set of N frames -prior to turning on the AM light -and finding the minimal power on the F F T {X[n]}.</p><p>Camera exposure was set to 1 ms in order to be able to capture up to 1000 FPS (our camera could effectively reach 700 FPS). This fast exposure time forces the lens aperture to open at its maximum value -in order to get enough light into the camera. Frames used in all of the experiments were captured by a camera with 400 frames per second and a resolution of 640x480.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>This section presents experimental results showing applications and analysis of AM video camera output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Applications</head><p>The application part shows color correction, shadow removal, and contrast enhancement under different types of background illumination conditions and objects. Dynamic background light varied between natural (sunlight) and artificial (tungsten), while scene type varied between static and dynamic -shadows and objects. The analysis part assessed noise and reconstruction error on the output. AM video camera output shows several example applications in <ref type="figure" target="#fig_6">Fig.8</ref>, where captured frames are presented in <ref type="figure" target="#fig_6">Fig.  8(a)</ref> and <ref type="figure" target="#fig_6">Fig. 8(c)</ref> and processed frames are presented in <ref type="figure" target="#fig_6">Fig. 8(b)</ref> and <ref type="figure" target="#fig_6">Fig. 8(d)</ref>. Contrast enhancement occurs when the input image has high intensity differences between the modulated light and the background light as in the second row of <ref type="figure" target="#fig_6">Fig. 8(a)</ref> and fourth and third rows of <ref type="figure" target="#fig_6">Fig. 8(c)</ref>. Color correction occurs when the background light and the modulated light have large color differences, as in the last row of <ref type="figure" target="#fig_6">Fig. 8(a)</ref> and first and second row of <ref type="figure" target="#fig_6">Fig. 8(c)</ref>. Shadow caused by the background lights are removed in all of the resulting AM frames. 3 . <ref type="bibr" target="#b2">3</ref> Real time videos available in the paper site</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Measurements</head><p>This part compares noise and reconstruction error of the AM video camera, presented in 2.2, with the standard camera. A static scene was captured at a frame rate of f s = 704Hz with different constant light illuminations and modulated light with a frequency of f 1 = 105Hz. <ref type="figure" target="#fig_7">Fig.9</ref> presents the values of a single pixel as a function of time, and demonstrates how its values are only due to the modulated illumination. The blue curve shows the pixel value captured by the camera when no background lights are present. The green and the red curves show the processed pixel's valuesÂ 1 . N = 20 frames were used to calculate the green curveÂ 1 , and N = 67 frames was used to calculate the red curve. For these values of N the value of N f 1 /f s is close to an integer and the value of M SE C in Eq. (10) is very small. Note N = 67 and N = 20 were chosen according to the local minima of the relative root mean squared error (RRMSE) defined by <ref type="figure" target="#fig_8">(Fig. 10)</ref>.</p><formula xml:id="formula_15">E[Â1−A1] 2 A1</formula><p>Performance of the reconstruction system was evaluated by the following features:  • Standard deviation of a reconstructed pixelÂ 1 when the scene is static.</p><p>• Coefficient of variation defined by ST D(Â1) µÂ 1 , where µÂ 1 is the expected value ofÂ 1 . • RRMSE <ref type="figure" target="#fig_8">Fig. 10</ref> shows the RRMSE as a function of N (number of frames for calculation ofÂ 1 ). The green curve represents the empirical RRMSE, The red curve represents a theoretical upper bound -based on Eq. (10) -and the blue curve represents the empirical coefficient of variation. To calculate the theoretical upper bound for the RRMSE the following parameters were estimated from the blue curve in <ref type="figure" target="#fig_7">Fig. 9</ref>: C A1 = 1.23, A2 A1 = 0.13, A3 A2 = 0.03, where A 2 , A 3 are the amplitudes of additional harmonics of our modulated illumination. The frequencies of these harmonics are f k = k · f 1 , for k = 2, 3. The value of (σ q f 1 T s ) 2 is taken to be 1.6 · 10 −3 . The function g(µ) used to estimate the last term of Eq. (10) is the g(µ) for blue pixels shown in <ref type="figure">Fig. 11</ref> Since σ q is not zero,Â 1 tends to zero and the RRMSE tends to 1 as N tends to infinity. The reason is because <ref type="figure">Figure 11</ref>. In red/green/blue: the STD of a red/green/blue pixel in a standard camera as a function of its mean value. In black and magenta: the STD of a both red, green and blue pixels in AM video camera (the STD ofÂ1) as a function of its mean value, when no background illumination is present. in the inner product</p><formula xml:id="formula_16">N −1 n=0 X[n]e −i2πf1Tsn the component of X[n]</formula><p>that should be proportional to e i2πf1Tsn contains a phase noise. That noise makes this component to be in the same phase as e i2πf1Tsn for some times and in the opposite phase for other times. The sum of these terms would cancel, leading the whole sum to grow slower then N . The multiplication of the whole sum by 2/N makes it tend to zero as N tends to infinity. If, on the other hand, σ q = 0 thenÂ 1 would tend to A 1 and the RRMSE would tend to zero, since the upper bound in Eq. (10) would tend to zero. <ref type="figure">Fig. 11</ref> shows the STD of the pixels as a function of their mean value for standard and AM video camera. The red, green and blue curves represent the function g(µ) (for red, green and blue pixels), which is the STD of a pixel in a standard camera given that its mean value is µ. The STD ofÂ 1 are shown in the magenta curve (for N = 20) and the black curve (for N = 67) for the scenario when there is no background illumination. These graphs are the same for the red,green and blue colors, and graw linearly as a function of µ. This can be explained by Eq. (10), since all terms in that bound, except for the last term that is much smaller than the others, are proportional to A 2 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Our AM video camera system demonstrates a low complexity and effective system performing shadow removal, color correction, and contrast enhancement on real time video frames <ref type="figure" target="#fig_6">(Fig. 8)</ref>. Performance analysis demonstrated how precision of the AM video camera grows as number of sampled frames N is higher up to a limit -which was proven analytically and experimentally <ref type="figure" target="#fig_8">(Fig. 10)</ref>. AM video camera is unaffected by the intensity of the background assuming the modulated amplitude A 1 is bigger than the camera noise values. In practice, AM light intensities surpassing 20 percent of background light level gave appreciable results. For cases of very high intensities of background light our  system could work in sunlight using method such as <ref type="bibr" target="#b17">[18]</ref>, with some minor adjustments to work with a rolling shutter camera. Reconstruction analysis proved that there is an analytical and experimental upper bound to the MSE <ref type="figure" target="#fig_8">(Fig.  10</ref>). Noise levels of AM video camera resemble the noise level of the standard camera <ref type="figure">(Fig. 11</ref>) and get closer to its performance as N grows. This paper differs from Schechner et. al <ref type="bibr" target="#b18">[19]</ref> by assuming a dynamic background light. Our work is closely related to <ref type="bibr" target="#b19">[20]</ref>, but in our case non-synchronized light was used without any spatial patterns. It also improves <ref type="bibr" target="#b20">[21]</ref> by capturing dynamic video scenes outside the lab, using less frames to generate a single reconstruction, and no need for synchronization between the light source and the camera.</p><p>Flash no flash assume constant illumination between two subsequent frames. Therefore when such techniques apply to indoor video with changing background lights, such as incandescent/flourecent light, they may produce a video sequence with 1. Low contrast output <ref type="figure" target="#fig_0">(Fig. 12</ref>) -due to negative change in the background lights between two subsequent frames.</p><p>2. Inconsistent flickering video <ref type="figure" target="#fig_1">(Fig. 13</ref>) -due to incon-sistent changes in the background lights between two subsequent frames.</p><p>The AM camera technique actively eliminate the influence of illumination changes. Therefore the video produced by the AM camera is much more consistent than the one produced by flash no flash techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future work</head><p>The highlights of the AM video camera are:</p><p>• We used a principle from the AM radio field and applied it to computational photography. AM demodulation filtered out all of the background lights and reconstructed the scene illuminated only by the AM light source.</p><p>• Light source separation can be used as an application for shadow removal, color correction, and contrast enhancement. The shadows in the reconstructed image are minimized by placing the modulated light source near the camera. Color correction is easier to perform because the reconstructed AM frame contains a single light source. Contrast is enhanced due to uniform light conditions in the AM frame.</p><p>• A highly parallelizable algorithm, which has good potential to work on currently available smart phones, was presented. The algorithm works separately on each pixel and its results are non-dependent on neighboring pixels.</p><p>• The presented method requires no synchronization between light and video camera, thus needing less hardware, and is simpler to implement.</p><p>• Prototype was built and tested extensively under different light conditions showing real time video color correction, shadow removal, and contrast enhancement.</p><p>The suggested technique can be implemented directly on available hardware using software alone. Future work will include methods for integration of the processing stage into the sensor, improved control of the AM light source, and mitigating the edge artifacts 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgment</head><p>The authors would like to thank undergraduate students Boris Bessarabov and Eitam Man for their help in building the modulated light system used in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>AM video camera enable working in stable illumination conditions independent of the background illumination (a) Image with shadows, low contrast, and color cast. (b) Proposed method with no shadows, correct color (white wall), and improved contrast.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Pixel measurement is a superposition of the incident light of the modulated light source and the background light.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Complex light conditions create multiple shadows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>In radio communication (Left Side) the AM Carrier is used to modulate the Sound signal into a Amplitude Modulated (AM) Radio. Proposed method (Right Side) uses a Modulated Light to illuminate a Patch Reflectance in the scene and create a Reflected Light Onto a Single Pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Data of a patch/pixel across time. A patch affected by the modulated light Iω(t)(first line) having an amplitude A1, is also affected by background lights I b (t)(second line) and captured by a video camera I(t)(third line) in time. The captured values in time are processed into a single reconstructed amplitudeÂ1(last line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The prototype -System diagram of the modulated light source.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>(a) and (c) Original scene (b) and (d) AM video camera frames performing real time video contrast enhancement, color correction, and shadow removal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Pixel values for different times. In blue: the captured by the camera pixel values, without background light. In green and red: the processed pixel valuesÂ1 for N = 20 and N = 67, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>The green curve: the empirical RRMSE ofÂ1. The red curve: a theoretical upper bound for the RRMSE ofÂ1. The blue curve: the empirical coefficient of variation ofÂ1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>A sampled pixel in time showing flash points Lmax and no-flash points Lmin creating an image with low contrast F nf (middle bottom part) using Flash No-Flash method. A spectral analysis of the sampled pixel is shown at bottom right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Inconsistency of Flash No-Flash when the background light changes compared with our method</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Camera can measure radiance by normalizing its measurements with Exposure Value(EV)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For each pair of phases ϕ j , ϕ k the expected value E e i(ϕ j −ϕ k ) is zero.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Real time videos available in the paper site</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Homogeneous codes for energy-efficient illumination and imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Backscatter compensated photometric stereo with 3 sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tsiotsios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Angelopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2259" to="2266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exemplar-based color constancy and multiple illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R V</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="860" to="873" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Light mixture estimation for spatially varying white balance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating combinational illumination estimation methods on real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Funt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1194" to="1209" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computational color constancy: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Saliency detection with flash and no-flash image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="110" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Preserving natural scene lighting by strobe-lit video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gotchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">919</biblScope>
			<biblScope unit="page" from="939" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Same frame rate ir to enhance visible video conference lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gunawardane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2011 18th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flash cut: Foreground extraction with flash and no-flash image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Illuminant chromaticity from image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3320" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Color constancy with spatio-spectral statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1509" to="1519" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic feature learning for robust shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1939" to="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting moving shadows: algorithms and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mikic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="918" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grassmann averages for scalable robust pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3810" to="3817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quaternion structural similarity: a new quality index for color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yadid-Pecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1526" to="1536" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Noiseoptimal capture for high dynamic range photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Homogeneous codes for energy-efficient illumination and imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;toole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Achar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A theory of multiplexed illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="808" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiplexed illumination for scene recovery in the presence of global illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="691" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Light source separation from image sequences of oscillating lights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hagege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electrical &amp; Electronics Engineers in Israel (IEEEI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>2014 IEEE 28th Convention of</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
