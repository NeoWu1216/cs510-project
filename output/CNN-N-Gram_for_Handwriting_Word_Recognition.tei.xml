<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CNN-N-Gram for Handwriting Word Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arik</forename><surname>Poznanski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<email>wolf@cs.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CNN-N-Gram for Handwriting Word Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given an image of a handwritten word, a CNN is employed to estimate its n-gram frequency profile, which is the set of n-grams contained in the word. Frequencies for unigrams, bigrams and trigrams are estimated for the entire word and for parts of it. Canonical Correlation Analysis is then used to match the estimated profile to the true profiles of all words in a large dictionary. The CNN that is used employs several novelties such as the use of multiple fully connected branches. Applied to all commonly used handwriting recognition benchmarks, our method outperforms, by a very large margin, all existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The most prominent method in the current application of deep learning to computer vision is the Convolutional Neural Network (CNN) <ref type="bibr" target="#b29">[30]</ref>. Developed initially for the task of reading handwritten digits, this method is now utilized for almost every perceptual task relating to image and video data. In many such tasks, CNNs lead to state of the art performance. It is, therefore, somewhat surprising that in the field of handwriting recognition, CNNs are currently not leading the performance charts.</p><p>CNNs are trained in a supervised way. The first question when training it, is what type of supervision to use. Previous applications of CNNs in the field of printed word recognition <ref type="bibr" target="#b24">[25]</ref> have demonstrated that a word based encoding is preferable to encoding using bag-of-n-grams. The results presented in this work show that, at least for handwriting recognition, a very effective system can be built using an attributes based encoding, in which the input image is described as having or lacking a set of n-grams in some spatial sections of the word.</p><p>The attributes used in our work are heavily based on the earlier work of Almazán et al. <ref type="bibr" target="#b2">[3]</ref>. These binary attributes check whether the word contains a specific n-gram in some part of the word. For example, one such property may be: Does the word contain the bigram "ou" in the second half of the word? If the examined word is "ingenious" then the answer is positive, whereas if the checked word is "outstanding" the answer is negative.</p><p>While the original work of <ref type="bibr" target="#b2">[3]</ref> uses SVMs over Fisher Vectors of SIFTs, we employ CNNs directly over raw pixel values. Moreover, in order to reach high levels of performance, multiple novelties had to be incorporated. These include the creation of specialized sub-networks that focus on subsets of the set of 600-1200 attributes used (this number depends on the size of the alphabet).</p><p>The obtained network is fairly large, and training it on the relatively small datasets available for handwriting recognition is challenging. We incorporate gradual training and other ideas in order to tackle this.</p><p>The resulting method is extremely potent. The same architecture applied to all significant handwriting benchmarks that we are aware of, achieves a very sizable improvement over state of the art. While the network was not designed for printed text, it also achieves state of the art results on the Street View Text (SVT) benchmark <ref type="bibr" target="#b54">[54]</ref> and results comparable to state of the art on IIIT5K <ref type="bibr" target="#b36">[37]</ref> benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Currently, the leading handwriting recognition benchmarks are IAM, RIMES and IFN/ENIT. The performance charts for these datasets are currently dominated by the use of Recurrent Neural Networks (RNNs) and its extensions such as Long-Short-Term-Memory (LSTM) networks, Hidden Markov Models (HMMs), and various combinations of these methods. CNNs are absent from current results.</p><p>The IAM dataset <ref type="bibr" target="#b32">[33]</ref> contains images of handwritten English words. The current state of the art on the IAM benchmark is a system by Bluche et al. <ref type="bibr" target="#b9">[10]</ref> which uses the ROVER voting scheme <ref type="bibr" target="#b14">[15]</ref> for combining four models. Two of them are based on Bidirectional Long Short-Term Memory (BDLSTM) RNNs, and the other two are based on deep Multi-Layer Perceptrons (MLPs).</p><p>A close second to the state of the art method on IAM is a system by Doetsch et al. <ref type="bibr" target="#b10">[11]</ref>, which uses an LSTM-RNN with an additional parameter that controls the shape of the squashing functions in the gating units.</p><p>The RIMES dataset <ref type="bibr" target="#b4">[5]</ref> is composed of images of handwritten French words. The current state of the art on RIMES is a system by Menasri et al. <ref type="bibr" target="#b33">[34]</ref> that uses a single RNN. To further improve their state of the art results, they also present a system that uses an optimized combination of seven recognizers, including one grapheme based MLP-HMM, two variants of sliding window based GMM-HMMs, and four variants of their proposed RNNs. IFN/ENIT <ref type="bibr" target="#b41">[41]</ref> is a dataset of Arabic handwritten word images. This dataset has four different test scenarios, named "abc-d", "abcd-e", "abcde-f" and "abcde-s". The current state of the art method depends on the test scenario. In scenario "abcde-f", the current state of the art is a system by Graves and Schmidhuber <ref type="bibr" target="#b12">[13]</ref>, which uses a hierarchy of multidimensional LSTM-RNNs. In all other IFN/ENIT scenarios, the leading method is the one by Stahlberg and Vogel <ref type="bibr" target="#b49">[49]</ref>, which uses fully connected deep neural networks for optical modeling with features extracted from raw pixel gray-scale intensity values of foreground segments. A comparable method, with regards to overall performance, is an HMM system by Azeem and Ahmed <ref type="bibr" target="#b5">[6]</ref>.</p><p>The work by Almazán et al. <ref type="bibr" target="#b2">[3]</ref> is closely related to our work. Their method encodes the input word image as Fisher Vectors (FV) <ref type="bibr" target="#b43">[43]</ref>, i.e., as an aggregation of the gradients of a Gaussian Mixture Model (GMM) over some low-level descriptors, SIFT in this case. It then trains a set of linear SVM classifiers, one per each binary attribute contained in a set of word properties. Canonical Correlation Analysis (CCA) is used to link the vector of predicted attributes and the binary attributes vector generated from the actual word. The CCA method finds a common vector subspace where the predicted attributes vector and binary attributes vector are naturally comparable. To find the transcription, a simple nearest neighbor search is made in the transformed lexicon space, which was projected to the common subspace.</p><p>Almazán et al.'s method is currently not among the state of the art methods in handwriting recognition, on any of the datasets. In our work, using almost the same binary attributes as in <ref type="bibr" target="#b2">[3]</ref>, we replace the FV based classifiers with a specific type of CNN. Unlike <ref type="bibr" target="#b2">[3]</ref>, we train over raw pixel values and, in addition, we benefit from using a single classifier that predicts all the binary attributes, instead of using one classifier per attribute. Instead of relying on the predictions, we apply CCA to the representation vector obtained from one layer below the prediction layers. Using our method, we obtain a sizable improvement over all commonly used handwriting recognition benchmarks, halving, in almost all cases, the best reported error rate.</p><p>Attributes based methods are now commonplace throughout computer vision. As an example, Movshovitz-Attias et al. <ref type="bibr" target="#b37">[38]</ref> predict a category from a given hierarchical set of categories. Instead of learning to predict the category directly, they have flattened the hierarchy and learned to predict all the categories to which the object belongs. This conversion from single label to multiple binary attributes enables them to better exploit the data since every low level category is also an example of its ancestors categories. This is similar in spirit to the attributes based model we employ.</p><p>Another recent use of attributes prediction using CNNs is the work by Zhang et al. <ref type="bibr" target="#b58">[57]</ref>. In their work they propose a method for inferring human attributes such as gender, hair style, cloth style, and more from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. They propose a method which combines part-based models and deep learning by training a pose-normalized CNNs. They use a network which shares the layers including the first fully connected layer, and only then they branch out to a fully connected layer per attribute. In our network we only share the conventional layers, and use a three layers deep fully connected layers per each group of attributes.</p><p>Another related work is the work by Jaderberg et al. <ref type="bibr" target="#b24">[25]</ref>, which uses CNNs trained on synthetic data for Scene Text Recognition. Although they get state of the art results on the SVT <ref type="bibr" target="#b54">[54]</ref> dataset, their method is not evaluated on handwriting recognition since, unlike for scene text, synthetic fonts lack the full variability which handwritten text poses. In their work, three different CNNs are presented, one of which is trained on words encoded as bag-of-n-grams. That n-gram based CNN achieves inferior results, compared to their other CNNs, on both SVT and SVT-50. Our n-gram based method, when applied outside our main scope of handwriting recognition, achieves results better than their best network on SVT. This is done, when training on the same synthetic dataset used for training the system of <ref type="bibr" target="#b24">[25]</ref>.</p><p>We identify four differences that we believe make our method work better than Jaderberg at al.'s. The first is our use of CCA to factor out dependencies between attributes. The second difference is that we take into consideration the spatial location of the n-gram inside the word. The third difference is our network structure, which is deeper and uses multiple parallel fully connected layers, each handling a different set of attributes. Lastly, our method uses considerably less n-grams than the list of 10,000 n-grams they have used.</p><p>The architecture we propose employs multiple fully connected "arms" diverging from the CNN body. This is done in order to create spatial and type of n-gram specialization. Spanning multiple branches from a neural network in order to support more effective learning is done in <ref type="bibr" target="#b50">[50]</ref>, where intermediate networks were added in order to support the training of a very deep network. Unlike our usage, these sub-networks were not used during test time.</p><p>The success of the training process relies on gradual training of the network, adding one such arm at each training phase. Other forms of gradual learning have been pro-posed in the literature. For example, the work of <ref type="bibr" target="#b24">[25]</ref> mentioned above, learns the lexicon gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In offline handwriting recognition, we are given two disjoint sets, which we name train and test. Each of the sets contains pairs (I, t) such that I is an image and t is its textual transcription. The goal is to build a system which, given an image, produces a prediction of the image transcription. The construction of the system is done using information from the train set only.</p><p>In order to evaluate the method's performance, we apply it to the test images and for each image compare the predicted transcription with the actual image transcription. The result of such an evaluation can be reported by one of several related measures. These include Word Error Rate (WER), Character Error Rate (CER), and Accuracy (1-WER). WER is the ratio of the reading mistakes, at the word level, among all test words. CER measures the Levenshtein distance normalized by the length of the true word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">From a text word to attributes vector</head><p>Our method leverages the inherent structure of a textual label by considering common attributes that are shared between different words. The attributes that we use were named Pyramidal Histogram of Characters (PHOC) in <ref type="bibr" target="#b2">[3]</ref>.</p><p>The simplest attributes are based on unigrams and pertain to the entire word. An example of a binary attribute in English is "does the word contain the unigram 'A'?" There are as many such attributes as the size of the character set of the benchmark that we tackle. The character set may contain lower and upper case Latin alphabet, digits, accented letters (e.g.é,è,ê,ë), Arabic alphabet, etc. We call the attributes that check whether a word contains a specific unigram, Level-1 unigram attributes.</p><p>We define Level-2 unigram attributes as attributes that observe whether a word contains a specific unigram in the first or second half of the word, e.g., -"does the word contain the unigram 'A' in the first half of the word?". For example, the word "BABY" contains the letter 'A' in the first half of the word ("BA"), but doesn't contain the letter 'A' in the second half of the word ("BY").</p><p>To address words with an odd number of characters and other fractional cases, we need to properly define the decision rule used to determine whether a letter appears in a specific section of the word. The decision is made purely using the word text, without any image involved. The process does not require any information about the location of the letters in the image.</p><p>The decision rule is that a word's letter is part of a specific section of the word if that section contains at least 50% of the letter location. For example, in the word "KID", the first half of the word contains 1.5 letters, including the letter 'K' and 50% of the letter 'I'. Therefore, according to our definition, the first half of the word "KID" contains the letters 'K' and 'I'. Similarly, The second half of the word "KID" contains the letter 'D' in full, and exactly 50% of the letter 'I'. Therefore, the second half of the word "KID" contains both 'D' and 'I'. While it may seem as if we are assuming that every letter has an equal width, we do not explicitly cut the images. As long as we are consistent with our definition of the attributes, the supervision given to the network is consistent.</p><p>Similarly, Level-3 unigram attributes are also defined, breaking the word into three equal parts. In addition, Level-2 bigram attributes are defined as binary attributes that indicate whether the word contains a specific bigram. We also include extensions to trigrams. <ref type="figure" target="#fig_0">Fig. 1</ref> demonstrates the attributes associated with a word.</p><p>Other attributes are possible, but we did not find these to be helpful. For example, we tried attributes pertaining to the first letters or to the end of the word, e.g., "does the word end with an 'ing'?"</p><p>Ideally, enough binary features would be defined such that every word in our lexicon has a unique attributes vec-tor. We will later use this bijective mapping to map a given attributes vector to its respective generating word.</p><p>The set of attributes used throughout our experiments, unless otherwise noted, contains the unigram attributes based on the entire list of each benchmark's character set, inspected in levels 1 to 5; the 50 most common bigrams in level 2, and the 20 most common trigrams in level 2. Given that the character set of a given benchmark contains k symbols, the total number of binary attributes would be: k(1 + 2 + 3 + 4 + 5) + 50 × 2 + 20 × 2 = 15k + 140.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning attributes vectors for images</head><p>While the transformation from words to attributes is technical, the transformation from an image to an estimated vector of attributes is learned, from examples, using a CNN. One of the strengths of CNNs, compared to the per-attribute classifiers used by Almazán et al. <ref type="bibr" target="#b2">[3]</ref>, is the sharing of intermediate computations. Many of the attributes are similar in nature. For example, "does the word contain the unigram 'A' in its first half?" and "does the word contain the unigram 'A' in the second half of the word?" are similar classification problems; "does the word contain the bigram 'AB'?" is also related to both. A shared set of filters can tackle these problems successfully, and the CNN benefits from solving multiple classification problems at once.</p><p>Compared to the approach of <ref type="bibr" target="#b24">[25]</ref>, which enjoyed a practically unlimited training set, handwriting recognition is based on smaller datasets. The advantage of attributes in such cases is that the training set is utilized much more efficiently. For example, consider the case of a training set of size 1,000. The word "SLEEP" may appear only twice, but attributes such as "does the word contain the unigram 'S' in the first half of the word?" will probably have many more instances. Therefore, learning to detect the attribute will be easier to train than learning to detect the word. Since CNNs benefit substantially from a larger training set, the advantage of the attributes based method for handwriting recognition is significant.</p><p>Another advantage of learning attributes rather than the words themselves, is that similar words may confuse the network. For example, consider the words "KIDS" and "BIDS". A "KIDS" word image is a negative sample for the "BIDS" category, although a large part of their appearance is shared. This similarity between some categories makes a category based classifier harder to learn, whereas an attributes based classifier uses this for its advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network architecture</head><p>The basic layout of our CNN is a VGG <ref type="bibr" target="#b47">[47]</ref> style network consisting of small (3×3) convolution filters. Starting with a small 100×32 input image, a relatively deep network structure of 12 layers is being used.</p><p>The employed CNN has nine convolutional layers and three fully connected layers. In forward order, the convolutional layers have 64, 64, 64, 128, 128, 256, 256, 512 and 512 filters of size 3 × 3. Convolutions are performed with a stride of 1 and there is input feature map padding by 1 pixel to preserve the spatial dimension. The layout of the fully connected layers is not conventional, and is detailed below. We use maxout <ref type="bibr" target="#b17">[18]</ref> activation for each layer, including, what seems to be less conventional, the convolutional layers. We apply batch normalization <ref type="bibr" target="#b22">[23]</ref> after each convolution, and before each maxout. The network also contains 2 × 2 max-pooling layers, with a stride of 2, following the 3rd, 5th and 7th convolutional layers.</p><p>An important design novelty that we introduce is the usage of multiple separate and parallel fully connected layers. Each layer leads to a separate group of attributes predictions. We divide the attributes to groups of unigrams, bigrams, or trigrams, and for levels and spatial locations. For example, one group of attributes contains only level 2, 2nd word-half, bigram attributes.</p><p>Thus, instead of using a single fully connected layer, as is customary, to generate the entire attributes vector, we employ one fully connected layer per each group of attributes. In our configuration, we have a total of 19 groups of attributes, 1 + 2 + 3 + 4 + 5 for unigram based attributes at levels one to five, 2 for bigram based attributes at level two, and 2 for trigram based attributes at the same level.</p><p>The layers leading up to this set of fully connected layers are all convolutional and are shared. The motivation for this network structure is that the convolutional layers learn to recognize the letters' appearance, regardless of their position in the word, and the fully connected layers learn the spatial information, i.e., the approximate position of the ngram in the word. Hence, splitting the one fully connected layer into several parts, one per spatial section, allows the fully connected layers to specialize, leading to an improvement in accuracy. This is verified in Sec. 5, where we demonstrate that there is a clear advantage for using multiple, separated, fully connected layers. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the structure of the network. The output of the last convolutional layer is fed into 19 branches. Each such branch contains three fully connected layers. These layers have 128 units, 2048 units, and as many units as the relevant group of binary attributes. In our configuration, for unigram based groups that number is equal to the size of the character set: 52 for IAM, 78 for RIMES, 44 for IFN/ENIT, and 36 for SVT and IIIT5K. For bigram based groups the size is 50, and for trigram based groups the size is 20. The activations of the last layer are transformed into probabilities using a sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and implementation details</head><p>The network is trained using the aggregated sigmoid cross-entropy (logistic) loss. Stochastic Gradient Descent (SGD) is employed as the optimization method, with a momentum set to 0.9, and dropout <ref type="bibr" target="#b48">[48]</ref> after the two fully connected hidden layers with a parameter set to 0.5.</p><p>An initial learning rate of 0.01 is used, and is lowered, as is customary, when the validation set performance stops im-proving. Each time the learning rate is divided by 10 with this process repeated three times. The batch size is set in the range of 10 to 100, depending on the dataset that we train on and the memory load. When enlarging the network and adding more FC layers, the GPU memory becomes congested and the batch size is lowered. The network is initialized using Glurot and Bengio's <ref type="bibr" target="#b15">[16]</ref> initialization scheme.</p><p>Training is done in stages, by gradually adding more attributes groups as training progresses. We start by training on a single group of attributes, the Level-1-Unigrams, using a single fully connected arm. When the loss stabilizes, another group of attributes is added and the training continues. Groups of attributes are being added in the following order: Level-1-Unigrams, Level-2-Unigrams, ..., Level-5-Unigrams, Level-2-Bigrams, and Level-2-Trigrams. When adding groups, the initial learning rate is used. Only when all 19 groups have been added, do we begin to lower the learning rate. We have found this gradual way of training to generate considerably superior results over the alternative of directly training on all the attributes groups at once.</p><p>In the SVT and IIIT5K experiments, when training the network on the huge synthetic dataset of <ref type="bibr" target="#b24">[25]</ref>, training was initialized on a partial subset of the training set. In detail, we train the network using 10k images out of the 7M images until partial convergence, after which we continue training using 100k images until partial convergence. We repeat this process with 200k and 1M, and finally we train on the entire train set. When trying to train directly on the entire train set the network did not converge. Our solution is similar to the incremental training used by <ref type="bibr" target="#b24">[25]</ref> where a multiclass classifier was trained on 90k different words. They first train on 5k words, and once converged, increased the training set to 10k words. This was repeated until all classes were covered.</p><p>We used a custom version of Caffe framework <ref type="bibr" target="#b26">[27]</ref> to implement our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Regularization and training data augmentation</head><p>In order to avoid overfitting, dropout, as mentioned, is applied after the first and second fully connected layers of each branch. In addition, a weight decay of 0.0025 is applied to learned weights.</p><p>The inputs to the network are grayscale 100 × 32 images. Where needed, we stretch the input image to this size, without preserving the aspect ratio. Since the handwriting datasets are rather small and we are training a deep neural network with tens of millions of parameters, data augmentation is warranted.</p><p>The data augmentation is performed as follows. For each input image, we apply rotation around the images center with each of the following angles (degrees):  image, dramatically increasing the amount of training data. This image augmentation process is described in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>This data augmentation technique employed is rather basic. We conjecture that the system can benefit from an even more sophisticated data augmentation technique. For example, elastic distortion <ref type="bibr" target="#b46">[46]</ref> can be used.</p><p>Note that our method augments the test data as well. This is described below in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Recognition</head><p>Each word in our lexicon is represented by its ground truth attributes vector. This is an efficient process that is only done once.</p><p>Given an image, it is run through the network. One can then directly compare the set of predicted attributes to the attributes of the lexicon words. However, this is suboptimal for several related reasons. First, the network was not trained for the purpose of matching lexical words. Rather, it was trained for per feature success. Second, such a method ignores the correlations that exist between the various coordinates due to the nature of the attributes. For example, a word which contains the letter 'A' in the first third of the word will always contain the letter 'A' in the first half of the word. Third, the relative importance and discriminative power of attributes or subsets of attributes is not taken into account in a direct comparison. In addition to these reasons, a direct comparison also requires a careful calibration of the network's output probabilities.</p><p>To solve these issues, we use Canonical Correlation Analysis (CCA) in order to learn a common linear subspace to which both the binary attributes of the lexicon words and the network representations are projected. This shared subspace is learned such that images and matching words would be projected as close as possible.</p><p>For added accuracy, the regularized CCA method <ref type="bibr" target="#b52">[52]</ref> is used. The regularization parameter is fixed to be the largest eigenvalue of the cross correlation matrix between the image representations and the matching attributes vectors.</p><p>Since CCA does not require that the matching input vectors of the two domains are of the same type or the same size, we are able to use "neural codes" in lieu of the attributes probability estimations provided by the network. The neural codes are the activations of the network in the layer just below the classification, i.e., in our case, the concatenation of the second fully connected layers from all branches of the network.</p><p>When doing so, we obtain a rather long representation. There are 19 groups of classifiers and each of them has 2, 048 units in the second locally connected layer (post maxout). The total representation size is, therefore, 38, 912. In order to avoid memory explosions, we subsample 12, 000 vector elements out of the 38, 912 and use the subsampled vectors as input to the CCA. We observe a very small change when resampling the subset: the change is less than 0.1 percent in all experiments we performed.</p><p>Similar to Almazán et al. <ref type="bibr" target="#b2">[3]</ref> work, and other applications of CCA, we L2-normalize the input of the CCA algorithm, and use the cosine distance in order to efficiently find the nearest neighbor in the shared space. Test side data augmentation In order to further improve results, test-side data augmentation is being employed. Given a test image, we generate 36 additional variants using the same method described in Sec. 3.5. The 37 images are all encoded using the network. The final representation is taken to be the mean vector of all 37 representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and results</head><p>We present results on the commonly used handwriting recognition benchmarks. The datasets used are: IAM, RIMES and IFN/ENIT, which contain images of handwritten English, French and Arabic, respectively. The same network was run in all cases, using the same parameters. Hence, no language specific information was needed except for the character set of the benchmark. In addition, while the network was not developed with printed text in mind, we report results on two scene text recognition benchmarks: SVT and IIIT5K.</p><p>The IAM Handwriting Database <ref type="bibr" target="#b32">[33]</ref> is a known offline handwriting recognition database of mostly cursive English word images. The database contains 115,320 words written by 500 authors. The database comes with a standard split into train, validation and test sets, such that every author contributes to only one set. In other words, it is not possible that the same author would contribute handwriting samples to both the train set and the test set.</p><p>RIMES <ref type="bibr" target="#b4">[5]</ref> contains more than 60,000 words written in French by over 1000 authors. The RIMES database has several versions with each one a super-set of the previous one. In our experiments, we employ the latest version presented in an ICDAR 2011 contest. IFN/ENIT <ref type="bibr" target="#b41">[41]</ref> is a cursive Arabic handwriting benchmark which contains several sets and has several scenarios that can be tested and compared to other works. The Database IAM RIMES Model WER CER WER CER Bertolami and Bunke <ref type="bibr" target="#b6">[7]</ref> 32.83 ---Dreuw et al. <ref type="bibr" target="#b11">[12]</ref> 28.80 10.10 --Boquera et al. <ref type="bibr" target="#b13">[14]</ref> 15. most common scenarios are: "abcde-f", "abcde-s", "abcde" (older) and "abc-d" (oldest). The naming convention specifies the train and the test sets. For example, the "abcdef" scenario refers to a train set comprised of the sets a, b, c, d, and e; testing in this scenario is done on set f. In addition, while the network was not developed with printed text in mind, we also report results on SVT <ref type="bibr" target="#b54">[54]</ref> and IIIT5K <ref type="bibr" target="#b36">[37]</ref> datasets. Two benchmarks exist for SVT: using a general lexicon and using the SVT-50 subset, in which the task is to recognize among 50 words. Similarly, IIIT5K contains two benchmarks: IIIT5K-50 and IIIT5K-1000. Protocol In Offline Handwriting Recognition, the goal is to find the transcription given a test image. The transcription is limited to a lexicon associated with the tested dataset. On IAM and RIMES, we use the lexicon of all the dataset  <ref type="bibr" target="#b53">[53]</ref> 57.0 ---Mishra et al. <ref type="bibr" target="#b36">[37]</ref> 73.57 -64.10 57.50 Novikova et al. <ref type="bibr" target="#b38">[39]</ref> 72.9 ---Wang et al. <ref type="bibr" target="#b55">[55]</ref> 70.0 ---Goel et al. <ref type="bibr" target="#b16">[17]</ref> 77.28 ---PhotoOCR <ref type="bibr" target="#b7">[8]</ref> 90.39 77.98 --Alsharif and Pineau <ref type="bibr" target="#b3">[4]</ref> 74.3 ---Almazán et al. <ref type="bibr" target="#b2">[3]</ref> 89.18 -91.20 82.10 Yao et al. <ref type="bibr" target="#b56">[56]</ref> 75.89 -80.20 69.30 Jaderberg et al. <ref type="bibr" target="#b25">[26]</ref> 86.1 ---Gordo <ref type="bibr" target="#b18">[19]</ref> 91.  <ref type="table">Table 3</ref>. Comparison to previous work on SVT and IIIT5K datasets. Shown is accuracy in percents. The term "unigrams" refers to training on unigrams only, without bigrams and trigrams. The term "trigrams" refers to training on unigrams, bigrams and trigrams. The term "probs CCA" means the input to CCA was the final probabilities layer, whereas the term "FC CCA" means the input is the last fully connected layer. The term "no/with test" refers to whether test-side data augmentation was employed.</p><p>words, both train and test sets, as done in all the lexicon based methods in the literature, e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>. On IFN/ENIT, we use the official lexicon attached to the benchmark. On SVT, we use the general purpose, 90k words lexicon used in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>, whereas for SVT-50 we use the 50 words lexicon associated with this reduced benchmark. On IIIT5K-50 and IIIT5K-1000 we use the 50 words lexicon and 1000 words lexicon respectively associated with the benchmarks. The method's prediction is compared with the actual image transcription. The different benchmarks use several different measures as detailed in Sec. 3. To ease comparison to other methods, we report using the same measure commonly used in the respected benchmarks. Specifically, on IAM and RIMES, we show our results using WER and CER measures, whereas on IFN/ENIT, SVT and IIIT5K, the results are shown using the accuracy measure.</p><p>Since the benchmarks are in different languages, different character sets were used. Specifically, for IAM, the character set contains the lower and upper case Latin alphabet. Digits were not included since they are rarely used in this dataset. However, when they appear we did not ig- nore them. Therefore, if a prediction is different from the ground truth label only by a digit, it is still considered a mistake. In RIMES, the character set used contains the lower and upper case Latin alphabet, digits and accented letters.</p><p>In IFN/ENIT, the character set was built out of the set of all unigrams in the dataset. This includes the Arabic alphabet, digits and symbols. In SVT the character set used contains the Latin alphabet, disregarding case, and digits. The network used for SVT and IIIT5K was slightly different from the networks used for handwriting recognition. Since the synthetic dataset used to train for the SVT and IIIT5K benchmarks has many training images, we doubled the number of neurons in the first fully connected layer. Comparison with previous work We compare to the state of the art on IAM and RIMES in <ref type="table">Table 1</ref> and on IFN/ENIT in <ref type="table">Table 2</ref>. Our method achieves state of the art results on all benchmarks, including all versions of the IFN/ENIT benchmark. The improvement over the state of the art, in these competitive datasets, is such that the error rates are cut in half throughout the datasets: IAM (6.45% vs. 11.9%), RIMES (3.9% vs. 8.9% for a single recognizer), IFN/ENIT set-f (3.24% vs. 6.63%) and set-s (5.91% vs. 11.5%),</p><p>In <ref type="table">Table 3</ref>, our results are compared to other state of the art methods on the popular SVT and IIIT5K datasets. As can be seen, we achieve state of the art results when using the same global 90k dictionary used in <ref type="bibr" target="#b24">[25]</ref>. State of the art results are also achieved on the more common SVT-50 variant. In addition, we get state of the art results on IIIT5K-50 and results comparable to state of the art on IIIT5K-1000. Interestingly, we have also compared the accuracy on the test set of the synthetic data: we obtain 96.88% compared to 95.2% obtained by the best network of <ref type="bibr" target="#b24">[25]</ref>.</p><p>In order to study the influence of each of the method's elements to its success, we have included several variants in <ref type="table">Table 4</ref>. Somewhat reassuringly, the method is robust to various design choices. For example, using CCA on the aggregated probability vectors provide a compatible level of performance. Similarly, bigrams and trigrams do not seem to consistently affect performance. We tried removing these only from the test stage, or from both train and test stages, without any clear indications.</p><p>What does seem to matter is the separation of the fully connected layers into multiple branches. Reducing the number of branches from 19 to 7 by merging related attributes groups (e.g., using a single branch for all level 5 unigram attributes instead of 5 branches), or to one branch of fully connected hidden layers, hurts performance. Increasing the number of hidden units in order to make the total number of hidden units the same (not shown in the table), hinders convergence during training. Lastly, test-side data augmentation seems to consistently improve performance.</p><p>For completeness we also present the results of using the full system without CCA. There are several ways to obtain results without CCA. Out of a few such alternatives, the best results seem to be obtained by thresholding the attribute scores and then using the Jaccard similarity between the binary prediction and binary ground truth. As expected, removing CCA hurts performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Handwriting recognition is the birthplace of deep learning. CNNs were created in order to read handwritten postal codes, and RNNs success in multiple handwriting benchmarks preceded, by a few years, the success of CNNs on imagenet. It is therefore somewhat disappointing that the recent wave of leaps in performance achieved using deep learning throughout computer vision has so far skipped handwriting recognition.</p><p>We believe that there is nothing inherently challenging in handwriting recognition that prevents machines from achieving a human level of performance in the same way that such performance is achieved in face recognition <ref type="bibr" target="#b51">[51]</ref> and even in some object recognition metrics <ref type="bibr" target="#b21">[22]</ref>.</p><p>The success of our method suggests that a successful handwriting recognition system can be built without investing much effort in atomic tasks such as image binarization and letter segmentation. This follows the end-to-end trend of deep learning. It could be the case that future systems would benefit from multiple sub-systems. Similarly, we employ only one network and do not use ensembles or voting schemes in order to obtain a slight improvement in performance. However, future systems might benefit from employing multiple systems in order to utilize multiple tactics depending on the nature of the script.</p><p>Looking forward, the ultimate goal of handwriting recognition is the construction of a universal reading system, which is adaptable not only for different scribes (as is done here) but also for different script types. In such a system, learning to read the next script would be easier than training from scratch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of the attributes which are set for the word "optimization". Since we use only common bigrams and trigrams, not every bigram and trigram is an attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The network architecture used. Nine shared convolutional layers, with intermediate batch normalization layers, maxout activations, and max pooling layers are employed, in sequence, to an input of size 100 × 32 pixels. The network then splits into 19 parallel branches. In each branch, there are two hidden layers and an output layer that is converted to probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>{−5, −3, −1, +1, +3, +5}. In addition, we apply shear using the following angles {−0.5, −0.3, −0.1, 0.1, 0.3, 0.5}. This way, 36 additional images are generated per each input</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The data augmentation process. Each input image is transformed using both rotation and shear to generate additional similar, but different, samples for a given label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Table 4. Comparison to different variants of the CCN-n-gram system. The full system contains 19 FCs, with bigrams and trigrams, with test-side data augmentation, CCA gets the FC layer as input. For reasons of table consistency, IFN/ENIT (abcde-s) results are given in terms of WER instead of Accuracy (1-WER).</figDesc><table>Database 

IAM 
RIMES 
abcde-s 
Model 
WER CER WER CER 
WER 
Full CNN-n-gram system 
6.45 
3.44 
3.90 
1.90 
5.91 
v1: CCA on probabilities 
6.56 
3.46 
3.85 
1.73 
6.42 
v2: no trigrams during test 
6.33 
3.34 
3.95 
1.86 
6.10 
v3: no bi-and tri-grams during test 
6.29 
3.37 
3.78 
1.89 
6.10 
v4: no trigrams during train 
6.32 
3.33 
4.15 
1.91 
5.91 
v5: no bi-and tri-grams during train 
6.36 
3.36 
3.85 
1.82 
5.85 
v6: 7 FCs instead of 19 FCs 
7.16 
3.95 
4.93 
2.34 
6.48 
v7: 1 FC instead of 19 FCs 
7.81 
4.33 
4.93 
2.31 
7.63 
v8: no test-side augmentation 
6.94 
3.71 
4.27 
2.02 
6.42 
v9: without using CCA 
8.83 
5.93 
5.17 
3.43 
6.68 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improvements in sub-character hmm model based arabic text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahmoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="537" to="542" />
		</imprint>
	</monogr>
	<note>14th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An effective approach to offline arabic handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alabodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Word spotting and recognition with embedded attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fornes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2552" to="2566" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end text recognition with hybrid hmm maxout models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1811</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rimes evaluation campaign for handwritten mail processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grosicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Brodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Prêteux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Frontiers in Handwriting Recognition</title>
		<meeting>the Workshop on Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective technique for the recognition of offline arabic handwritten words using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Azeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="412" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hidden markov model-based ensemble methods for offline handwritten text line recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3452" to="3460" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tandem hmm with convolutional neural network for handwritten word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2390" to="2394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparison of sequence-trained deep neural networks and recurrent neural networks optical modeling for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Language and Speech Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="199" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and robust training of recurrent neural networks for offline handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kozielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="279" to="284" />
		</imprint>
	</monogr>
	<note>14th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical hybrid mlp/hmm or rather mlp features for a discriminatively trained gaussian hmm: a comparison for offline handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dreuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3541" to="3544" />
		</imprint>
	</monogr>
	<note>Image Processing (ICIP)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Icdar 2009-arabic handwriting recognition competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">El</forename><surname>Abed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Märgner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving offline handwritten text recognition with hybrid hmm/ann models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Espana-Boquera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Castro-Bleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gorbe-Moya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zamora-Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="779" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding, 1997. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Whole is greater than sum of parts: Recognizing scene text words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2013 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="398" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Supervised mid-level features for word image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5224</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ICDAR 2011: French handwriting recognition competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grosicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>El-Abed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Document Analysis and Recognition</title>
		<meeting>of the Int. Conf. on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1459" to="1463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<title level="m">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improvements in rwth&apos;s system for off-line handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kozielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2013 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A framework for arabic handwritten recognition based on segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lawgali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouridane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Hybrid Information Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="413" to="428" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Icdar 2011-arabic handwriting recognition competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Margner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2011 International Conference on</title>
		<imprint>
			<biblScope unit="page" from="1444" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Icfhr 2010-arabic handwriting recognition competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Märgner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Abed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2010 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The iam-database: an english sentence database for offline handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U.-V</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The A2iA French handwriting recognition system at the Rimes-ICDAR2011 competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Menasri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bianne-Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8297</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Abbyy finereader professional 9.0. PC Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mendelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Over-generative finite state transducer n-gram for out-of-vocabulary word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis Systems (DAS), 2014 11th IAPR International Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="212" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2012-23rd British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">BMVA</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ontological supervision for fine grained classification of street view storefronts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Largelexicon attribute-consistent text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<biblScope unit="page" from="752" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Icdar 2007 arabic handwriting recognition competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maddouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mägner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ellouze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Amiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Colloque International Francophone sur l&apos;Ecrit et le Document (CIFED)</title>
		<meeting><address><addrLine>Hammamet, Tunis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ifn/enit-database of handwritten arabic words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Maddouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Märgner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ellouze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Amiri</surname></persName>
		</author>
		<imprint>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hmm based approach for handwritten arabic word recognition using the ifn/enitdatabase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Maergner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">890</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
	<note>14th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05717</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">958</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The qcri recognition system for handwritten arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Processing-ICIAP 2015</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Canonical ridge and econometrics of joint production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vinod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="166" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4042" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
