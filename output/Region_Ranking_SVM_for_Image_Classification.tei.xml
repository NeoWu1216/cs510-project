<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Region Ranking SVM for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Wei</surname></persName>
							<email>zijwei@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11794</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
							<email>minhhoai@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11794</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Region Ranking SVM for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of an image classification algorithm largely depends on how it incorporates local information in the global decision. Popular approaches such as averagepooling and max-pooling are suboptimal in many situations.</p><p>In this paper we propose Region Ranking SVM (RRSVM), a novel method for pooling local information from multiple regions. RRSVM exploits the correlation of local regions in an image, and it jointly learns a region evaluation function and a scheme for integrating multiple regions. Experiments on PASCAL VOC 2007, VOC 2012 show that RRSVM outperforms the methods that use the same feature type and extract features from the same set of local regions. RRSVM achieves similar to or better than the state-of-the-art performance on all datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image classification is one of the most fundamental and challenging tasks in computer vision. Image classification aims at recognizing the semantic category of an image, such as whether the image contains a certain object (e.g., bicycle, car), depicts a certain scene (e.g., beach, bedroom), or captures a certain action (e.g., answering phone, riding horse). Recognizing the semantic category of an image, however, is challenging because the location of the semantic region, the image area that corresponds to the semantic category that we need to recognize, is unknown.</p><p>A popular approach is to tackle this problem is to aggregate the information computed at multiple regions of an image. Recent examples of this approach are to average CNN feature vectors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref> that are computed at multiple locations and scales <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref>. These approaches lead to impressive recognition performance on a number of datasets, including PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> an ImageNet <ref type="bibr" target="#b26">[27]</ref>. However, average pooling does not always yield good performance, especially when the semantic region is small and has little or no overlap with the majority of the local regions being considered. In this case, average pooling harms recognition due to too much background noise. Another popular approach is to treat the semantic region as a latent variable and use a region classifier to localize the semantic region. A region classifier is used to evaluate multiple regions of an image, and the one that yields the maximum classifier score is considered the semantic region. This approach assumes the semantic region is among the local regions being considered, or significantly overlaps with one of them. This assumption, however, does not usually hold in practice because: (i) the local regions may be required to have some certain shape (e.g., axis parallel rectangular regions, while the semantic region can be free-form and dis-contiguous), and (ii) the total number of regions that can be considered must be limited to meet a computational budget. For example, CNN features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref> can only be computed on square regions of a certain size. Also, computing CNN features is time consuming so the total number of regions is usually limited to ten <ref type="bibr" target="#b17">[18]</ref> or at most several hundred <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref>. Furthermore, this approach might not work well even when the semantic region is among the regions being considered: there is no guarantee that the region that yields the maximum score would actually correspond to the semantic region. This is especially true when the region classifier is far from perfect; it is often trained without region-level labels because training images are generally not divided into annotated regions.</p><p>Several prior works also suggest the inefficacy of using the maximum score for classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. Hu et al. <ref type="bibr" target="#b15">[16]</ref> considered both the maximum and the average scores, and reported better classification performance for the maximum score in many experiments. Hoai and Zisserman <ref type="bibr" target="#b12">[13]</ref> considered the problem of recognizing human actions in video where the exact locations of human actions are unknown. To tackle temporal uncertainty, they posed it as a multiple instance learning problem where subsequences of a video clip were considered as candidates for the actual location of a human action. Assuming there was a base classifier, they observed the inadequacy of using the maximum score of the subsequences as the decision value. Instead, they proposed Subsequence Score Distribution (SSD), a decision function that was based on the distribution of the scores of the video subsequences. They showed that SSD outperformed the method that used the maximum score by a wide margin. Although SSD was more robust than the maximum score, its performance largely depends on the performance of the base classifier, which had been learned independently of SSD.</p><p>In this paper, we propose Region Ranking SVM (RRSVM), a novel formulation for the classification of multiple regions. RRSVM is based on SSD, which uses all region scores instead of using just the max (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>) or the mean for classification. RRSVM trains an SVM base classifier and uses the scores of all the regions in an image to predict the image label. The scores of the regions in an image are ranked and combined using a weight vector, called region-combination vector. This vector is common to all images of a category, and it is jointly learned with the base classifier. The region-combination vector can be considered as a classifier of which the input is the distribution of region scores. The distribution preserves more information than both extreme (i.e., max) and summary (i.e., mean) statistics, and it will be empirically shown to be more robust and effective. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the essence of our approach.</p><p>RRSVM bares some similarities to bilinear models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref>. Bilinear models also consider the correlation between multiple data instances and learn two weight vectors for factorizing 'styles' and 'contents'. However, bilinear models require a fixed and known ordering of the instances, which is not applicable to image classification problems. In contrast, RRSVM automatically determines the order of regions by ranking them using the scores obtained from the base classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image Classification</head><p>Image classification is one of the most important problems in computer vision. Training SVM classifiers on Fisher Vectors (FVs) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref> and variant methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref> have once been the dominating methods for large scale image classification problems. Recent development in deep learning and Convolution Neural Networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref> have significantly advanced the performance of large scale image classification. Many algorithms, either based on Fisher Vectors or CNN, often compute an average feature vector over multiple local regions. This often oversimplifies the situations that actually the different local regions of an image contain different amount of information for image classification. Unlike existing works, our proposed method selectively integrates information into the classification decision.</p><p>Many works have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41]</ref> to utilize local information for classification tasks. Among these the most widely used is the spatial pyramid representation <ref type="bibr" target="#b19">[20]</ref>. Spatial pyramid representation, however, relies on rigid geometric correspondence of grid division, which ignores the importance of semantic or discriminative localization. This model has limited discriminative power for recognizing semantic category with huge variance in location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Subsequence Score Distribution (SSD)</head><p>SSD <ref type="bibr" target="#b12">[13]</ref> is a method for considering the score distribution of multiple feature vectors. SSD was initially proposed for human action recognition. The technique is general and can be applied to image classification. For convenience, here we review it in the context of image classification.</p><p>SSD assumes a base classifier f for evaluating image regions has been learned. Consider an image region x, ideally, f (x) should be 1 if x is a region that contains the target object and -1 otherwise. Given a test image B, instead of using the maximum score max x∈B f (x) as the decision value for B, SSD proposes to learn an aggregation operator as below.</p><p>Consider the training data that consists of n images {B i } n i=1 and associated image labels {y i } n i=1 . Assume for now that all images have the same number of sampled regions. Let m be the number of regions and d the dimension of each region descriptor. SSD represents each image as a matrix B i ∈ ℜ d×m , but the order of the columns can be ar-</p><formula xml:id="formula_0">bitrary. Let b 1 i , · · · , b m i be the region descriptors in image B i . Let a i = [sort(f (b 1 i ), · · · , f (b m i ))] T .</formula><p>Here, sort is the function that reorders the inputs in descending order. a i represents the score distribution of regions from B i . SSD learns score-combination vector s by optimizing the following objective:</p><formula xml:id="formula_1">minimize s,b n i=1 max(1 − y i (s T a i + b), 0) (1) s.t. s 1 ≥ s 2 ≥ · · · ≥ s m ≥ 0,<label>(2)</label></formula><formula xml:id="formula_2">m j=1 s j = 1.<label>(3)</label></formula><p>The above optimization problem seeks a weight vector s and the bias term b for separating between the score distribution vectors of positive and negative data. The objective in Eq. (1) is the sum of Hinge losses, as used in the objective of SVMs <ref type="bibr" target="#b36">[37]</ref>. SSD has been shown to yield impressive results on human action recognition <ref type="bibr" target="#b12">[13]</ref>. However, the performance of SSD depends heavily on the base classifier. In <ref type="bibr" target="#b12">[13]</ref>, the base classifier is simply trained to separate the mean vectors of positive videos and the mean vectors of negative videos. This base classifier may not work well for other problems. Unfortunately, a straight forward formulation for iterative learning of the base classifier and the SSD classifier will lead to a degenerate solution. This will be explained in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Region-Ranking SVM (RRSVM)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">RRSVM Formulation</head><p>RRSVM is a framework that jointly learns the base classifier and the SSD decision function. Using the same notation as in Sec. 2.2, RRSVM is posed as the following optimization problem:</p><formula xml:id="formula_3">minimize w,s,b λ||w|| 2 + n i=1 (w T Γ(B i ; w)s + b − y i ) 2 (4) s.t. s 1 ≥ s 2 ≥ · · · ≥ s m ≥ 0, (5) h({Γ(B i ; w)s}) ≤ 1.<label>(6)</label></formula><p>In the above formulation, w and b are the weight vector and the bias term of the SVM base classifier. Γ(B; w) denotes a matrix that can be obtained by rearranging the columns of the matrix B so that w T Γ(B; w) is a sequence of nonincreasing values. This sequence is the sorted list of the SVM scores of individual regions, which will be referred to as the region-score vector. This region-score vector is essentially a non-parametric representation for the score distribution of the regions in an image B. Vector s is the weight vector for combining the SVM region scores for each image; this vector is common to all images of a class. The objective of the above formulation consists of the regularization term λ||w|| 2 and the sum of regression losses. There, λ is the only parameter of RRSVM and it is tunable. This objective is analogous to the objective of Least-Squares SVMs (LSSVM) <ref type="bibr" target="#b32">[33]</ref>. LSSVM, also known as kernel Ridge regression <ref type="bibr" target="#b28">[29]</ref>. LSSVM has been shown to perform equally well as SVM in many classification benchmarks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39]</ref>. We base our formulation on LSSVM instead of SVM <ref type="bibr" target="#b36">[37]</ref> because LSSVM has a closed-form solution, which is a computational advantage over SVM.</p><p>Constraint (5) requires s to be non-negative and monotonic. This is to emphasize the relative importance of higher region-classification scores (recall that we take the dot product between s and w T Γ(B; w), the sorted vector of local region scores).</p><p>Constraint <ref type="formula" target="#formula_3">(6)</ref> requires the feature vectors to be bounded. Here h(·) is the function that measures the spread of a set of vectors:</p><formula xml:id="formula_4">h({x i } n i=1 ) = n i=1 x i − 1 n n i=1 x i 2<label>(7)</label></formula><p>Recall that Γ(B i ; w)s is a linear combination of local region feature vectors of B i ; it is essentially the representation vector for the image B i . Constraint (6) requires the spread of these representation vectors to be bounded by 1.</p><p>In theory, this constraint governs the compactness of the feature vectors, which is important for the theoretical analysis of the max-margin learning framework <ref type="bibr" target="#b36">[37]</ref>. In practice, this constraint prevents s to become extremely large. To see why this is important, consider the loss terms in Eq. (4). These loss terms remain the same if w and s are scaled inversely. Thus, if s can be made extremely large then w can be made extremely small; rendering the margin term λ||w|| 2 ineffective. The right hand side of Constraint <ref type="formula" target="#formula_3">(6)</ref> is rather arbitrary. We can replace 1 by any positive value γ, and obtain an equivalent formulation by adjusting the value of λ as follows: λ new := γλ old .</p><p>Once (w, b, s) have been learned, the classification decision value for a test image B is taken as:</p><formula xml:id="formula_5">w T Γ(B; w)s + b.<label>(8)</label></formula><p>As can be seen, this decision function is the same as the decision function of SSD, which has been shown to be very effective <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Alternative Formulations</head><p>Compare SSD (Eq. 1) and RRSVM (Eqs. <ref type="bibr">4 &amp; 11)</ref>, one can notice the two differences in the formulations for learning s. First, SSD uses Hinge loss while RRSVM uses squared loss. This modification is necessary to ensure that w and s optimizes the same objective.</p><p>Second, the L 1 -norm constraint for s in Eq. <ref type="formula" target="#formula_2">(3)</ref> is replaced by the bound on the spread of representation vectors in Eq. <ref type="bibr" target="#b5">(6)</ref>. Apart from the theoretical reasons for using Eq. (6) as explained above, there are potential issues for using the L 1 -norm constraint.</p><p>Consider an alternative RRSVM formulation where Eq. <ref type="formula" target="#formula_3">(6)</ref> is replaced by Eq. <ref type="formula" target="#formula_2">(3)</ref>. This formulation often leads to a solution where s degenerates to the solution s 1 = 1 and s 2 = · · · = s m = 0. This has been observed in many cases of our experiments. This tendency can be explained as follows. In the learning formulation, the margin term λ||w|| 2 is included in the objective. Thus, the formulation prefers small ||w||. Consider the loss terms in the objective, these terms remain the same if ||w|| and ||Γ(B i ; w)s|| are scaled inversely. Thus, in general, the bigger {||Γ(B i ; w)s||} are, the smaller ||w|| can become. But, for any image B, the vector s that leads to highest value of ||Γ(B; w)s|| is the degenerated solution given above. To see this, suppose</p><formula xml:id="formula_6">Γ(B; w) = [b 1 , · · · , b m ] and the local feature vectors of B are L 2 -normalized (i.e., ||b 1 || = · · · = ||b m || = 1).</formula><p>Using the Cauchy-Schwarz inequality, we have:</p><formula xml:id="formula_7">||Γ(B; w)s|| 2 = || m i=1 s i b i || 2 (9) ≤ ( m i=1 s i )( m i=1 s i ||b i || 2 ) = 1<label>(10)</label></formula><p>In general, b i = b j , so equality holds only when s 1 = 1 and s 2 = · · · = s m = 0. Thus, the alternative learning formulation is biased toward the degenerated solution. The formulation for joint learning of w and s should avoid using the L 1 -norm constraint. Note that the usage of this constraint in the SSD formulation is fine because w is fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>The learning formulation given in Eq. 4 can be optimized with block coordinate descent, alternating between the following two procedures: For Procedure (A), w is fixed, and let B i be Γ(B i ; w) and a i be B T i w. Procedure (A) is equivalent to:</p><formula xml:id="formula_8">minimize s,b n i=1 (a T i s + b − y i ) 2 (11) s.t. s 1 ≥ s 2 ≥ · · · ≥ s m ≥ 0,<label>(12)</label></formula><formula xml:id="formula_9">s T n i=1 B T i B i − 1 n ( n i=1 B i ) T ( n i=1 B i ) s ≤ 1. (13)</formula><p>The above is a Quadratically Constrained Quadratic Program (QCQP). This optimization problem is convex, and it can be solved efficiently and globally using a QCQP solver, such as IBM Cplex 1 . For Procedure (B), s is fixed, and let (w old , b old ) are the current values of (w, b). Procedure (B) seeks new values for (w, b) to decrease the objective of Eq. (4). Let u i be Γ(B i ; w old )s, we first solve the following ridge regression problem to get (w * , b * ):</p><formula xml:id="formula_10">w * , b * := argmin w,b λ||w|| 2 + n i=1 (w T u i + b − y i ) 2 (14)</formula><p>There exists a closed form solution for the above optimization problem, and therefore (w * , b * ) can be found efficiently. (w * , b * ) define the line search direction for new values of (w, b); we perform binary search until we find a feasible solution with lower objective value as follows. Otherwise, let w * := (w * + w old )/2, b * := (b * + b old )/2 and we check the objective value an the constraint satisfaction again. If the number of binary splits exceeds 10, we terminate the procedure and return w old , b old . In practice, this procedure usually requires no binary split and always terminate within six splits.</p><p>The block-coordinate descent algorithm is guaranteed to converge because each procedure does not increase the objective value. In our experiments, it usually converges within 15 iterations.</p><p>We initialize the algorithm by: i) set s 1 = · · · s m = 1/m, ii) represent each image by the mean of its local region features, and iii) train a binary SVM to get initial w and b. This initialization and the optimization algorithm discussed in this section work well in our experiments, but smarter initialization strategies or better optimization policies can be used, e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>As discussed in Sec. 3.1, the right hand side of Constraint (6) is rather arbitrary. In practice, it is convenient to set it to be the spread of the initial image representation vectors (the means of images' local region descriptors in our experiments). Learning s can be considered as learning the new feature representation, and this constraint requires the space contraction of the new feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Extensions</head><p>One way of extending the formulation presented in Sec. 3.1 is to replace Γ by another appropriate function. This function must take an unordered set of vectors and return a matrix of fixed dimensions. These dimensions must be matched by the dimensions of w and s. For example:</p><p>• If the number of regions of each image (m) is high, Γ can be replaced by a function that returns the m ′ highest scored regions in descending order and set the dimension of s to m ′ , with m ′ ≪ m. • If it is impossible to enumerate and find regions with the highest scores, Γ can be combined with random sampling, which represents a distribution by its random samples. • If the number of the regions from each image differ, Γ can be replaced by a function that combines sorting and region selection. This selection procedure could simply use random sampling with replacement, or it could deterministically cycle through the regions in the decreasing order of the region scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform experiments on three classification benchmarks: image classification on the PASCAL VOC 2007 <ref type="bibr" target="#b6">[7]</ref> dataset, human action recognition in still images on the PASCAL VOC 2012 <ref type="bibr" target="#b7">[8]</ref> dataset, and the object classification challenge of the ImageNet Large Scale Visual Recognition Challenge 2014 (ILSVRC 2014) <ref type="bibr" target="#b26">[27]</ref>. In our experiments, we compare RRSVM with several competing methods that use the same feature representation. While RRSVM is agnostic to the particular feature representations, we use features extracted from the pre-trained VGG-D network <ref type="bibr" target="#b30">[31]</ref>, which has 16 layers and is commonly referred to as VGG16. We use VGG16 because of its excellent performance, being the best among methods that use a single neural network. Our feature extraction is based on the MatConvNet library <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Extraction</head><p>Our feature extraction pipeline is the same as described in <ref type="bibr" target="#b30">[31]</ref>. The last fully connected layer of VGG16 is removed and the remaining fully connected layer is converted to a fully convolutional layer, similar to <ref type="bibr" target="#b29">[30]</ref>. To compute feature vectors for an image, the image is first resized isotropically so that the smallest size is equal to a predefined size Q, where Q ∈ {256, 384, 512}. Additionally, the resized images are flipped horizontally, creating six images at three scales. The fully-convolutional net is applied to each of the six images, leading to a feature map with 4096 channels. The size of feature map depends on the size of the resized image. Combining the feature vectors at three scales and with and without flipping, each input image typically yields 200 to 400 densely sampled feature vectors, each feature vector can be mapped back to a subwindow of the original input image.</p><p>Our work differs from <ref type="bibr" target="#b30">[31]</ref> only in the feature pooling step. In <ref type="bibr" target="#b30">[31]</ref>, average pooling is applied on each image scale. Meanwhile, we compute the averaged of all feature vectors of all scales and subsequently perform L 2normalization to create a global feature descriptor. We also perform L 2 normalization on each densely sampled feature vectors to create local feature descriptors. Finally, we stack the global feature descriptor with each local feature descriptor to get a 8192-dim vector to represent a local region of the input image. We do not use multi-crop sampling <ref type="bibr" target="#b34">[35]</ref>, which is very computationally demanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">PASCAL VOC 2007 Object Classification</head><p>We first perform experiments on the PASCAL VOC 2007 dataset <ref type="bibr" target="#b6">[7]</ref>. This dataset consists of three disjoint subsets for training, validation and testing; and they contain 2501, 2510 and 4952 images respectively. Each image is annotated with one or several labels, corresponding to 20 object categories. Classification performance for each category is measured by Average Precision. The mean Average Precision of all categories is denoted as mAP.</p><p>The baseline of our method is the one that trains a classifier on the average of all feature vectors computed for an image, as in <ref type="bibr" target="#b30">[31]</ref>. We consider this as the fairest method to compare with RRSVM because they use the same set of feature vectors. In this paper, we use LSSVM as the classifier because of its computational efficiency and competitive performance <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>. The top two rows in <ref type="table" target="#tab_1">Table 1</ref> show that LSSVM performs similar to SVM (with the best C tuned using 5-fold cross validation). Hereafter, we use LSSVMs as baselines for all the remaining experiments. We used the validation set for tuning parameter λ of LSSVM and RRSVM. We found that the best performance is achieved when λ = 10 −4 n, where n is the total number of training data. For final classification results, we trained classifiers on the combined training and validation data and the average precision is reported in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>There are several noticeable results in <ref type="table" target="#tab_1">Table 1</ref>. First, for the baseline method (denoted as LSSVM), the mAP is slightly higher than 89.3% of VGG16 reported in <ref type="bibr" target="#b30">[31]</ref>. This may be due to the slight difference in the feature normalization step. Second, LSSVM-SSD and LSSVM-Max perform similarly. This might be because SSD elects to use a single effective region in this situation. Third, there is a significant performance gap between RRSVM and the baseline LSSVM, and RRSVM also outperforms all other meth-  ods in every object category especially in the ones where the baseline classifiers have relatively low accuracy (e.g., for categories bottle and plant, the performance gaps are 10.2% and 9.2% respectively). Some previous state-of-theart methods are listed at the bottom of Tab. 1, and RRSVM outperforms them by a wide margin (approximately 30% error reduction). <ref type="figure" target="#fig_3">Figure 2</ref> shows the local regions used by RRSVM on some representative test images. From these images we see that RRSVM assigns weights to meaningful regions of the images that contain relevant object information. More examples are provided in the supplemental material.</p><formula xml:id="formula_11">- - - - - - - - - - - - - - - - - - - -89.3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">VOC 2012 Action Classification</head><p>In this section, we describe the experiments on the Action dataset from the PASCAL VOC2012 Challenge. The task is to recognize actions in still images, given the bounding box of a person performing the action. The dataset contains 2296 training images with 3134 bounding boxes, 2292 validation images with 3144 bounding boxes, and 4569 test images with 6283 bounding boxes.</p><p>This classification task is similar to the image classification task of VOC2007 described in the previous section. The key difference is the availability of the regions of interest, which are the human bounding boxes. Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref>, for each method in consideration, we train two classifiers, one for the entire images and the other for the human regions. Finally, we combine two classifiers by averaging their scores. <ref type="table" target="#tab_3">Table 2</ref> compares the performance of RRSVM and LSSVM. Both methods use the same sets of feature vectors produced by VGG16. As can be seen, RRSVM outperforms LSSVM on all action categories. The gap in mean average precision is 3.2%. <ref type="table" target="#tab_4">Table 3</ref> compares RRSVM with several state-of-the-art methods on the test set. For this experiment, we use both VGG16 and VGG19 as also used by <ref type="bibr" target="#b30">[31]</ref>. We train the RRSVMs for VGG16 and VGG19 separately and average their scores. We train them on both train and validation sets. The results on the test set are obtained by submitting the output to the PASCAL evaluation server, conforming to the rules of PASCAL VOC Challenge. As can be seen in <ref type="table" target="#tab_4">Table 3</ref>, our proposed RRSVM, without any task-specific heuristics, outperforms the previous state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">ILSVRC 2014 Image Classification</head><p>We also evaluate RRSVM on ILSVRC 2014 classification challenge <ref type="bibr" target="#b26">[27]</ref> to study its benefits for large-scale image classification. The ILSVRC 2014 classification challenge requires classifying images into one of the 1000 leaf-node categories of the ImageNet dataset. There are 1,281,167 images for training, 50,000 for validation, and 100,000 for testing. The number of positive training examples for each class ranges from 732 to 1300, and all images of the other classes are considered as negative examples. Two numbers are usually reported: the top-1 error rate, which compares the class with highest prediction score with the ground truth class, and the top-5 error rate, which considers the prediction correct if the ground truth is within the top 5 predicted classes. The top-5 error is used to address the potential problem of inexhaustive ground truth annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Experiment setup</head><p>The experiment setup for the ILSVRC dataset is similar to the setup for the PASCAL VOC 2007 dataset (Section 4.2) with two modifications to: 1) be possible to train 1000 classifiers on a large dataset of more than one million images, and 2) calibrate the scores of 1000 binary one-versus-rest classifiers for a multi-class categorization task.</p><p>The first modification is to train LSSVM and RRSVM with a smaller number of negative examples. In theory, it is possible to train LSSVM and RRSVM with all the ex-   The second modification is to integrate multiple binary classifiers in to a single multi-class classifier. Since RRSVMs (and also LSSVMs) are independently trained for each class, they must be calibrated to be used for multi-class</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mAP</head><p>Oquab et al. <ref type="bibr" target="#b23">[24]</ref> 70.2 Hoai et al. <ref type="bibr" target="#b13">[14]</ref> 70.5 Gkioxari et al. <ref type="bibr" target="#b9">[10]</ref> 73.6 Hoai <ref type="bibr" target="#b11">[12]</ref> 76.3 Simonyan and Zisserman <ref type="bibr" target="#b30">[31]</ref> 84.0 RRSVM (ours) 85.5 classification. We use a 3-layer neural network to reconcile the outputs of the 1000 classifiers. The first layer of the network is a fully connected layer with 1000 units followed by a rectified linear unit (ReLU). The second layer is the same as the first layer but without ReLU. The last layer is a softmax layer with 1000 outputs for 1000 classes. The network is trained to minimize the negative log likelihood using all training examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Classification performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Category based analysis</head><p>The diversity and size of ILSVRC 2014 enable us to further analyze the performance of RRSVM. In this section we use AP to measure the performance on the validation set for individual object categories.</p><p>We first of all consider the differences between APs of RRSVM and results of VGG16, which will be referred to as AP gaps. We sort the AP gaps for 1000 classes and plot them in <ref type="figure" target="#fig_4">Figure 3a</ref>. As can be seen, the AP gaps are positive for more than70% of the classes. The mean of the AP gaps is 1.6%. The category with the largest AP gap is n03673027 (ocean liner) with 13% AP gap. The lowest AP gap is -23.7% for n02447721 (gong), which seems to be an outlier case as the second lowest AP gap is -9.8%. There are two possible reasons for poor performance on this category: (i) the images of gong have various shapes/textures and with relatively uniform backgrounds, and the classifier being trained by RRSVM deteriorates through the iterations of the iterative optimization; and (ii) we did not use enough negative training examples (recall we only use 15K negative training samples for efficiency). Examples of the failure cases are showed in the supplementary material.</p><p>We also consider the number of non-zero values of the s vector. It corresponds to the number of local regions used by RRSVM. <ref type="figure" target="#fig_4">Figure 3b</ref> plots the distribution of the numbers of effective regions. RRSVM uses no more than 10 regions for the majority of the categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Timing</head><p>The optimization of RRSVM typically converges within 15 iterations. Each iteration requires solving a QCQP and one or several least square problems. On an Intel Xeon 2.5GHz machine with 16 cores for parallel data loading, it takes roughly 10 minutes to train an RRSVM classifier for each ILSVRC 2014 category. This excludes the feature computation using VGG16.  <ref type="table" target="#tab_5">Table 4</ref>: Error rates on ILSVRC 2014 dataset (%). RRSVM performs similarly to methods that use multiple nets and use multiple cropped regions (computationally expensive). RRSVM uses the same setup as the first method shown in this table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have proposed Region Ranking SVM (RRSVM), a novel framework for image classification. RRSVM is developed on the foundation of a robust decision function that is based on the distribution of multiple region scores. During training, RRSVM jointly learns a region evaluation function and a region-combination vector. We have showed that RRSVM outperforms the baseline methods that use the same features as RRSVM by a wide margin. Tested on the image classification task of PASCAL VOC 2007, the action recognition task of PASCAL VOC 2012, and the object classification task of ILSVRC 2014, RRSVM is better than or comparable to the state-of-the-art methods that use more powerful features or require more computational resources.</p><p>Currently, each RRSVM is a binary classifier, and multiple RRSVMs must be calibrated before they can be used for multi-class classification. A future direction is to extend the current framework to jointly train multiple RRSVMs and the multi-class classifier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Image classification with unknown object location. A popular approach is to consider multiple local regions (b), and uses either max-pooling (c) or averagepooling (d). We propose a method that selectively uses the regions and combine them for classification (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>A) Fix w, optimize Eq. (4) w.r.t. s and b, (B) Fix s, optimize Eq. (4) w.r.t. w and b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Let g(w, b, s) denote the objective value of Eq. (4). If g(w * , b * , s) ≤ g(w old , b old , s) and h({Γ(B i ; w * )}) ≤ 1 then we terminate the Procedure (B) and output w new := w * , b new := b * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Local regions used by RRSVM for classification. These are representative images from the test set of PASCAL VOC 2007. The blue boxes with numbers correspond to the regions and their weights used for classification. Yellow boxes with labels are the ground truth regions of interest for the specific category. Best viewed on a digital device.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>(a): AP gaps between RRSVM and VGG16 for individual classes on ILSVRC 2014. The majority of classes have positive AP gaps. (b): distribution of the numbers of effective regions used by RRSVM on ILSVRC 2014. Most classes require 20 effective regions or less.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Model aero bike bird boat bottle bus car cat chair cow table dog horse mbike prsn plant sheep sofa train tv mAP</figDesc><table>SVM 
98.6 95.9 97.1 95.8 71.2 91.0 93.6 96.1 72.9 86.6 88.3 96.3 96.7 94.0 97.4 70.5 92.8 82.5 97.4 89.9 90.2 
SVM-Max 
98.6 97.3 97.4 96.6 78.3 93.0 95.2 96.9 76.3 87.2 88.4 96.8 97.0 95.0 98.5 75.1 93.2 83.7 97.7 92.4 91.7 
LSSVM 
98.9 95.6 97.2 95.4 69.5 91.1 93.5 96..4 73.9 87.4 88.7 96.5 96.9 94.1 97.1 70.3 93.7 83.5 98.3 88.4 90.3 
LSSVM-Max 98.9 96.9 97.9 95.7 75.1 92.8 94.7 97.1 76.9 88.7 88.9 97.2 97.1 95.2 98.2 75.5 95.0 84.3 98.4 91.4 91.8 
LSSVM-SSD 98.9 96.8 97.7 95.8 75.0 92.7 94.4 97.0 76.9 89.0 88.9 97.3 97.1 95.2 98.2 75.5 95.0 84.3 98.4 91.3 91.8 
RRSVM 
99.2 97.4 98.1 96.7 79.7 94.5 95.9 97.4 79.3 89.3 88.9 97.7 97.1 95.7 98.8 79.5 95.4 84.8 98.6 93.1 92.9 

Chatfield [4] 95.3 90.4 92.5 89.6 54.4 81.9 91.5 91.9 64.1 76.3 74.9 89.7 92.2 86.9 95.2 60.7 82.9 68.0 95.5 74.4 82.4 
Wei et al. [40] 96.0 92.1 93.7 93.4 58.7 84.0 93.4 92.0 62.8 89.1 76.3 91.4 95.0 87.8 93.1 69.9 90.3 68.0 96.8 80.6 85.2 
VGG16 [31] 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Average</figDesc><table>Precision (%) on VOC 2007 test set. The entries with the highest APs for each object category are printed 
in bold. The top four methods use VGG16 features where LSSVM-Max is for max pooling and LSSVM-SSD learns SSD 
based on LSSVM scores. RRSVM has the highest AP in all categories. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Average precision (%) on VOC2012 Action validation set. Both methods use VGG16 features. LSSVM average all feature vectors while RRSVM uses them selectively. RRSVM outperforms LSSVM on all action categories. The LSSVM obtained is subsequently used to select a new set of hard negative examples for retraining the LSSVM. The new set of hard negative examples is selected from negative examples with the highest classification scores. Additionally, we maintain the cardinality of the negative set and ensure that each negative class has at least 10 examples. The procedure for training an LSSVM and picking a new set of negative examples is repeated for three iterations (it often converges in three iterations). The final selected set of negative examples is then used to train both an LSSVM and an RRSVM.</figDesc><table>amples. In practice, however, this creates a bottleneck in 
data loading because the data is too big to be fit in memory, 
and this is especially problematic for iterative optimization. 
To circumvent this problem, we train LSSVM and RRSVM 
with a reduced set of negative examples. The set of nega-
tive examples is based on hard negative mining as follows. 
For each class, we first train an LSSVM with all positive 
training examples and a random set of negative examples. 
To maximize the diversity, we randomly pick 15 negative 
examples in each of the 999 negative classes. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Mean average precision on VOC2012 action recognition test set. RRSVM outperforms the previous state-of-the-art by 1.5%.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4</head><label>4</label><figDesc>compares the performance of RRSVM with several other methods on the validation and the test sets of the ILSVRC 2014 dataset. Overall, the results of RRSVM are comparable with the state-of-the-art methods even though it uses only one neural network and without the expensive multi-cropping procedure. The fairest comparison is between RRSVM and VGG16 that does not use multi-crop (first method ofTable 4) because both methods use the same features and have the same setup. In this comparison, RRSVM outperforms the baseline by 0.8% and 1.9% using Top-5 and Top-1 error measurement respectively.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www-01.ibm.com/software/commerce/optimization/cplex-optimizer/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This project is partially supported by the National Science Foundation Award IIS-1566248.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Good practice in large-scale learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representing shape with a spatial pyramid kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval</title>
		<meeting>the ACM International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visor: Towards on-the-fly large-scale object category retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A regularization framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="www.pascal-network.org/challenges/VOC/voc2012/workshop/,2012.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deterministic annealing for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2604</idno>
		<title level="m">Actions and attributes from wholes and parts</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regularized max pooling for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving human action recognition using score distribution and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Action recognition from weak alignment of body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning discriminative localization from weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1523" to="1534" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple-instance ranking: Learning to rank images for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale image classification: fast feature extraction and svm training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised discriminative localization and classification: a joint learning process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilinear classifiers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>doi: 10.1007/ s11263-015-0816-y. 1</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High-dimensional signature compression for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ridge regression learning algorithm in dual variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Least Squares Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Gestel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>World Scientific</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Leave-one-out kernel optimization for shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">Cnn: Single-label to multi-label</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Svm versus least squares svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image classification using super-vector coding of local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
