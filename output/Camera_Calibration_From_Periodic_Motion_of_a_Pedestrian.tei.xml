<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Camera Calibration from Periodic Motion of a Pedestrian</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Center for Information Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghua</forename><surname>Ying</surname></persName>
							<email>xhying@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Center for Information Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangpeng</forename><surname>Rong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Center for Information Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Shang</surname></persName>
							<email>shangzeyu@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Center for Information Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
							<email>zha@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Center for Information Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Camera Calibration from Periodic Motion of a Pedestrian</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Camera calibration directly from image sequences of a pedestrian without using any calibration object is a really challenging task and should be well solved in computer vision, especially in visual surveillance. In this paper, we propose a novel camera calibration method based on recovering the three orthogonal vanishing points (TOVPs), just using an image sequence of a pedestrian walking in a straight line, without any assumption of scenes or motions, e.g., control points with known 3D coordinates, parallel or perpendicular lines, non-natural or pre-designed special human motions, as often necessary in previous methods. The traces of shoes of a pedestrian carry more rich and easily detectable metric information than all other body parts in the periodic motion of a pedestrian, but such information is usually overlooked by previous work. In this paper, we employ the images of the toes of the shoes on the ground plane to determine the vanishing point corresponding to the walking direction, and then utilize harmonic conjugate properties in projective geometry to recover the vanishing point corresponding to the perpendicular direction of the walking direction in the horizontal plane and the vanishing point corresponding to the vertical direction. After recovering all of the TOVPs, the intrinsic and extrinsic parameters of the camera can be determined. Experiments on various scenes and viewing angles prove the feasibility and accuracy of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the field of computer vision, camera calibration is one of the most fundamental issues for many applicants including 3D reconstruction, object recognition, metrology, and surveillance. Considerable efforts have been made to compute the intrinsic and extrinsic parameters of the camera with consideration of speed, accuracy and robustness <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>. Vanishing point based calibration methods have been proved to be suitable for the situation of structured scenes. Caprile and Torre <ref type="bibr" target="#b2">[3]</ref> proposed a method to calibrate _____________________________ * Corresponding Author a camera with known aspect ratio and skew from a single view of the TOVPs. They demonstrated that the principal point of the camera coincides with the orthocenter of the triangle with vertices being the TOVPs. Cipolla et al. <ref type="bibr" target="#b3">[4]</ref> proposed a simple and geometrically intuitive method using the TOVPs and one reference point to determine both intrinsic and extrinsic parameters, and the method was realized with various viewpoints in indoor and outdoor architectural scenes when the TOVPs are available.</p><p>Since calibration objects are often absent in surveillance scenes and parameters of surveillance cameras may be changed over time, using vanishing points extracted from image sequences of a pedestrian seems to be an imperfect, but not-that-bad choice for camera calibration, though the extracted vanishing points are not always as accurate as those in structured scenes like buildings with sufficient and strong rigidity constraints of parallelism and orthogonality. Lv et al. <ref type="bibr" target="#b15">[16]</ref> recovered the vertical vanishing point and the horizon line by detecting leg-crossings of a walking human. In order to determine the two orthogonal vanishing points in the horizontal plane, they need to point out two orthogonal lines on the ground which must be simultaneously taken with the pedestrian. Krahnstoever and Mendonca <ref type="bibr" target="#b12">[13]</ref> presented a Bayesian method for calibration using the foot-to-head homology acquired from the visual surveillance of human activity, where the probabilistic spatial distribution of the tracks on the horizontal plane is required. Junejo and Foroosh <ref type="bibr" target="#b10">[11]</ref> used the detected head and feet locations to compute two epipoles as two orthogonal vanishing points. They assumed that the camera's intrinsic parameters are almost fully known except for the focal length. The Total Least Squares method was applied to the observation points to estimate the focal length. Micusik and Pajdla <ref type="bibr" target="#b17">[18]</ref> proposed a method for automatic simultaneous camera calibration and the foothead homology estimation by observing a person standing at various locations in the scene with the same pose, e.g., standing to attention and facing the same direction in front of the camera. They formulated the calibration of intrinsic and extrinsic camera parameters as a Quadratic Eigenvalue Problem. Kusakunniran et al. <ref type="bibr" target="#b13">[14]</ref> utilized the cross-ratio relationship in projective geometry to directly estimate a full projection matrix. However, they required observing three or more positions of person walking on a ground plane, where the three positions are not collinear. It means that the person would walk around in the field of view of the camera. It is not difficult to find out that all these previous methods require assumption or prior knowledge about scenes <ref type="bibr" target="#b15">[16]</ref>, non-natural or pre-designed special motion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>, or only two orthogonal vanishing points extracted but not all of the TOVPs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>In this paper, we are motivated to develop a method which does not necessitate any assumption of specific scenes or motions but just exploits the periodic motion of human walking in a straight line. In the absence of favorable information in the scenes or pre-designed special motion, we recover the TOVPs just from the image sequence of a pedestrian walking in a straight line. Indeed, for real surveillance scenes, pedestrians often walk only one pass in the field of view of the camera, unusually walk around as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>. In many cases, the assumption of walking approximate in straight line is not very difficult to be satisfied, since the minimal data for our calibration method are just continuous three steps, namely, four continuous shoe prints on the ground. The main contribution of the paper is that we consider the shoe prints as the stable and easily detected features in the image sequence of pedestrians, and the TOVPs are recovered from these features more robustly, and then for camera calibration. To the best of our knowledge, this is the first work showing that it is possible to calibrate camera through the images of shoes of pedestrians. Since the techniques for detecting of periodic motion of human <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21]</ref> and foot pose estimation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12]</ref> have been well exploited, we are able to extract the toes of the shoes on the ground plane efficiently from the periodic motion. We are especially interested in special case when the two legs are maximally separated as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, since it corresponds to a critical phase where the toe-to-toe distance reaches maximum value, and usually shoes contact the ground plane. We called such case as "lambda-shaped" one. Since the toes of the shoes on the ground plane will keep fixed on the ground for a relatively long time, they can be detected easily and robustly, with comparisons of the head and feet locations as used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. We divide the set of detected toes into two sets related to the left and right toes, respectively. The vanishing point corresponding to the walking direction can be detected by computing the intersection of the two parallel lines formed by the left and right toes. Furthermore, we use the harmonic conjugate properties in projective geometry to recover the vanishing point corresponding to the perpendicular direction of the walking direction in the horizontal plane and the vertical vanishing point. After recovering all of the TOVPs, the intrinsic and extrinsic parameters of the camera can be determined from the detected TOVPs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview of the proposed method</head><p>When a pedestrian is walking in a straight line, it easily ensures the pace of a pedestrian as constant within a short period, thus the adjacent toes corresponding to the same shoe on the ground plane are equidistant. If we divide the detected toes into two sets related to the left and right toes, we can acquire two sets of 2D points whose correspondences in 3D are equidistant and respectively lying on two parallel lines (see <ref type="figure" target="#fig_2">Figure 2a</ref>). During the periodic motion of human walking, the two legs will separate to the max distance when the front shoe just touches the ground, which will remain almost stationary on the ground plane for a relatively long time until the back shoe moves forward and replaces it as the front shoe. We define this special case when the two legs are maximally separated as "lambda-shaped" one, and its corresponding frames in the image sequence as lambda-shaped frames.</p><p>Since the toes of the shoes on the ground plane will keep fixed on the ground for a relatively long time, they can be detected easily and robustly.</p><p>Given an image sequences of a pedestrian walking in a straight line, we extract the toes of the shoes from the pedestrian blobs in the lambda-shaped frames, as shown in <ref type="figure" target="#fig_0">Figure 1a</ref> (see Section 3 for details). We construct two image lines corresponding to the left and right toes on the ground plane, respectively, and then the vanishing point corresponding to the walking direction, v x can be determined as shown in <ref type="figure" target="#fig_0">Figure 1b</ref>.</p><p>The images of the midpoints of adjacent toes can be recovered by constructing harmonic conjugate systems, with the help of v x , as shown in <ref type="figure" target="#fig_0">Figure 1c</ref> and <ref type="figure" target="#fig_2">Figure 2a</ref>. We connect the midpoints and its corresponding toes in the other side in order to construct lines perpendicular to the direction of the walking direction on the horizontal plane, as shown in <ref type="figure" target="#fig_2">Figure 2a</ref>. Then, the vanishing point v y is determined by computing their common intersection, as shown in <ref type="figure" target="#fig_0">Figure 1c</ref>. We also propose another approach to detect v y : Firstly connect the left and right toes to get two groups of lines, as shown in <ref type="figure" target="#fig_2">Figure 2b</ref>. Secondly compute the common intersections of these lines to acquire two novel vanishing points v l and v r on the horizon line. Finally, construct harmonic conjugate system to determine v y , as shown in <ref type="figure" target="#fig_0">Figure 1d</ref> (see Section 4 for details).</p><p>The detection of the vertical vanishing point v z is illustrated in Section 4.3 (see <ref type="figure">Figure 6</ref>). The detailed implementation of calibration algorithm using the recovered TOVPs is described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Detect image points of toes on ground plane</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Extract pedestrians in lambda-shaped frames</head><p>Given an image sequence, a statistical background model <ref type="bibr" target="#b7">[8]</ref> can help extract the moving foreground objects. Many efforts have been made to detect pedestrian <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>. For each frame of the sequence, the blob of the pedestrian can be fast extracted with a blob tracker if no strong shadow exists <ref type="bibr" target="#b9">[10]</ref>. We need to first pick out the lambda-shaped frames to enable the next step to detect the toe positions from the corresponding blobs in these frames.</p><p>We first apply PCA to blob in each of frames. Denote the first and second eigenvalue of the covariance matrix at i-th frame as and , then we define = / , where the superscripts (1) and (2) are used to distinguish the first and second eigenvalue. The curve related to k i as shown in <ref type="figure" target="#fig_3">Figure 3a</ref> has the following properties: It reaches peaks in lambda-shaped frames and valleys in leg-crossing frames, thus pick out the peaks and we can determine the lambdashaped frames, as shown in <ref type="figure" target="#fig_3">Figure 3a</ref>. Due to the tiny change of step frequency and noise, the curve may not keep to a strictly fixed period and appear unsmooth, we can apply quadratic curve fitting around the local peaks to deal with </p><formula xml:id="formula_0">V Y V X V R V X V L</formula><p>it, as illustrated in <ref type="figure" target="#fig_3">Figure 3b</ref>. If the included angle between the walking direction and the projection of the viewing direction on the ground plane is too small, the two legs in the blobs will not separate, thus the above procedure cannot pick out the lambda-shaped frames exactly but just provides some initials for the lambda-shaped frames. We need to detect the fixed pixels of the blobs around the initial frames.</p><p>Once the percentage of the fixed pixels has been higher than the preset threshold starting from some frames, it means the front shoe has been fixed on the ground, and the lambdashaped frames are determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Detect toe positions on pedestrian blobs</head><p>If the colors of the pedestrian's shoes favorably contrast with those of the background, the shoes can be extracted perfectly <ref type="bibr" target="#b11">[12]</ref>. Nevertheless, a pedestrian in a surveillance scene does not necessarily wear particular shoes. Therefore we provide a method to detect the front toe position from a pedestrian's blob in the lambda-shaped frame.</p><p>We denote the blob's center as c and its first eigenvector as e, illustrated in <ref type="figure" target="#fig_3">Figure 3c</ref>. For each pixel in the blob, we define the vector from c to the i-th accessed pixel as t i , initial toe position f is the pixel that corresponds to the minimal dot product of e and t:</p><formula xml:id="formula_1">=arg min •<label>(1)</label></formula><p>Note that the minimal dot product is a negative value. In case that two or more pixels correspond to the minimal scalar product, we choose the one that is most apart from the principle axis as the optimal initial toe position. Making use of the property that the toes will keep fixed on the ground for a relatively long time, we can optimize the initial toe position f to acquire the refined toe position on the ground plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Recover the three orthogonal vanishing points 4.1. Recover the vanishing point corresponding to the walking direction</head><p>Denote the left and right toe positions as { } ,…, and { } ,…, , where |M−N| ≤ , and the superscripts (l) and (r) are used to distinguish the left and right toes, and the subscripts i and j run over all left and right toes (see <ref type="figure" target="#fig_4">Figure 4</ref>). Let the lines best fitting these points be = , and = , , where + = , and the subscript l and r are used to distinguish the lines related to the left and right toes. S l and S r are easily determined as: * , * =a r g * , * min | + | / where Σ k is the covariance matrix of f k , and the subscript k runs over all left toes or right toes. The vanishing point corresponding to the walking direction on the horizontal plane v x is then detected by computing the intersection of S l and S r (see <ref type="figure" target="#fig_4">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Recover the vanishing point corresponding to the perpendicular direction of the walking direction on the horizontal plane</head><p>We present two approaches to detect the vanishing point v y corresponding to the perpendicular direction of the walking direction on the horizontal plane in Step 1 and Step 2, and combine them into a unique solution in Step 3.</p><p>Step 1: A natural idea to recover v y is to find two or more image lines whose corresponding 3D lines are mutually parallel and perpendicular to the walking direction, then v y can be determined by computing common intersection of these image lines. We propose a construction procedure of these desired image lines as follows: Denote as the image point of the midpoint related to and in 3D, and similarly denote related to and (see <ref type="figure" target="#fig_4">Figure 4)</ref>.</p><p>We use the harmonic conjugate properties in the projective geometry to compute and . Assume that in 3D space, is the midpoint of and , X is the point at infinity in the line determined by . The cross ratio <ref type="bibr" target="#b21">[22]</ref> of these four points is determined as:</p><formula xml:id="formula_2">, = = • =−</formula><p>(3) As the cross ratio of these four points equals -1, we say that , , and X make up a harmonic system of points,  <ref type="bibr" target="#b1">(2)</ref> or and are harmonic conjugate points relative to and X. According to the property of projection transformation that cross ratio is a projective invariant <ref type="bibr" target="#b21">[22]</ref>, the projections of these four points , , and X in the image plane, namely , , , and v x , also satisfy the harmonic conjugate relationship:</p><formula xml:id="formula_3">(a) (b) (c)</formula><formula xml:id="formula_4">, = , = −<label>(4)</label></formula><p>can be determined by solve <ref type="bibr" target="#b3">(4)</ref>. Use the exactly same properties we can also compute from the harmonic conjugate relationship: where the subscript t runs over all parallel lines. Denote q t as the midpoint of and , v y is then detected as:</p><formula xml:id="formula_5">, =−<label>(5)</label></formula><p>=arg min + + + (6) where ( , ) is the line determined by q t and v.</p><p>Step 2: v y can be recovered in another way which constructs harmonic conjugate system by vanishing points directly. As illustrated in <ref type="figure">Figure 5a</ref>, we can acquire a group of parallel lines by connecting with in sequence, and another group of parallel lines by connecting and . Denote the common intersection of the first group of lines as v l , and the common intersection of the second group of lines as v r . These two intersections are both vanishing points collinear with v x and v y on the horizon line. We can also construct harmonic conjugate system to detect v y as:</p><formula xml:id="formula_6">, , , =−<label>(7)</label></formula><p>The reason is as follows: As shown in <ref type="figure">Figure 5b</ref>, X is the point at infinity in the direction of walking, Y is the point at infinity in the direction perpendicular to the direction of walking, L and R are the points at infinity respectively corresponding to the direction of the lines connecting the left and right toes in the specific way we described above (see <ref type="figure" target="#fig_2">Figure 2b)</ref>, O can be any point not at infinity. Denote four lines = , = , = , = , then the cross ratio of these four lines equals the cross ratio of the four points <ref type="bibr" target="#b21">[22]</ref>, i.e.:</p><formula xml:id="formula_7">, = ,<label>(8)</label></formula><p>Due to the symmetry of left and right toes during the periodic motion of straight walking: x is the internal angular bisector of ∠LOR , y is the external angular bisector of ∠LOR , namely, = ,θ=φ (see <ref type="figure">Figure 5b</ref>). Thus we can compute the cross ratio of the four lines as well as that of the four points:</p><formula xml:id="formula_8">, = , = sin , sin , • sin , sin , = sin − sin • sin −θ sin −φ =−</formula><p>According to the property of projection transformation that cross ratio is a projective invariant, the correspondences of the four points L, R, X, Y in the image plane, namely v l , v r , v x , v y , also satisfy the harmonic conjugate relationship:</p><formula xml:id="formula_9">, , , = , =−<label>(10)</label></formula><p>Since , , have been determined, by solving equation <ref type="bibr" target="#b9">(10)</ref>, we can obtain v y .</p><p>Step 3: Denote v y acquired in step 1 and step 2 as and , respectively. We combine both results to uniquely determine v y in the balance of different situations. If the viewpoint's height is similar to the pedestrin's height, the two lines fitting the left toes and right toes are close to overlap, we tend to prefer instead of because in the first approach, the extremely short distance between a is more reliable than , as in the second approach, the lines connecting the left and right toes do not always intersect at a point in the horizon line due to noise and outliers. Except for these two extreme conditions, we utilize both and to determine v y , and employ a parameter to reasonably balance their weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Recover the vertical vanishing point</head><p>We recover the vanishing point v z corresponding to the vertical direction by constructing W vertical poles and compute their common intersection (W=M+N− ). As shown in <ref type="figure">Figure 6a</ref>, the top of a pole is the head position in a lambda-shaped frame, which is the tangent point on the common tangent line to all the lambda-shaped blobs, and the tangent line should pass through v x . The corresponding bottom is the midpoint of the two toes in a lambda-shaped frame, which can also be acquired by using the harmonic conjugate property. Denote the image point of midpoint related to and in 3D as , and the image point of midpoint related to and in 3D as , as shown in <ref type="figure">Figure 6b</ref>. By utilizing the harmonic conjugate properties, and v l and v r being available, we can compute and as:</p><formula xml:id="formula_10">, = , =−<label>(11)</label></formula><p>By computing the common intersection of these vertical poles, the vanishing point v z is detected with the similar approach to detect v y as illustrated in (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Calibration algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Calibration with the recovered TOVPs</head><p>If a camera with zero skew and unit aspect ratio, i.e., a threeparameter camera model, the camera parameters left for us to determine are the focal length f, the principal point , , the rotation matrix R and the translation vector T. Now we have obtained the TOVPs as shown in previous sections. The calibration algorithm goes as the following steps:</p><p>Determine the intrinsic matrix: For triangle v x v y v z , its orthocenter coincides with the principal point = </p><formula xml:id="formula_11">v y v x v r v l Vanishing Line f 3 (r) f 2 (r) f 1 (r) f 3 (l) f 2 (l) f 1 (l) O L X R Y l x r y α β θ φ v x v l Vanishing Line Common Tangent Line v l ,</formula><p>of the image plane. If denote the coordinates of v x , v y and v z as (x 1 , y 1 ) T , (x 2 , y 2 ) T and (x 3 , y 3 ) T , when the origin of the coordinate system is located at the principal point, then focal length f can be determined by the following equations:</p><formula xml:id="formula_12">+ + = + + = + + =<label>(12)</label></formula><p>Detailed proof and explanations for these properties can be found in <ref type="bibr" target="#b2">[3]</ref>. Determine the rotation matrix: From obtained X = (x 1 , y 1 , f) T , Y = (x 2 , y 2 , f) T and Z = (x 3 , y 3 , f) T , rotation matrix can be easily acquired as:</p><formula xml:id="formula_13">= /norm /norm /norm (13)</formula><p>Determine the translation vector: Given a 3D point, denote its camera coordinates as , and its world coordinates as , they satisfy:</p><formula xml:id="formula_14">= +<label>(14)</label></formula><p>For the translation vector T = (T X , T Y , T Z ) T , T Z is the camera height where the origin of the world coordinate system is on the horizontal plane. If the height of walking human H is given, T Z can be determined by the cross ratio of the four points A, B, C, D:</p><formula xml:id="formula_15">= − ,</formula><p>where D is the vanishing point v z , B and C are the head and bottom positions in lambda-shaped frames, A is the intersection of the horizon line l Inf (determined by the two ground vanishing points v x and v y ) and the line passing through B, C and D. If we assign a point in the image plane as the correspondence of the world coordinate system's origin, T X and T Y can be computed by solving the equation of perspective projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Minimal data and robust calibration</head><p>We need at least adjacent two left toes and two right toes to construct enough lines and detect all of the TOVPs, thus the minimal data for our calibration method are continuous three steps, which generate four continuous toe positions of the shoe prints on the ground. It is obvious that a minimal data with such short path lengths easily satisfies the assumption of walking approximate in a straight line, namely, a little change of walking direction is not so serious. Additionally, if provided more data, we can complete even more accurate and robust calibration against noise and outliers, by dividing the extracted toes into groups of four, namely, groups of minimal data. From each group and its corresponding head position, we can detect a set of TOVPs and then compute camera parameters. For the intrinsic parameter sets as { , } ,..., , where f i and = , , , are the focal length and principle point determined by the i-th group, we can adopt RANSAC to eliminate unreasonable sets first and then determine the ultimate parameters by majority voting or least square methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Degenerate case</head><p>The degenerate case is that two lines determined by the left and right toes become almost coincided. In this case, v z and v x are still available, with v y undetermined. Under assumption that the principle point = , coincides with the center of the image plane, we can solve the third equation in <ref type="bibr" target="#b11">(12)</ref> and obtain focal length f. Substituting f into the first two equations in <ref type="formula" target="#formula_1">(12)</ref>, we can estimate the vanishing point v y . R and T can be determined with the similar procedure as described in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In order to verify the proposed method, we use both sequences recorded by ourselves and downloaded from EPFL data set <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. We recorded sequences of pedestrians in various scenes from different heights and viewing angles. The sequences were recorded with Cannon LEGRIA HFS21 and have a resolution of × . Some images taken in two different sequences Seq. #1 and Seq. #2 are shown in <ref type="figure" target="#fig_6">Figure 7</ref>. Seq. #1 was shot downwards from a balcony on the second floor of a building (see <ref type="figure" target="#fig_6">Figure 7ab</ref>), Seq. #2 was shot from the top corner of a corridor (see <ref type="figure" target="#fig_6">Figure 7cd</ref>), which are typical sequences of surveillance scenes shot at close or medium range. The detected left toes, right toes and head positions have been marked with different colors in these images.</p><p>For each sequence, we also record a sequence of a checkerboard in different positions and orientations. The calibration results obtained from the method of <ref type="bibr" target="#b25">[26]</ref> are used as the ground truths. The comparisons between the ground truths and the intrinsic parameters estimated by the proposed method are presented in <ref type="table">Table 1</ref>.  <ref type="bibr" target="#b25">[26]</ref> and the proposed method <ref type="bibr" target="#b14">(15)</ref> From <ref type="table">Table 1</ref>, we may find that the calibration results of the proposed method are not so far from the ground truths. It is satisfying as expected and shows that camera calibration through the shoes of pedestrians is feasible. Most of the previous pedestrian based calibration methods are suitable for sequences took at medium or long ranges, but may not work or fail when dealing with sequences took at close ranges. In the two typical surveillance scenes as shown in <ref type="figure" target="#fig_6">Figure 7</ref>, where a pedestrian may well walk in a straight line for one pass in only a few steps and then goes out of sight, the proposed method may be the only solution so far.</p><formula xml:id="formula_16">Seq. # f u 0 v 0 1 Ground</formula><p>We also use the multi-camera pedestrian sequences downloaded from EPFL data set <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> to verify the proposed method. The available sequences have an original resolution of × . As shown in <ref type="figure">Figure 8</ref>, Seq. #3 was shot outside a building on a terrace, which is also a common surveillance scene. The ground truths provided by EPFL data set <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> are obtained from the Tsai model <ref type="bibr" target="#b23">[24]</ref>. Comparisons between the ground truths obtained from <ref type="bibr" target="#b23">[24]</ref> and the intrinsic parameters estimated by the proposed method are presented in <ref type="table">Table 2</ref>. As presented in <ref type="table">Table 2</ref>, the calibration results of the proposed method are basically consistent with the ground truths obtained from <ref type="bibr" target="#b23">[24]</ref>. Experiments on sequences recorded by ourselves and downloaded from EPFL data set <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> in different scenes and viewing angles prove the feasibility and accuracy of the proposed method, especially for sequences shot in common surveillance scenes at close or medium ranges.</p><formula xml:id="formula_17">Seq. # f u 0 v 0 3 Ground</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In some typical surveillance scenes, such as subway stations, supermarkets, hospitals, hotels and etc., a majority of surveillance cameras are mounted in some top corners of the ceilings and at close or medium ranges from monitored persons. We find that in such situations, the shoes of pedestrians are very prominent in frames, especially the shoe prints, which are easily detectable, and may generate very regular pattern or, more precisely, repeatable pattern on the ground. As we known, repeatable pattern may be a very good choice for camera calibration, e.g., the commonly used checkerboard pattern. Therefore, this paper aims at employing such repeatable patterns for camera calibration. To the best of our knowledge, this is the first work showing that it is possible to calibrate camera through the images of shoes of pedestrians. By recognizing the "lambda-shaped" frames when two legs are maximally separated and left and right shoes both contact the ground, we can determine the image positions of the toes on the ground plane, as well as the corresponding head positions. Then we recover the TOVPs by utilizing the harmonic conjugate property to mine the metric information implicitly existing among the left and right shoe prints. After detecting all of the TOVPs, the intrinsic and extrinsic parameters of the camera can be determined. The minimal data for calibration in the proposed method are just continuous three steps, namely, four continuous shoe prints on the ground, thus easily ensuring the assumption of walking approximate in straight line. The degenerate case when left and right toes become almost collinear on the ground plane is also well discussed in this paper. Our ongoing work is to utilize traces of shoes of pedestrians to calibrate multiple cameras.  <ref type="figure">Figure 8</ref>. Two detected lambda-shaped frames in Seq. #3 from EPFL data set <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. The detected left toes are marked as red, right toes are marked as green and head positions are marked as pink. Note that the pedestrian's height and detected head positions in the lambda-shaped frames hardly change with the slight rotation of his head.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Recover the vanishing point vx corresponding to the walking direction, and the vanishing point vy corresponding to the perpendicular direction of the walking direction in the horizontal plane. (a) Extract the toes of the shoes from the pedestrian blobs in the lambda-shaped frames when the two legs are maximally separated. Left toes are marked as red, and right toes as green. (b) Use the left and right toes respectively to construct two image lines. The vanishing point vx is their intersection. (c) Use harmonic conjugate properties in projective geometry to recover the images of the midpoints of adjacent toes, marked as black, and construct lines perpendicular to the walking direction on the ground, the vanishing point vy is their intersection. (d) Another method to detect vy: Acquire two other vanishing points v l and vr on the horizon line, and construct harmonic conjugate system to determine vy (see Section 4 for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Top view of shoe prints, related to Figure 1. (a) The constructing procedures corresponding to Figure 1c. (b) The constructing procedures corresponding to Figure 1d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Extract the front toe position of a pedestrian in a lambdashaped frame. (a) The plot of = / . (b) Adopt quadratic curve fitting near the local peaks of (a). Pick out the curve peaks to determine the lambda-shaped frames. (c) A pedestrian's blob in a lambda-shaped frame. c is the center position. e is the first eigenvector. For each pixel of the blob, ti is the vector from c to the i-th accessed pixel. The initial toe position is the pixel that corresponds to the minimal dot product of e and t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The first approach to recover the vanishing point vy corresponding to the perpendicular direction of the walking direction on the horizontal plane. Use the harmonic conjugate property to compute the images of the midpoints and of adjacent toes, then construct lines connecting the midpoints and the corresponding toes of the other shoe, vy is detected by computing their common intersection. midpoint on one side and the toe on the other side makes the construction of parallel lines difficult to implement. If the viewing direction is nearly parallel with the walking direction,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>The second approach to recover the vanishing point vy corresponding to the perpendicular direction of the walking direction on the horizontal plane. (a) Connect the adjacent left and right toes to acquire two groups of lines, then compute their common intersections to locate v l and vr, finally construct harmonic conjugate system to detect vy. (b) Top view of the constructed harmonic conjugate system. Construct vertical poles to detect the vertical vanishing point vz. (a) The head positions are the tangent points on the common tangent line to all of the lambda-shaped blobs, and marked as pink. The tangent line should pass through vx. The midpoints of two toes in lambda-shaped frames are computed using harmonic conjugate property, and marked as blue. (b) Connect the head positions and corresponding midpoints to construct vertical poles. vz is detected by computing their common intersection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Some detected lambda-shaped frames in two of our sequences used in experiments. (a) and (b) are from Seq. #1, (c) and (d) are from Seq. #2. The detected left toes are marked as red, right toes are marked as green and head positions are marked as pink. (The personal information has been hidden to comply with the blind review policy.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1. The comparisons of calibration results obtained from</figDesc><table>truth [26] 
Ours 

5324.54 1011.57 521.29 
4433.11 1012.83 470.45 
2 Ground truth [26] 
Ours 

2229.51 996.65 270.06 
2107.92 1165.38 329.04 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 2. The comparisons of calibration results obtained from<ref type="bibr" target="#b23">[24]</ref> and the proposed method</figDesc><table>truth [24] 
Ours 

856.36 
355.51 
241.21 
740.98 
407.43 
223.48 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">v y v x Vanishing Line m 2 (r) m 1 (r) m 2 (l) m 1 (l) f 3 (r) f 2 (r) f 1 (r) f 3 (l) f 2 (l) f 1 (l)<ref type="bibr" target="#b8">(9)</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">f 3 (r) f 2 (r) f 1 (r) f 3 (l) f 2 (l) f 1 (l)h 3 (l) h 2 (l) h 1 (l) n 3 (l) n 2 (l) n 1 (l)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generic temporal segmentation of cyclic human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Albu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Quirion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="21" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1806" to="1819" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using vanishing points for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caprile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Camera Calibration from Vanishing Points in Image of Architectural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust real-time periodic motion detection, analysis, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="781" to="796" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2179" to="2195" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Three-dimensional computer vision: a geometric viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multicamera people tracking with a probabilistic occupancy map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lengagne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="282" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Trajectory rectification and path modeling for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D pose estimation for foot motion tracking from image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Consumer Electronics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian autocalibration for surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krahnstoever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mendonca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A direct method to selfcalibrate a surveillance camera by observing a walking pedestrian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kusakunniran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Periodic motion detection and segmentation via approximate sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Camera calibration from video of a walking human</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1513" to="1518" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A theory of self-calibration of a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="151" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simultaneous surveillance camera calibration and foot-head homology estimation from human detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Foot-based mobile interaction with games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Paelke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stichling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Computer Entertainment Technology</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pedestrian detection via periodic motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="160" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D reconstruction of periodic motion from a single view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ribnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="44" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Algebraic projective geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Semple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Kneebone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A versatile camera calibration technique for highaccuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="323" to="344" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Virtual and real world adaptation for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geroimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="797" to="809" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
