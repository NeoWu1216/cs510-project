<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Egocentric Look at Video Photographer Identity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University of Jerusalem Jerusaem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University of Jerusalem Jerusaem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Egocentric Look at Video Photographer Identity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>An important message in this paper is that photographers should be aware that sharing egocentric video will compro-mise their anonymity, even when their face is not visible.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Egocentric cameras are being worn by an increasing number of users, among them many security forces worldwide. GoPro cameras already penetrated the mass market, reporting substantial increase in sales every year. As headworn cameras do not capture the photographer, it may seem that the anonymity of the photographer is preserved even when the video is publicly distributed.</p><p>We show that camera motion, as can be computed from the egocentric video, provides unique identity information. The photographer can be reliably recognized from a few seconds of video captured when walking. The proposed method achieves more than 90% recognition accuracy in cases where the random success rate is only 3%.</p><p>Applications can include theft prevention by locking the camera when not worn by its rightful owner. Searching video sharing services (e.g. YouTube) for egocentric videos shot by a specific photographer may also become possible. An important message in this paper is that photographers should be aware that sharing egocentric video will compromise their anonymity, even when their face is not visible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The popularity of head worn egocentric cameras is increasing. GoPro reports an increase in sales of 66% every year, and cameras are widely used by extreme sports enthusiasts and by law enforcement and military personnel.</p><p>Special features of egocentric video include:</p><p>• The camera is worn by the photographer, and is recording while the photographer performs normal activities. • The camera moves with the photographer's head.</p><p>• The camera does not record images of the photographer. In spite of this we show that photographers can often be identified.</p><p>Photographers feel secure that sharing their egocentric videos on social media does not compromise their identity ( activity and operations of special forces recorded by wearable cameras are widely published on YouTube. Some have even recorded and published their own crimes. A consequence of our work is that the photographer identity of such videos can sometimes be found from camera motion. Body motion is an accurate and replicable feature for identifying people over time. It is often recorded by accelerometers ( <ref type="bibr" target="#b15">[16]</ref>) or by an overlooking camera. Egocentric video can effectively serve as a head mounted visual gyroscope and can accurately capture body motion information. It follows that any egocentric video which includes walking contains body motion information that can accurately reveal the photographer.</p><p>Specifically, we use sparse optical flow vectors (50 flow vectors per frame) taken over a few steps (4 seconds). This results in a set of time-series, one for each component of each optical flow vector. In <ref type="figure">Fig 2 we</ref> show the temporal Fourier Transform of one flow vector for three different sequences, showing visible differences between different photographers.</p><p>As a first approach for determining photographer identity, we computed LPC (Linear Predictive Coding 1 ) <ref type="bibr" target="#b2">[3]</ref> coefficients for each of the optical flow time series. All LPC coefficients of all optical flow sequences were used as a descriptor. Photographer recognition using a non-linear SVM trained on the LPC descriptor gave 81% identification accu- <ref type="bibr">Figure 2</ref>. Comparison of the temporal frequency spectra for three videos. Two videos were recorded using camera D1 by users A and B, the third video was recorded by user A using camera D2. It is readily seen that the spectra of the two videos recorded by photographer A are very similar to each other despite being recorded by different cameras and at different times. This suggests that a photographer's physique is expressed in the motion observed in his video. racy (vs. accuracy of 3% in random) and verification EER (Equal Error Rate) of 10%.</p><p>Our second approach learns the descriptor and classifiers using a Convolutional Neural Network (CNN) which includes layers corresponding to body motion descriptor extraction and to recognition. The CNN is trained on the optical-flow features described above. Using CNN improves the results over the LPC coefficients, yielding 90% identification rate (vs. accuracy of 3% in random) and verification EER (Equal Error Rate) of 8%.</p><p>The above experiments were performed on both a small (6 person) public dataset <ref type="bibr" target="#b5">[6]</ref> (originally collected for Egocentric Activity Analysis) and on a new, larger (32 person) dataset collected by us especially for Egocentric Video Photographer Recognition (EVPR).</p><p>The ability to recognize the photographer quickly and accurately can be important for camera theft prevention and for forensic analysis (e.g. who committed the crime). Other applications are web search by egocentric video photographer and organization of video collections. Wearing a mask does not reduce recognition rate, of course.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>Determination of the painter of an artwork for preventing forgery and fake artists has attracted attention for centuries. Computer vision researchers have presented several approaches for automatic artist and style classification mainly utilizing low-level and object cues <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Recognizing the unseen photographer of a picture is an interesting related problem. In this setting the photographic style <ref type="bibr" target="#b23">[24]</ref> and the location of the photograph <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref> can be used as cues for photographer recognition. Both methods are unable to distinguish between photographers using cameras on default settings (such as most wearable cameras) and at the same locations. Another approach is automatic recognition of the photographer's reflection (e.g. in the subject's eyes <ref type="bibr" target="#b17">[18]</ref>), but this relies on having reflective surfaces in the pictures.</p><p>Photographer recognition from wearable camera video is a novel problem. Such video is jittery due to the motion of the photographer's head and body. Although typically a nuisance, we show that frame jitters can accurately determine photographer identity.</p><p>Human body motion was already used for recognition. Gait recognition is typically done by a video camera observing a person's shape and dynamic walking style. These features are able to recognize a person accurately <ref type="bibr" target="#b16">[17]</ref>. In our scenario, however, the photographer is not seen by the camera which is worn on his head. Recognition from accelerometers carried on the user's body <ref type="bibr" target="#b15">[16]</ref> is also reported. Shiraga et al. <ref type="bibr" target="#b22">[23]</ref> studied people recognition wearing a backpack with stereo cameras. Rotation and period of motion were computed using 3D geometry, and users were accurately recognized. Unlike all prior art, we are interested in recognizing photographers of videos taken by standard wearable cameras (e.g. as exist on video sharing websites), nearly all of which are monocular, head or chest mounted.</p><p>Using optical flow for activity recognition from headmounted cameras has been done by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14]</ref> and others. Papers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> used head motion to retrieve headmounted camera users observed in other videos recorded at the same time. We, on the other hand, use camera motion to recognize the users of wearable cameras across time.</p><p>Feature design for time series data has been extensively studied, particularly for speech recognition systems ( <ref type="bibr" target="#b20">[21]</ref>). Speaker verification is a long standing problem which is related to this work. Linear Predictive Coding (LPC) descriptors are very popular for speaker recognition <ref type="bibr" target="#b6">[7]</ref>. We show that an LPC-based descriptor is highly effective also for user recognition from egocentric camera video.</p><p>In this paper we also take an end-to-end approach of learning features along with the classifier, instead of hand designing the features. We perform this using convolutional neural networks (CNN). For an overview of deep networks see <ref type="bibr" target="#b1">[2]</ref>. Learned features are sometimes better than handdesigned features <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Photographer Recognition from Optical Flow</head><p>Egocentric video suffers from bouncy and unsteady motion caused by photographer head and body motion. Al-  though usually a nuisance, we show that this motion forms the basis for accurate photographer recognition methods. We present our basic features in Sec. 3.1. Two alternative descriptors and classifiers are described in Sec. 3.2 and Sec. 3.3.</p><formula xml:id="formula_0">−→ x −→ t −→ t −→ x −→ t −→ t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>In the following sections we assume that the video frames were pre-processed in the following way (see <ref type="figure" target="#fig_1">Fig. 3</ref>):</p><p>1. Frames are partitioned into a small number (m x × m y ) of non-overlapping blocks. 2. m x × m y optical flow vectors are computed for each frame using the Lucas Kanade algorithm <ref type="bibr" target="#b14">[15]</ref>. We use 10×5 optical flow vectors per frame. 3. A block of T seconds of such optical flow vectors is taken. We used T = 4 seconds, which is long enough to include a few steps. At 15 fps this results in 60 frames. 4. Each feature vector covers a period of 4 seconds, and we computed feature vectors every 2 seconds. There is an overlap of 2 seconds between two successive feature vectors.</p><p>We used optical flow features for photographer recognition, rather than pixel intensities, as the body motion is eventually expressed by the pixel motion. On the other hand, recognition should be invariant to the specific objects seen in the environment, objects that are represented by pixel intensities. CNNs may be able to learn optical flow from pixel intensities, but learning this will require much more data than we can collect.</p><p>If dense optical flow were used as a feature, the high feature dimensionality would have lead to overfitting on small datasets. Using a smaller number of flow vectors gave reduced accuracy. In looking for the optimal feature size we found out that a grid size of 10×5 optical flow vectors was a good compromise between overfitting and accuracy.</p><p>The feature extraction process is shown in <ref type="figure" target="#fig_1">Fig 3.</ref> Visualization of two extracted feature vectors is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Full details are in Sec. 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LPC Descriptor + Kernel SVM</head><p>LPC <ref type="bibr" target="#b2">[3]</ref> is a popular time-series descriptor (e.g. for speaker verification). LPC assumes the data is generated by a physical system, here the photographer's head and body. It attempts to learn a linear regression model for its equations of motion, predicting for each optical flow series the flow value in the next frame given the flow values of previous k frames. Given a feature vector, we calculate an LPC model for each component of each 4s flow time series (100 models in total). Using too few coefficients yields less accurate predictions, while too many coefficients causes overfitting. We found k=9 to work well for our case. The final LPC descriptor consisted of all coefficients of all time-series models (100×9).</p><p>An RBF-SVM classifier was used for learning both identification (classify LPC descriptor into 1 of M known photographers) and verification (classify LPC descriptor into target photographer or rest-of-the-world). The non-linear (RBF) classifier outperformed linear SVM in almost all cases. As mentioned before, photographer recognition using a non-linear SVM trained on the LPC descriptor gave 81% identification rate (vs. random 3%), and the verification EER (Equal Error Rate) was 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convolutional Neural Network</head><p>In Sec. 3.2 we described a hand-designed descriptor for identity recognition. The LPC descriptor suffers from several drawbacks:</p><p>• The LPC regression model is learned for each timeseries separately and ignores the dependence between optical flow vectors.  <ref type="figure">Figure 5</ref>. A diagram of our CNN architecture for photographer recognition from a given flow feature vector. The operations on the data are shown on top, the sizes of subsequent data layers are shown on the bottom. The Neural Network learns the descriptor jointly with the classifier, therefore automatically creating a descriptor optimal to this task.</p><p>• The LPC descriptor and SVM classifier are learned independently, the labels cannot directly influence the design of the descriptor.</p><p>To overcome the above drawbacks, we propose to learn a CNN model for photographer recognition. The CNN learns descriptor and classifier end to end, and is able to take advantage both of dataset labels and the dependence between features when calculating filter coefficients. The CNN is a more general architecture, the LPC descriptor is a subset of descriptors learnable by the network.</p><p>Due to the limited number of data points available in our datasets, we limit our CNN to only 2 hidden layers. Using more layers increases model capacity but also increases over-fitting, and this architecture yielded the best performance. The architecture is illustrated in <ref type="figure">Fig. 5</ref>.</p><p>Our architecture is tailored especially for egocentric video. As we use sparse optical flow we do not assume much spatial invariance in the features (differently from most image recognition tasks). On the other hand the precise temporal offset of the photographer's actions is usually not important, e.g. the precise time of the beginning of a photographer's step is less important than the time between strides. Our architecture should therefore be temporally invariant. The first layer was thus designed to be convolutional in time but not in space.</p><p>The kernel size spans all the blocks across the x and y components over K T frames (we use K T = 20 which is a little longer than the typical step duration). The convolutional layer consists of M kernels (we use M = 128). The outputs of the kernels z 1 m = W m * x are passed through a ReLU non-linearity (max(z 1 m , 0)). We pool the outputs substantially in time, as the feature vector is of high dimension compared to the amount of training data available. To correspond to the typical time interval between steps we use kernel length of 20 and stride of 15.</p><p>The data is then passed through two fully connected (affine) layers each followed by a sigmoid non-linearity (σ(z) = 1 1+e −z ). The first fully connected hidden layer has N 1 hidden nodes (we used N 1 = 128). The output of this layer is the learned CNN descriptor.</p><p>The second fully connected layer is a linear soft-max classifier and has the same number of nodes as the number of output classes: 2 classes for verification, and 20 or 32 classes for identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Joint Prediction from Several Descriptors</head><p>Sec. 3.2 and Sec. 3.3 described a method to train a photographer classifier on a short (4 seconds) video sequence. The video used for recognition is usually significantly longer than 4 seconds.</p><p>We split the video into 4 second subsequences (overlapping by 2 seconds) and extract their feature vectors V t (t is the subsequence number). We compute the identity label (L t ) probability distribution for each feature vector V t using LPC or CNN classifiers trained as de- <ref type="figure" target="#fig_4">Figure 7</ref>. Classification accuracy vs. video length when one feature vector covers T = 4 seconds (Using CNN on the FPSI Dataset). Longer video allows extraction of more feature vectors. MAP classification consistently beats mode classification. Both methods can exploit longer sequences and thus improve on 4s sequence recognition. All methods perform far better than random. scribed above, We then classify the entire video into the globally most likely label, argmax i t P (L t = i|V t ) = argmax i t log(P (L t = i|V t )). While this classifier assumes that feature vectors are IID, we have found that this requirement is not necessary for the success of the method. See <ref type="figure" target="#fig_3">Fig. 6</ref> for an example on the FPIS dataset. MAP classification has helped boost the recognition performance on the EVPR dataset to around 90% (an increase of 13%) over the 4s rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Several experiments were performed to evaluate the effectiveness of our method. As there is no standard dataset for Egocentric Video Photographer Recognition, we use both a small (6 person) public dataset -FPSI <ref type="bibr" target="#b5">[6]</ref> that was originally collected for egocentric activity analysis. For each photographer -morning sequences were used for training, and afternoon sequences for testing.</p><p>In order to evaluate our method under more principled settings, we collected a new larger (32 person) dataset -EVPR -specifically designed for egocentric photographer video recognition. In the EVPR dataset all photographers recorded two 7 minute sequences (from which we extracted around 200 four second sequences each) on the same day with different head-mounted cameras (D1,D2) for training and testing. 20 of the photographers also recorded another 7 minute sequence with yet another camera (D3) a week later. Both datasets are described in detail in Sec. 6.1. The detailed experimental protocol is described in Sec. 6.2.  Both methods far outperform the random rate of 3% (Top-1) and 6% (Top-2). Both descriptors also beat the raw features by a large margin. <ref type="figure">Figure 9</ref>. CMC rates for recognition 1 week later (for 12s sequences). LPC accuracy: 76% (Top-1) and 86% (Top-2). The CNN further improves the performance with 91% (Top-1) and 96% (Top-2). Both methods far outperform the random rate of 5% (Top-1) and 10% (Top-2). Both descriptors also beat the raw features by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Photographer Identification</head><p>Test videos are usually longer than 4 seconds, and we have multiple feature vectors for each person. We combine predictions over a longer video using the MAP rule in Sec. 3.4. In <ref type="figure" target="#fig_4">Fig. 7</ref> we compare the MAP strategy vs. taking the most frequent 4s prediction in the test video (Mode). We observe that using longer sequences further improves recognition performance, reaching around 91% accuracy for 50 seconds of video. We also observe that MAP classifiers consistently beats the Mode classifier and use it in all other experiments.</p><p>To evaluate the recognition performance on a larger dataset, we show the performance of our method on our new dataset -EVPR. In this experiment the network was trained on video sequences for each photographer using Camera D1 and is evaluated on video sequences recorded on the same day using Camera D2 and a week later recorded using Camera D3. In <ref type="figure" target="#fig_5">Fig. 8</ref> and <ref type="figure">Fig. 9</ref> we present the cumulative match curve (CMC) for the same day and week later recognition results respectively. We use the Top-k notion, indicating that the correct result appeared within the top k predictions of the classifier. In addition to LPC and CNN, an RBF-SVM trained on the raw optical flow features is used as baseline to evaluate the quality of our descriptors. High accuracy was achieved in both scenarios, same day CNN recognition accuracy is 90% (top 1) and 93% (top 2). The recognition performance a week later is better with 91% (top 1) and 96% (top 2). The improved performance numbers a week later are expected due to the smaller dataset size (20 vs 32), but are nonetheless encouraging as many photographers wore different shoes from the D1 training sequence recorded a week before. This result shows that our method can obtain good recognition performance on meaningful numbers of photographers and across at least a week.</p><p>To test the possibility that stabilization would take away some or all the body motion information in the frame motions, the identification experiments were redone with the following pre-processing stage: for each frame (50 flow vectors) the mean framewise vector was calculated and then subtracted from each of the vectors in the frame. As motion between frames is small and some lens distortion correction was performed, this is similar to 2D stabilization. Table. 1 shows that such "stabilization" degrades performance somewhat (4-9%), but accuracy still remains fairly high. We note however that more complex stabilization might remove more body motion information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Photographer Verification</head><p>We also test the verification performance obtained by our method. In order to evaluate verification performance by a single number it is common to use the Equal Error Rate (EER), the error rate at which the False Acceptance Rate (FAR) and False Rejection Rate are equal.</p><p>The EER for both the CNN and LPC descriptors for videos of length 4s (one feature vector) and 12s (five feature vectors) is presented in <ref type="table">Table.</ref> 2 while the ROC curves are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. A detailed description of our protocol can be found in Sec. 6. It can be seen from our results that high accuracy (low EER) can be obtained by both descriptors: LPC 14% (4s), 10% (12s) and CNN 11% (4s), 8% (12s). The CNN obtains better performance for both durations with a larger improvement for 4s.</p><p>It should be noted that all test probe photographers apart from the target photographer had never been used in training. By focusing on modeling the target photographer we can separate him from the rest of the world, and are thus able to generalize to unseen test photographers.</p><p>Descriptor 4s 12s LPC 13.6% 9.6% CNN 11.3% 8.1% <ref type="table">Table 2</ref>. Verification equal error rates for LPC and CNN descriptors with 4s and 12s sequence duration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Analysis of CNN features: In order to analyze the features learned by the CNN we visualize the filters learned by the first layer. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the horizontal and veritical components of a first layer temporal filter learned by the network. For illustration purposes, only the weights of the central line of pixels are shown. Looking at the weights, we see that the horizontal component filter is tuned to respond to some specific frequencies, while the vertical component looks for sharp rotations. This behavior appears in several other filters suggesting that the network might be using both spectral and transitive cues.</p><p>Transfer Learning for verification: In some scenarios it may not be possible to train a verification classifier for each photographer. In such cases Nearest Neighbors may be a good alternative. The following approach is taken: An identification CNN is trained on half the photographers in the training dataset. We choose a video by a target photographer (that was not used for training the CNN), and extract its CNN descriptors (as in Sec. 3.3), this set of descriptors forms our gallery. Similarly we extract CNN descriptors from all video sequences of photographers not used for training the CNN, this forms our probe set (excluding the sequence used as gallery). For each probe descriptor we check if the euclidean distance from its nearest neighbor in the gallery is smaller than some threshold, and if so we classify it as the target photographer. We used Camera D1 sequences for training and D2 sequences for test. 16 randomly selected photographers were used for training the CNN, and the rest for verification. The same procedure was carried out for LPC (without training a CNN). Multiple 4s sequence predictions are aggregated using simple voting. The average EER for 12s sequences was 15.5% (CNN) and 22%(LPC). Although less accurate than trained classifiers, this shows the network learns to identity features that are general and can be transferred to identify unseen photographers. Nearest Neighbors classification on the optical flow raw features yielded very low performance in accordance with the findings of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>. Verification on FPSI: We tried learning verification classifiers by choosing one photographer from the FPSI dataset as target, and 4 other photographers as negative training data. The morning sequences of the target photographer were used for training and the afternoon for testing. We tested the classification performance between the afternoon sequences of the target photographer and the remaining 6th non-target photographer from the FPSI dataset. The network however, fit to the train non-target photographers and has not been able to generalize to the unseen probe photographer. We therefore conclude that a significant number of photographers (such as present in the EVPR dataset) is required for training a verification classifier.</p><p>Failure cases: In <ref type="figure" target="#fig_0">Fig. 12</ref> several cases are shown where the 4 second descriptor failed to give correct recognition. Failure can be caused by sharp head movements (sometimes causing significant blur), by large moving objects, or by lack of features for optical flow computation. It is likely that by identifying such cases and removing their descriptors, higher recognition performance may be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Procedure</head><p>In this section we give a detailed description of the experimental procedure used in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Dataset Description</head><p>Two datasets were used for evaluation: a public general purpose dataset (FPSI) and a larger dataset (EVPR) collected by us to overcome some of the weaknesses of FPSI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">FPSI Dataset</head><p>The First-Person Social Interactions (FPSI) dataset was collected by Fathi et al. <ref type="bibr" target="#b5">[6]</ref> for the purpose of activity analysis. 6 individuals (5 males, 1 female) recorded a day's worth of egocentric video each using head-worn GoPro cameras. Due to battery and memory limitations of the camera, the photographers occasionally took the cameras off and put them on again, ensuring that camera extrinsic parameters were not kept constant.</p><p>In this work we learn to recognize video photographers while walking, rather than sitting or standing. We therefore extracted the walking portions of each video using manual labels. It is possible to use a classifier such as described in <ref type="bibr" target="#b18">[19]</ref> to find the walking intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">EVPR Dataset</head><p>The FPSI dataset suffers from several drawbacks: it contains video only for a small number of photographers <ref type="bibr" target="#b5">(6)</ref> and each photographer wears the same hat and camera all the time. It is therefore conceivable that learning camera parameters can help recognition. To overcome these issues we collected a larger dataset -Egocentric Video Photographer Recognition (EVPR).</p><p>The EVPR consists of head-mounted video sequences collected from 32 photographers. Each video sequence was recorded with a GoPro camera attached to a baseball cap worn on the photographer's head (as in <ref type="figure" target="#fig_0">Fig. 13</ref>). Each photographer was asked to walk normally for around 7 minutes along the same road. All photographers recorded two 7 minute video sequences on a single day using two different cameras (and caps). 20 photographers also recorded another sequence a week later. The use of different cameras for different sequences came to ensure that motion rather than camera calibration is learned. No effort was made to ensure that the same shoes would be used on both days (and in fact several persons had changed shoes between sequences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation Protocol</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Photographer Identification</head><p>Photographer identification sets to recognize a photographer from a closed set of M candidates. For this task it is assumed that we have training data from all subjects. We tested our method both on the FPSI the EVPR datasets. In the FPSI dataset we used for each individual the first 80% of sequences (taken in the morning) for training, and the last 20% sequences recorded in the afternoon for testing. This is done to reduce overfitting to a particular time or camera setup. Data were randomly sub-sampled to ensure equal number of examples for each photographer in both training and testing sets. The results are described in Sec. 4.</p><p>For the EVPR dataset we used sequences from Camera D1 for training. For testing we use both sequences from Camera D2 (taken on the same day) and Camera D3 (taken a week later, when available). The results on each camera are compared to analyze whether recognition performance degrades within a week.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Photographer Verification</head><p>Given a target photographer with a few minutes of training data, and negative training examples by other non-target photographers, we verify whether a probe test video sequence was recorded by the target photographer. Recognition on longer sequences is done by combining the predictions from subsequent short sequences. As the FPSI dataset contains only 6 photographers it was not suitable for this task (this was elaborated upon in Sec. 5) therefore only the EVPR dataset was used for evaluating performance on this task. For each of 32 photographers: i) photographer is designated target ii) we selected sequences of the target photographer and 15 non-target photographers (randomly selected) for training a binary classifier. All training sequences were 7 minutes (200 descriptors) long and were recorded by camera D1. iii) Another sequence recorded by the target photographer and the remaining 16 participants that were not used for training, were used to test the classifier. Test sequences were recorded by camera D2. iv) The ROC curve and EER ware computed. Average EER and ROC for all photographers is finally obtained. As each sequence contained about 200 descriptors this formed a significant test set. Care was taken to ensure that all photographers (apart from the target) would appear in the training or test datasets but not in both. This was done to ensure we did not overfit to specific non-target photographers. We replicated positive training examples to ensure equal numbers of negative and positive training and test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Implementation Details</head><p>Features: In all experiments the optical flow grid size used was 10×5. In the CNN experiments, all optical flow values were divided by the square-root of their absolute value, this was found to help performance by decreasing the significance of extreme values. Feature vectors of length 60 frames at 15 fps (4s) were used. Feature vectors were extracted every 2s (with a 2s overlap).</p><p>Normalization: We followed the standard practice -For the LPC descriptor, all feature vectors were mean and variance normalized across the training set before being used by the SVM. For the CNN, feature vectors were mean subtracted before being input to the CNN.</p><p>Training: The SVM was trained using LIBSVM <ref type="bibr" target="#b3">[4]</ref>. We used σ = 1e − 4 and C = 1 for LPC, C = 10 for the raw features. The CNN was trained by AdaGrad <ref type="bibr" target="#b4">[5]</ref> with learning rate 0.01 on a GPU using the Caffe <ref type="bibr" target="#b8">[9]</ref> package. The mini-batch size was 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>A method to recognize the photographer of head-worn egocentric camera video has been presented. We show that photographer identity can be found from body motion information as expressed in camera motion when walking. Recognition was done with both physically motivated hand designed descriptors, and with a Convolutional Neural Network. Both methods gave good recognition performance. The CNN classifier was shown to generalize and improve on the LPC hand-designed descriptor.</p><p>The time-invariant CNN architecture presented here is quite general and can be used for other video classification tasks relying on coarse optical flow.</p><p>We have tested the effects of simple 2D video stabilization on classification accuracy, and found only slight degradation in performance. It is possible that more elaborate stabilization would have a greater effect.</p><p>The implication of our work is that photographers' headworn egocentric videos give much information away. This information can be used benevolently (e.g. camera theft prevention, user analytics on video sharing websites) or maliciously. Care should therefore be taken when sharing such video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Fig. 1). Police forces routinely release footage of a) A GoPro video uploaded to YouTube allegedly capturing a crime from the POV of the robber. Can the robber be recognized? b) A GoPro video uploaded by US soldiers in combat. Are their identities safe?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>a) 50 Optical flow vectors are calculated for each frame (only 12 shown here), and represented as two columns (each of 50 values), for the x and y optical flow components. b) The feature vector consists of optical flow columns for 60 frames, stacked into two 50×60 arrays, for the x and y components of the flow. Horizontal flow component Vertical flow component</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Two examples of the flow feature vectors. Each feature vector consists of 50 optical flow vectors per frame, computed for each of 60 frames. Here only the central row, having 10 flow vectors, is shown. The left and right images show the horizontal and vertical components of the optical flow. Note the rich temporal structure along the time axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>The MAP rule operated on the FPIS dataset: a) Ground truth labels. b) Raw CNN probabilities. c) MAP rule probabilities (for T =12 seconds.). The MAP classifier visibly 'cleaned up' the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7</head><label>7</label><figDesc>presents the photographer recognition test performance of our network on the FPSI database (6 people). The average correct recognition rate on a single feature vector (describing only 4 seconds of video) is 76% against the random performance of 16.6%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>CMC rates for same day recognition (for 12s sequences). LPC accuracy: 81% (Top-1) and 88% (Top-2). The CNN further improves the performance with 90% (Top-1) and 93% (Top-2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>ROC curves for the verification performance of our method for LPC and CNN descriptors of 4s and 12s sequences. For both methods we show the mean ROC curve. The EER of each method is given by the point of intersection between the linear line and its ROC curve. Examples of a temporal filter for the horizontal (left) and vertical (right) flow components. Horizontal axis is time, and vertical axis is location along the central line. The horizontal component filter appears to be sensitive for certain left-right frequencies, while the vertical component filter is sensitive to oscillating rotations: When the right side is moving up the left side is moving down, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Common failure cases for the 4-second descriptor: a-b) Sharp turns of the head result in atypical fast motions, sometimes causing motion blur. c) Large moving objects can also cause atypical optical flow patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 .</head><label>13</label><figDesc>The apparatus used to record the EVPR dataset.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The LPC coefficients of a time series are k values that when scalar multiplied with the last k measurements of the time series, will optimally predict the next measurement.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This research was supported by Intel ICRI-CI and by the Israel Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification of artistic styles using binarized features derived from a deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014 Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI. Foundations and trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Speaker recognition: a tutorial. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="1437" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Social interactions: A first-person perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cepstral analysis technique for automatic speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Im2gps: estimating geographic information from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;08</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image processing for artist identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Berezhnoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast unsupervised ego-action learning for first-person sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting geoinformative attributes in large-scale image collections using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="550" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying users of portable devices from gait pattern with accelerometers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mantyjarvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vildjiounaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Makela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ailisto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Moving object recognition in eigenspace representation: gait analysis and lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognotion Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<title level="m">Corneal imaging system: Environment from eyes. IJCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="23" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal segmentation of egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Head motion signatures from egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust textindependent speaker identification using gaussian mixture speaker models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="83" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">First-person activity recognition: What are they doing to me</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gait-based person authentication by wearable cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shiraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitsugami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mukaigawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INSS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Who&apos;s behind the camera? identifying the authorship of a photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05038</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ego-surfing first person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yonetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
