<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilinear Hyperplane Hashing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
							<email>xlliu@nlsde.buaa.edu.cnchdeng.xd@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhujin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Hao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">QCIS and FEIT</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">§</forename><surname>Dacheng</surname></persName>
							<email>dacheng.tao@uts.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><forename type="middle">♮</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multilinear Hyperplane Hashing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hashing has become an increasingly popular technique for fast nearest neighbor search. Despite its successful progress in classic pointto-point search, there are few studies regarding point-to-hyperplane search, which has strong practical capabilities of scaling up applications like active learning with SVMs. Existing hyperplane hashing methods enable the fast search based on randomly generated hash codes, but still suffer from a low collision probability and thus usually require long codes for a satisfying performance. To overcome this problem, this paper proposes a multilinear hyperplane hashing that generates a hash bit using multiple linear projections. Our theoretical analysis shows that with an even number of random linear projections, the multilinear hash function possesses strong locality sensitivity to hyperplane queries. To leverage its sensitivity to the angle distance, we further introduce an angular quantization based learning framework for compact multilinear hashing, which considerably boosts the search performance with less hash bits. Experiments with applications to large-scale (up to one million) active learning on two datasets demonstrate the overall superiority of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed the success of fast approximated nearest neighbor search in many domains and applications including large-scale visual search <ref type="bibr" target="#b5">[6]</ref>, objec- * corresponding author t detection <ref type="bibr" target="#b2">[3]</ref>, classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref> and recommendation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. Locality-sensitive hashing (LSH) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> pioneered the hash based solution with a good balance between the search performance and computational efficiency. In LSH, the linear projection paradigm was adopted to efficiently generate the hash codes for the specific similarity metric like l p -norm (p ∈ (0, 2]), and has been widely accepted in many following research for compact hash codes learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>, equipped with a number of techniques like double bit quantization <ref type="bibr" target="#b8">[9]</ref>, bit selection <ref type="bibr" target="#b16">[17]</ref>, asymmetric quantization <ref type="bibr" target="#b23">[24]</ref> and discrete optimization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Despite the progress of hashing research, most of existing methods mainly deal with the classic point-to-point nearest neighbor search, where according to certain distance measure the closest database points to the query one are desired as the nearest neighbors. However, in practice there are also a variety of cases that require the nearest points to a hyperplane, namely, point-to-hyperplane search problem. Typical instances includes the large-scale active learning with support vector machines (SVM) <ref type="bibr" target="#b27">[28]</ref>, maximum margin clustering <ref type="bibr" target="#b31">[32]</ref> and large-margin dimensionality reduction <ref type="bibr" target="#b29">[30]</ref>. In the task of large-scale active learning with SVMs, the active learning iteration process selects the unlabeled point closest (with minimum-margin) to current SVM's decision hyperplane into the training set, and retrains the SVM classifier with the increasing training data, gaining a provable performance improvement in a few iterations. A common solution for point-to-hyperplane search is the exhaustive search, which nevertheless spends expensive computation and memory on large-scale datasets.</p><p>To address this problem, <ref type="bibr" target="#b21">[22]</ref> proposed the concomitant hashing to accelerate the min/max inner product, which exploits properties of order statistics of statistically correlated random vectors. Different from the idea of concomitant hashing, prior research <ref type="bibr" target="#b7">[8]</ref> first successfully devised hyperplane hashing paradigm with applications to large-scale active learning. Based on the theoretical guarantee of colli- to the hyperplane P(w) than point x1; (b) an illustration for the scale-invariance of multilinear hash function with an even order m, where x1, x2 and x3 below the hyperplane P(w) respectively share the same angle distance to P(w) and thus same hash code with x ′ 1 , x ′ 2 and x ′ 3 locating above P(w); (c) The collision probabilities p1 of different hyperplane hashing with respect to angle distance r.</p><p>sion probability using linear projections, two random hashing methods (AH and EH) have been presented for point-tohyperplane search with respect to the angle distance and Euclidean distance. The hyperplane hashing permits efficient large-scale search for points near to a hyperplane in sublinear time with a certain accuracy. Following the paradigm based on angle distance, <ref type="bibr" target="#b14">[15]</ref> further provided a more strongly locality-sensitive hashing using bilinear hash functions (BH). To meet the same level of search accuracy, BH requires much less hash bits compared with AH and EH. Since the bilinear hash functions are randomly generated in BH, there inevitably exists heavy redundancy among them. Similar to prior hashing research for point-to-point search, a learning based BH (LBH) was designed for compact yet discriminative hash codes, which substantially reduce the storage and computation cost. As the locality-sensitive property successfully promises fast point-to-hyperplane search, this paper further enhances the property with a much higher collision probability for angle distance. To this end, we first design a novel hyperplane hashing scheme with multilinear hash functions, which generates a hash bit based on the product of a series of linear random projections. By devising the coding scheme for both the hyperplane query and the database points, our theoretical analysis shows that with an even number of projections, the multilinear hash function is more locality sensitive to the angle distance than AH and BH (BH can be treated as a special case of our multilinear hashing with two projections). This property enables us to achieve satisfying performance using fewer hash bits for practical applications. Besides, we introduce an angular quantization based learning framework for compact multilinear hyperplane hashing, better migrating the difficulty of the minimum angle distance search. On two large datasets up to one million, we empirically demonstrate that our approach outperforms state-of-the-arts in active learning with SVMs.</p><p>The remaining sections are organized as follows. Section 2 presents the formulation of the point-to-hyperplane search problem. In Section 3 we first review the related hyperplane hashing methods and then introduce the proposed random multilinear hashing with a theoretical analysis. To pursue compact hash codes, we further devise a learning based multilinear hashing algorithm in Section 4. Comprehensive experiments on the task of large-scale active learning over two popular datasets are presented in Section 5, followed by conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation</head><p>Let us formally state the point-to-hyperplane search problem: given a database D = {x 1 ,...,x n } of n points in R d , the goal is to find the closest points in D to a given hyperplane query P w determined by the normal vector w ∈ R d . Take the margin-based SVM active learning as an example. The margin |w T x+b| w of any point x describes the distance to the decision hyperplane P w of SVM classifier (w,b), and in each iteration the active selection prefers the points with the minimum margin.</p><p>Without loss of generality, we append x with a constant 1, and assume that both x and w are normalized and P w passes through the origin. Then it is easy to see that the point-to-hyperplane nearest neighbor search actually minimizes the absolute value of the cosine between w and x:</p><formula xml:id="formula_0">| cos θ w,x | = |w T x| w x .</formula><p>(1) <ref type="figure" target="#fig_0">Fig. 1</ref> (a) displays the geometric relationship between the hyperplane normal vector w and database point x.Since | cos θ w,x | =sin| π 2 − θ w,x |, the distance from x to the hyperplane P w is monotonically proportional to the angle measure α w,x ∈ [0, π 2 ]:</p><formula xml:id="formula_1">α w,x = | π 2 − θ w,x |.<label>(2)</label></formula><p>Therefore, α w,x can serve as the distance metric in our problem: a narrow α w,x indicates a close point locating near to the hyperplane P w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1</head><p>The distance between a vector x and a hyperplane P w is measured by α ω,x , i.e., d(P w , x)=α ω,x .</p><p>Under the angle distance metric, the point-to-hyperplane search problem can equivalently be regarded as a point-topoint search problem between the hyperplane normal vector w and the database points in D. However, the distance metric is quite different from the conventional nearest neighbor search. Here the nearest neighbors to the hyperplane should be the points almost perpendicular to w, rather than those with small angles in point-to-point search. To achieve fast approximate hyperplane query, we will devise a new family of locality sensitive hash function that preserves the specific angle distance with a large probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multilinear Hyperplane Hashing</head><p>In this section, we will briefly review the existing hyperplane hashing methods, and then propose a random projection based locality sensitive hash of multilinear form with a series of theoretic analysis. Before that, we have to point out that in the whole paper we suppose the binary codes have values from {−1, 1} (equivalent to 0/1 code in prior hashing research) for concise derivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Related Work</head><p>To our best knowledge, there are only three related hyperplane hash families: angle hyperplane hash (AH), embedding hyperplane hash (EH) <ref type="bibr" target="#b7">[8]</ref> and bilinear hash (BH, including its learning based version LBH) <ref type="bibr" target="#b14">[15]</ref>. We first give a brief review on these related works.</p><p>Angle Hyperplane Hashing: AH as the first hyperplane hashing in the literature employs the two-bit hash function to encode the input x ∈ R d :</p><formula xml:id="formula_2">h AH (x)= h u,v (x, x), x is a database point h u,v (x, −x), x is a hyperplane normal where h u,v (a, b)=[ sgn(u T a), sgn(v T b)], with u, v ∼ N (0,I d×d ),</formula><p>i.e., u and v are independent vectors from Gaussian distribution. Embedding Hyperplane Hashing: Jain et al. also proposed another hyperplane hashing (EH) based on the Euclidean distance <ref type="bibr" target="#b7">[8]</ref>. It first computes the high dimensional embedding by vectorizing the rank-1 ma-</p><formula xml:id="formula_3">trix of the input vector x as V (x)=vec(xx T )= (x 2 1 ,x 1 x 2 , ··· ,x 1 x d , ··· ,x 2 d )</formula><p>, and then generates the hash bit using random projection u ∼N(0,</p><formula xml:id="formula_4">I d 2 ×d 2 ) h EH (x)= sgn(u T V (x)), x is a database point sgn(−u T V (x)),</formula><p>x is a hyperplane normal at the cost of expensive computation and storage. Bilinear Hyperplane Hashing: To suppress the computation cost and simultaneously guarantee high collision probabilities, Liu et al. discovered a new hyperplane hash family named bilinear hashing (BH) <ref type="bibr" target="#b14">[15]</ref>. With two projection vectors u, v ∼N (0,I d×d ), BH hash function is defined as:</p><formula xml:id="formula_5">h BH (x)=sgn(u T xx T v),<label>(3)</label></formula><p>respectively encoding the database point x and the hyperplane query P w into h BH (x) and −h BH (w). <ref type="table" target="#tab_0">Table 1</ref> compares the collision probability and time complexity of different hash families.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Random Multilinear Hashing</head><p>In this paper, we propose a new generic hash family named multilinear hash function (MH for short). </p><formula xml:id="formula_6">h m (x)=sgn(u 1 T x ···u m T x), i.i.d. u l ∼N(0,I d×d ),l=1,...,m.<label>(4)</label></formula><p>Note that following the above definition, when m =2 , the multilinear hash function degenerates to BH <ref type="bibr" target="#b14">[15]</ref>. The multilinear form enjoys several advantages in hyperplane hashing. First, it helps us focus on the angle distance in our hyperplane hashing, eliminating the effect of the data scale (even the negative one, see <ref type="figure" target="#fig_0">Fig. 1</ref>(b) for an illustration with an even m). Second, since the projection on a vector can preserve the data locality to some extent, multiple projections together can significantly boost the probability collision. We will show its locality sensitive property in the following theoretical analysis.</p><p>Lemma 1 Given a query point w ∈ R d and a database point x ∈ R d , the probability of collision for these two points under h m is</p><formula xml:id="formula_7">P[h m (w)=h m (x)] = (1 − 2θw,x π ) m +1</formula><p>2 Proof (1) When m =1 , the equation holds according to the fact from <ref type="bibr" target="#b0">[1]</ref>.</p><p>(2) When m&gt;1, assuming the equation holds for all k&lt;m ,wehave,</p><formula xml:id="formula_8">P[h m (w)=h m (x)] = P[h m−1 (w)=h m−1 (x)]P[sgn(u m T w)=sgn(u m T x)] + P[h m−1 (w) = h m−1 (x)]P[sgn(u m T w) = sgn(u m T x)] = P[h m−1 (w)=h m−1 (x)](1 − θ w,x π ) +(1− P[h m−1 (w)=h m−1 (x)]) θ w,x π = (1 − 2θw,x π ) m +1 2 .</formula><p>This completes the proof.</p><p>Lemma 2 Given a hyperplane query P w with the normal vector w ∈ R d , define h m (P w )=−h m (w). Then, the probability of collision for P w and x under the random multilinear hash h m with an even m is</p><formula xml:id="formula_9">P[h m (P w )=h m (x)] = 1 2 − 2 m−1 α m w,x π m Proof Using h m (P w )=−h m (w) for P,weget P[h m (P w )=h m (x)]=1− P[h m (w)=h m (x)] =1− (1 − 2 θw,x π ) m +1 2 = 1 2 − 2 m−1 ( π 2 − θ w,x ) m π m .</formula><p>Then by applying the facts α w,x = |θ w,x − π 2 | and m is even, one can complete the proof.</p><p>Lemma 2 gives us several hints to the design of multilinear hyperplane hash functions. First, m should be even, otherwise, one can find that the collision probability of P w and x under h m reaches the highest when θ w,x equals π. That is to say it is still very likely that the collision happens even if α w,x equals to π 2 , which contradicts to our intention. Second, under the mild condition that m is even, the collision probability can be amplified considerably by increasing the order m of the multilinear function. <ref type="figure" target="#fig_0">Fig. 1(b)</ref> demonstrates the necessity of an even m for the angle distance preservation, possessing the invariance to the magnitude and reverse of the point vector.</p><p>Theorem 1 Under the condition that m is even, the multilinear hyperplane hash function family h m is</p><formula xml:id="formula_10">r, r(1 + ǫ), 1 2 − 2 m−1 r m π m , 1 2 − 2 m−1 (r(1+ǫ)) m π m -sensitive to the distance measure d(P w , x)=α w,x with r, ǫ &gt; 0. Proof Using Lemma 2,ifd(x, P w ) ≤ r,wehave P[h m (P w )=h m (x)] = 1 2 − 2 m−1 d m (P w , x) π m ≥ 1 2 − 2 m−1 r m π m = p 1 . Likewise, when d(P w , x) &gt;r(1 + ǫ) we have P[h m (P w )=h m (x)] &lt; 1 2 − 2 m−1 (r(1 + ǫ)) m π m = p 2 ,</formula><p>and p 1 &gt;p 2 . This completes the proof.</p><p>Theorem 1 indicates that the locality sensitivity to the angle distance is bounded by the collision probability which monotonically increases with respect to even m, and for any even m&gt;2 this probability is larger than that of AH, EH and BH (see <ref type="figure" target="#fig_0">Fig. 1</ref>(c) and <ref type="table" target="#tab_0">Table 1</ref>). Note that using the similar multilinear trick, we can also improve the collision probability of AH and EH theoretically. Meanwhile, we can see that there is a tradeoff between the high collision probability and low time complexity for bit generation. Next, we give the theoretical performance guarantee and computation bound when using multilinear hash functions for point-tohyperplane nearest neighbor search.</p><p>Theorem 2 Given a database D with n points and a hyperplane query P w , if there exists a database point x * such that d(x * , P w ) ≤ r, then with ρ = ln p1 ln p2 (1) using n ρ hash tables with log 1/p2 n hash bits, the random multilinear hyperplane hash of an even order is able to return a database pointx such that d(x, P w ) ≤ r(1 + ǫ) with probability at</p><formula xml:id="formula_11">least 1 − 1 c − 1 e , c ≥ 2;</formula><p>(2) the query time is sublinear to the entire data number n, with n ρ log 1/p2 n bit generations and cn ρ pairwise distances computation.</p><p>The theorem guarantees the practicality of MH that the nearest neighbors can be located fast (in a sublinear time) with a large probability. The proof can be easily completed based on the locality sensitivity. Please refer to the proofs in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning Multilinear Hash Functions</head><p>Though randomized hashing methods enjoy elegant theoretic guarantee and computational simplicity, they usually require long hash codes for satisfying performance, and consequently need much computation and memory cost in practice. Conventional hashing research for point-to-point search problem attempted to address such issue by learning hash functions from data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref>, which can generate compact hash codes by eliminating the redundancy among functions, and thus largely save computation and storage <ref type="bibr" target="#b1">[2]</ref>. Following this line, in this section we introduce an angle quantization based learning method for the multilinear hyperplane hash functions (LMH for short).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Angle Quantization</head><p>Our motivation comes from the fact that hyperplane-topoint search problem is tightly connected to the point-topoint one. In detail, as shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref> and guaranteed in Lemma 2, a hyperplane query P ω can be equivalently represented by its normal ω with the multilinear hash bit h m j (P w )=−h m j (w) generated by the j-th multilinear function h m j (w)=sgn((u j 1 ) T w ···(u j m ) T w). If we denote the k-length multilinear hash code of x by</p><formula xml:id="formula_12">H m (x)=(h m 1 (x), ··· ,h m k (x)) T ,<label>(5)</label></formula><p>then H m (P w )=−H m (w), and searching the nearest neighbors to P ω turns to finding those points x having the most similar hash codes (or smallest Hamming distances) to H m (P w ). Namely, for desired points H m (P w ) T H m (x) → k with a small angle distance α w,x .</p><p>Therefore, preserving the angle plays the most important role in the discriminative hash function learning.</p><p>Since the hash codes actually serve as a binary representation for points in a more complex feature space <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>, in our problem the learnt binary codes should maximally preserve the angle (or cosine similarity) among the points. To this end, we propose angle quantization based learning algorithm, which aims at capturing the angles among the projected database points using the binary hash codes. Instead of random generation, here the projection vectors of each multilinear hash function will be learnt to minimize the angle quantization loss, and thus those points close to the hyperplane query can be discriminatively distinguished and located quickly in the Hamming space.</p><p>Formally, given a training set X =( x 1 , ··· , x n ′ ), our goal is to preserve the angles among them by learning k multilinear hash functions of m order, characterized by m projection matrice U l =( u 1 l , ··· , u k l ), l =1 ,...,m. Denote the projection values before binary quantization by</p><formula xml:id="formula_13">Y = U 1 T X ⊙···⊙U m T X,</formula><p>where the symbol ⊙ represents the Hadamard product, then the binary codes will be B = sgn(Y )=H m (X), and the projection and the binary code of each x i will be the corresponding columns of Y and B, denoted by Y i and B i . Therefore, for each point x i its hash code B i should maximally approximate Y i . The learning problem can be formulated as follows:</p><formula xml:id="formula_14">max B,U l n ′ i=1 cos(B i , Y i ) s.t. B1 =0, U T l U l = I,l =1,...,m.<label>(6)</label></formula><p>The constraint U T l U l = I makes sure the k hash functions are independent to each other, while B1 = 0 forces the data to be evenly distributed over -1 and 1 for a balanced coding, and we relax it to Y 1 = 0 due to the optimization difficulty stemming from the discrete constraint of B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Iterative Optimization</head><p>Note that in problem (6), if we denote the projection of all training data using j-th hash function by y j = (Y j1 , ··· ,Y jn ′ ) T ∈ R n ′ ×1 , and the binary codes by b j = (B j1 , ··· ,B jn ′ ) T ∈{−1, 1} n ′ ×1 , then the problem can be approximately reformulated as: This rearrangement of the formula inspires our sequential optimization solution: one by one we learn u j l , 1 ≤ l ≤ m and the bit values b j for all training data. At each iterative step, b j and u j l are optimized in an alternating manner by fixing others. Specifically at the j-th step, we repeat the following updating until the objective converges:</p><formula xml:id="formula_15">max n ′ i=1 cos(B i , Y i ) ≈ n ′ i=1 B T i √ k Y i = 1 √ k n ′ i=1 k j=1 B ji Y ji = 1 √ k k j=1 b j T y j</formula><formula xml:id="formula_16">(Step 1) Update b j . When u j 1 , ··· , u j m are fixed, y j = X T u j 1 ⊙···⊙X T u j m is determined, we solve b j by max bj b T j y j s.t. b j ∈{−1, 1} n ′ ×1<label>(7)</label></formula><p>Clearly, the optimal solution is given by b j = sgn(y j ).</p><p>(</p><p>Step 2) Update u j l , l =1,...,m. This can be efficiently done by alternatingly optimizing each u j l while fixing u j l ′ for any l ′ = l. Since all u j ′ l , j ′ &lt;jhave been learnt in previous steps, by introducing the vector</p><formula xml:id="formula_17">e = ⊙ l ′ =l X T u j l ′ ,<label>(8)</label></formula><p>the optimization problem with respect to u j l turns to be</p><formula xml:id="formula_18">max u j l a T u j l s.t. c T u j l =0, (u j ′ l ) T u j l =0,j ′ &lt;j,<label>(9)</label></formula><p>where a = X(e ⊙ b j ) and c = Xe corresponds to the relaxed balanced constraint 1y j =0 , and (u j ′ l ) T u j l =0is imposed to enforce the diversity of the projections. This is a standard linear programming problem, whose optimal solution can be efficiently obtained using a number of optimizing techniques like primal-dual interior point <ref type="bibr" target="#b19">[20]</ref>. In the above alternating optimization, we get a suboptimal solution in each update, and with a few iterations we can easily obtain the optimized multilinear hash functions h m j in the j-th step.</p><p>To start the learning, we randomly initialize the projection vectors of each hash function, and then learn a series of multilinear functions that boost the accuracy of the pointto-hyperplane search. The whole learning based multilinear hash method is listed in Algorithm 1. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the objective curves with respect to the number of iterations, Update each binary codes b j by solving (7); <ref type="bibr">6:</ref> Update each projection matrice u j l by solving <ref type="formula" target="#formula_18">(9)</ref>, l =1,...,m; <ref type="bibr">7:</ref> until Converge 8: end for 9: Output: the hashing projection vectors U l , l = 1,...,mand the binry codes B.</p><p>where we can observe that our algorithm can converge fast, and thus the hash functions can be learnt efficiently at the training stage.</p><p>Note that in LMH we mainly devote our efforts to preserving the angle values instead of the absolute ones. Nevertheless, the scale invariance of the multilinear hash makes us only have to focus on the half of the subspace partitioned by the hyperplane query, and a bit flipping over the query code can further improve the recall performance in practice. Though the learnt MH can hardly be proved locality sensitive, exploiting the data structures faithfully helps us pursue more discriminative functions than random way. Our experiments in the next section will demonstrate this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The studied point-to-hyperplane search problem appears in many tasks like classification with active learning and cutting-plane based maximum margin clustering. In our experiments, we adopt the widely-studied active learning with SVMs as the application, and comprehensively evaluate the proposed MH and LMH in terms of search accuracy and efficiency. Besides, four state-of-the-art hyperplane hashing algorithms AH, EH, BH and its learning version LBH, and two naive but common methods including exhaustive linear scan and random selection are also evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Settings</head><p>Experiments are conducted on two large widely-adopted datasets in classification including MNIST <ref type="bibr" target="#b10">[11]</ref> and Tiny-1M appended with CIFAR-10 <ref type="bibr" target="#b9">[10]</ref>. MNIST is a dataset of handwritten digits, comprising 60,000 training images and 10,000 test images associated with digits from 0 and 9 (i.e., 10 different classes). Each image is represented by a 784dimensional vector corresponding to its 28 × 28 grayscale pixel intensities. Tiny-1M comprises two parts from 80M tiny image set <ref type="bibr" target="#b28">[29]</ref>: one million images randomly sampled without labels and CIFAR-10 with 60,000 32×32 color images associated with 10 semantic categories, each of which has 6,000 samples. Following the setting in <ref type="bibr" target="#b14">[15]</ref>, we treat the first part as the "other" class, and represent each image using the provided 384-dimensional GIST descriptors.</p><p>In the SVM based classification with active learning, we separately train linear SVMs in the one-vs-all setting for each class with random initializations, i.e., 5 labeled samples for each class on both datasets. Then different pointto-hyperplane methods are adopted to select the unlabeled samples nearest to the current SVM's hyperplane. In all experiments we run 300 active selection iterations, and retrain SVMs using the training set appended with the selected samples in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Discussions</head><p>We compare the proposed MH and the learn based version LMH to two naive methods: exhaustive and random selection, and four state-of-the-art hyperplane hashing methods: AH, EH <ref type="bibr" target="#b7">[8]</ref>, BH, and LBH <ref type="bibr" target="#b14">[15]</ref>.</p><p>At the offline stage, we first generate hash functions for each hashing methods, and encode the database points using the same number of hash bits for fair comparison. According to the size of each set, we respectively generate 16 hash bits for hash tables on MNIST and 20 bits on Tiny-1M. This is because the theoretical and practical analysis show that the optimal code length should be close to log 2 n <ref type="bibr" target="#b6">[7]</ref>. When it comes to the online point-to-hyperplane search in each active learning step, from each table we lookup the buckets with hash codes in a small Hamming radius from the query code, and then obtain the nearest point to the hyperplane by ranking candidates in the buckets. To balance the efficiency and accuracy, in our experiments we empirically set the radius to 5 for both datasets. For the case that no points fall within the radius, we alternatively choose the random selection as a supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Performance in Large-Scale Active Learning</head><p>We first report the results using different methods on M-NIST. <ref type="figure" target="#fig_4">Fig. 3(a)</ref> plots the corresponding mean average precision (MAP) curves with respect to 300 active learning iterations. Clearly, BH and LBH outperform AH, EH and random selection, and our MH and LMH (m =4 ) further improve the MAP and consistently obtains the best performances among all hyperplane hashing methods, mainly owning to the fact that the multilinear functions preserve the discriminative power of the angle distance, and thus retrieve the most perpendicular points to the hyperplane normal. Moreover, we can notice that at the first few iterations, LMH even gets a better performance than the exhaustive way, which indicates that there is no guarantee that the exhaustive selection can serve as the best choice for better  test accuracy. In all iterations, LMH and MH consistently achieve very close performance to the exhaustive selection. This is mainly because MH can faithfully find the nearest neighbors for each hyperplane query. The minimum margins shown in <ref type="figure" target="#fig_4">Fig. 3</ref>(b) also illustrate this point, where LMH and MH get very small margins with a slight variation. <ref type="figure" target="#fig_4">Fig. 3</ref>(c) further depicts the number of nonempty table lookup per class, accumulated in the 300 iterations. It can be observed that in certain cases AH and EH fail to retrieve candidate points for minimum-margin based active selection. Instead, owning to the higher collision probability, our MH and LMH can enjoy the 100% success rate for hash table lookup given the specific Hamming radius.</p><p>To demonstrate the practicability of our hyperplane hashing method for large-scale point-to-hyperplane search problem, we perform active selection on the Tiny-1M dataset with 1.06 millon images. We use the provided 10K images from CIFAR-10 as the testing samples, and the rest 1.01 millon as the database for active sample selection. <ref type="figure" target="#fig_5">Fig.  4</ref> shows the similar results on the Tiny-1M in terms of MAP, minimum margins and success lookup number. Namely, as the exhaustive selection does, both of our hashing methods (m =4 ) select the points with smaller margins than other hashing baselines ( <ref type="figure" target="#fig_5">Fig. 4(b)</ref>), and hit the desired points with a higher success rate <ref type="figure" target="#fig_5">(Fig. 4(c)</ref>). Subsequently, they can obtain satisfying performances for the practical applications, and our LMH can further learn more compact yet discriminative codes by exploiting the angle information among data, and again obtains the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Performance using Different Settings</head><p>In above experiments, we adopt one hash table and a relatively large lookup radius to achieve the desired recall performance, which grantees that the nearest neighbor to the hyperplane query can be located. To comprehensively study the performance of MH with different settings, we further vary the parameters including the lookup radius r, the table number L and the order m of the multilinear hash function.</p><p>Lookup radius: <ref type="figure">Fig. 5</ref> investigates the precision and the recall performance in terms of the active learning curves and the number of the success lookups. Here we vary the lookup radius r from 4 to 6 in one 20-bits hash table and treat all points belonging to the buckets within r as the ranking candidates. It can be observed that for each method, as the radius increases, higher precision and success lookups can be attained. This is because that the enlarged search range guarantees a high probability that the true nearest neighbors can be identified among the retrieved candidates. Besides, we can see that in all cases our MH can get the best performance. For instance, MH using a mall lookup radius r =4 can even get a much better performance than the baselines using a large one, i.e., BH r =5and AH using r =6.  <ref type="figure" target="#fig_6">Fig. 6</ref> we further study the performance of different hyperplane hash methods with more than one (4 and 8) hash tables. In order to clearly observe the effect when using more hash tables, in this experiment we set the lookup radius to a  small one (i.e., r =2 ). In this case, the ranking candidates are those points in each table that fall in the buckets within r Hamming distance to the hyperplane query. According to <ref type="figure" target="#fig_6">Fig. 6(a)</ref>, it is obvious that more hash tables can largely boost the point-to-hyperplane search for different methods, mainly owing to the improved recall of the nearest neighbors. This conclusion can also be verified by the results of success table lookups in <ref type="figure" target="#fig_6">Fig. 6(b)</ref>. Moreover, in all cases our MH again obtains the best performance, with the encouraging locality sensitivity.</p><p>Multilinear order: The above experimental results have demonstrated that MH can achieve satisfying performance with a high collision probability. Our theoretical analysis implies that a large m will further increase such probability, enhancing the locality sensitive property. To study its effect in the application of active learning, <ref type="table" target="#tab_3">Table 2</ref> lists the MAP and minimum margins of the 300th active learning iteration using m =4, 8, 16. Consistent with Theorem 1, using more projection in one hash function will amplify the collision probability of the informative samples, and thus increase the precision of SVM classifiers, meanwhile decreasing the minimum margins.</p><p>Computational efficiency: The computational efficiency is regarded as a critical issue in large-scale applications. Among all methods, only LBH and LMH require offline training, where the iterative optimization in LMH is faster than Nesterov's gradient method in LBH <ref type="bibr" target="#b14">[15]</ref>. As to the search time, <ref type="figure" target="#fig_7">Fig. 7</ref> shows the averaged time (in seconds) of the active selection using different methods on Tiny-1M. Exhaustive selection requires linearly scanning all the points in the unlabeled database and thus suffers from much  more time consumption than others, while random selection serves as the most fast method, but wastes most labeling time on those uninformative samples. Utilizing the table indexing, the hashing methods can avoid the expensive computation by the sublinear search and ranking on a quite small candidate set. Due to the dimension expansion, the code generation in EH takes more computation and memory than others. Although our hyperplane hashing methods depend on more linear projections than AH and BH (i.e., the hashing time t h linearly increases with m), but it is ignorable (≤ 10 −3 s when m ≤ 16) compared to other parts of the active selection time, i.e., ranking time. From the figure, we can observe that its total selection time varies very slightly, and is quite close to baseline hashing methods. Therefore, it can be concluded that MH can give a sufficiently satisfying performance without consuming too much computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we considered the point-to-hyperplane search problem and proposed a new family of hyperplane hash function named multilinear hashing. The proposed multilinear hash function enjoys strong locality sensitivity for the angle distance, and thus is able to retrieve the nearest points for a hyperplane query in a sublinear time. By learning the projections from the data, we can further preserve the angles among data using compact yet discriminative hash codes, which largely enables the practicability of the multilinear hyperplane hashing in many applications. Large-scale active learning experiments on two datasets have demonstrated the superior performance of the multilinear hashing in terms of both accuracy and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustrations for point-to-hyperplane problem: (a) an simple example, where the point x2 gives a smaller angle distance d(w, x2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2</head><label>2</label><figDesc>The multilinear hash function h m (·):R d → {−1, 1} of m-order comprises m linear projection vectors u l ∼N(0,I d×d ),l =1,...,m, with the following form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The objective with respect to the iteration number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Input: training data X, the order of the multilinear hash m and the code length k; 2: Initialize: the projection matrice U l =( u 1 l , ··· , u k l ), u l ∼N(0,I d×d ), l =1,...,m; 3: for j =1,...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Results of the active learning using different methods on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Results of the active learning using different methods on Tiny-1M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Results of the active learning using different methods with a different number of tables on Tiny-1M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Selection time (seconds) of active learning using MH with different m on Tiny-1M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>comparison of collision probability p1 and hash time t h of different hash families.</figDesc><table>AH 
EH 
BH 
MH (ours) 

p1 

1 

4 − 

α 2 
w,x 
π 2 

cos −1 sin 2 (αw,x ) 
π 

1 

2 − 

2α 2 
w,x 
π 2 

1 

2 − 

2 m−1 α m 
w,x 
π m 

t h 
2kd 
kd 2 or k d 

2 

2kd 
mkd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table number :</head><label>number</label><figDesc>In practice, either adopting a large lookup radius or building multiple hash tables can significantly increase the recall for nearest neighbor search. Therefore, in</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>(b) # success table lookups Figure 5. Results of the active learning using different methods with different lookup radius on Tiny-1M.</figDesc><table>0 

50 
100 
150 
200 
250 
300 

0.25 

0.3 

0.35 

0.4 

0.45 

# iteration 

MAP 

AH r=4 
BH r=4 
MH r=4 
AH r=5 
BH r=5 
MH r=5 
AH r=6 
BH r=6 
MH r=6 

(a) active learning curves 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
50 

100 

150 

200 

250 

300 

class 

# lookup 

AH r=4 
BH r=4 
MH r=4 
AH r=5 
BH r=5 
MH r=5 
AH r=6 
BH r=6 
MH r=6 

0 
50 
100 
150 
200 
250 
300 
0.22 

0.24 

0.26 

0.28 

0.3 

0.32 

0.34 

0.36 

# iteration 

MAP 

AH 4 tables 
BH 4 tables 
MH 4 tables 
AH 8 tables 
BH 8 tables 
MH 8 tables 

(a) active learning curves 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
50 

100 

150 

200 

250 

300 

class 

# lookup 

AH 4 tables 
BH 4 tables 
MH 4 tables 
AH 8 tables 
BH 8 tables 
MH 8 tables 

(b) # success table lookups 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy and minimum margins of the 300th iteration using MH with different m on Tiny-1M.</figDesc><table>m =4 
m =8 
m =16 
MAP 
38.82±1.03 40.32±0.81 41.67±0.41 
min |w T x+b| 

w 

7.13±2.11 
6.32±1.77 
4.41±0.83 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was partially supported by the National Natural Science Foundation of China (61402026 and 61572388), and Australian Research Council Projects DP-140102164, FT-130101457, and LE140100061.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Locality-sensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOCG</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast, accurate detection of 100,000 object classes on a single machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Angular quantization-based binary codes for fast similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mobile product search with bag of hash bits and boundary reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3005" to="3012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the difficulty of nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Double-bit quantization for hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2604" to="2623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning hash functions using column generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discrete graph hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3419" to="3427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compact hyperplane hashing with bilinear functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collaborative hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2147" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reciprocal Hash Tables for Nearest Neighbor Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="626" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view complementary hash tables for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1107" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compact hashing for mixed image-keyword query over multi-label images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On the implementation of a primal-dual interior point method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hash-svm: Scalable kernel machines for large-scale visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="979" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accelerated large scale optimization by concomitant hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="414" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-metric locality-sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>M. Fox and D. Poole</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The power of asymmetry in binary hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2823" to="2831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H. Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-margin weakly supervised dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="865" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Circulant binary embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient maximum margin clustering via cutting plane algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="751" to="762" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
