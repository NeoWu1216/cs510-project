<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umur</forename><surname>Ciftci</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaun</forename><surname>Canavan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Reale</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Horowitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion is expressed in multiple modalities, yet most research has considered at most one or two. This stems in part from the lack of large, diverse, well-annotated, multimodal databases with which to develop and test algorithms. We present a well-annotated, multimodal, multidimensional spontaneous emotion corpus of 140 participants. Emotion inductions were highly varied. Data were acquired from a variety of sensors of the face that included high-resolution 3D dynamic imaging, high-resolution 2D video, and thermal (infrared) sensing, and contact physiological sensors that included electrical conductivity of the skin, respiration, blood pressure, and heart rate. Facial expression was annotated for both the occurrence and intensity of facial action units from 2D video by experts in the Facial Action Coding System (FACS). The corpus further includes derived features from 3D, 2D, and IR (infrared) sensors and baseline results for facial expression and action unit detection. The entire corpus will be made available to the research community.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the last 10 years, research on facial expression analysis has shifted its focus from posed behavior to non-posed (i.e., spontaneous) behavior <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>. This shift has increased the difficulty of such analyses, but also their ecological validity and practical utility. In the next 10 years, a similar shift will occur from single modality to multimodal analyses, with increasing research integrating 2D and 3D videos, temperature dynamics, and physiological responses.</p><p>Researchers are already beginning to use 3D sensors and models to improve facial feature tracking and expression recognition <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32]</ref>. However, because of the difficulty in collecting and labeling spontaneous behavior, these studies mainly focused on posed expressions.</p><p>Infrared imaging technology has also been employed for facial expression analysis <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b14">15]</ref> due to its sensitivity to skin temperature and relative insensitivity to lighting conditions and skin color. However, existing work mainly utilized the temperature information as a single modality. Because the temperature distribution may not align well with facial appearance, it is challenging to extract temperaturebased features for expression recognition.</p><p>Research has also shown the correlation of the physiological state to the emotion state of individuals <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>. A number of databases have been developed successfully in recent years <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1]</ref>. The utility of the physiological signals needs to be further investigated.</p><p>Complex human behavior can only be fully-understood by integrating physical features from multiple modalities (e.g., facial expressions and physiological responses). Many studies have theoretically and empirically demonstrated the advantage of integrating multiple modalities in human emotion perception relative to using a single modality <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>. However, the emotion-related modalities are typically studied separately.</p><p>To our knowledge, there is no database of emotional behavior that combines following multiple emotion related modalities: 2D and 3D face visual dynamics, skin temperature dynamics, and physiological responses.</p><p>Although there are several facial expression databases that include 3D data (e.g., BU-3DFE <ref type="bibr" target="#b32">[33]</ref>, BU-4DFE <ref type="bibr" target="#b31">[32]</ref>, Bosphorus <ref type="bibr" target="#b20">[21]</ref>, ITC-3DRFE <ref type="bibr" target="#b24">[25]</ref>, ETH-3DAV <ref type="bibr" target="#b8">[9]</ref>, and 3D AU-DB <ref type="bibr" target="#b4">[5]</ref>), they are all based on posed behavior and typically include few subjects, little diversity, limited ground truth labels, and limited metadata.</p><p>Recently, a 3D spontaneous facial expression database (BP4D) <ref type="bibr" target="#b34">[35]</ref> with extensive labeling, metadata, and diversity was released to the research community. The 2D videos of this dataset were included in the second Facial Expression Recognition and Analysis Challenge (FERA) <ref type="bibr" target="#b27">[28]</ref>. However, this dataset only includes 41 subjects, which lim-its its statistic power and discriminative capacity for emotion classification.</p><p>Another recent database <ref type="bibr" target="#b0">[1]</ref> includes multimodal data on body motion and electromyographic signals. However, this data is limited to multiple views other than the range data for study of chronic pain related emotions.</p><p>In short, as of yet, there is no corpus of sufficiently large size and ethnic diversity that includes the following information: 2D and 3D video of spontaneous facial behavior, thermal imaging, physiological data, expert FACS labels <ref type="bibr" target="#b7">[8]</ref>, and derivatives (e.g., features).</p><p>These findings motivated us to develop a multimodal 3D dynamic spontaneous emotion corpus with metadata (i.e., labels and feature derivatives). In this paper, we present a corpus that includes 140 subjects from various ethnic/racial ancestries: Black, White, Asian (including East-Asian and Middle-East-Asian), Hispanic/Latino, and others (e.g., Native American). The emotion-related modalities include facial expressions, thermal, 2D and 3D dynamics, and physiological data.</p><p>Each subject experienced 10 tasks corresponding to 10 different emotion categories. The physiological data was collected by a vital sign sensor (e.g., heart rate, blood pressure, respiration rate, skin conductivity (EDA)). The skin temperature was also collected by a thermal camera. To elicit authentic and ecologically-valid emotional expressions, we designed a protocol with four approaches integrated seamlessly, including social interview, film watching, physical experience, and controlled activities. A 3D dynamic imaging system is used to capture high-resolution 3D dynamic facial geometric data and video texture data. Such high-definition 3D dynamic (aka 4D) facial representation allows us to examine the fine structural change as well as the precise time course for spontaneous expressions. We have also processed and analyzed the dataset to provide a set of labels and feature derivatives in 2D/3D/IR in order to facilitate the utility of the new corpus. FACS codes (partial AUs) are annotated manually with respect to both their occurrence and intensity. The self-report and data validation have also been reported in the database.</p><p>The contribution of this work is three-fold: 1. This is the first multimodal data corpus with a large set of well-synchronized and aligned sensor modalities including high-definition 3D geometric facial sequences, 2D facial videos, thermal videos, physiological data sequences (heart rate, blood pressure, skin conductance (EDA), respiration rate). 2. The data is significantly expanded in terms of number of subjects with diverse ethnic/racial ancestries as compared to the existing databases. A procedure with 10 seamlessly-integrated tasks was applied by a professional performer/interviewer, resulting in the effective elicitation of spontaneous emotions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Acquisition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Participants</head><p>140 subjects have been recruited to participate in data collection at the Binghamton University. There are 58 males and 82 females, with ages ranging from 18 to 66 years old. Ethnic/Racial Ancestries include Black, White, Asian (including East-Asian and Middle-East-Asian), Hispanic/Latino, and others (e.g., Native American). <ref type="table">Table 1</ref> shows the ethnic distribution. Following the IRB approved protocol, the informed consent form was signed by each subject before the start of data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recording System Setup and Synchronization</head><p>The data capture system (as shown in <ref type="figure" target="#fig_0">Figure 1a</ref>) includes a 3D dynamic imaging system, a thermal signal sensor, and a physiological signal sensor system. The 3D dynamic imaging system (Di3D 1 ) contains a 3D stereo imaging sensor and a 2D video sensor. The thermal sensor is mounted on the 3D dynamic imaging system with a tripod in a fixed position (as shown in <ref type="figure" target="#fig_0">Figure 1b)</ref>, and all these sensors are positioned in the same distance to a subject. The physiological signals are collected using the Biopac 2 MP150 system. It captures vital sign signals in a very high sample rates, including blood pressure, respiration rate, heart rate and electrodermal activity (EDA). Detailed configurations are depicted in the following subsections. Note that the system synchronization is critical for data collection from various modality sensors. Due to each sensor has its own machine to control, we developed a program to trigger the recording from the start to the end across all three sensors simultaneously. This is realized through the control of a master machine by sending a trigger signal to three sensors concurrently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">3D dynamic imaging system</head><p>The 3D face model sequences and their corresponding 2D texture image sequences are captured by the Di3D dynamic imaging system. Associated with two symmetric lights, the system is composed of a 3D sensor with a pair of stereo monochrome cameras aligned vertically and an RGB 2D color camera placed in between the stereo cameras. The 3D model of each frame is created by the dense passive stereo photogrammetry method. Each facial model contains about 30k -50k vertices giving much detailed geometric information at RMS accuracy of 0.2 mm. And the resolution of each 2D texture image is 1040 × 1392 pixels. Consider the trade off between emotion granularity and computing complexity, we set the frame rate to 25fps. This is also consistent to the video rate of the thermal sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Thermal sensor</head><p>The thermal camera that we used is FLIR 3 A655sc Longwave infrared camera. This camera captured thermal videos in resolution of 640 × 480 per frame with 25 • Lens and 17 micron pixels with temperature range of −40 and 150 • C. The frame rate is 50 fps with the full resolution of 640 × 480. The spectral range is 7.5 − 14.0µm. In order to better synchronize all sensors in our system, we set the capture rate of the thermal sensor to 25 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Physiological signal sensing system</head><p>The physiological data were collected by Biopac MP150 data acquisition system. Its measurement capacity is in the range [-25mmHg, 300mmHg] for blood pressure, [0, 200 breaths/minutes] for respiration rate, and [30, 300 beats/minute] for heart rate. The blood pressure signal 3 http://www.flir.com/ (mmHg) is captured through noninvasive blood pressure (Biopac NIBP100D) monitoring system containing two units, finger unit and an inflatable cuff. The finger unit captures data from an index finger and a middle finger of a hand and an inflatable cuff is placed on an arm for calibration.</p><p>Having the blood pressure signal with peak count, the pulse rate (beat/minute), systolic blood pressure (mmHg), and diastolic blood pressure (mmHg) are derived. The respiration signal (measured in voltage) is captured by a respiration belt wearing around the chest. Given the peak count, it derives the parameter -respiration rate (breaths/minute).</p><p>The electrodermal activity (EDA) (measured in micro Siemens) is captured through two leads placed on a right palm connecting a wrist watch. The EDA signal is the indication of arousal level with various skin conductivity.</p><p>In general, the system captures physiological signals in a very high sample rate at 1000Hz. The resulting data include heart rate, respiration rate, systolic blood pressure, diastolic blood pressure, and electrodermal activity (EDA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Emotion Elicitation</head><p>In order to evoke a range of authentic emotions in a laboratory environment, we designed a protocol of ten tasks with seamless transitions. Motivated by the work <ref type="bibr" target="#b34">[35]</ref>, a professional actor was hired to host the entire interview procedure during data collection. Interviews with a skilled interviewer can elicit a wide range of emotional expressions and interpersonal behavior.</p><p>Four methods were employed in the protocol, which include interpersonal conversation, film clip watching, cold pressor, and designed physical experiences. Ten activities (tasks T1-T10 as shown in <ref type="table" target="#tab_2">Table 2</ref>) were conducted with a natural transition from positive emotions to negative emotions. Between any two tasks, there was a brief pause for self-report.</p><p>Data collection started with a social interview in which the interviewer told a joke for a relaxed and amused atmosphere. Then the subject's 3D avatar was created on-site and displayed to the subject for a surprising effect. A negative feeling was then elicited by showing the subject a video clip of a 911 emergency call, followed by a sudden burst of sound for a startled expression. After a pause, the interviewer posed a question to induce a skeptical expression, followed by an embarrassment induction by asking the subject to do a silly performance. Then a fearful feeling was generated through a dart game experience, followed by a physical discomfort experience by having the subject submerge a hand into ice water. After that, the interviewer induced an upset feeling in the subject by pretending to complain about the subject's poor performance on the ice water task. Finally, an unpleasant odor was presented to the subject to induce a disgusted feeling.</p><p>Note that this emotion elicitation protocol has more tasks  than the other reported methods. According to the compound emotions theory <ref type="bibr" target="#b6">[7]</ref>, the surprised feeling could be positive or negative. In our experiment, a fearful surprise was triggered by a siren in T4 and a joyful surprise was induced by seeing a self 3D face in T2. We treat them in two different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Self Report</head><p>As stated in Section 2.3, immediately after each task, every participant was provided with a short period to report the feeling that he/she had experienced.</p><p>A tablet was used to choose emotions and their intensities from a list of possible choices (e.g., relaxed, surprised, sad, happy/amusement, skeptical, physical pain, disgusted, embarrassed, nervous, scared/fear, angry/upset, frustrated, and startled/shocked). 5-point Likert-type scales from "very slightly" to "extremely" were used to rate the emotion intensity. Participants were allowed to choose multiple emotion categories as well as to input other emotion categories if none of provided options fit their experience.</p><p>Among the data collected, we conducted statistical tests on all ten tasks. The top three emotions voted by all participants of each task are displayed in <ref type="figure" target="#fig_1">Figure 2</ref>. As seen in <ref type="figure" target="#fig_1">Figure 2</ref>, the majority vote of each task fits well with the emotion that the task was intended to elicit, which demonstrates that the designed elicitation protocol was effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Database Organization</head><p>The new corpus is structured by participants. Each participant is associated with 10 tasks including highresolution 3D model sequences, 2D RGB videos, thermal  videos, and sequences of physiological signals (i.e., respiration rate, blood pressure, EDA, and heart rate). <ref type="figure" target="#fig_2">Figure 3</ref> shows the overall structure of database. The average size of each subject is about 100GB, resulting in over 10TB with about 1.4 million frames in total. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates sample data sequences of four modalities from a subject.</p><p>In addition, the metadata are also generated, including manually labeled action units (both occurrence and intensity) on four tasks, automatically tracked head poses, and 3D/2D/IR facial landmarks. Detailed annotations and method will be described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data Annotation and Descriptive Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">FACS Coding</head><p>Expert FACS coders annotated facial action units during four tasks (i.e., happiness/amusement, embarrassment, fear/nervous, and physical pain) for all 140 subjects. Therefore, we have 560 (i.e., 4 × 140) data sessions coded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">AU occurrence</head><p>Segments of the most facially-expressive 20 seconds of each task and a total of 34 facial action units were occurrence coded by five expert FACS coders. Coders annotated onsets when the action units reached the B-level of intensity (as defined by the FACS manual) and offsets when they   <ref type="table" target="#tab_4">Table 3</ref> shows the base rate for all 34 action units.</p><p>To assess inter-rater reliability, 94 sessions were randomly selected for comparison coding. Two or more of the five coders coded these videos. Across the action units with base rates higher than 5%, the mean value (S) as seen from <ref type="figure" target="#fig_4">Figure 5</ref> was 0.79, ranging from 0.59 for AU 11 to 0.94 for AU 4. Among all chance-adjusted reliability indices, the S index is robust to the most problems (e.g., skewed base rates) and is especially suited to binary occurrence coding <ref type="bibr" target="#b36">[37]</ref>. According to Altman's benchmarks, these results indicate very good reliability (&gt;0.8) for 8 action units, good reliability (&gt;.6)for 5 action units, and moderate reliability (&gt;.4)for 1 action unit. For consistency with the past, results are also presented in <ref type="figure" target="#fig_4">Figure 5</ref> using the overall Matthew's Correlation Coefficient (MCC).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">AU intensity</head><p>AUs 6, 10, 12, 14, and 17 were intensity coded for a subset from the whole database. Coding was completed by two expert coders. The distribution of intensity levels was similar across action units, with B-level frames being the most common, followed by C-level frames, D-level frames, Alevel frames, and E-level frames, in descending order. Although occurrence coders delimited events at the B-level, intensity coders annotated additional frames before and after each event. Many of these additional frames were Alevel frames.</p><p>Percentage of frames at each intensity level is illustrated in <ref type="figure" target="#fig_5">Figure 6</ref>. Across the action units that were coded for intensity, the mean inter-rater reliability (weighted S) was 0.76, ranging from 0.70 for AU 6 to 0.84 for AUs 10 and 12 <ref type="table">(Table 4</ref>). Although many interval-level performance metrics (e.g., PCC, ICC, and MSE) have been used to calculate the reliability of intensity coding, intensity codes are ordinal in nature and require a categorical reliability metric. Here, we apply ordinal weighting to the S index. According to Altman's benchmarks, these results indicate good reliability for three AUs and very good reliability for two AUs.  <ref type="table">Table 4</ref>: Intensity reliability across 5 action units. This analysis is based on redundant coding of 9 sessions by both coders; the sessions selected differ between AUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature Point Tracking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">3D feature tracking</head><p>To track feature points from the 3D geometric face surface directly, we apply a so-called shape index-based statistical shape model (SI-SSM) <ref type="bibr" target="#b2">[3]</ref> for such a task. Similar to <ref type="bibr" target="#b2">[3]</ref>, 83 landmark points are defined on the face surface including the eyes, nose, mouth, eyebrows, and face contour. Utilizing the SI-SSM allows us to track a range of dynamic expressions and model transformations (translation, rotation, etc.), by making use of both the global and local shapes of the input face model. The global face shape is constructed by a parameterized model S G from a set of training data, each with 83 patches centered at 83 feature landmarks. Similarly, the local face shape S L is represented by the shape index values of each patch. PCA is applied to both the global and local shape models to learn the modes of variation from the training data. Both the global and local feature vectors are then combined into one model S GL = {S G , S L }, representing the face surface shape with expression deformation more adaptively.</p><p>To detect and track features on 3D face models, the classic cross correlation template matching scheme is applied to compute the correlation score between the each patch of the SI-SSM and the input mesh model patches. We compared the tracked features from the SI-SSM approach to manually annotated ground truth resulting in a mean squared error of 2.5. Example tracked frames can be seen in <ref type="figure" target="#fig_7">Figure 7</ref> (first row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">2D feature tracking</head><p>Two-dimensional facial expression sequences were automatically tracked using the zface software <ref type="bibr" target="#b11">[12]</ref>. By applying cascade regression to person-independent 3D registration (inferred from the 2D video), zface tracked 49 facial feature points with various head poses in each video frame. Using this approach, facial feature points remain invariant across head pose over a range of approximately 60 degrees. <ref type="figure" target="#fig_7">Figure 7</ref> (second row) shows several sample frames with tracked points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Thermal feature tracking</head><p>In order to make three modalities face data (3D, 2D, IR (Infra-red)) easy to align each other, we have also tracked the 28 facial landmarks from the thermal temperature data directly.</p><p>Initially we pre-process the thermal temperature data to increase their local contrast. Then the Constrained Local Model (CLM) <ref type="bibr" target="#b5">[6]</ref> is applied to sequentially perform independent facial landmark detection and global refinement based on the face shape pattern constraint. In detection, we first initialize the facial landmark locations using the mean face shape based on the initialized thermal eye locations. Then, we search for each facial landmark independently in the local region with the Gabor wavelet, phase-based displacement estimation method <ref type="bibr" target="#b37">[38]</ref>, and a pre-built offline feature databases. Given the independent facial landmark detection results, the face shape is refined with the Active Shape Model <ref type="bibr" target="#b3">[4]</ref>. In tracking, the facial landmarks are initialized as the locations in the last framework. For local independent landmark searching, both the online template from the last frame and the offline databases are combined for better prediction. ASM is also applied to refine the independent detection results. We tested the thermal facial landmark tracking approach on the thermal sequences of the database, and calculated the landmark tracking error as the distance between the tracked landmark locations and the manually annotated ground truth landmark locations, which is normalized by the inter-ocular distance. If we consider the images with error less than 10% of the inter-ocular distance as successfully detected images, the detection rate is 91.57%. <ref type="figure" target="#fig_7">Figure 7</ref> (third row) shows the tracking result on a sample set of thermal images with different participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Head Pose</head><p>Head pose is an important cue for understanding emotional expressions. It is tracked and included in the database as one of the meta-data. Three orientations (yaw, roll and pitch) are estimated through video sequences. Since the head pose information can be derived directly from our tracked 3D points on the 3D face model sequences, here we focus on tracking the head pose from 2D videos. A cylindrical head tracker <ref type="bibr" target="#b10">[11]</ref> is used to get head pose from 2D videos. The tracker works person-independently, and has concurrent validity with 2D+3D AAM <ref type="bibr" target="#b15">[16]</ref> with magnetic motion capture device <ref type="bibr" target="#b10">[11]</ref>.</p><p>We randomly select 60 subjects for the statistical analysis. As shown in <ref type="table">Table 6a</ref>, over 90% of frames are less than 10 degree with respect to the front view. To show the head pose variations with different emotions, we compute the standard derivations of all 60 subjects across 10 tasks. In <ref type="figure" target="#fig_8">Figure 8</ref>, vertical axis stands for standard derivation of head pose. Among all 10 tasks, tasks T6-T10 have clearly larger pose variations in pitch than the other tasks. Except for T3-T4, the pitch variation is more dramatic than roll and yaw. Except for T2, the roll variation appears to be the smallest among the three orientations, meaning that the rolling head is not common nor comfortable for exhibiting emotions. Having such a finding, pitch could be used for disclosing more clues on emotional status than the other head orientations. The physical experience method in the process of emotion elicitation could have more dramatic head motions than the other emotion induction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Validation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">3D Dynamic Spontaneous Expression Analysis</head><p>To validate the usefulness of the data, we applied the approach reported by Huang et al. <ref type="bibr" target="#b9">[10]</ref> to adapt a generic model to the model sequences. The hybrid approach using two vertex mapping algorithms, displacement mapping and point-to-surface mapping, and a regional blending algorithm are used to reconstruct the facial surface detail. The adapted models have the same number of vertices across the corresponding 3D video sequence. Thereby, the vertex correspondence across the range model sequence is established. The vertices on the adapted model can be tracked by finding the displacement of tracked vertices of two neighboring frames.</p><p>In order to make it comparable to the state of the art <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b25">26]</ref>, we implemented the 3D dynamic facial expres-sion descriptor based on primitive feature labels and HMM classifier as described in <ref type="bibr" target="#b25">[26]</ref>. 60 subjects were selected for performance evaluation. Among ten tasks, six tasks <ref type="table" target="#tab_4">(T1, T3, T4, T7, T9</ref>, and T10, corresponding to six prototype expressions) of each subject were used for classification. A 10-fold cross validation procedure was applied with 90% subjects for training and 10% subjects for testing. The result shows that the average correct recognition rate is 74.8%. As comparison, the same approach was applied to the BP4D database <ref type="bibr" target="#b34">[35]</ref>, resulting in a 73.7% recognition rate for classifying spontaneous 3D dynamic expressions (joy, anger, surprise, disgust, fear, and sadness) on average. We have also applied the same approach to 3D dynamic posed expressions using a public database BU-4DFE <ref type="bibr" target="#b31">[32]</ref>, where 81.2% recognition rate was achieved for classifying six posed expressions. Apparently, our new dataset has the comparable quality to the BP4D database in terms of the 3D modality. The 3D dynamic spontaneous facial expressions show much more variety and subtlety in appearance and timing than the posed expressions. This still poses a big challenge for analysis of naturally occurred facial behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Facial Expression Analysis on Thermal Data</head><p>To validate the utility of the thermal data, we have also conducted experiments on facial expression recognition using the thermal videos of randamly selected 60 subjects. We applied the thermal video descriptor reported in <ref type="bibr" target="#b14">[15]</ref> for such a task. The face region of each thermal video clip is warped to the frontal view based on scale-invariant feature transform(SIFT) flow, generating a corresponding SIFT flow video clip. Then, the thermal video cuboids are segmented from each thermal video clip based on max pooling and motion video cuboids are segmented from each SIFT flow video clip based on average pooling. Thermal video words and motion video words are clustered by k-means cluster. Finally, each video is represented by a histogram of the bag of SIFT Flow and facial temperature changes video words. The resulting histogram is used as a descriptor for classification by the support vector machine(SVM). The recognition accuracy is 91%.</p><p>For comparison, we have also applied a state-of-the-art approach reported in <ref type="bibr" target="#b30">[31]</ref> to test on our database. The features derived from the temperature difference matrix on forehead, left cheek and right cheek were used. The recognition accuracy is 62%. We further compared the two methods on randomly selected 22 subjects from USTC-NVIE database <ref type="bibr" target="#b29">[30]</ref> and achieved recognition accuracy 71% and 59%, respectively, on classifying six prototype expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Task Classification on Physiological Data</head><p>To validate the utility of the collected physiological data, we conducted emotion recognition experiments based on those data. In our experiments, we randomly selected 45 subjects, and classify the 10 tasks (emotions) from the set of features <ref type="bibr" target="#b17">[18]</ref> extracted from the physiological signals, including 8 features from EDA (i.e., mean and variance of the normalized signal (Mn, Vn), mean and RMS of the 1st derivative (Md, RMSd), average rising time and recover time of SCRs, average of negative derivative, and their proportion to all derivative values), and 7 features from blood pressure signals (i.e., Mn, Vn, Md, RMSd, and pulse rate, diastolic blood pressure, systolic blood pressure variance), and 5 features from respiration signals (i.e., Mn, Vn, Md, RMSd, and respiration rate variance).</p><p>In our first experiment we have selected five tasks (Tasks 1, 3, 4, 7, and 10) which target happiness, sadness, startle, fear and disgust emotions. Using 10-fold cross validation and RBF kernel SVM, the average accuracy of five-class emotion recognition is 59.5%. Moreover, we mapped the emotion classes into binary classes of low and high arousal using the emotion semantic space described in <ref type="bibr" target="#b21">[22]</ref>. Based on the new classes, we used the same classifier and achieved 60.5% accuracy in classifying 10 tasks (emotions) from our database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">AU Detection and Recognition</head><p>We performed experiments to detect and recognize FACS Action Units based on 3D dynamic facial model sequences. We developed a log-normal (LN) based 4D polynomial fitting approach for generating spatio-temporal features <ref type="bibr" target="#b18">[19]</ref>. Given the 3D dynamic model sequences, depth information is extracted for each frame, and the neighborhood around each spatio-temporal point is fit to a 4D polynomial (using the best-fitting log-normal function to model temporal behavior). The dynamic curvature values, the static curvature values, and the shape index values are computed from each feature, and histogram for each spatial region is formed from the corresponding features. More details of the algorithm (the LN-based 4D feature approach) can be found in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Action units are tested for individually in subsequences (31 frames long) extracted from the full video sequences. Positive samples are subsequences that contain a single AU preceded and followed by the absence of that particular AU. For classification, the Leave-One-Subject-Out approach was employed using SVM. Subsequences for 7 AUs were extracted, which resulted in 213 sample subsequences extracted from a subset of the database. The method automatically finds the "best" 7-frame window in a 31-frame subsequence (referred to LN(31)). We also list the results from using only the 7-frame windows containing the AUs on both our approach (referred to as LN <ref type="formula">(7)</ref>) and LBP-TOP <ref type="bibr" target="#b35">[36]</ref> (referred to as LBP <ref type="formula">(7)</ref>). The results are presented in <ref type="table" target="#tab_6">Table 5</ref>. We also performed a cross-database test by training on data extracted from the BP4D-Spontaneous database <ref type="bibr" target="#b34">[35]</ref> and testing on the new data. AUs 1, 2, 6, 16, and 17 AUC F1 AU LBP(7) LN(7) LN(31) LBP(7) LN <ref type="formula">(7)</ref>    <ref type="table">Table 6</ref>: Statistics of head pose and AU recognition were tested. The results can be seen in <ref type="table">Table 6b</ref>. These experiments again used the 31-frame subsequences with the LN(31) approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we have presented a new multimodal spontaneous emotion database (MMSE) for the research community in order to facilitate the research of the field. We have employed the state-of-the-art algorithms to label and validate the data. Partial data have also been used successfully for application of video-based heart rate estimation <ref type="bibr" target="#b26">[27]</ref>. However, our current work has certain limitations, which give rise to our future work as follows: (1) we will expand the database from several aspects, including more subjects, AUs, and intensity coding; (2) we will also extract the physiological features and study the cross-correlation of multimodal data and their fusion schemes. As a result, the database will be sustained and updated progressively by including all the derivatives for the research community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Recording system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Emotion distribution from self-report.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Database overall structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Sample data sequences from a participant including original 2D texture (first row), shaded model (second row), textured model (third row), thermal image (fourth row), physiology signal(fifth row: respiration rate, blood pressure, EDA, heart rate) and corresponding action units(last row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Occurrence reliability with two kinds of metrics. dropped below it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Percentage of frames at each intensity level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Samples of feature points tracking on 3D models (1st row: 83 feature points tracked by SI-SSM), 2D texture sequences (2nd row: 49 feature points tracked by zface), and thermal frames (3rd row: 28 feature points).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Head pose variations under different tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Ten tasks for spontaneous emotion elicitation.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>AU occurrence for 140 subjects(BR = base rate).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>AU Depth Data Results (WA = Weighted Average based on AU sequence counts) (b) AU Depth Cross-Database Results</figDesc><table>Angle Pitch Yaw Roll 
&lt; 5 • 72.8 64.5 82.6 
&lt; 10 • 92.7 92.2 97.6 
&lt; 15 • 98.7 98.1 99.6 
&lt; 20 • 99.5 99.6 99.9 

(a) Proportion of frames with 
certain range of rotation angles 

AU Acc. AUC F1 
1 
.818 .876 .817 
2 
.875 .938 .873 
6 
.900 .920 .899 
16 .765 .875 .765 
17 .714 .780 .714 
WA .776 .850 .776 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.di4d.com 2 http://www.biopac.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This material is based upon the work supported in part by the National Science Foundation under grants CNS-1205664 and CNS-1205195. We would like to thank Nicki Siverling for technical assistance. We would also like to thank Dr. Peter Gerhardstein for his help in data collection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The automatic detection of chronic pain-related expression: requirements, challenges and a multimodal dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaltwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic recognition of facial actions in spontaneous expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Littlewort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="22" to="35" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Landmark localization on 3d/4d range data using a shape index-based statistical shape model with global and local constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="136" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active shape mod-els&amp;mdash;their training and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A facs valid 3d dynamic action unit database with applications to 3d dynamic morphable facial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cosker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krumhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic feature localisation with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3054" to="3067" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NAS</title>
		<meeting>of the NAS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">111</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Manual for the facial action coding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Consulting Psychologists Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A 3-d audio-visual corpus of affective communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Romsdorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reshaping 3d facial scans for facial appearance modeling and 3d facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<idno>2012. 7</idno>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust 3d head tracking by online feature registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense 3d face alignment from 2d videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deap: A database for emotion analysis; using physiological signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic coding of facial expressions displayed during posed and genuine pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spontaneous facial expression analysis based on temperature changes and head motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2d vs. 3d deformable face models: Representational power, construction, and real-time fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward machine emotional intelligence: Analysis of affective physiological state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vyzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1175" to="1191" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic analysis of facial activity for multimodal human-machine applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Binghamton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD dissertation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Static and dynamic 3d facial expression recognition: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sandbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<idno>2012. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bosphorus database for 3d face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alyüz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dibeklioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics and Identity Management</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">What are emotions? and how can they be measured? Social science information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="695" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multimodal database for affect recognition and implicit tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Methodological considerations in the psychophysiological study of emotion. Handbook of affective sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="225" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the effect of illumination on automatic expression recognition using the ict-3drfe database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<idno>2012. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on 3d dynamic range model sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-adaptive matrix completion for heart rate estimation from face videos under realistic conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FERA&apos;15 2nd facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d facial expression recognition based on primitive surface feature distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A natural visible and infrared facial expression database for expression recognition and emotion inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyses of a multimodal spontaneous facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A high-resolution 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BP4D-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">19 assumptions behind intercoder reliability indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Yearbook</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">419</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust pose invariant facial feature detection and tracking in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
