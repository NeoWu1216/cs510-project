<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Multi-Object Tracking via Structural Constraint Event Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Keti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ryeol</forename><surname>Lee</surname></persName>
							<email>crlee@gist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
							<email>jhyoon@keti.re.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">CV Lab</orgName>
								<orgName type="institution">GIST</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">CV Lab</orgName>
								<orgName type="institution">GIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Online Multi-Object Tracking via Structural Constraint Event Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-object tracking (MOT) becomes more challenging when objects of interest have similar appearances. In that case, the motion cues are particularly useful for discriminating multiple objects. However, for online 2D MOT in scenes acquired from moving cameras, observable motion cues are complicated by global camera movements and thus not always smooth or predictable. To deal with such unexpected camera motion for online 2D MOT, a structural motion constraint between objects has been utilized thanks to its robustness to camera motion. In this paper, we propose a new data association method that effectively exploits structural motion constraints in the presence of large camera motion. In addition, to further improve the robustness of data association against mis-detections and false positives, a novel event aggregation approach is developed to integrate structural constraints in assignment costs for online MOT. Experimental results on a large number of datasets demonstrate the effectiveness of the proposed algorithm for online 2D MOT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-object tracking (MOT) aims to estimate object trajectories according to the identities in image sequences. Recently, thanks to the advances of object detectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>, numerous tracking-by-detection approaches have been developed for MOT. In this type of approaches, target objects are detected first and tracking algorithms estimate their trajectories using detection results. Tracking-by-detection methods can be broadly categorized into online and offline (batch or semi-batch) tracking methods. Offline MOT methods generally utilize detection results from past and future frames. Tracklets are first generated by linking individual detections in a number of frames, and then iteratively associated to construct long trajectories of objects in the entire sequence, or in a time-sliding window with a temporal delay (e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>). On the other hand, online MOT algorithms estimate object trajectories using only detections from the current as well as past frames (e.g. <ref type="bibr" target="#b3">[4]</ref>), and online MOT algorithms are more applicable to real-time applications such as advanced driving assistant systems and robot navigation.</p><p>In MOT, object appearances are used as important cues for data association which solves the assignment problems of detections-to-detections, detections-to-tracklets, and tracklets-to-tracklets. However, appearance cues alone are not sufficient to discriminate multiple objects, especially for tracking similar objects (e.g., pedestrians, faces, and vehicles). Tracking-by-detection methods typically exploit motion as well as appearance cues, and use certain (e.g., linear or turn) models to describe the object movements. However, for online 2D MOT in scenes acquired from moving cameras, observable motion cues are complicated by global camera movements and not always smooth or predictable. In other words, even when the individual object motion model is updated with consecutive detections, it is not reliable enough to predict the next location of an object when the camera moves severely. The situation becomes worse when objects are not correctly detected since, without correct detections, object motion models cannot be updated to take camera motion into account. While significant advances on batch (or semi-online) trackers have been made (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>), online MOT using motion constraints from detection results has not yet been much explored.</p><p>In this paper, we propose a new data association method for effectively exploiting the structural motion constraints between objects for online 2D MOT, which considers camera motion as well as ambiguities caused by the frequent mis-detections. The structural constraints are represented by the location and velocity differences between objects. Using these constraints, we introduce a new cost function which takes global camera motion into account to associate multiple objects. In addition, to reduce the assignment ambiguities caused by mis-detections as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we propose the event aggregation approach which considers the structural constraints and assignment events.</p><p>We incorporate the proposed data association and the structural constraints into a two-step online 2D MOT framework, which consists of two data association steps. In the first step, by using the proposed structural constraint event aggregation, even under large camera motion or fluctuations, we can robustly estimate continuously tracked objects where structural constraints are sufficiently reliable due to consecutive updates at each frame. In the second step, we infer and recover the missing objects between frames to alleviate the problems of mis-detection from detectors. Using the structural constraints of objects between frames, we can re-track the missing ones from the tracked objects from the first step. We demonstrate the merits of the proposed algorithm for online MOT using a large number of challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review related MOT methods that utilize the structural motion constraints. Numerous MOT methods directly utilize the first or the second order motion models to locate objects <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>. However, those 2D independent motion models do not work properly under unpredictable camera motion, especially when tracking methods do not exploit the visual information from future frames.</p><p>Pellegrini et al. <ref type="bibr" target="#b20">[21]</ref> and Leal-Taix√© et al. <ref type="bibr" target="#b17">[18]</ref> use social force models which consider pairwise motion (such as attraction and repulsion) and visual odometry to obtain 3D motion information for tracking multiple objects. Different from the proposed online 2D MOT algorithm, this method requires 3D information to project objects and detections on the top-view plane for association. In addition, this method does not consider scenes with large camera motion.</p><p>Grabner et al. <ref type="bibr" target="#b12">[13]</ref> propose to exploit the relative distance between feature points for single object tracking and reduce tracking drifts caused by drastic appearance changes. In <ref type="bibr" target="#b6">[7]</ref>, a mutual relation model is proposed to reduce tracking errors when target objects undergo appearance changes. To reduce ambiguities caused by similar ap-pearances in MOT, motion constraints between objects are used along with object appearance models using the structured support vector machines <ref type="bibr" target="#b29">[30]</ref>. Unlike the aforementioned methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30]</ref>, our method exploits structural constraints to solve the online 2D MOT problem with the frame-by-frame data association that assigns objects to correct detections.</p><p>Yang and Nevatia <ref type="bibr" target="#b27">[28]</ref> use conditional random field for MOT in which the unary and binary terms are based on linear and smooth motion to associate past and future tracklets in sliding windows. Recently, Yoon et al. <ref type="bibr" target="#b28">[29]</ref> exploit structural spatial information in terms of relative motion to handle camera motion. This method basically assumes that the camera motion is small and smooth to guarantee that at least a few objects are well predicted and tracked by linear motion models. Different from the aforementioned methods, the proposed method aggregates structural constraints along with assignment events taking abrupt camera motion and ambiguities caused by mis-detections into account for online MOT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structural Constraint Event Aggregation</head><p>The trajectory of an object is represented by a sequence of states denoting the position, velocity, and size of an object in the image plane with time. We denote the state of an object i at frame t as s i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Structural constraint cost function</head><p>The MOT task can be considered as a data association problem, which finds the correct assignment event between objects and detections. In this paper, the assignment event a i,k ‚àà A describes the state of assignments between objects and detections. If the detection k is assigned to the object i, the assignment is denoted by {a i,k = 1}. Otherwise, it is denoted by {a i,k = 0}. For data association, the dissimilarity cost between objects and detections is computed based on the cost function. The best assignment event is then estimated by minimizing total assignment costs. In this section, we introduce a new cost function that considers the structural motion constraints between objects. The tracked objects and their detections are represented by red boxes and yellow boxes, respectively, and the green lines connecting objects denote structural constraints. Black boxes represent assignments. d 0 stands for the case of mis-detections. As shown in this figure, in the anchor assignment a 2,2 of the object 2 and the detection 2, we move the object 2 to align the center location of the object 2 with that of the detection 2. Then, in the structural constraint cost, we compute the assignment costs of other objects and detections based on their structural constraints. From the different anchor assignments, the structural constraint costs for the same assignment event are computed. For instance, the costs of the assignment event (a 1,0 = a 2,2 = a 3,3 = 1) are obtained from the anchor assignments a 2,2 = 1 and a 3,3 = 1, respectively. The event aggregation fuses these structural constraint costs having the same assignment event but the different anchor assignment. Œ£ represents the summation of the structural constraint costs.</p><p>We denote a detection k resulting from detectors at frame t as</p><formula xml:id="formula_0">d k t = [x k d,t , y k d,t , w k d,t , h k d,t ]</formula><p>‚ä§ and the set of the detections at frame t used for MOT as D t (d k t ‚àà D t ) with its index set as k ‚àà M t . Without loss of generality, we remove the time index t for simplicity in the following sections. Since each object is assigned with at most one detection, the structural constraint cost function with the assignment event A is described b≈∑</p><formula xml:id="formula_1">A = arg min A C(S, E, D), s.t. i‚ààN k =0 a i,k ‚â§ 1 ‚àß k‚ààM ‚à™{0} a i,k = 1 ‚àß i‚ààN a i,0 ‚â§ |N |,<label>(2)</label></formula><p>where each assignment is a binary value a i,k = {0, 1}, a i,k ‚àà A, k ‚àà M ‚à™ {0}, and a i,0 stands for the case of misdetected objects. Hence, the sum of a i,0 along i is equal to the number of objects |N | when all objects are mis-detected.</p><p>To deal with large camera motion, we first set an anchor assignment by associating the object i and the detection k as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Anchor assignment a i,k makes the center location of the object i coincide with that of the detection k. Based on the anchor assignment and the structural constraint E i , we conduct all possible assignment events between the remaining objects and detections. By doing this, the structural constraint cost evades the error caused by the global camera motion. Based on this concept, the proposed structural constraint cost function is formulated by</p><formula xml:id="formula_2">C(S, E, D) = i‚ààN k‚ààM a i,k ‚Ñ¶ i,k + j‚ààN j =i q‚ààM ‚à™{0} q =k a j,q Œò j,q i,k ,<label>(3)</label></formula><p>where the subscripts i, k denote the index for costs computed based on the anchor assignment a i,k = 1, and the cost of the anchor assignment is represented by</p><formula xml:id="formula_3">‚Ñ¶ i,k = F s (s i , d k ) + F a (s i , d k ).<label>(4)</label></formula><p>Here, we compute the size and appearance costs as</p><formula xml:id="formula_4">F s (s, d) = ‚àí ln 1 ‚àí |h ‚àí h d | 2(h + h d ) ‚àí |w ‚àí w d | 2(w + w d ) , F a (s, d) = ‚àí ln B b=1 p b (s)p b (d),<label>(5)</label></formula><p>where (w, h) and (w d , h d ) denote width and height of an object and a detection, respectively. In addition, p n (s) and p n (d) denote the histogram of an object and a detection, respectively. b is the bin index and B is the number of bins. From the anchor position, we calculate the cost of the structural constraint which is described by</p><formula xml:id="formula_5">Œò j,q i,k = Ô£± Ô£≤ Ô£≥ F s (s j , d q ) + F a (s j , d q ) +F c (s j , e j,i , d k , d q ) if q = 0 œÑ if q = 0 ,<label>(6)</label></formula><p>where we empirically set the cost œÑ to some non-negative value (e.g., 4 in this work) for the case of mis-detected objects, d 0 . The constraint cost is formulated by</p><formula xml:id="formula_6">F c (s j , e j,i , d k , d q ) = ‚àí ln area(B(s j,k )‚à©B(d q )) area(B(s j,k )‚à™B(d q )) , s j,k = [x k d , y k d , 0, 0] ‚ä§ + [œá j,i , œÖ j,i , w j , h j ] ‚ä§ .<label>(7)</label></formula><p>Here, we determine the position of the object j by the position of the detection k and the structural constraint e j,i . The constraint cost is measured by using the overlap ratio <ref type="bibr" target="#b8">[9]</ref> of the object bounding box and the detection bounding box to compute a normalized cost since it automatically compensates bias errors caused by the size of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Event aggregation</head><p>Based on the different anchor assignments, we obtain different costs due to the different sizes of detections and detection noises even if the assignment event A is the same. Hence, we aggregate all the costs that have the same assignment event but with the different anchor assignments. Compared to conventional one-to-one matching process for the data association as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, this process significantly reduces ambiguity caused by false positives near objects, mis-detections, and constraint errors since we can measure the cost of each assignment event several times according to the number of corresponding anchor assignments as described in <ref type="figure" target="#fig_1">Figure 2</ref>. This aggregation process is described by</p><formula xml:id="formula_7">C(A) = i‚ààN,k‚ààM a i,k =1 a i,k ‚Ñ¶ i,k + j‚ààN j =i q‚ààM ‚à™{0} q =k a j,q Œò j,q i,k ,<label>(8)</label></formula><p>where A ‚äÇ A all and A all denotes all possible assignment events. Finally, we select the best assignment event having the minimum aggregated cost a≈ù</p><formula xml:id="formula_8">A = arg min A C(A) ‚àÜ , ‚àÜ = i‚ààN,k‚ààM a i,k ,<label>(9)</label></formula><p>where ‚àÜ denotes the normalization term that is equal to the number of the anchor assignments from the same assignment event A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Assignment event initialization and reduction</head><p>Since considering all of assignment events is not computationally efficient, we propose a simple but effective reduction approach. First, we adopt the simple gating technique <ref type="bibr" target="#b1">[2]</ref> before conducting the structural constraint event aggregation. This approach is widely used in the MOT literature. We roughly remove the negligible assignments based on two conditions as where p i and p k d represent the position of the object i and the detection k, respectively, and (w i , h i ) denotes the size of the object i. We empirically set œÑ s = 0.7. If the above conditions are satisfied, a i,k = 1. Otherwise, the assignment is set to a i,k = 0, and this assignment is not considered for tracking at the current frame. Second, we propose a partitioning approach that splits the structural constraints to handle a large number of objects and detections as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The assignments of objects and detections in different paritions are set to a i,k = 0. For the partition p, we generate all possible assignment events A p ‚äÇ A p all based on the condition in <ref type="bibr" target="#b1">(2)</ref>. The structural constraint event aggregation is carried out for each partition. The final assignment event is then obtained by merging the assignment event results from each partition. In this work, we empirically set the maximum number of objects in each partition to 5. The number of partitions is determined by P = ‚åàthe number of objects/5‚åâ, and we then splits the partition possibly to have the same number of objects. Here, we use the center location as a partitioning condition. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, P K-means centers are obtained, and the objects located close to each K-means center are then gathered in the same partition. Another reduction approach <ref type="bibr" target="#b22">[23]</ref> can be alternatively modified and applied to our structural constraint event aggregation. The main steps of the proposed structural constraint event aggregation (SCEA) method is summarized in Algorithm 1.</p><formula xml:id="formula_9">p i ‚àí p k d &lt; (w i ) 2 + (h i ) 2 ‚àß exp ‚àíF s (s i , d k ) &gt; œÑ s ,<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Two-Step Online MOT via SCEA</head><p>We adopt a two-step approach for effectively exploiting the structural constraints between objects for online 2D MOT. Since the structural constraints of objects tracked in the previous frame have been also updated with their corresponding detections, their constraints are more robust than mis-detected objects. This allows us to more robustly and accurately assign detections to tracked objects. The overall process of the proposed online MOT via SCEA is described in Algorithm 2.</p><p>Data: objects S, detections D, structural constraints E Result: assignment event A begin</p><p>Step 1: Initializing possible assignment events (Section 3.3) ¬∑ Remove negligible assignments by using the gating ( <ref type="formula" target="#formula_9">(10)</ref>). ¬∑ Divide objects, structural constrains, and detections into the subset S p ‚äÇ S, E p ‚äÇ E, D p ‚äÇ D by the partitioning <ref type="figure" target="#fig_2">(Fig. 3)</ref>. ¬∑ Generate all possible assignment events of each partition A p all from the S p and D p based on the condition in (2).</p><p>Step 2: Aggregating assignment event costs ( <ref type="formula" target="#formula_7">(8)</ref> and <ref type="formula" target="#formula_8">(9)</ref></p><formula xml:id="formula_10">) A = œÜ; for p = 1 : P do C(A p ) = i‚ààN p ,k‚ààM p a i,k =1 a i,k ‚Ñ¶ i,k + j‚ààN p j =i q‚ààM p ‚à™{0} q =k a j,q Œò j,q i,k , A p = arg min A p (C(A p )/‚àÜ) , A p ‚äÇ A p all ; A := A ‚à™√Ç p ; end end Algorithm 1: Structural Constraint Event Aggregation.</formula><p>We denote the set of tracked objects in the previous frame by S w , and their structural information is represented by E w . Using S w , E w , and the current detections D, we conduct the first data association via the SCEA introduced in Section 3. Then, we obtain the new assignment event√Ç w from which we store the position of associated detections for the object i as</p><formula xml:id="formula_11">s i 1 = [x k d , y k d ] ‚ä§ , s i 1 ‚àà S w 1 if a i,k = 1</formula><p>, and the set of associated object index is represented by i ‚àà N w . In the second step, similar to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>, we recover missing objects, which are not associated with any detections in the previous frame but re-detected in the current frame. The recovery process is conducted by using the tracked objects in the first step and their structural constraint information as described in <ref type="figure" target="#fig_4">Figure 4</ref>. The mis-detected objects are denoted by S m , and the structural constraints between mis-detected objects and tracked objects are represented by E m . Using S m , E m , and S w 1 , we recover the re-detected objects a≈ù</p><formula xml:id="formula_12">A m = arg min A C(S m , E m , S w 1 ,D), s.t. i‚ààN m a i,q = 1 ‚àß q‚ààM a i,q = 1,<label>(11)</label></formula><p>where N m denotes the set of the mis-detected object index, andM represents the index set of the detectionsD. Here, detectionsD contains the not-assigned detections in the first step and dummy detections d 0 for the case of missing objects. The structural constraint cost function for missing objects is defined as</p><formula xml:id="formula_13">C(S m , E m , S w 1 ,D) = i‚ààN m q‚ààM a i,q Œ¶ i,q Œ¶ i,q = Ô£± Ô£≤ Ô£≥ F s (s i , d q ) + F a (s i , d q ) +F r (s i , E m , S w 1 , d q ) if q = 0 œÑ if q = 0 ,<label>(12)</label></formula><p>Data: tracked objects S w , structural constraints of tracked objects E w , mis-detected objects S m , structural constraints between tracked objects and mis-detected objects E m , detections D Result: Trajectories of the targets for video frame f do</p><p>Step 1: Data association via SCEA ¬∑√Ç w = SCEA(S w , E w , D); (Section 3 and Algorithm 1)</p><formula xml:id="formula_14">¬∑ S w 1 = {s i 1 = [x k d , y k d ] ‚ä§ |a i,k = 1, ‚àÄi ‚àà N w , ‚àÄk ‚àà M };</formula><p>Step 2: Recovery of mis-detected objects ¬∑√Ç m = Recovery(S m , E m , S w 1 ,D); ( <ref type="formula" target="#formula_12">(11)</ref> and <ref type="formula" target="#formula_1">(12)</ref>) ¬∑√Ç =√Ç w ‚à™√Ç m ;</p><p>Step 3: Update ¬∑ Current tracking result:  where œÑ = 4 in this work. We recover the missing object i from the set of tracked objects using their structural constraint. The constraint cost is therefore formulated as</p><formula xml:id="formula_15">S w = {s i := KF (s i , d k )|a i,k = 1, ‚àÄi ‚àà N w ‚à™ N m , ‚àÄk ‚àà M } with</formula><formula xml:id="formula_16">F r (s i , E m , S w 1 , d q ) = ‚àí ln area(B(s i,Œ≥ )‚à©B(d q )) area(B(s i,Œ≥ )‚à™B(d q )) , s i,Œ≥ = [(s Œ≥ 1 ) ‚ä§ , 0, 0] ‚ä§ + [œá i,Œ≥ , œÖ i,Œ≥ , w i , h i ] ‚ä§ Œ≥ = arg max j‚ààN w 1 [œá i,j ,œÖ i,j ] .</formula><p>(13) Here, N w denotes the index of tracked objects at the first step, and the reliability of structural constraints between tracked objects and missing objects can be different according to the past motion coherence. To consider this constraint reliability, we select the object moving in the most similar direction and velocity by taking into account the motion coherence between objects, [œá i,j ,œÖ i,j ] . To solve (11), we reformulate (11) in a matrix form as</p><formula xml:id="formula_17">C = Œ¶ det |N m |√ó|M | Œ¶ 0 |N m |√ó|N m | ,</formula><p>where the matrices are obtained by Œ¶ det = [Œ¶ i,q ], ‚àÄi ‚àà N m , ‚àÄq ‚ààM and Œ¶ 0 = diag[Œ¶ i,0 ], ‚àÄi ‚àà N m . The offdiagonal etries of Œ¶ 0 are set to ‚àû. We then apply the Hungarian algorithm <ref type="bibr" target="#b15">[16]</ref> to get the assignment event having the minimum cost.  <ref type="bibr">[-7, 7]</ref>, <ref type="bibr">[-15, 15]</ref>) represent the range of the different levels of camera motion fluctuation noise in terms of pixel. The missing rate of the detections is set to 0%, 10%, 20%, and 30%. The proposed SCEA shows the best overall performance. We analyze the performance of each method in detail in Section. 5.1.</p><p>From√Ç w and√Ç m , we update the final tracking result as <ref type="bibr" target="#b24">[25]</ref> for smoothing, and the index set is represented by N w . After the update, other not-assigned objects are collected again in the set S m , and their index set is denoted by N m .</p><formula xml:id="formula_18">S w = {s i = KF (s i , d k )|a i,k = 1, ‚àÄi ‚àà N w ‚à™ N m , ‚àÄk ‚àà M } with the Kalman filter KF (¬∑)</formula><p>Structural constraint update: After tracking, we update the structural constraints between objects with their corresponding detections based on the same approach proposed in <ref type="bibr" target="#b28">[29]</ref>,</p><formula xml:id="formula_19">using z i,j t = [x i d,t , y i d,t ] ‚ä§ ‚àí [x j d,t , y j d,t ] ‚ä§ as an obser- vation, where [x i d,t , y i d,t ] ‚ä§</formula><p>represents the location of a detection assigned to the object i. We assume that the structural constraint change follows piece-wise linear motion model. With the observation z i,j t , we indirectly update the structural constraint variations by using the standard Kalman filter <ref type="bibr" target="#b24">[25]</ref>. The structural constrains of missing objects are simply based on the linear motion model.</p><p>Object management: For any MOT method, an object initialization and termination steps are typically required to manage targets according to their statuses. In this work, objects are initialized in a way similar to <ref type="bibr" target="#b3">[4]</ref>. Here, we use the distance and the appearance between two detections as an initialization cue. If the distances between a detection in the current frame and unassociated detections in the past a few (e.g., 4) frames are smaller than a certain threshold, we then initialize this detection as a new object. The structural constraint between the new object and all other objects are then generated by (1), where their initial variation is set t»Ø</p><formula xml:id="formula_20">œá i,j t =œÖ i,j t = 0.</formula><p>On the other hand, we simply delete or terminate objects if they are not associated with any detections for two frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present the experimental evaluation of the proposed online MOT algorithm and comparison against the state-of-the-art methods especially for the scenes acquired from moving cameras. For reproducibility, we will open source codes of the structural constraint cost aggregation at cvl.gist.ac.kr/project/scea.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance validation</head><p>To show the effectiveness of each component of the proposed method, we utilize the synthetic datasets which are generated based on the ground truth of the ETH sequences (Bahnhof, Sunnyday, and Jelmoli sequences) <ref type="bibr" target="#b7">[8]</ref>. We apply the different levels of motion fluctuation noises and detection missing rate as shown in <ref type="figure" target="#fig_5">Figure 5</ref>. The low level fluctuation represents the original camera motion in the ETH sequences where the camera moves smoothly. The medium level fluctuation and the high level fluctuation represent fluctuation noises synthetically generated by the uniform distribution within [‚àí7, 7] and [‚àí15, 15] pixels, respectively. In addition, for all scenarios, we include at most 10 false detections per each frame. To measure the accuracy of the data association, the number of true positives, false positives, false negative, and mis-matches are counted per each frame.</p><p>Data association evaluation: The performance of four different data association approaches is shown in <ref type="figure" target="#fig_5">Figure 5</ref>. The relative motion network (RMN) approach <ref type="bibr" target="#b28">[29]</ref> performs well under the low level fluctuation as this assumes accurate linear prediction of the well-tracked objects under smooth camera motion. The linear motion (LM) method is a baseline method where the data association is carried out without the structural constraints or event aggregation. It is similar to the joint probabilistic data association (JPDA) in that both methods consider the assignment events. A fast and efficient version of JPDA has been recently proposed and applied to the vision-based MOT in <ref type="bibr" target="#b22">[23]</ref>. As the fluctuation increases, the performance of the LM method is degraded due to large camera motion where the linear motion model dose not work well. The structural constraint nearest neighbor (SCNN) is a data association method with the structural constraint cost function but without event aggregation. Due to the structural constraint cost function, the SCNN can deal with the large camera motion. However, since the structural constraint costs are obtained by the local nearest neighbors, the performance of the SCNN shows limited performance caused by the ambiguities as discussed in Section 1 and shown in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="figure" target="#fig_5">Figure 5</ref> demonstrates that the SCEA performs better than other approaches since it robustly deals with large fluctuations based on the structural constraint cost function, and it can efficiently reduce ambiguities by aggregating costs of the same events computed based on the different anchor assignment.</p><p>Efficiency of the event reduction: In the experiments, the same event reduction techniques described in Section 3.3 are applied to the LM, SCNN, and SCEA methods for computational efficiency. Here, the gating technique has long been applied to MOT, and without this, the data association is computationally intractable when considering all possible assignment events as pointed out in <ref type="bibr" target="#b1">[2]</ref>. For that reason, we only evaluate the efficiency of the partitioning technique using the SCEA method with and without partitioning (SCEA-w/o-P). Even with the gating technique, the SCEA-w/o-P method becomes computationally intractable when more than a certain number of objects or detections are given as shown in <ref type="figure" target="#fig_6">Figure 6</ref>. For this reason, the Sunnyday sequence, the Jelmoli sequence, and the roughly half of the Bahnhof sequence (i.e., frame #0-#450) are used for the evaluation. <ref type="figure" target="#fig_6">Figure 6</ref> shows that the SCEA method is more applicable to online MOT thanks to the low computational complexity with similar performance to the SCEA-w/o-P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons with State-of-the-Art Methods</head><p>We name the proposed algorithm as SCEA (Online MOT via Structural Constraint Event Aggregation) and evaluate it on a large number of benchmark datasets: 29 sequences from the KITTI dataset <ref type="bibr" target="#b11">[12]</ref> and 22 sequences from the MOT Challenge dataset <ref type="bibr" target="#b16">[17]</ref>. The datasets contain test sequences from a static camera as well as a dynamic camera. The detections of the KITTI dataset 1 and the MOT Challenge dataset 2 are also provided. Note that, since this work focuses on 2D MOT with a single camera, we do not use any other information from stereo images, camera calibration, depth maps, or visual odometry. In addition, we utilize the same detections used for other methods in all experiments for fair comparison. We compare the SCEA with the state-of-the-art online MOT methods including MDP <ref type="bibr" target="#b25">[26]</ref>, TC ODAL <ref type="bibr" target="#b0">[1]</ref>, RMOT <ref type="bibr" target="#b28">[29]</ref>, NOMT-HM <ref type="bibr" target="#b4">[5]</ref>, and ODAMOT <ref type="bibr" target="#b10">[11]</ref>. Here, online methods produce the solution instantly at each frame by a causal approach.</p><p>Evaluation metrics: We adopt the widely used evaluation metrics, Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP) from <ref type="bibr" target="#b2">[3]</ref>. In addition, we also consider the number of mostly tracked (MT), the number of mostly lost (ML), the fragment (FG), the identity switch (ID), the Recall (Rec), and the Precision (Prec) from <ref type="bibr" target="#b18">[19]</ref>. The runtime is also considered as a metric in terms of Hz or sec. Motivated by the MOT Challenge evaluation, we also use the average ranking (AR) computed by averaging all metric rankings. Although the AR does not reflect the MOT performance directly, it can be used as a reference to compare overall MOT performance Benchmark dataset: The KITTI dataset provides two sets of detections, one from the DPM <ref type="bibr" target="#b9">[10]</ref> and the other from the regionlet <ref type="bibr" target="#b23">[24]</ref>. The regionlet detector generates more accurate detections than the DPM as illustrated on the KITTI website. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the AR indicates that the SCEA method performs fairly well compared to other stateof-the-art online trackers. The OMDAMOT method utilizes the additional local detector to deal with missing objects caused by partial occlusions, and the NOMT-HM additionally utilizes the optical flow information to reduce ambiguities caused by similar appearance of objects. Although our method utilizes the information only from detections and does not exploit those additional local detector or optical flow information, it shows comparable or better performance compared to the OMDAMOT and the NOMT-HM. The RMOT also uses the structural motion cues between objects to track missing objects robustly. However, the RMOT method does not perform well in the car sequences where large camera panning motion frequently occurs as explained in Section 5.1. Compared to the RMOT, the proposed SCEA algorithm shows much better performance in terms of MOTA, Prec, IDS, and Frag, which indicate the proposed data association method is more accurate than the RMN data association used in the RMOT.</p><p>For KITTI pedestrian sequences, the SCEA algorithm achieves better performance in MOTA and in Prec compared to the NOMT-HM, and it also shows better performance in IDS. This is because the optical flow information from pedestrians is less reliable compared to that in the car sequences due to the small size and non-rigid appearance of a pedestrian. In addition, the motion cue (the optical flow) becomes less discriminative when motion of objects is small. In the KITTI dataset, the motion of pedestrians is much smaller than that of cars. Since the SCEA method extracts structural motion information only from detections, its performance is less affected by the object size, appearance, and small motion. As shown in the results on the MOT Challenge dataset (pedestrian sequences, Table 2), the SCEA method performs well compared to other online methods overall. The TC ODAL utilizes the linear motion model to link the tracklets based on the Hungarian algorithm. For this reason, it shows limited performance under camera motion. The MDP shows better performance in MOTA, MT, ML, and FN compared to the SCEA. This is because the MDP learns the target state (Active, Tracked, Lost and Inactive) from a training dataset and its ground truth in an online manner. Therefore, it can initialize and terminate the objects more robustly than other methods. In addition, due to the use of the optical flow for local template tracking, it generates longer trajectories compared to other online methods. However, the SCEA algorithm has advantages over the MDP method in that it does not require any training datasets and it runs faster because it does not conduct template tracking based on dense optical flow. To show the performance dependency on the training dataset, we compare the SCEA with the MDP on the KITTI dataset. For pedestrian sequences, we run the MDP with original trained model provided with the original source code by the authors (MDP-MOTC). In addition, we also train the MDP with the KITTI training dataset for car sequences (MDP-KITTI). As shown in <ref type="table">Table 3</ref>, the performance of the MDP depends on the training dataset. Note that the performance of the MDP can be improved further if more training datasets are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In online 2D MOT with moving cameras, observable motion cues are complicated by global camera movements and thus not always smooth or predictable. In this paper, we propose a new data association method that effectively exploits structural motion constraints in the presence of large camera motion. In addition, to alleviate data association ambiguities caused by mis-detections and multiple detections, a novel event aggregation approach is developed to integrate structural constraints in assignment event costs for online MOT. Finally, the proposed data association and structural constraints are incorporated into the two-step online 2D MOT algorithm which simultaneously tracks objects and recovers missing objects. Experimental results on a large number of datasets demonstrate the effectiveness of the proposed algorithm for online 2D MOT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of structural constraint ambiguity: The tracked objects and their correct detections are represented by the red box and the yellow box, respectively. The overlap ratio costs of the ground truth assignment and the incorrect assignment are similar due to mis-detections and multiple false positive detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Structural constraint event aggregation (Algorithm 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Assignment event reduction concept: The gating and the partitioning reduces the number of assignment events. Gray circles represents the assignment region reduced by the gating. The objects are grouped based on the K-means center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Kalman filter KF (). ¬∑ Object management (Section 4) ¬∑ Structural constraint update (Section 4) end Algorithm 2: Two-Step Online MOT via SCEA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Recovery of missing objects. From the tracked objects (s 1 and s 2 ) and the structural constraints (the green lines), we recover missing objects when they are re-detected (detection d 1 and d 2 ). By doing this, we can continuously keep the identity of the missing objects under camera motion and occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Data association performance according to different levels of camera motion fluctuation and detection missing rates. MOTA = 1 ‚àí false negative+false positive+mis-match ground truth . The numbers ([0, 0],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of the SCEA (with the partitioning) and the SCEA without the partitioning (the SCEA-w/o-P).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Comparison to the online trackers on the KITTI dataset.</figDesc><table>(a) Car (based on the DPM detections) 

MOTA MOTP Rec Prec MT ML ID FG sec(core) AR 
NOMT-HM 60.2 78.7 63.8 96.9 27.0 30.3 28 250 0.09(16) 1.89 
RMOT 
51.5 75.2 57.2 92.9 15.2 33.5 51 382 0.01(1) 3.33 
ODAMOT 58.8 75.5 65.5 94.6 16.8 18.9 403 1298 1(1) 2.78 
SCEA 
56.3 78.8 58.1 98.9 20.0 29.3 17 468 0.05(1) 2.00 

(b) Car (based on the regionlet detections) 

MOTA MOTP Rec Prec MT ML ID FG sec(core) AR 
NOMT-HM 74.8 80.0 80.6 96.3 38.7 15.2 109 371 0.09(16) 1.78 
RMOT 
65.3 75.4 80.2 87.7 26.8 11.4 215 742 0.02(1) 2.56 
SCEA 
75.2 79.4 81.4 95.9 38.7 12.7 106 466 0.06(1) 1.56 

(c) Pedestrian (based on the DPM detections) 

MOTA MOTP Rec Prec MT ML ID FG sec(core) AR 
NOMT-HM 27.5 68.0 37.1 80.1 11.3 51.6 73 743 0.09(16) 2.67 
RMOT 
34.5 68.1 43.7 83.2 10.0 47.4 81 692 0.01(1) 1.56 
SCEA 
33.1 68.5 40.1 85.3 8.6 47.4 16 724 0.05(1) 1.67 

(d) Pedestrian (based on the regionlet detections) 

MOTA MOTP Rec Prec MT ML ID FG sec(core) AR 
NOMT-HM 39.3 71.1 50.4 83.3 17.2 42.3 186 870 0.09(16) 2.44 
RMOT 
43.7 71.0 53.5 85.8 16.8 41.2 156 760 0.02(1) 1.78 
SCEA 
43.9 71.9 49.3 90.7 14.1 43.3 56 649 0.06(1) 1.78 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Comparison to the online trackers on the MOT Challenge dataset (pedestrian sequences). FAF: the average number of false alarms per frame. FP: the number of false positives. FN: the number of false negatives. (The results of the NOMT-HM are from the original paper [5].)Table 3. Comparison to the MDP on the KITTI training dataset.</figDesc><table>MOTA 
MOTP 
FAF 
MT 
ML 
FP 
FN 
ID 
FG 
Hz(core) 
AR 
TC ODAL 
15.1 
70.5 
2.2 
3.2 
55.8 
12,970 
38,538 
637 
1,716 
1.7 (1) 
4.30 
RMOT 
18.6 
69.6 
2.2 
5.3 
53.3 
12,473 
36,835 
684 
1,282 
7.9 (1) 
3.70 
NOMT-HM 
26.7 
71.5 
2.0 
11.2 
47.9 
11,162 
33,187 
637 
1,716 
11.5 (16) 
2.50 
MDP 
30.3 
71.3 
1.7 
13.0 
38.4 
9,717 
32,422 
680 
1,500 
1.1 (8) 
2.30 
SCEA 
29.1 
71.1 
1.1 
8.9 
47.3 
6,060 
36,912 
604 
1,182 
6.8 (1) 
2.10 

(a) Car (except for the training sequences) 

MOTA MOTP Rec Prec MT ML ID FG 
MDP-KITTI 55.0 75.1 60.8 92.3 10.7 40.9 19 118 
SCEA 
58.8 78.6 61.3 96.5 11.6 32.9 6 100 

(b) Pedestrian (except for the training sequences) 

MOTA MOTP Rec Prec MT ML ID FG 
MDP-KITTI 23.8 71.2 49.1 66.4 3.5 36.0 8 204 
MDP-MOTC 25.1 71.2 47.8 68.6 3.5 34.9 32 209 
SCEA 
35.4 73.2 51.5 76.3 7.0 32.6 3 154 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = [x i t , y i t ,·∫ã i t ,·∫è i t , w i t , h i t ]‚ä§ and the set of the states at frame t as S t (s i t ‚àà S t ) with its index set i ‚àà N t . Each structural motion constraint is described by the location and velocity difference between two objects ase i,j t = [œá i,j t , œÖ i,j t ,œá i,j t ,œÖ i,j t ] ‚ä§ = [x i t ‚àí x j t , y i t ‚àí y j t ,·∫ã i t ‚àí·∫ã j t ,·∫è i t ‚àí·∫è j t ] ‚ä§ .(1)Here, (œá i,j t ,œÖ i,j t ) denotes the velocity difference to consider objects moving with different tendency. The set of structural constraints for the object i is represented by E i t = {e i,j t |‚àÄj ‚àà N t }, and the set of all structural constraints at frame t is denoted by E t = {E i t |‚àÄi ‚àà N t }.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">cvlibs.net/datasets/kitti/eval_tracking.php 2 motchallenge.net/data/2D_MOT_2015/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was supported by the Na- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bar-Shalom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Multitarget-multisensor Tracking: Principles and Techniques. YBS publishing</title>
		<meeting><address><addrLine>Storrs, CT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurasip Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Online multiperson tracking-by-detection from a single</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>uncalibrated camera. PAMI, 2011. 1, 2, 6</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Group tracking: exploring mutual relations for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online Domain Adaptation for Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tracking the invisible: Learning where the object might be</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online multitarget tracking by large margin structured learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feyereisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">Motchallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Everybody needs somebody: modeling social and grouping behavior on a linear programming multiple people tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to associate: Hybridboosted multi-target tracker for crowded scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Continuous energy minimization for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Globallyoptimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint probabilistic data association revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An introduction to the kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-object tracking through occlusions by local tracklets filtering and global tracklets association with detection responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An online learned CRF model for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian multi-object tracking using motion context from multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure preserving object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
