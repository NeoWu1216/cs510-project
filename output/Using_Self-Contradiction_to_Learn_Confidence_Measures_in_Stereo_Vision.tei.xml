<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Self-Contradiction to Learn Confidence Measures in Stereo Vision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Mostegel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Rumpler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Using Self-Contradiction to Learn Confidence Measures in Stereo Vision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learned confidence measures gain increasing importance for outlier removal and quality improvement in stereo vision. However, acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction, active sensing devices and/or synthetic scenes. To overcome this problem, we propose a new, flexible, and scalable way for generating training data that only requires a set of stereo images as input. The key idea of our approach is to use different view points for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm. This enables us to generate a huge amount of training data in a fully automated manner. Among other experiments, we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the KITTI2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data. * The research leading to these results has received funding from the EC FP7 project 3D-PITOTI (ICT-2011-600545) and from the Austrian Research Promotion Agency (FFG) as Vision+ project 836630 and together with OMICRON electronics GmbH as Bridge1 project 843450.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many works have demonstrated that machine learning can be greatly beneficial for stereo vision <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10]</ref>. All these works have one thing in common: They require training data -the more the better.</p><p>Previous approaches used three main sources of training data. The first source is manual labeling. While this is the traditional approach in the fields of classification and segmentation (e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29]</ref>), it requires hundreds of manhours even in 2D. Because the task becomes even more taxing in 3D, only very few manually labeled datasets exist in this domain (e.g. <ref type="bibr" target="#b17">[18]</ref>). The second source is synthetic data generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>. Unfortunately, pure synthetic data generation has not yet reached the level where it generalizes to natural images without an extreme modeling ef- <ref type="figure">Figure 1</ref>. Our approach automatically detects and classifies contradictions and consistencies between multiple depth maps to generate labeled training images, which can be used for training confidence measures. From top to bottom: RGB input images, depth maps created with a query stereo algorithm (here <ref type="bibr" target="#b25">[26]</ref>), label images based on laser ground truth <ref type="bibr" target="#b6">[7]</ref> and our automatically generated label images. In the label images, green stands for positive samples, red for negative and blue is ignored during training.</p><p>fort. The third source is to record ground truth data with active depth sensors, which is currently the most popular source <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. If a projector based setup is used <ref type="bibr" target="#b26">[27]</ref>, the ground truth can achieve a very high accuracy, but the data acquisition takes a lot of time and is restricted to indoor scenes. For outdoor scenes the method of choice is typically the use of a laser scanner <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>. Aside from requiring a non-trivial registration between the laser reconstruction and the recorded images, this method is also subject to a range of assumptions itself. This fact makes a manual removal of obviously incorrect ground truth data necessary for outdoor datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>. Some approaches, like <ref type="bibr" target="#b20">[21]</ref>, combine these three sources. They combine active sensing with synthetic car models and manual annotation to increase the quality of ground truth data.</p><p>None of these methods is easily portable to very specific application areas such as under water reconstruction or 3D reconstruction with micro aerial vehicles. Furthermore, none of these methods shows good scaling properties in the sense of required man-hours per training data.</p><p>This motivated us to propose a novel way of generating training data without a synthetic model, active devices or manual interaction. Instead of explicitly generating a ground truth, we compare multiple depth maps of the same scene obtained with the same stereo approach with each other and thus collect positive and negative training data. As input we require a set of stereo images observing a static scene from multiple viewing angles. After computing the relative poses and generating depth maps, we evaluate which parts of the depth maps can likely be trusted, which parts contradict each other and for which parts we simply do not have enough information available to make this decision. This results in a set of partially labeled images, similar to ground truth, which can be used for training (see <ref type="figure">Fig. 1</ref>).</p><p>For the evaluation of our method we use three publicly available datasets, which are namely the multi-view stereo dataset of Strecha et al. <ref type="bibr" target="#b31">[32]</ref>, the Middlebury2014 dataset <ref type="bibr" target="#b26">[27]</ref> and the KITTI2012 stereo dataset <ref type="bibr" target="#b6">[7]</ref>. On these datasets we demonstrate that the performance of learned confidence measures can be boosted by simply training them on large amounts of domain specific training data, which our approach can cheaply provide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>To the best of our knowledge, we are the first to approach the topic of stereo training data generation in a selfsupervised manner. In order to demonstrate the usefulness of our groundtruth generation, we show that existing stereo vision approaches, which already have been evaluated on one or more of the afore mentioned stereo datasets, can also be successfully trained on our generated data.</p><p>Thus, we first give a short overview of the most relevant learning based stereo approaches. While most approaches pose the problem of learning reconstruction errors as a binary classification problem (correct matches/depth values versus incorrect matches/depth values), Kong and Tao <ref type="bibr" target="#b16">[17]</ref> propose to use an additional class for failures due to foreground fattening. Using these predicted class probabilities they adjust the initial matching cost. Peris et al. <ref type="bibr" target="#b23">[24]</ref> train a multi-class Linear Discriminant Analysis (LDA) classifier to compute disparities together with a confidence map. Zbontar and LeCun <ref type="bibr" target="#b35">[36]</ref> improve the matching cost computation by learning a similarity measure between small image patches using a convolutional neural network.</p><p>The approaches mentioned above are very specific in their formulation, but many other works use a so called "confidence measure" as a basis for improving the stereo output. A confidence measure should predict the likelihood of a depth value being correct and is typically computed using image intensities, disparity values and/or matching costs. Some surveys about confidence measures are available in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In the simplest way a confidence measure can be used to remove very likely wrong measurements from the depth map. This process is called sparsification. The most common way for sparsification without training is the left-right consistency check <ref type="bibr" target="#b15">[16]</ref>. While this check already detects many outliers, it cannot detect errors caused by a systematic problem of an approach (e.g. foreground fattening). Haeusler et al. <ref type="bibr" target="#b9">[10]</ref> showed that ensemble learning of many different features with random decision forests can significantly improve the sparsification performance. Note that confidence measures are also learned in similar fashion in the domain of optical flow, e.g. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6]</ref>. Spyropoulos et al. <ref type="bibr" target="#b30">[31]</ref> used the confidence prediction as a soft-constraint in a Markov random field to improve the stereo output. In the very recent work of Park and Yoon <ref type="bibr" target="#b22">[23]</ref> the confidence prediction is used to modulate the matching cost of a semi-global matcher <ref type="bibr" target="#b14">[15]</ref> and thus increase its performance. As the performance of the above mentioned approaches depends on how well the confidence of a measurement can be predicted, the area under the sparsification curve is one of the most important evaluation criteria in this domain <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>. Hence, we found that this criterion is ideally suited to benchmark the quality of our training data generation, aside from comparing it directly to the ground truth. In our experiments, we use three recent approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b22">23]</ref> that compute confidence measures and analyze the change of performance depending on the used training data (laser ground truth vs. automatically generated training data) on the KITTI2012 dataset <ref type="bibr" target="#b6">[7]</ref>.</p><p>Aside from stereo vision, there exist some works that deal with learning the matchability of features. Some of these works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b10">11]</ref> use ground truth data collected by <ref type="bibr" target="#b0">[1]</ref>. To generate the ground truth data they use the dense multi-view stereo reconstruction algorithm provided by Goesele et al. <ref type="bibr" target="#b8">[9]</ref> and trust this approach to be accurate enough. The problem with applying this approach to dense stereo is that a learning algorithm will try to tune its output to reproduce any systematic error made by <ref type="bibr" target="#b8">[9]</ref>. Philbin et al. <ref type="bibr" target="#b24">[25]</ref> use SIFT <ref type="bibr" target="#b18">[19]</ref> nearest-neighbors together with a RANSAC verification to generate negative and positive training data, whereas Simonyan et al. <ref type="bibr" target="#b29">[30]</ref> first compute a homography between images using SIFT and RANSAC and then establish region correspondences using the homography. Hartmann et al. <ref type="bibr" target="#b13">[14]</ref> learn the matchability of SIFT features by collecting features that survive the matching stage and those which are rejected as positive and negative training data. All of these approaches focus on a specific type of sparse feature and do not generalize well to dense stereo. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fully Automatic Training Data Generation</head><p>The core idea of our training data generation approach is to relax the aim from labeling each pixel as correct/incorrect to labeling them as selfconsistent/contradicting. We use the word self-consistent in the sense that depth maps generated with the same algorithm from different view points shall not contradict each other through free space violation or occlusion. Note that the use of different view points is important as the errors in depth maps are in general strongly correlated, if they are generated from the same view point with the same stereo algorithm. In this work we use the observation that this correlation is small when the relative observation angle between two depth maps is large to reduce the influence of systematic errors. The basic steps of our approach are visualized in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>As input our approach requires a set of stereo images with known poses. First, we execute the query algorithm, which yields a set of depth maps (Setup). Then we assess which parts of a depth map are supported by other depth maps with a significantly different observation angle (Stage 1). In the next stage (Stage 2), we then use this support to influence the voting process. In the final stage (Stage 3), we detect outliers which were missed in the previous stage using an augmented depth map. In the remainder of this section, we describe all involved steps in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stage 1: Support Assessment</head><p>Given many depth maps of the same scene, we want to separate parts of the scene where many depth maps agree on the structure (consistent parts) from those where they either disagree or we simply do not have enough view points to rule out systematic errors.</p><p>In performing this separation, we have to account for two problems. The first problem is that if the camera poses of two stereo pairs are too similar, the depth maps will very likely contain the same systematic error. To remedy this situation, we try to decrease the error correlation in using different observation angles. The second problem originates from the finite precision of cameras, which introduces a query camera reference camera depth uncertainty. This means that a measurement with a high uncertainty is not well suited for determining whether a measurement with a lower uncertainty is correct or not.</p><p>To estimate this uncertainty we use the model proposed by <ref type="bibr" target="#b11">[12]</ref>. This model allows us to compute a covariance matrix for each 3D point corresponding to a depth value through first-order backward covariance propagation under the assumption of isotropic Gaussian image noise, which is explained in more detail in <ref type="bibr" target="#b12">[13]</ref>.</p><p>As we aim to produce 2D label images, we address this problem on a per-pixel basis. So for each pixel of a depth map, we first collect the support of other depth maps. A reference depth map is only allowed to express its support for the 3D point p query associated with a query pixel if it fulfills the following two criteria.</p><p>First, the viewpoints shall be sufficiently different. We define that a viewpoint is different enough if the observation angle difference between two stereo pairs is sufficiently large ( α diff &gt; α min ). We compute this observation angle as</p><formula xml:id="formula_0">α diff = ∡( − −−−− → p query c ref , − −−−−−− → p query c query ),</formula><p>where c x is the mean camera center of a stereo pair.</p><p>Second, the reference measurement shall be within a fixed theoretical tolerance σ max of the query measurement. For this evaluation we use the Mahalanobis distance based on the covariance matrix with the smaller uncertainty (either reference or query).</p><p>In order to avoid being too much biased by a single observation direction, a 3D point fulfilling these criteria is not directly allowed to vote, but instead can activate its corresponding bin depending on the observation angle. We use angular bins of α min degree, where each activated bin increases the support for the query point by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stage 2: Consistency Voting</head><p>The basic idea of this stage is to let all depth maps vote for the (in)consistency of a query depth map. Similar to works in depth map fusion (e.g. <ref type="bibr" target="#b21">[22]</ref>), negative votes are cast by free space violations and occlusions and positive votes are cast by measurements which are sufficiently close to each other (see <ref type="figure" target="#fig_1">Fig. 3</ref>). Opposed to fusion approaches, we aim for a completely different output. While works in depth map fusion try to improve/fuse the depth map, we only aim to decide which parts of the depth map cause contradictions and which parts are sufficiently consistent. Furthermore, we have to reduce the influence of systematic errors in the voting scheme, which we achieve with the support of a reference measurement computed in the previous stage. In particular this means that only parts which have a support from at least one significantly different observation angle are eligible for voting.</p><p>The proposed voting scheme looks as follows. For casting a positive vote v + a reference measurement has to fulfill two properties. First, it shall be more accurate than the query measurement. We evaluate this property with the largest Eigen value of the corresponding covariance matrix. Second, the reference measurement has to be within a fixed theoretical tolerance of σ max of the query measurement. For this evaluation we use the Mahalanobis distance based on the covariance matrix of the query 3D point. We define a positive vote as:</p><formula xml:id="formula_1">v + = i ref · support ref (1)</formula><p>where i ref is the smallest Eigen value of the Fisher information matrix of the reference 3D point. This means that measurements with a low theoretic uncertainty get a higher voting strength, as</p><formula xml:id="formula_2">√ i ref = 1/ √ u ref , where u ref is the largest</formula><p>Eigen value of the covariance matrix and hence √ u ref can</p><p>be interpreted as the standard deviation along the axis of the highest uncertainty. For casting a negative vote a reference measurement has to fulfill three properties. First, it also has to be more accurate than the query measurement. Second, it has to be outside the fixed theoretical tolerance of σ max . Third, it has to cause a free space violation or occlusion as depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>. In a free space violation, a reference measurement would block the line of sight of a query measurement (left side in <ref type="figure" target="#fig_1">Fig. 3</ref>), whereas the other way around would cause an occlusion (right side in <ref type="figure" target="#fig_1">Fig. 3</ref>). If these properties are met, a negative vote is cast:</p><formula xml:id="formula_3">v − = − i ref · support ref<label>(2)</label></formula><p>For each pixel in the query depth map the votes are collected. The label of a pixel with more than zero votes is then set depending on the sign of the final sum of votes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stage 3: Outlier Detection</head><p>In the previous step, we only allowed measurements with a minimal support from a different observation angle to vote and only then if they are more accurate than the query measurement. This restriction is necessary because otherwise the training data would contain a great percentage of incorrectly labeled samples, i.e. false positives (consistent but incorrect) and false negatives (inconsistent but correct). However, this also causes many regions to be missed in which absolutely no consensus can be reached, because they only contain outliers (e.g. top left corner in <ref type="figure" target="#fig_5">Fig. 5</ref>). In this stage, we aim to detect these outliers for enhancing our negative training data.</p><p>First, we label trivial outliers which either lie behind the camera or do not project into the second stereo camera. For the remaining unlabeled regions we compare the depth values of the query camera to a specially augmented depth map. Our procedure for obtaining this augmented depth map is inspired by the stability-based depth map fusion proposed by Merrell et al. <ref type="bibr" target="#b21">[22]</ref>. The main difference is that we do not aim for high performance or even the perfect depth map, but a depth map which rather prefers lower depth values which are sufficiently plausible. We found that underestimating the depth values helps us to keep the number of false negatives (inconsistent but correct) low, while at the same time allowing us to recover many true negatives (inconsistent and incorrect). Further, we avoid any smoothness assumptions to preserve fine objects.</p><p>For computing the augmented depth map, we collect all depth values of the other depth maps that would project into a pixel of the query image. Then we sort these depth values and search for the closest depth value which obtains a positive score in a voting scheme. This voting scheme is very similar to the one proposed in the previous stage, but many more depth values will end up with a positive score although they are incorrect.</p><p>There are 4 differences to the other voting scheme: (1) Every depth map can vote (without accuracy restrictions), (2) the border between consistent and contradicting vote is set to (1/</p><formula xml:id="formula_4">√ u query + 1/ √ u ref ) · σ max , (3) support ref = 1 for</formula><p>all measurements and (4) a depth value has to obtain at least three votes to be considered valid. If no such depth value is found, the original depth is kept. Using the augmented depth map, we now treat a depth value as a negative sample if the following two criteria are met. First, the query depth value has to be smaller than the depth value of the augmented depth map. Second, the difference between those two depth values has to be larger than σ max · 1/ √ u augmented , where u augmented stands for the largest Eigen value of the covariance matrix of the augmented measurement if we pretend that it is only visible from the query stereo pair. The final training data is then a combination of the negative samples from this stage with the positive and negative training samples from the previous stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In our experiments, we use three publicly available datasets, which are namely the KITTI2012 dataset <ref type="bibr" target="#b6">[7]</ref>, the Middlebury2014 dataset <ref type="bibr" target="#b26">[27]</ref>, and the Strecha fountain dataset <ref type="bibr" target="#b31">[32]</ref>.</p><p>The main focus of our experiments is on the KITTI2012 dataset <ref type="bibr" target="#b6">[7]</ref> because it is well-suited to demonstrate our ap-proach and has already been used before for the evaluation of confidence prediction algorithms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>. The KITTI2012 dataset does not only let us evaluate the coverage and accuracy of our approach, but also lets us highlight the usefulness of our approach in boosting the performance of confidence prediction approaches by simply training them on the automatically generated training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">General Setup</head><p>For all experiments we used the same set of parameters. The parameter α min (= 10 • ) can be used to adjust the tradeoff between coverage and label error. As a general rule, we can say that if one increases this parameter, the false positive rate becomes lower, but at the same time the label coverage decreases as well. The parameter σ max (= 2) can be used to express desired accuracy of a query algorithm as a multiple of the σ bound.</p><p>As query algorithms, we use two different stereo algorithms. The first algorithm is a Semi-Global Matching (SGM) <ref type="bibr" target="#b14">[15]</ref> implementation by Rothermel et al. <ref type="bibr" target="#b25">[26]</ref> which uses the census transform for computing the matching cost. As a second algorithm we chose the recently proposed Slanted Plane Smoothing (SPS) approach of Yamaguchi et al. <ref type="bibr" target="#b34">[35]</ref>. We chose this approach because it shows a very good performance on the KITTI datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>, and gives a completely different output than a SGM (piece-wise planar super pixels vs. unrestricted transitions).</p><p>For analyzing the benefit of our approach for learning, we have chosen three different recent approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b22">23]</ref> which are based on confidence prediction. All three approaches use random forests for the confidence prediction, which made it possible to reimplement them in a common framework. The difference between the approaches lies in which hand-crafted features they feed to the random forest. Ensemble learning <ref type="bibr" target="#b9">[10]</ref> uses the peak ratio, entropy of disparities, perturbation, left-right disparity difference, horizontal gradient, disparity map variance, disparity ambiguity, zero mean sum of absolute differences and the local SGM energy, which results through consideration of multiple scales in a feature vector of 23 dimensions. Ground Control Point (GCP) learning <ref type="bibr" target="#b30">[31]</ref> uses eight features, which are the matching cost, distance to border, maximum margin, attainable maximum likelihood, left-right consistency, left-right difference, distance to discontinuity and difference with median disparity. Park et al. <ref type="bibr" target="#b22">[23]</ref> use a feature vector with 22 dimensions, which contains the peak ratio, naive peak ratio, matching score, maximum margin, winner margin, maximum likelihood, perturbation, negative entropy, left-right difference, local curvatures, local variance of disparity values, distance to discontinuity, median deviations of disparities, left-right consistency, magnitude of image gradient and the distance to border.</p><p>For the implementation we used the publicly available   <ref type="bibr" target="#b25">[26]</ref> and SPS <ref type="bibr" target="#b34">[35]</ref>), confidence prediction algorithm (Ensemble <ref type="bibr" target="#b9">[10]</ref>, GCP <ref type="bibr" target="#b30">[31]</ref>, Park <ref type="bibr" target="#b22">[23]</ref>) and training data (Laser and Ours). As a baseline method we also show the Left-Right disparity Difference (LRD). random forest framework of Schulter at al. <ref type="bibr" target="#b27">[28]</ref>. For training the forest we used the same settings in all our experiments. We used 20 trees with a maximum depth of 20 and a minimum leaf size of 100. For choosing a split function we use the standard entropy and draw 2000 random samples per node and 500 random thresholds per feature channel. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">KITTI Dataset</head><p>We use the KITTI2012 dataset <ref type="bibr" target="#b6">[7]</ref> to evaluate three properties of our ground truth generation, which are namely accuracy, coverage and training performance. The first two, we obtain by comparing our automatically generated label images to label images produced with the laser ground truth provided for the training dataset. For the SGM <ref type="bibr" target="#b25">[26]</ref> data we reach an accuracy of 97.3% (STD: 1.4%) at an average coverage of the laser ground truth of 47.8% (STD: 11.8%). For the SPS <ref type="bibr" target="#b34">[35]</ref> data we obtain an accuracy of 95.3% (STD: 5.7%) at an average coverage of 48.6% (STD: 13.4%). Note that the coverage mostly depends on the camera motion. In the first column we show the depth maps of SGM <ref type="bibr" target="#b25">[26]</ref> and SPS <ref type="bibr" target="#b34">[35]</ref> together with the RGB input image. The second column shows the resulting label images once produced with the laser ground truth (Laser) and once with our approach (Ours). Note that our approach only assigns a positive label to parts of the scene that are observed under significantly different view points (the car is making a turn to the left in the sequence). The remaining 3 columns show the confidence prediction output of Ensemble <ref type="bibr" target="#b9">[10]</ref>, GCP <ref type="bibr" target="#b30">[31]</ref> and Park <ref type="bibr" target="#b22">[23]</ref> once trained on Laser and once on Ours. The confidence ranges from low (black) to high (white). Note the confidence prediction is much smoother for Ours and contains less artifacts (especially for GCP).</p><p>The ideal case to demonstrate our approach would be a circular motion around an object, whereas no motion will result in no labeled images. As the KITTI dataset contains some sequences with very little motion, this results in a high standard deviation of the coverage.</p><p>While accuracy and coverage are relevant, the much more interesting factor is how well the data is suited for training an algorithm. To analyze this factor, we benchmark the change of the confidence prediction performance of three recent confidence prediction approaches, which we further refer to as Ensemble <ref type="bibr" target="#b9">[10]</ref>, GCP <ref type="bibr" target="#b30">[31]</ref> and Park <ref type="bibr" target="#b22">[23]</ref>. For benchmarking this performance we evaluate the Area Under the Sparsification Curve (AUSC) as in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>. A sparsification curve plots the bad pixel rate over the sparsification factor. For drawing the curve the pixels are sorted by confidence values and always the lowest values are removed. The AUSC is a very good indicator for the prediction performance of a confidence measure. Sparsification curves for frame 102 of the dataset are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, while further sparsification curves can be found in the supplementary material.</p><p>For training on the laser ground truth, we follow the evaluation protocol of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>. This means that we select the frames 43, 71, 82, 87, 94, 120, 122 and 180 of the KITTI training dataset for training. The labels correct/incorrect are set by comparing the query depth maps with the laser ground truth using the standard three pixel disparity threshold. Further on, we will mark a confidence measure trained on this data with the suffix "Laser". As our approach requires multiple images that view the same scene, we use the 195 sequences of 21 stereo pairs of the KITTI testing dataset for automatically generating our label images. Further on, we will mark a confidence measure trained on this data with the suffix "Ours". Example label images can be found in <ref type="figure" target="#fig_5">Fig. 5</ref> and the supplementary material. For testing we once again follow the protocol of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> and evaluate the confidence prediction on the KITTI training dataset mi-nus the eight sequences that were used for training on the laser ground truth. Thus, there is no overlap between training and testing for Laser as well as Ours. Also note that Ours has not seen a single ground truth laser scan. In training, we used all available training samples from the laser ground truth and roughly ten times this number from our automatically generated data. Note that this is less than one percent of all available training data. With this setup our implementation used ∼20GB of memory for training.</p><p>In <ref type="figure">Fig. 6</ref> we show the mean, minimum and maximum AUSC values of the three confidence prediction algorithms for all combinations of query algorithm and training data. In Tab. 1 we show the AUSC for each approach divided by the optimal AUSC over all evaluated sequences of the KITTI dataset. In all cases, using our training data resulted in a performance boost. In some cases the AUSC even dropped by 10%. A visual comparison of the difference in the confidence prediction can be found in <ref type="figure" target="#fig_5">Fig. 5</ref> and the supplementary material. Note that our training data leads to a smoother confidence prediction with significantly fewer artifacts.</p><p>As a matter of completeness, we executed our training data generation only on the eight same sequences that were used for training Laser. One has to note that the coverage of our approach depends on the camera motion and one of the sequences (180) contains no useful motion, which leaves our approach with 7 sequences. Using only this limited amount of training data, the AUSC increased by ∼10% for all approaches compared to using the 195 testing sequences. This is not surprising, as each of our training images can be considered as weaker compared to the laser ground truth, in the sense that consistency alone cannot uncover all errors and that the coverage of our labeling depends on the camera motion. But this experiment clearly shows that using ten times more "weak" training samples, which can be cheaply generated with our method, still leads to a better performance than fewer "strong" training samples.  <ref type="figure">Figure 6</ref>. Mean, minimum and maximum AUSC values over the three confidence prediction algorithms (Ensemble <ref type="bibr" target="#b9">[10]</ref>, GCP <ref type="bibr" target="#b30">[31]</ref>, Park <ref type="bibr" target="#b22">[23]</ref>) for all frames of the KITTI training dataset minus the eight frames used for training. We display all combinations of query algorithm (SGM <ref type="bibr" target="#b25">[26]</ref> and SPS <ref type="bibr" target="#b34">[35]</ref>) and training data (Laser and Ours). The frames were sorted according to mean AUSC value of Ours. As a baseline method we also show the Left-Right disparity Difference (LRD). Note that Ours (red) is lower than Laser (blue) in most cases. For SGM, all approaches perform always better than LRD if they are trained on Ours, while if they are trained on Laser they sometimes perform worse (e.g. 142). For SPS stereo, the number of severe errors is significantly higher for Laser than for Ours (compare blue versus red peaks above 160). LRD Ens. <ref type="bibr">[</ref>  <ref type="bibr" target="#b25">[26]</ref> and SPS <ref type="bibr" target="#b34">[35]</ref>), confidence prediction algorithm (Ensemble <ref type="bibr" target="#b9">[10]</ref>, GCP <ref type="bibr" target="#b30">[31]</ref>, Park <ref type="bibr" target="#b22">[23]</ref>) and training data (Laser and Ours). The reduction is computed as 1 − AU SCOurs/AU SCLaser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Middlebury Dataset</head><p>The Middlebury2014 <ref type="bibr" target="#b26">[27]</ref> dataset contains a set of 23 high resolution stereo pairs for which known camera calibration parameters and ground truth disparity maps obtained with a structured light scanner are available. The set is divided into 10 stereo pairs for training and additional 13 stereo pairs that we used for testing. The images in the Middlebury dataset all show static indoor scenes with varying difficulties including repetitive structures, occlusions, wiry We display all combinations of query algorithm (SGM <ref type="bibr" target="#b25">[26]</ref> and SPS <ref type="bibr" target="#b34">[35]</ref>), confidence prediction algorithm (Ensemble <ref type="bibr" target="#b9">[10]</ref>, GCP <ref type="bibr" target="#b30">[31]</ref>, Park <ref type="bibr" target="#b22">[23]</ref>) and training data (Kitti <ref type="bibr" target="#b6">[7]</ref> and Middle <ref type="bibr" target="#b26">[27]</ref>). As a baseline method we also show the Left-Right disparity Difference (LRD). Note that the red symbols (Middle) are in many cases drastically lower than their blue counter parts (Kitti  <ref type="table">Table 2</ref>. Area under the sparsification curve divided by optimal area on the Middlebury dataset. We display all combinations of query algorithm (SGM <ref type="bibr" target="#b25">[26]</ref> and SPS <ref type="bibr" target="#b34">[35]</ref>), confidence prediction algorithm (Ensemble <ref type="bibr" target="#b9">[10]</ref>, GCP <ref type="bibr" target="#b30">[31]</ref>, Park <ref type="bibr" target="#b22">[23]</ref>) and training data (Kitti <ref type="bibr" target="#b6">[7]</ref> and Middle <ref type="bibr" target="#b26">[27]</ref>). The reduction is computed as 1 − AU SC M iddle /AU SCKitti.</p><p>objects as well as untextured areas.</p><p>Due to the limitation that only stereo pairs and no multiview sequences are provided, we are not able to evaluate the accuracy performance of our ground truth generation. But we can still evaluate the performance of the confidence measures previously learned on the KITTI to evaluate their generalization performance from outdoor to indoor scenes. <ref type="figure" target="#fig_7">Figure 7</ref> shows the resulting AUSC curve for SGM <ref type="bibr" target="#b25">[26]</ref> and SPS <ref type="bibr" target="#b34">[35]</ref>, respectively. In Tab. 2 we show the AUSC over the optimal values.</p><p>For all combinations of query algorithm and confidence prediction approach, training on the Middlebury increased the performance compared to training on the KITTI and evaluating on the Middlebury. The percentage of area reduction strongly depends on the used confidence prediction approach. We assume that the large variation in area reduction (3%-30%) is caused by features which are very setup specific (e.g. distance to border). Despite the large reduction variation, all approaches benefit from training on the  <ref type="figure">Figure 8</ref>. Sparsification curves for testing stereo pair on the Strecha fountain dataset. We display all combinations of confidence prediction algorithm (Ensemble <ref type="bibr" target="#b9">[10]</ref>, GCP <ref type="bibr" target="#b30">[31]</ref>, Park <ref type="bibr" target="#b22">[23]</ref>) and training data (Kitti <ref type="bibr" target="#b6">[7]</ref>, Middle <ref type="bibr" target="#b26">[27]</ref> and Ours) for the SGM output <ref type="bibr" target="#b25">[26]</ref>. As a baseline method we also show the Left-Right disparity Difference (LRD).</p><p>Middlebury rather than the KITTI. This means that tuning towards a special setup can make a large difference in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Strecha Dataset</head><p>To further demonstrate the value of our approach, we analyze the sparsification performance in a completely different setup. For this experiment we used the multi-view stereo dataset of Strecha et al. <ref type="bibr" target="#b31">[32]</ref>. This dataset provides images together with camera poses and two ground truth meshes. From the two available meshes, the Herz-Jesu mesh is a good example that also active sensors have their limitations. In this mesh all the thin structures (hand rails and bars) are simply missing. As these errors would cause problems in the evaluation, we only used the second dataset (Fountain), which does not contain any thin structures. This dataset consists of 11 images aligned to the ground truth mesh. For this experiment we split the images into a training set containing 3 image pairs and a test set with 2 image pairs. The training pairs are made of images 0+1, 4+5 and 8+9 and the testing pairs of 2+3 and 6+7. Each pair was then rectified using <ref type="bibr" target="#b25">[26]</ref>. As the SPS implementation <ref type="bibr" target="#b34">[35]</ref> failed to produce any reasonable output on this kind of data, we limit this experiment to the SGM <ref type="bibr" target="#b25">[26]</ref> reconstruction.</p><p>In this setup our ground truth generation reached an accuracy of 95.1% (STD: 2.6%) at a coverage <ref type="bibr" target="#b29">30</ref>  <ref type="table">Table 3</ref>. Area under the sparsification curve divided by optimal area (Relative Area RA) on the Strecha fountain dataset. We display all combinations of confidence prediction algorithm (Ensemble <ref type="bibr" target="#b9">[10]</ref>, GCP <ref type="bibr" target="#b30">[31]</ref>, Park <ref type="bibr" target="#b22">[23]</ref>) and training data (Kitti <ref type="bibr" target="#b6">[7]</ref>, Middle <ref type="bibr" target="#b26">[27]</ref> and Ours) for the SGM output <ref type="bibr" target="#b25">[26]</ref>. The reduction is computed as 1 − AU SCx/AU SCOurs for each confidence prediction approach.</p><p>(STD: 5.0%). In <ref type="figure">Fig. 8</ref> we show the resulting two sparsification curves and the AUSC reduction statistics in Tab. 3. All combinations of query algorithms and confidence prediction approaches performed better trained on the Middlebury than on the KITTI. In all cases the performance was further increased by tuning them specifically to this scene in using our automatically generated training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we present a novel way to train confidence prediction approaches for stereo vision in a cheap and scalable manner. We collect positive and negative training data by analyzing the consistency between depthmaps that observe the same physical scene. Consistency is a necessary but not sufficient criterion for correctness. On the one hand, this means that consistency is perfectly suited for unveiling incorrect depth values and thus to collect negative training data. On the other hand, it can never be guaranteed that all incorrect depth values are detected through consistency alone, as they can be consistent and incorrect at the same time. To keep the number of incorrect samples in the positive training data low, we only consider parts of the scene which have been viewed from significantly different observation angles for the generation of positive training data. In our experiments, we demonstrate that the resulting training data can be a great benefit for learning-based confidence prediction. On the KITTI2012 dataset, the amount and diversity of our training data allowed us to improve the average confidence prediction performance of three different approaches by 1 to 10% without changing the algorithms themselves. Further, we demonstrated that all three confidence prediction approaches can significantly benefit from learning application specific properties. With our approach, these specific properties can be learned at low cost; even for applications, such as aerial or under water robotics, that typically lack ground truth data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Fully Automatic Training Data Generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Consistency Voting. There are three possibilities for voting. A positive vote (center) is only cast if the reference measurement is within the uncertainty boundary of the query measurement. A negative vote is either cast if a reference measurement would block the line of sight of the query camera (left) or the other way around (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Sparsification curves for sequence 102 of the KITTI training dataset. We display all combinations of query algorithm (SGM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>For every training setup we balanced the dataset on image basis. This means that every image contributed as many positive training examples as negative examples. For the final evaluation, we always considered the complete image.For obtaining the pose estimation on the KITTI2012 dataset we use<ref type="bibr" target="#b7">[8]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results for sequence 102 of the KITTI training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Area under the Sparsification Curve (AUSC) values for all 13 frames of the additional Middlebury dataset. The frames were sorted according to the optimal area under the curve value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>).</figDesc><table>LRD Ens.[10] Park[23] GCP[31] 
SGM-Kitti 
2.10 
1.24 
1.25 
1.78 
SGM-Middle 2.10 
1.19 
1.20 
1.50 
Reduction 
-
3.29% 
3.30% 
15.86% 
SPS-Kitti 
1.41 
1.48 
1.81 
2.05 
SPS-Middle 
1.41 
1.39 
1.42 
1.44 
Reduction 
-
6.32% 
21.63% 
29.82% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>(a) SGM Pair 2+3.</figDesc><table>GCP-Kitti 

GCP-Middle 

GCP-Ours 

Park-Kitti 

Park-Middle 

Park-Ours 

Ensemble-Kitti 

Ensemble-Middle 

Ensemble-Ours 

LRD 

Optimum 

Bad pixel rate [1] 

Sparsification [1] 

0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
0 

0.02 

0.04 

0.06 

0.08 

0.1 

0.12 

0.14 

0.16 

0.18 

0.2 

GCP-Kitti 

GCP-Middle 

GCP-Ours 

Park-Kitti 

Park-Middle 

Park-Ours 

Ensemble-Kitti 

Ensemble-Middle 

Ensemble-Ours 

LRD 

Optimum 

Bad pixel rate [1] 

Sparsification [1] 

0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
0 

0.05 

0.1 

0.15 

0.2 

0.25 

(b) SGM Pair 6+7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>.4% LRD Ens.<ref type="bibr" target="#b9">[10]</ref> Park<ref type="bibr" target="#b22">[23]</ref> GCP<ref type="bibr" target="#b30">[31]</ref> </figDesc><table>Kitti RA 
2.12 
1.81 
1.91 
3.54 
Middle RA 2.12 
1.43 
1.59 
2.60 
Ours RA 
2.12 
1.40 
1.51 
2.01 
Kitti Red 
-
22.34% 
21.04% 
45.30% 
Middle Red 
-
1.86% 
5.02% 
23.53% 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminative learning of local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A stereo confidence metric using single view imagery with comparison to five alternative approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Egnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings from the 15th International Conference on Vision Interface</title>
		<meeting>from the 15th International Conference on Vision Interface</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="943" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting binocular halfocclusions: empirical comparisons of five approaches. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Egnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A real-time multi-cue framework for determining optical flow confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scharwachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1978" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">StereoScan: Dense 3D Reconstruction in Real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view stereo for community photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ensemble learning for confidence measures in stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeusler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3279" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal view path planning for visual SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heyden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Scandinavian Conference on Image Analysis</title>
		<meeting>the 17th Scandinavian Conference on Image Analysis</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting matchability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A quantitative evaluation of confidence measures for stereo vision. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A method for learning matching errors for stereo computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint optimization for object class segmentation and dense stereo reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bastanlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Clocksin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="133" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a confidence measure for optical flow. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1107" to="1120" />
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time visibility-based fusion of depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Leveraging stereo matching with learning-based confidence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards a simulation driven stereo vision system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohkawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Descriptor learning for efficient retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV 2010</title>
		<editor>K. Daniilidis, P. Maragos, and N. Paragios</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6313</biblScope>
			<biblScope unit="page" from="677" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SURE: Photogrammetric Surface Reconstruction from Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fritsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Haala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings LC3D Workshop</title>
		<meeting>LC3D Workshop</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nešić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<editor>X. Jiang, J. Hornegger, and R. Koch</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">8753</biblScope>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accurate object detection with joint classification-regression random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors using convex optimisation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1573" to="1585" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to detect ground control points for improving the accuracy of stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spyropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1621" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On benchmarking camera calibration and multi-view stereo for high resolution imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Thoennessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Labelme: Online image annotation and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning image descriptors with the boosting-trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="269" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient joint segmentation, occlusion labeling, stereo and flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV</title>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="756" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Žbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
