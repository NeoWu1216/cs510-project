<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">First Person Action Recognition Using Deep Learned Descriptors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suriya</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Arora</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">First Person Action Recognition Using Deep Learned Descriptors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We focus on the problem of wearer's action recognition in first person a.k.a. egocentric videos. This problem is more challenging than third person activity recognition due to unavailability of wearer's pose and sharp movements in the videos caused by the natural head motion of the wearer. Carefully crafted features based on hands and objects cues for the problem have been shown to be successful for limited targeted datasets. We propose convolutional neural networks (CNNs) for end to end learning and classification of wearer's actions. The proposed network makes use of egocentric cues by capturing hand pose, head motion and saliency map. It is compact. It can also be trained from relatively small number of labeled egocentric videos that are available. We show that the proposed network can generalize and give state of the art performance on various disparate egocentric action datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With availability of cameras from GoPro [2], Google Glass [1], Microsoft SenseCam <ref type="bibr" target="#b0">[3]</ref> etc., the wearable cameras are becoming a commodity allowing people to generate more and more egocentric video content. By making first person point of view available, egocentric cameras have become popular in applications like extreme sports, law enforcement, life logging and home automation.</p><p>The egocentric community has been trying to develop or adapt solutions to a wide variety of computer vision problems in the new emerging context. Work done in last few years has ranged from problems like object recognition <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b33">36]</ref>, activity recognition <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b28">31,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b39">42]</ref> to more applied problems like summarization <ref type="bibr" target="#b1">[4,</ref><ref type="bibr" target="#b19">22,</ref><ref type="bibr" target="#b23">26]</ref>, and predicting social interactions <ref type="bibr" target="#b5">[8]</ref>. Interesting ideas which exploit special properties of egocentric videos have been proposed for problems like temporal segmentation <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b30">33]</ref>, frame sampling <ref type="bibr" target="#b31">[34,</ref><ref type="bibr" target="#b46">49]</ref> and hyperlapse <ref type="bibr" target="#b15">[18]</ref>. Newer areas specific to egocentric vision such as gaze detection <ref type="bibr" target="#b21">[24]</ref> and camera wearer identification <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b29">32]</ref> have also been explored. Figure 1: Hands and object motion pattern are important cues for first person action recognition. The first two columns show 'take' action whereas last two show 'stir'. Notice the wide difference in appearance. Second and third row shows hand mask and saliency map derived from dominant motion. In several cases hands are occluded by the handled object and hence the partial hand mask is obtained. We train a compact convolutional neural network using such egocentric cues. We achieve state of the art accuracy for first person action recognition. Our approach can be applied to datasets that differ widely in appearance and actions, without requiring any hand tuning. We further improve the performance by using pretrained networks for third person videos in a multi stream setting.</p><p>We focus on the recognition of wearer's actions (or first person actions) from egocentric videos in each frame. We consider short term actions that typically last few seconds, e.g., pour, take, open etc. We do not assume any prior temporal segmentation. First row in <ref type="figure">Figure 1</ref> shows the frames corresponding to some example actions.</p><p>Recognition of wearer's actions is a natural first step in many egocentric video analysis problems. The problem is more challenging than third person action recognition because of unavailability of the actor's pose. Unlike third person actions where the camera is either static or smoothly moving, there are large shakes present in the egocentric videos due to head motion of the wearer. The sharp change in the viewpoint makes any kind of tracking impossible which makes it difficult to apply third person action recognition algorithms. Therefore, hands and handled objects become the most important cues for recognizing first person actions. <ref type="figure">Figure 1</ref> helps in visualizing the same.</p><p>Researchers have understood the importance of egocentric cues for the first person action recognition problem. In last few years several features based on egocentric cues such as gaze, motion of hands and head, and hand pose have been suggested for first person action recognition <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b7">10,</ref><ref type="bibr" target="#b22">25]</ref>. Object centric approaches which try to capture changing appearance of objects in the egocentric video have also been proposed <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b28">31]</ref>. However, the features have been hand tuned in all these instances and shown to be performing reasonably well for limited targeted datasets.</p><p>Convolutional neural networks (CNNs) have emerged as a useful tool for many computer vision tasks. However, training such deep networks require huge amount of labeled samples. Unavailability of large amounts of data in egocentric context makes their direct use non-trivial for first person action recognition. This paper proposes a framework for general first-person action recognition with following specific contributions:</p><p>1. We propose and demonstrate the utility of deep learned egocentric features for the first person action recognition. We show that these features alone can surpass the state of the art. They are also complementary to the popular image and flow features.</p><p>2. We provide an extensive evaluation of our deep learned features on various datasets widely different in appearance and actions. Our method performs well on four different egocentric video datasets with no change in the parameters.</p><p>3. The specific focus of the earlier works have restricted the standardization across various datasets and evaluation methodologies. To overcome this, we annotated a large number of publicly available egocentric first person action videos. We make the annotated datasets, network models and the source code available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action recognition has traditionally been from a third person view, for example, from a static or a handheld camera. A standard pipeline is to encode the actions using hand crafted features based on keypoints and descriptors. Some notable contributions in this area includes STIP <ref type="bibr" target="#b18">[21]</ref>, 3D-SIFT <ref type="bibr" target="#b35">[38]</ref>, HOG3D <ref type="bibr" target="#b14">[17]</ref>, extended SURF <ref type="bibr" target="#b45">[48]</ref>, and Local Trinary Patterns <ref type="bibr" target="#b47">[50]</ref>. Recent methods <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b16">19,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b43">46]</ref> have shown promising results which leverage the appearance and motion information around densely sampled point trajectories instead of cuboidal video volume.</p><p>Deep learned features for action recognition have also been explored. Works like Convolutional RBMs <ref type="bibr" target="#b40">[43]</ref>, 3D ConvNets <ref type="bibr" target="#b10">[13]</ref>, C3D ConvNet <ref type="bibr" target="#b41">[44]</ref>, Deep ConvNets <ref type="bibr" target="#b12">[15]</ref> take video frame as input. Recently, Two-stream ConvNets <ref type="bibr" target="#b36">[39]</ref> introduced spatial and flow streams for action recognition.</p><p>Hand-crafted descriptors along trajectories lack discriminative property while deep learned features fail to capture salient motion. The TDD features proposed by <ref type="bibr" target="#b44">[47]</ref> try to establish a balance by using deep convolutional descriptors along the trajectories.</p><p>We show later that both trajectory based techniques <ref type="bibr" target="#b42">[45,</ref><ref type="bibr" target="#b43">46]</ref> as well as deep neural network approaches <ref type="bibr" target="#b36">[39,</ref><ref type="bibr" target="#b44">47]</ref> do not capture the egocentric features. This restricts their performance for first person action recognition. Our egocentric features alone perform better than the state of the art. The proposed features can be complemented by the features suggested for the third person action recognition to get a further boost in the performance.</p><p>Appearance models are hard to develop from foreground or background objects due to quickly changing view field in typical egocentric videos. Spriggs et al. <ref type="bibr" target="#b38">[41]</ref> have proposed to use a mix of GIST <ref type="bibr" target="#b27">[30]</ref> features and IMU data to recognise first person actions. Their results confirm the importance of head motion in first person action recognition. Pirsiavash and Ramanan <ref type="bibr" target="#b28">[31]</ref> attempt to recognise the activity of daily living (ADL). Their thesis is that the first person action recognition is "all about the objects" being interacted with. Lee et al. <ref type="bibr" target="#b19">[22]</ref> present a video summarization approach for egocentric videos and use region cues indicative of high-level saliency such as the nearness to hands, gaze, and frequency of occurrence. Fathi et al. <ref type="bibr" target="#b7">[10]</ref> recognize the importance of hands in the first person action recognition. They propose a representation for egocentric actions based on hand-object interactions and include cues such as optical flow, pose, size and location of hands in their feature vector. In sports videos, where there are no prominent handled objects, Kitani et al. <ref type="bibr" target="#b13">[16]</ref> use motion based histograms recovered from the optical flow of the scene (background) to learn the action categories performed by the wearer. Singh et al. <ref type="bibr" target="#b37">[40]</ref> proposed a generic framework which uses trajectory aligned features along with simple egocentric cues for first person action recognition. They have released a challenging 'Extreme Sports' dataset of egocentric videos and showed that their method works even when there is no hand or object present in the video. Other works <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b30">33]</ref> have focussed on recognising long term activities of the wearer lasting several minutes such as walking, running, working etc. Several of these methods are shown to be effective for limited targeted datasets of interest. In this paper we propose convolutional neural networks for first person action recognition trained using egocentric cues which can generalize to variety of datasets at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Ego ConvNet</head><p>Hand-eye coordination is a must to accomplish any object handling task. Whenever a wearer interacts with objects (grasps or reaches for the object), hands first reach out for the object, the pose of the hand is determined by the grasp type for the object. Assuming that the wearer is looking straight, the head motion pattern is same as the gaze pattern, which follows the hands or the handled object. Hands often get occluded behind the handled objects. In this case, dominantly moving parts of the scene may give useful hints about the focus of the action. This coordination pattern is common for a vast variety of hand-object interactions. Capturing this information enables us to learn how the wearer interacts with his surrounding in order to carry out various actions.</p><p>We train an 'Ego ConvNet' to learn the coordination patterns between hands, head and eye movement. There are various challenges in this approach:</p><p>• Pixel level annotation of hand and object is very costly.</p><p>• Gaze or saliency map is usually captured with the help of separate eye tracking sensors.To make our approach generic we do not want to use extra sensors. We use egocentric video as the only input.</p><p>• Egocentric video analysis, being a relatively new field in computer vision, lacks a large enough annotated data for training a neural network.</p><p>To overcome these issues, we have used computer vision approaches to generate egocentric cues, which can be used for training a compact neural network with relatively small amount of labeled training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Input</head><p>Hand Mask Hand pose in egocentric videos provides useful information for understanding hand-object manipulation and analyzing hand-eye coordination. However, egocentric videos present new challenges such as rapid changes in illuminations, significant camera motion and complex hand-object manipulations. Without relying on manually segmented hand mask as input for the network, we automatically segment out hand regions by modelling local appearance of hand pixels and global illumination. We learn the models from a small set of manually segmented hand regions over a diverse set of imaging conditions. To account for different illumination conditions, Li and Kitani <ref type="bibr" target="#b20">[23]</ref> have suggested to use a collection of regressors indexed by a global color histogram. We follow their approach and use the response of a bank of 48 Gabor filters (8 orientations, 3 scales, both real and imaginary components) to capture local textures. We create appearance descriptors using RGB, HSV and LAB color spaces from 900 super pixels.</p><p>The posterior distribution of a pixel x given a local appearance feature l and a global appearance feature g, is computed by marginalizing over different scenes c,</p><formula xml:id="formula_0">p (x | l , g) = c p (x | l , c) p (x | c, g) Figure 2</formula><p>: If the camera was static, salient regions in the image for an action recognition task can be computed as parts with moving objects. In egocentric setting, where the camera is also moving, we first cancel the component of flow due to camera motion. We note that the motion of the egocentric camera is 3D rotation for the considered actions. Such motion can easily be compensated by cancelling a 2D homography. Left and center images shows the original and compensated flow. We use the dominant motion direction from the compensated flow and use the component of flow in the direction of dominant motion direction to generate a saliency map (right image). See the text for the details.</p><p>where p (x | l , c) is the output of a discriminative global appearance-specific regressor and p (c | g) is a conditional distribution of a scene c given a global appearance feature g.</p><p>We perform k-means clustering on the HSV histogram of each training image. For each cluster we generate different global appearance models by learning separate random tree regressor. Histogram allows for encoding both the appearance as well as illumination of the scene. We train n = 11, models in our experiments. We assume that the hands viewed under similar global appearance share a similar distribution in the feature space. The conditional distribution p (c | g) is approximated using a uniform distribution over the n nearest models.</p><p>Second row in <ref type="figure">Figure 1</ref> shows the hand masks obtained for some example actions using the described method.</p><p>Head Motion Egocentric camera is often mounted on the wearer's head and mimics its motion. Due to the natural pivot of the head on the neck, the induced motion is a 3D rotation which can be easily captured using a 2D homography transformation of the image. We use optical flow for correspondence (avoiding hand regions) and estimate frame to frame homography using RANSAC. Use of optical flow instead of feature matching using local image descriptor such as SIFT or SURF avoids extra computation overhead. It also ensures robustness against moving objects that may be present in such videos. We set the bound on head motion between two consecutive frames to be between −5 pixels and +5 pixels.Head motion in x and y direction is then normalized to the range [0, 255] and encoded as grayscale image separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saliency Map</head><p>The background in a first person action is often cluttered and poses serious challenges for an action classifier. There are a variety of objects present in the scene <ref type="figure">Figure 3</ref>: Being a relatively new field, availability of labeled dataset for first person action recognition is limited. We propose a new compact CNN architecture for recognizing the wearer's actions, which takes as input egocentric cues and can be trained from limited training samples. The first and second rows show the architecture of the proposed Ego ConvNet with 2D and 3D convolutions respectively. We collect the information from a video segment of EL frames and generate a 3D input stack of size EN × EM × (4 × EL). The features learnt from 2D and 3D Ego ConvNet seem to be complementary and we use the two networks together achieving state of the art accuracy of 58.94% on GTEA dataset <ref type="bibr" target="#b7">[10]</ref>.</p><formula xml:id="formula_1">E N E M 4 x E L E L 5 5 E N E M 4 x E L 4 5 5</formula><p>during the action, but usually only the objects being handled are important. Such salient objects can easily be determined if the gaze information is available. But this requires extra sensors. We note that there is hand-eye coordination in any first person action, and therefore saliency map obtained from the gaze resembles the dominantly moving parts in the scene. If the camera was static, such objects could be easily distinguished from others on the basis of observed optical flow only. In egocentric videos, motion of the camera has to be taken care of before using this approach. As described in the last section, this motion is easily approximated using a 2D homography. We use the homography to cancel the component due to camera motion in the observed flow. After camera motion compensation, the dominant motion in the scene comes from handled objects or hands. <ref type="figure">Figure 2</ref> shows the same for an example action.</p><p>We use dominant flow regions to generate a saliency map per frame. We take orientation/direction of compensated optical flow and quantize it into 9 bins over an interval of 0 − 360 • . Each flow vector votes proportional to its magnitude. The bin with the highest number of votes is declared the dominant motion direction. Saliency value for a pixel is evaluated as the magnitude of flow in the direction of dominant motion direction. The saliency values are normalized to the range [0, 255] and encoded as grayscale image for input to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture</head><p>We use a convolutional neural network to learn the coordination patterns between hands, head motion and saliency map while the wearer performs an action. We encode hand mask as a binary image. Camera motion (x and y direc-tions separately) and saliency map are encoded as grayscale images. These are used as input to the network. We scale the input images to E N × E M pixels. We collect the information from a video segment of E L frames generating a 3D input stack of size E N × E M × (4 × E L ). For each frame we use 4 different egocentric features. Hence, for E L frames the input dimension depth becomes 4 × E L . We preserve the original video aspect ratio while setting E N and E M .</p><p>Our Ego ConvNet architecture ( <ref type="figure">Figure 3</ref>) consists of 2 convolution layers each followed by MAX pooling, RELU non-linearity, and local response normalization (LRN) layers and 2 fully connected layers. We use infogain multinomial logistic loss instead of popular multinomial logistic loss during training to handle class imbalance.</p><formula xml:id="formula_2">E = −1 N N n=1 H ln log(p n ) = −1 N N n=1 K k=1 H ln,k log(p n,k ) H m,p =      1 − |L m | K k=1 |L k | , if p = m 0, otherwise</formula><p>where E is infogain multinomial logistic loss, N is training batch size, K is number of classes, l n is ground truth label for sample n , p n,k is probability of sample n being classified as class k and |L m | is number of training samples belonging to class m. We also use dropout ratio of 0.5 with fully connected layer to avoid overfitting during training phase. During test phase, we estimate the class label by applying SOFTMAX on f c2 layer output. Without any appearance and motion cues, i.e., using egocentric cues alone, <ref type="figure">Figure 4</ref>: We extend our Ego ConvNet by adding two more streams corresponding to spatial and temporal streams in twostream architecture <ref type="bibr" target="#b36">[39]</ref>. Intuitively spatial and temporal streams capture the generic appearance and flow based features whereas the egocentric stream captures the coordination between wearer's hands, head and eyes, which is not captured by appearance and flow features. Using multiple stream architecture improves the accuracy of proposed method from 58.94% to 68.5% on GTEA dataset <ref type="bibr" target="#b7">[10]</ref>.</p><p>our Ego ConvNet is able to outperform state of the art by a large margin. This outlines the importance of egocentric cues for first person action recognition task.</p><p>3D Convolution Tran et al. <ref type="bibr" target="#b41">[44]</ref> extended 2D convolution to allow 3D convolution and 3D pooling, which has been shown to capture the temporal structure of an action. C3D <ref type="bibr" target="#b41">[44]</ref> performs 3D convolutions and 3D pooling, propagating temporal information across all the layers in the network. We have also experimented with 3D convolutions in the proposed network. The architecture for the 3D Ego ConvNet is slightly different from that of 2D Ego ConvNet. We use filters of size 5 × 5 × 4, temporal pooling of size 5 × 5 × 1 for first convolution layer and 5 × 5 × 2 for second convolution layer where last dimension represents depth of the filter. The first pooling layer has kernel of depth 1 with the intention of avoiding the merge of the temporal signal too early. <ref type="figure">Figure 3</ref> shows the architecture. The performance of 2D and 3D Ego ConvNet are similar. However, the features learnt seem to be complementary. Therefore, we combine the two networks by using two-streams architecture (similar to the one proposed by Simonyan and Zisserman <ref type="bibr" target="#b36">[39]</ref>) by adding a new SVM classifier at the end. While the accuracy for Ego ConvNet using 2D and 3D convolutions is 57.61% and 55.79% respectively, the accuracy when the two networks are used together is 58.94%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Three-Stream Architecture</head><p>Dense Trajectory <ref type="bibr" target="#b42">[45]</ref> and its variants <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b16">19,</ref><ref type="bibr" target="#b43">46]</ref> constitute a popular approach for third person action recognition. These techniques track interest points across time and compute hand designed flow and appearance based descriptors (HOG, HOF, and MBH) along the trajectories. Recently Wang et al. <ref type="bibr" target="#b44">[47]</ref> have proposed to replace hand designed features with 'Deep Convolution' descriptors. They use two-stream architecture proposed by Simoyan and Zisserman <ref type="bibr" target="#b36">[39]</ref> for computing deep convolution descriptors. The first 'Spatial Stream' uses individual video frame as input, effectively performing action recognition from still images using ImageNet <ref type="bibr" target="#b17">[20]</ref> architecture. The spatial stream takes a single RGB video frame as input. The second 'Temporal Stream' uses stacked optical flow as input to capture motion information. The temporal stream takes the stacked dense optical flow displacement fields between L consecutive frames as input. The x and y components of flow fields are encoded as grayscale image separately after normalising to [0, 255]. The temporal convnet has the same architecture as the spatial stream.</p><p>We extend our Ego ConvNet by adding two more streams corresponding to spatial and temporal streams in the TDD or two-stream architecture. Intuitively, spatial and temporal streams capture the generic appearance and flow based features whereas the egocentric stream captures the coordination between the wearer's hands, head and eyes, which is not captured by appearance and flow features. Fusion of these cues results in a more meaningful feature to describe first person actions.</p><p>We perform fusion of the three streams: spatial, temporal and egocentric, by combining weighted classifier scores. The weights are learnt using cross validation. For egocentric features we use softmax score as the classifier score. For the spatial and temporal streams we use SVM classification score. We learn a multiclass SVM classifier using improved Fisher Vector representation of trajectory pooled features from appearance and flow features.</p><p>The architecture of our convnet can be seen in <ref type="figure">Figure 4</ref>. Using multiple stream architecture with pre-trained spatial and temporal streams improves the accuracy of proposed method from 58.94% to 68.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Protocol</head><p>We consider short term actions performed by different subjects while performing different activities. In our work, we use four different publicly available datasets of egocentric videos: GTEA <ref type="bibr" target="#b7">[10]</ref>, Kitchen <ref type="bibr" target="#b38">[41]</ref>, ADL <ref type="bibr" target="#b28">[31]</ref> and UTE <ref type="bibr" target="#b19">[22]</ref>. Out of these, only GTEA and Kitchen datasets have frame level annotations for the first person actions. For ADL and UTE datasets, where similar action level labelling was not available, we selected a subset of the original dataset and manually annotated the short term actions in the parts where a wearer is manipulating some object. Other kinds of actions, such as walking, watching television etc. are labelled as 'background'. The speed and nature of actions vary across subjects and activities (e.g., consider the ac- <ref type="figure">Figure 5</ref>: Examples of wearer's action categories we propose to recognize in this paper from different datasets: GTEA <ref type="bibr" target="#b7">[10]</ref> (top row), Kitchen <ref type="bibr" target="#b38">[41]</ref> (middle row) and ADL <ref type="bibr" target="#b28">[31]</ref> (bottom row). The columns represent the actions 'pour', 'take', 'put', 'stir' and 'open'. The actions vary widely across datasets in terms of appearance and speed of action. Features and technique we suggest in this paper is able to successfully recognize the wearer's actions across different presented scenarios, showing the robustness of our method. The GTEA dataset consists of 7 long term activities captured using head mounted cameras. Each activity is approximately 1 minute long. We follow 'leave-one-subject-out' experimental setup of <ref type="bibr" target="#b4">[7]</ref> for all datasets.</p><p>Kitchen dataset <ref type="bibr" target="#b38">[41]</ref> is captured using head mounted camera and IMUs. Camera point of view is from top, and severe camera motion is quite common. Similar to <ref type="bibr" target="#b38">[41]</ref>, we select 7 subjects from 'Brownie' activity. We use videos of 6 subjects for training and test on the video of the remaining subject. ADL dataset consists of videos of subjects performing daily life activities, captured using chest mounted cameras with 170 degrees of viewing angle. UTE dataset <ref type="bibr" target="#b19">[22]</ref> contains 3 to 5 hours long videos captured from head-mounted cameras in a natural and uncontrolled setting. The annotated datasets and the source code along with the pre-trained CNN models for the paper are available at the project page: http://cvit.iiit.ac.in/ projects/FirstPersonActions/.</p><p>Evaluation Protocol For systematic evaluation, we use leave-one-subject-out policy for training and validation and report classification accuracy on the unseen test subject. Formally, classification accuracy for first person action recognition task is defined as the number of frames (or video segments) classified correctly divided by total number of frames (or video segments) in the videos used for testing. Frame level action recognition is important for continuous video understanding. This is also crucial for many other applications (e.g., step-by-step guidance based on wearer's current actions). We also evaluate our method at the video segment level. In this case, there is only one action in each video segment. However, length of the segment is not fixed. In this setting, we have an approximate knowledge of action boundaries which naturally improves action recognition results.</p><p>Unlike <ref type="bibr" target="#b22">[25]</ref>, we are interested in classification of first person action in different settings and not the specific object which is involved in the action. For example, the action where the wearer is 'pouring' 'water' from the 'bottle' into the 'cup' and where the wearer is 'pouring' 'mayonnaise' on to the 'bread'. In our experiments we consider both actions as 'pouring' despite different objects being involved. We believe that such evaluation is more challenging and removes the bias of object instance specific appearance while learning action models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We use Caffe's CNN implementation <ref type="bibr" target="#b11">[14]</ref> due to its speed and efficiency. In all experiments, we normalise data to the range [0, 1] and use infogain multinomial logistic loss to counter imbalanced class population in the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Ego ConvNet</head><p>We use 343 pre-segmented hand masks and implementation for hand segmentation provided by <ref type="bibr" target="#b20">[23]</ref>. Since GTEA dataset is too small to properly train a convnet, we add more data by using videos from the Interactive Museum dataset <ref type="bibr" target="#b2">[5]</ref>, consisting of 700 videos at 800×450 resolution and 25 frames per second, all shot with a wearable camera mounted on the head. We manually segment hands from randomly selected 60 frames in order to train the hand models.</p><p>We chose Interactive Museum dataset for pre-training due to its similarity to GTEA dataset in which same actions are performed by various subjects. Further, the hand gesture videos emphasize on actions using hands under similar camera or head movement.</p><p>Prior to training on GTEA dataset, we pre-train the network on <ref type="bibr" target="#b2">[5]</ref>. This takes care of the small dataset size available for training. Pre-training is done in leave-one-subjectout manner. Videos from subjects 1-4 are used for training and validation is done on videos from subject 5. We select non-overlapping windows of E L frames (depending on architecture) as input to the network. Pre-training is done for 20 epochs in all experiments. For Kitchen, ADL and UTE datasets, we fine-tune Ego Convnet which has been pre-trained on Interactive Museum and GTEA datasets, to avoid training from scratch.</p><p>Training Similar to pre-training, training is done in leaveone-subject-out manner as well. We use video segments from subject 1 and 3 for training, subject 4 for validation and subject 2 for testing. A video segment is an overlapping sliding window of E L frames (depending on architecture) as input to the network. Since the input is overlap-  <ref type="bibr" target="#b44">[47]</ref> Temporal 57.12 TDD <ref type="bibr" target="#b44">[47]</ref> Spatial + Temporal 59.47 <ref type="table">Table 3</ref>: Performance of trajectory based methods when used with various features for GTEA dataset. Accuracy in terms of percentage is reported for frame level action recognition.</p><p>ping frames, the training set is shuffled at random to avoid all samples belonging to the same class for batch training.</p><p>Frames are resized while maintaining the original aspect ratio, as we found warping to square size input reduces the performance. Training is done by SGD with minibatch size of 64 examples. Initial learning rate is 0.001, and is divided by 10 every 10K iterations. The optimization is stopped at 25K iterations (about 60 epochs).</p><p>Varying Network Architecture We trained a compact network with 2 convolution layers and 2 fully connected layers. Such a compact network has been shown to work fairly well despite having limited training data. To search for a good Ego ConvNet architecture, we varied both spatial, E N ×E M , (64×36, 32×18 and 16×9 pixels) and temporal, E L , (28, 16, 10 and 5 frames) input dimensions and found 32 × 18 × 5 to be the best. This size is small enough to keep number of parameters low without losing relevant information. We also tried skipping frames (by selecting every third or sixth frames) from deeper input volume (15 frames or 30 frames) keeping input depth to 5. However, this approach did not improve recognition performance. We also investigated with different filter sizes (3 × 3, 5 × 5 and 7 × 7 ) and number of filters (32, 64, 128, 256 for convolution layers and 256, 512, 1024 and 2048 for fully connected layer) and found 5 × 5 filter with 32 filters in first convolution layer, 128 filters in second convolution layer and 512 channel output for f c1 layer to be the best performer. For 3D Ego ConvNet, keeping all other parameters same, we use filters having depth-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and Discussion</head><p>We first present our experiments and analysis of the proposed action descriptor on GTEA dataset to bring out salient aspects of the suggested approach. Experiments with other datasets have been described later.</p><p>We follow experimental setup of Fathi et. al. <ref type="bibr" target="#b4">[7]</ref> for GTEA dataset. They perform joint modelling of actions, activities and objects, on activities of three subjects and predict actions on activities of one subject. They report an accuracy of 47.70%. <ref type="table" target="#tab_2">Table 2</ref> shows performance of our Ego ConvNet and <ref type="table">Table 3</ref> for trajectory based approaches on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Features Accuracy  GTEA dataset. Although, the standalone performance for two types of features is similar, we found the learnt features to be complementary. <ref type="figure" target="#fig_0">Figure 6</ref> shows confusion matrices for the two sets of features to support our claim. Therefore, we fuse the trajectory pooled and egocentric features in a three-stream architecture. <ref type="table" target="#tab_5">Table 4</ref> gives the effect of different features on the performance of our algorithm on the dataset. Experimental protocol leaving subject 2 is what is done by Fathi et. al. in <ref type="bibr" target="#b4">[7]</ref> and is followed by us to do a fair comparison. We also show cross-validated results in leave-one-subject-out manner (see <ref type="table">Table 1</ref>). We extend our experiments to other publicly available egocentric video datasets. Results on these datasets are shown in <ref type="table" target="#tab_2">Table 2</ref> and 5. We follow the same experimental setup as <ref type="bibr" target="#b38">[41]</ref> and perform frame level action recognition for 'Brownie' activity for 7 subjects. Spriggs et al. <ref type="bibr" target="#b38">[41]</ref> reports an accuracy of 48.64% accuracy when using first person data alone and 57.80% when combined with IMU data. We achieve 54.35% accuracy using our method with egocentric stream alone and 66.23% with our three-streams approach.</p><p>The ADL dataset has been used for long term activity recognition by <ref type="bibr" target="#b28">[31]</ref> in the past. We annotated the dataset Dataset Accuracy Frame level Segment level Chance level GTEA <ref type="bibr" target="#b7">[10]</ref> 68.50 82.40 11% Kitchen <ref type="bibr" target="#b38">[41]</ref> 66.23 71.88 3.4% ADL <ref type="bibr" target="#b28">[31]</ref> 37.58 39.02 4.7% UTE <ref type="bibr" target="#b19">[22]</ref> 60.17 65.30 4.7% <ref type="table">Table 5</ref>: Our results for first person action recognition on different egocentric videos datasets. Sliding window based approach for classification used in our algorithm performs poorly at action boundaries. Therefore, the accuracy for segment level classification, when the action boundaries are clearly defined, comes out higher.</p><p>with the short term actions and tested our method on it. Similar to our experiment on GTEA, we test our model on one subject while using other for training. We achieve 37.58% accuracy at frame level and 39.02% at video segment level using the proposed method. The UTE dataset has been used for video summarization by <ref type="bibr" target="#b19">[22]</ref> in the past. Motion blur and low image quality is fairly common in this dataset. For action recognition we achieve 60.17% accuracy at frame level and 65.30% at video segment level using the proposed method.</p><p>The proposed action descriptor improves upon the state of the art on all four datasets (see <ref type="table">Table 1</ref> for the details about dataset and comparison). <ref type="figure">Figure 5</ref> shows some sample actions correctly classified by our approach. Note the difference in appearance between the datasets. The experiments show that the proposed approach consistently outperforms state of the art accuracy for each of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Previous approaches for first person action recognition have explored various hand tuned features based on egocentric cues such as hand pose, head motion and objects present in the scene. These approaches do not leverage existing work in third person video analysis and do not generalize beyond the dataset considered.</p><p>We have proposed a new convolutional neural network based framework for first person action recognition. We propose a three-stream architecture which uses egocentric cues in the first stream and complements it with pre trained spatial and temporal streams from third person video analysis. We show that with the egocentric stream alone, we can achieve state of the art accuracy. The performance improves further after using complementary features from spatial and temporal streams. The generality of the approach is validated by achieving state of the art accuracy on all available public datasets at the same time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 6 :</head><label>6</label><figDesc>Confusion matrix for different deep learned descriptors on GTEA dataset. Classification using Ego ConvNet descriptors (left) and TDD descriptors (right). The Ego ConvNet and TDD features are clearly complementary and improve overall action recognition accuracy when used together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1: Statistics of egocentric videos datasets used for experimentation. The proposed approach uses deep learned appearance, motion and egocentric features and improves the state of the art on all the datasets we tested. Results are reported in terms of percentage of accuracy. The datasets vary widely in appearance, subjects and actions being performed, and the improvement on these datasets validates the generality of suggested descriptor for egocentric action recognition task. Note that originally ADL dataset has been used for activity recognition and UTE for video summarization and not for action recognition as in this paper. Therefore, comparative results are not available for these datasets.</figDesc><table>Dataset 
Subjects Frames Classes 
State of the art 
Accuracy 
Ours 
Ours 
(cross validated) 

GTEA [10] 
4 
31,253 
11 
47.70 [7] 
68.50 
64.41 
Kitchen [41] 
7 
48,117 
29 
48.64 [41] 
66.23 
66.23 
ADL [31] 
5 
93,293 
21 

N.A. 

37.58 
31.62 
UTE [22] 
2 
208,230 
21 

N.A. 

60.17 
55.97 

tion 'open' in two scenarios, 'open' water bottle and 'open' 
cheese packet). Statistics related to the datasets are shown 
in Table 1. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Detailed analysis of spatial, temporal and egocentric fea-
tures used in our work. H: Hand masks, C: Camera/Head motion, 
M: Saliency Map, S: Deep learned Spatial descriptors, T: Deep 
learned Temporal descriptors. Results are reported in terms of 
percentage of accuracy. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Effect of various CNN features on first person action recognition. The experiments are done on GTEA dataset. Accuracy reported is for frame level action recognition. H: Hand masks, C: Camera/Head motion, M: Saliency Map, S: Deep learned Spatial descriptors, T: Deep learned Temporal descriptors.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://cvit.iiit.ac.in/projects/ FirstPersonActions/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement The authors would like to thank Kohli Center on Intelligent Systems (KCIS) for the partial financial support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Microsoft sensecam</title>
		<ptr target="http://research.microsoft.com/en-us/um/cambridge/projects/sensecam/.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Novelty detection from an ego-centric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aghazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gesture recognition in ego-centric videos using dense trajectories and hand segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting daily activities from egocentric images using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Social interactions: A firstperson perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Egocentric video biometrics. CoRR, abs/1411</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7591</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Better exploiting motion for better action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast unsupervised ego-action learning for first-person sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">First-person hyperlapse videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Motion based foreground detection and poselet motion features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pixel-level hand detection in ego-centric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to predict gaze in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An attention-based activity recognition for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object-centric spatio-temporal pyramids for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mccandless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coupling eyemotion and ego-motion features for first-person activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ogaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Head motion signatures from egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal segmentation of egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Egosampling: Fastforward and stereo for egocentric videos. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3596</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Figure-ground segmentation improves handled object recognition in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Egocentric recognition of handled objects: Benchmark and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">First-person activity recognition: What are they doing to me</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Trajectory aligned features for first person action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<idno>abs/1604.02115</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal segmentation and activity classification from first-person sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Spriggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">High level activity recognition using low resolution wearable vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W M</forename><surname>Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detecting snap points in egocentric video with a web photo prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Local trinary patterns for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yeffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
