<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyperDepth: Learning Depth from Structured Light Without Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">Ryan</forename><surname>Fanello</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Tankovich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Escolano</forename><surname>David</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Shahram Izadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HyperDepth: Learning Depth from Structured Light Without Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structured light sensors are popular due to their robustness to untextured scenes and multipath. These systems triangulate depth by solving a correspondence problem between each camera and projector pixel. This is often framed as a local stereo matching task, correlating patches of pixels in the observed and reference image. However, this is computationally intensive, leading to reduced depth accuracy and framerate. We contribute an algorithm for solving this correspondence problem efficiently, without compromising depth accuracy. For the first time, this problem is cast as a classification-regression task, which we solve extremely efficiently using an ensemble of cascaded random forests. Our algorithm scales in number of disparities, and each pixel can be processed independently, and in parallel. No matching or even access to the corresponding reference pattern is required at runtime, and regressed labels are directly mapped to depth. Our GPU-based algorithm runs at a 1KHz for 1.3MP input/output images, with disparity error of 0.1 subpixels. We show a prototype high framerate depth camera running at 375Hz, useful for solving tracking-related problems. We demonstrate our algorithmic performance, creating high resolution real-time depth maps that surpass the quality of current state of the art depth technologies, highlighting quantization-free results with reduced holes, edge fattening and other stereo-based depth artifacts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consumer depth cameras have revolutionized many aspects of computer vision. With over 24 million Microsoft Kinects sold alone, structured light sensors are still the most widespread depth camera technology. This ubiquity is both due to their affordability, and well-behaved noise characteristics, particularly compared with time-of-flight cameras that suffer from multipath errors <ref type="bibr" target="#b17">[17]</ref>; or passive stereo techniques which can fail in textureless regions <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Structured light systems date back many decades; see * Authors equally contributed to this work. <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b16">16]</ref>. Almost all follow a similar principle: A calibrated camera and projector (typically both near infrared-based) are placed at a fixed, known baseline. The structured light pattern helps establish correspondence between observed and projected pixels. Depth is derived for each corresponding pixel through triangulation. The process is akin to two camera stereo <ref type="bibr" target="#b43">[43]</ref>, but with the projector system replacing the second camera, and aiding the correspondence problem. Broadly, structured light systems fall into two categories: spatial or temporal. The former uses a single spatially varying pattern, e.g. <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b45">45]</ref>, and algorithms akin to stereo matching to correlate a patch of pixels from the observed image to the reference pattern, given epipolar constraints. Conversely, the latter uses a varying pattern over time to encode a unique temporal signature that can be decoded at each observed pixel, directly establishing correspondence.</p><p>Temporal techniques are highly efficient computationally, allowing for a simple, fast lookup to map from observed to projected pixels, and estimate depth. However, they require complex optical systems e.g. MEMS based projectors and fast sensors, suffer from motion artifacts even with higher framerate imagers, and are range limited given the precision of the coding scheme. Therefore many consumer depth cameras are based on spatially varying patterns, typically using a cheap diffractive optical element (DOE) to produce a pseudo-random pattern, such as in Kinect.</p><p>However, spatial structured light systems carry a fundamental algorithmic challenge: high computational cost associated with matching pixels between camera and projector, analogous to stereo matching. This computational barrier has also motivated many local stereo methods; see <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b4">5]</ref>. Whilst progress has been made on efficient stereo methods, especially so called O(1) or constant time methods <ref type="bibr" target="#b4">[5]</ref>, these often trade accuracy or precision for performance, and even then very high framerates cannot be achieved.</p><p>Just a single disparity hypothesis often requires two local patches (in left and right images) to be compared, with many pixel lookups and operations. Spatial structured light algorithms e.g. in Kinect <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b25">26]</ref>, attempt to reduce these comparisons, but even then ∼20 patch comparisons are re-quired per pixel. These are even higher for dense stereo methods. In addition, there are further sequential operations such as region growing, propagation or filtering steps <ref type="bibr" target="#b4">[5]</ref>. This explains the fundamental limit on resolution and framerate we see in depth camera technologies today (typically 30-60Hz VGA output).</p><p>In this paper we present HyperDepth, a new algorithm that breaks through this computational barrier without trading depth accuracy or precision. Our approach is based on a learning-based technique that frames the correspondence problem into a classification and regression task, instead of stereo matching. This removes the need for matching entirely or any sequential propagation/filtering operations. For each pixel, our approach requires less compute than a single patch comparison in Kinect or related stereo methods.</p><p>The algorithm independently classifies each pixel in the observed image, using a label uniquely corresponding to a subpixel position in the associated projector scanline. This is done by only sparsely sampling a 2D patch around the input pixel, and using a specific recognizer per scanline. Absolutely no matching or even access to the corresponding reference pattern is required at runtime. Given a calibrated setup, every pixel with an assigned class label can be directly mapped to a subpixel disparity and hence depth.</p><p>To train our algorithm, we capture a variety of geometric scenes, and use a high-quality, offline stereo algorithm <ref type="bibr" target="#b6">[7]</ref> for ground truth. This allows our recognizers to learn a mapping for a given patch to a (discrete then continuous) class label that is invariant to scene depth or affine transformations due to scene geometry. Using this approach, we demonstrate extremely compelling and robust results, at a working range of 0.5m to 4m, with complex scene geometry and object reflectivity. We demonstrate how our algorithm learns to predict depth that even surpasses the ground truth. Our classifiers learn from local information, which is critical for generalization to arbitrary scenes, predicting depth of objects and scenes vastly different from the training data.</p><p>Our algorithm allows each pixel to be computed independently, allowing parallel implementations. We demonstrate a GPU algorithm that runs at 1KHz on input images of 1.3MP producing output depth maps of the same resolution, with 2 17 disparity levels. We demonstrate a prototype 375Hz camera system, which can be used for many tracking problems. We also demonstrate our algorithm running live on Kinect (PrimeSense) hardware. Using this setup we produce depth maps that surpass the quality of Kinect V1 and V2, offline stereo matching, and latest sensors from Intel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Work on structured light dates back over 40 years <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>. At a high level these systems are categorized as temporal or spatial <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>Temporal techniques require multiple captures of the scene with a varying dynamic pattern (also called multishot <ref type="bibr" target="#b16">[16]</ref>). This projected pattern encodes a temporal signal that is uniquely decoded at each camera pixel. Examples of patterns include binary <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b20">20]</ref> gray code, <ref type="bibr" target="#b35">[35]</ref>, and fringe patterns <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b53">53]</ref>. These techniques have one clear advantage, they are computationally very efficient, as the correspondence between camera and projector pixel is a biproduct of decoding the signal. Depth estimation simply becomes a decode and lookup operation. However, systems require multiple images, leading to motion artifacts in dynamic scenes. To combat this, fast camera and projector hardware is required, such as demonstrated by the Intel F200 product. However, these components can be costly and fast motions still lead to visible artifacts. Systems are also range limited given temporal encoding precision. Spatial structured light instead use a single unique (or pseudo unique) 1D <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b51">51]</ref> or 2D pattern <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b45">45]</ref> for single shot depth estimation. These techniques appeal as they use simple optical elements, i.e. no dynamic projector and regular framerate cameras, and are more robust to motion artifacts and range limitations. However, they also suffer from a fundamental challenge: the correspondence problem becomes far more challenging. Almost all methods frame this problem as a local stereo matching problem. For example, the PrimeSense algorithm inside the Kinect <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b25">26]</ref>, first extracts observed dots, then matches a patch around each dot, with corresponding patches in the reference image using NCC. This leads to ∼20 NCC patch matches per pixel, with the minimum NCC score being selected. Then a sequential region growing process creates a dense disparity map. Each of these steps: dot extraction, NCC patch matching, disparity selection, and region growing takes considerable time with many pixel lookup and operations.</p><p>There is a large body of work on stereo matching, some of which is relevant for structured light systems. For example, <ref type="bibr" target="#b15">[15]</ref> first detects sparse support points in stereo images and performs sparse correspondence search among them before a dense propagation step. Others approximate global optimization methods <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref> based on dynamic programming and achieve reasonable frame rates but are restricted to low-resolution images and operate on a strongly quantized depth range (typically at 64 discrete depth values).</p><p>Local stereo algorithms are generally faster than their global counterparts, because they identify corresponding pixels only based on the correlation of local image patches. Many correlation functions can be implemented as a filter with a computational complexity independent of the filter size <ref type="bibr" target="#b43">[43]</ref>. Recent real-time stereo approaches focus on filters that weight each pixel inside the correlation window based on image edges, e.g. based on bilateral filtering <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b31">31]</ref> or guided image filtering <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b9">10]</ref>. These approaches show good computational performance with small disparity levels, but do not scale to high precision estimation.</p><p>PatchMatch stereo <ref type="bibr" target="#b6">[7]</ref> has been shown to achieve high quality dense depth maps by leveraging slanted support windows and sub-pixel disparities within a PatchMatch framework <ref type="bibr" target="#b1">[2]</ref>. This technique has recently been extended to real-time performance by assuming fronto parallel windows and reducing the number of iterations of disparity propagation <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b54">54]</ref>. <ref type="bibr" target="#b24">[25]</ref> bring together concepts from PatchMatch stereo and cost volume filtering within a unified framework, but the reliance on superpixels limits structured light use.</p><p>Whilst all this work leads to less computation for stereo matching, these approaches ultimately still require a large number of computations (still fundamentally relying on computing matching costs across a large number disparity levels), with often expensive preprocessing, filtering and propagation steps that can be sequential. This has meant that even GPU, FPGA or ASIC stereo implementations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b14">14]</ref> can only operate on limited input/output resolution, at speeds rarely exceeding 30Hz, often with a trade in accuracy.</p><p>Other work has looked at combining a pair of cameras with either fixed <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">54]</ref> or dynamic structured light patterns <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b52">52]</ref>. The former referred to as active stereo, is a technique used in the recent Intel R200 depth camera. The latter is used to extend stereo to the temporal domain (spacetime stereo) but suffers from motion artifacts and complex hardware setups, similar to other temporal structured light systems. Both these techniques again rely on local stereo algorithms with computational limitations.</p><p>Our work attempts to bring the computational benefits of temporal structured light to more widespread and appealing single-shot spatial structured light systems. To achieve this we take a radical departure from the literature, reformulating this correspondence problem to a classificationregression rather than stereo matching task. As we will show in the remainder of this paper, this learning-based approach brings some extremely compelling computational and accuracy benefits. In essence, our approach allows each pixel in the camera image to be evaluated independently, with a computational effort similar to testing a single disparity hypothesis in local stereo methods.</p><p>Techniques that employ machine learning for depth estimation have begun to appear. <ref type="bibr" target="#b49">[49]</ref> explore deep nets for computing stereo matching costs, but still require multiple disparity hypothesis evaluation using computationally expensive learning architectures. <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b42">42]</ref> predict depth from a single image, but are expensive and lack the accuracy in general scenes. <ref type="bibr" target="#b13">[13]</ref> uses diffuse infrared light to learn a shape from shading mapping from infrared intensity to depth, but this technique fails in general scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learning to Recognize Structured Light</head><p>In this section we reformulate the spatial structured light correspondence problem from a machine learning perspec-tive, and show how disparity maps with subpixel accuracy can be predicted extremely efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem formulation</head><p>We use a setup analogous to the Kinect, where an infrared (IR) camera is placed at a fixed baseline to a structured light DOE projector. The IR camera captures images at 1280×1024 resolution. The IR projector generates a pseudorandom dot pattern in the scene that is observed by the IR camera as image I. The pattern projector can be seen as a virtual camera that always observes the same constant image, referred to as the reference pattern R. We assume images I and R to be calibrated and rectified. Therefore, for each pixel p = (x, y) in I the corresponding pixelp = (x, y) in R, that shows the same local dot structure, lies on the same scanline y. The shift along the x coordinatex − x is known as disparity d and is inversely proportional to the scene depth Z via Z = bf d , where b is the baseline of the system and f is the focal length.</p><p>The local dot structure in a small spatial neighborhood in the reference pattern uniquely identifies each pixelp = (x,ŷ) along a scanlineŷ. Therefore, we can assign each pixel p along a scanline in R to a unique label c =x according to itsx coordinate. If we are able to recognize the class c for a pixel p = (x, y) in the observed IR image I, we can simply infer the disparity of p via the direct mapping d = c − x.</p><p>Motivated by this observation, we cast the depth estimation problem into a machine learning problem, where given training data, we learn to recognize the class label in the observed image I from the local dot structures and, as a consequence, the depth. Note that in the Kinect reference pattern, the same local structures reappear in differentx coordinates of different scanlines y. Finding these repetitions is a challenging task itself <ref type="bibr" target="#b29">[29]</ref>. To overcome this issue, we simply use a classifier per line, which also provides additional robustness against the distortion of the projected pattern. This allows us to reuse the same class labels c =x in different scanlines y. Each of these classifiers has to disambiguate C = 1280 classes, equal to the width of the image. The reference pattern is symmetric around the central projector pixel and repeats three times along the X and Y dimension. Using a classifier per scanline circumvents this symmetry problem and in practice, due to distortion of the projector, classes never repeat within a scanline. As future work, we could exploit these repetitions to reduce the total number of class labels and classifiers.</p><p>Subpixel Accuracy Class labels need to support subpixel shifts of the pattern to avoid quantization of disparity maps. In order to obtain subpixel accuracy, each class is additionally divided into subclasses, equal to the desired level of subpixel precision. In our case, each step is 0.05 pixels, meaning class i to class i + 1 has 20 additional subpixel labels. From now on the labels c can assume continuous values. It is natural to recast this second step into a regression problem: we first predict the class, then a regression function will produce the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">HyperDepth Algorithm</head><p>The core part of our algorithm is a recognizer that predicts continuous class labels independently for each pixel in the image I. Pixels on the same scanline will share the same recognizer. In this work we resorted to an ensemble of random forests per scanline, which have shown great performances in pixel-wise classification using very simple sparse features <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b13">13]</ref>. Decision trees <ref type="bibr" target="#b7">[8]</ref> can be naturally used to train a mixed objective function: we start with a classification objective and then we switch to a regression one to obtain subpixel accuracy.</p><p>Given an input pixel p = (x, y) and the infrared image I, a random forest infers a probability distribution p(c|p, I) over the C classes. The forest learns to map the pixel into one of the classes looking at its spatial context.</p><p>Node Features: Each split node contains a set of learned parameters δ = (u, v, θ), where (u, v) are 2D pixel offsets and θ represents a threshold value. The split function f is evaluated at pixel x as</p><formula xml:id="formula_0">f (p; θ) = L if I(p + u) − I(p + v) &lt; θ R otherwise<label>(1)</label></formula><p>where I is the input IR image. This kind of pixel difference test is commonly used with decision forest classifiers due to its efficiency and discriminative power. The features are also invariant to illumination variations, which helps to generalize across different ambient light levels. The relative offsets u and v are sampled within a maximum patch size of 32 × 32 1 , which uniquely identifies all the patterns in the scanline.</p><p>Note the forest predictions are extremely efficient as only a small set of these simple feature tests are performed for each pixel. Furthermore, each pixels (and associated trees) can be processed in parallel.</p><p>Training We assume ground truth data for the class labels are available, in the next subsection we describe in details the acquisition procedure. For each scanline of the image we train multiple trees independently on a subset S of the training data. For the first few levels of the tree we consider classes as integer values, solving the classification problem. For our application, set S contains training examples (p, c) where p identifies a pixel within a particular training image and c is the pixel's ground truth label. Starting at the root, a set of candidate split function parameters δ are proposed at random. For each candidate, S is partitioned into left S L (δ) and right S R (δ) child sets, according to Eq. 1. The objective function</p><formula xml:id="formula_1">Q(δ) = E(S) − d∈{L,R} |S d (δ)| |S| E(S d (δ)) .<label>(2)</label></formula><p>is evaluated given each of these partitions, and the candidate δ that maximizes the objective is chosen. The entropy E(S) is the Shannon entropy of the (discrete) empirical distribution p(c|S) of the class labels c in S:</p><formula xml:id="formula_2">E(S) = − C c=1 p(c|S) log p(c|S), with<label>(3)</label></formula><formula xml:id="formula_3">p(c|S) = 1 |S| (·,·,c ′ )∈S [c = c ′ ] .<label>(4)</label></formula><p>Training then continues greedily down the tree, recursively partitioning the original set of training pixels into successively smaller subsets. Training stops when a node reaches a maximum depth, contains too few examples, or has too low entropy or differential entropy. After we learned integer disparities, we continue the training with a regression function for the last 6 levels of the trees in order to obtain subpixel accuracy. In particular we maximize Eq. 2, where the entropy is generalized to handle continuous values. We tested other regression objective functions, based on the variance computed from the samples (p, c) ∈ S, however they led to considerably worse results. We also tried a direct regression approach, but given the complexity of the problem we noticed substantial overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training Data Generation</head><p>In the previous section we assumed the availability of class labels for training. Recall that disparity d = c − x, where x is the horizontal position of the pixel in the image and c =x the class label. Hence, given a pixel p = (x, y) and disparity d, we can generate a class label via c = d + x. To compute disparities for training, we have to recover the reference IR pattern R, the relative pose of R with respect to I and then rectify both R and I. Finally, an accurate stereo matching algorithm computes a disparity map on the rectified images that are used for training. Note that when using the Kinect sensor with our approach, we could directly use the depth maps from the Kinect to generate training labels. However, we found that the depth maps from Kinect are affected by a high level of quantization. Moreover this procedure would not generalize to other structured light systems where Kinect output is not available.</p><p>To infer the reference pattern, we leverage the calibration procedure proposed by Mcllroy et al. <ref type="bibr" target="#b29">[29]</ref>. In contrast to this prior work that uses a static camera and a moving IR projector, in our setup we have a rigid assembly of the camera and projector, which we exploit in our calibration process. We first calibrate the intrinsic parameters of the IR camera and we capture images of a flat surface by moving the camera-projector assembly to multiple positions. Similar to <ref type="bibr" target="#b29">[29]</ref>, we then solve a non-linear optimization via Levenberg-Marquardt to obtain the pattern projected by the IR projector, and the extrinsics of the projector with respect to the camera. We assume that the projector is a virtual camera that is located at a known initial displacement from the camera (the approximate physical location of the projector with respect to the camera), which is used to initialize the solver. In addition to the extrinsics, we recover the reference pattern as seen at the image plane resulting in a simple camera model for the projector with zero distortion. The recovered reference pattern is shown in the supplementary material. Note that similar to <ref type="bibr" target="#b29">[29]</ref> we recover the distorted pattern as seen at the image plane. Given this reference pattern and the calibration parameters of camera-projector setup we perform a standard bilinear stereo rectification to recover the rectified reference pattern and the rectified IR image.</p><p>Once the rectified reference pattern and the current rectified IR image are available, we can generate disparity maps using an accurate, but offline, stereo matching method. For our training data generation we use PatchMatch stereo <ref type="bibr" target="#b6">[7]</ref>, which is a state of the art local stereo matching approach that in comparison to other methods gives excellent reconstruction quality for slanted/curved surfaces and estimates sub-pixel disparities without quantization. Note that a local method performs very well for our scenario since there are virtually no untextured regions in the input images. In the future, more sophisticated global stereo methods could be employed to further raise the quality of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Runtime Algorithm</head><p>At runtime the inputs of the algorithm are the IR image I and H random forest classifiers, one per scanline, where each random forest comprises of F trees. Given a pixel p = (x, y), the output of a single tree f will be:</p><formula xml:id="formula_4">c f = arg max c p (y,f ) (c|p, I),<label>(5)</label></formula><p>where p (y,f ) (·) denotes the probability of the floating point class c computed with the f -th tree for the scanline y. The aggregation strategy we used among different trees is based on their agreement on the current classĉ f . Letĉ 1 andĉ 2 the output of 2 different trees. If |ĉ 1 −ĉ 2 | &lt; 0.2 we aggregate the predictionĉ = p1ĉ1+p2ĉ2 p1+p2 as well as the expected probabilitiesp = p 1 + p 2 . The disparity d =ĉ − x is then assigned to the pixel p based on the highest scorep.</p><p>Invalidation Criteria We invalidate pixels with inaccurately predicted disparities by using the posterior p(c|p, I) as indication of the confidence for the prediction. In particular, we use the following invalidation criteria:</p><p>• Signal Check. We invalidate pixels that traverse the random forest and do not observe a sufficient amount of signal τ = 500 in the IR image I.</p><p>• Probability Check. If the probabilityp of the winning classĉ is smaller than 0.6 we invalidate the pixel.</p><p>• Winners Check. We make sure that the top 2 predictions are consistent, i.e. they lie in a very close disparity range: we invalidate if |ĉ 1 −ĉ 2 | &gt; 1.</p><p>• Disparity Check. If the predicted label is wrong the disparity d =ĉ − x could belong to a non valid range. We invalidate every d &lt; 0 and every d &gt; 422, which is the maximum number of possible disparities.</p><p>For our model configuration, with 4 trees and 12 levels, only 48 pixel differences are processed to compute depth per pixel. The running time does not depend on number of disparities or patch size and it is fully parallel for each pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We now systematically evaluate our proposed algorithm. It is important to highlight that all results shown were computed on completely new scenes that are not part of the training data. Unless noted otherwise, we use the cameraprojector hardware of the Microsoft Kinect and access the raw IR images using OpenNI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">HyperDepth Parameters</head><p>We first show how we set the HyperDepth parameters and analyze the computational performance. In order to train the algorithm we acquired 10000 frames generating training labels using PatchMatch stereo as described in Sec 2.3. We trained up to 5 trees per line with 15 levels per tree. Training 1 tree per line takes 1 day on a single GPU implementation on a NVIDIA Titan X.</p><p>We test the performance of our method on 1000 images that were acquired completely independently from our training data. <ref type="figure" target="#fig_1">Fig. 2</ref> evaluates different configurations of our models on the test data. The top graph in <ref type="figure" target="#fig_1">Fig. 2</ref> shows the absolute disparity difference between PatchMatch and our <ref type="figure">Figure 3</ref>. Qualitative Comparisons. We compare disparity maps generated on the test data with PatchMatch and HyperDepth. Note how we generalize even in regions where PatchMatch is invalidated. model with respect to the number of levels per tree. We can see that after level 14 little improvement is gained since the disparity error goes down by only 0.005 pixels on the average. Similarly, more than 4 trees show little improvement, only 0.001 in disparity error.</p><p>To see how this disparity error translates to depth error, <ref type="figure" target="#fig_1">Fig. 2</ref>, middle graph, shows a similar analysis in the depth domain. Notice how the method exhibits very low error (&lt; 1cm) up to 3 meters. As a baseline, we also plot the theoretical error for a high quality stereo system, with disparity precision of 0.25 pixels (note: this precision is below the Middlebury subpixel benchmark of 0.5 pixels). We show that our error would be 3 times lower than this baseline. This also shows that our algorithm is able to model the disparity prediction within the precision of our training data (PatchMatch stereo) and the quality of our results is currently limited by the accuracy of the training data. <ref type="figure" target="#fig_1">Fig. 2</ref>, bottom, reports the running time on 1.3 megapixel images using a NVIDIA Titan X GPU. The top accurate configuration with 4 trees and 15 levels has a disparity error of 0.064 pixels with a running time of 2.5msec. To reach 1KHz 3 trees with 12 levels are used with an disparity error of only 0.1 pixels. In <ref type="figure">Fig. 3</ref> we show the quality of the disparity maps generated on the test data with PatchMatch and HyperDepth. Our approach computes correct disparities in image regions where PatchMatch shows holes or fattened edges. This is impressive given the fact that our method was trained with results from PatchMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Error Analysis</head><p>To quantitatively analyze the error of single depthmaps we use a setup similar to <ref type="bibr" target="#b32">[32]</ref>: we place a camera in front of a (large) white plane at multiple (known) distances <ref type="bibr">20, 50, 100, 150, 200, 250, 300, 350cm</ref>. For each distance we record multiple frames and compute the error with respect to the (known) plane equation. We depict the results in <ref type="figure" target="#fig_2">Fig. 4</ref> and <ref type="figure" target="#fig_3">Fig. 5</ref>. We compare HyperDepth with Microsoft KinectV1 (PrimeSense), Intel RealSense F200, Intel RealSense R200 and PatchMatch stereo.</p><p>As shown our depth maps contain less error. KinectV1 depth maps suffer from heavy quantization. The F200 contains higher error within the working range of our sensor &gt; 50cm, but works at a closer distance up to 20cm. Note this sensor uses temporal structured light, and clearly exhibits motion artifacts, and limited working range (from 20cm to 100cm), with a large performance degradation after 50cm. The R200 is an active stereo camera, and this exhibits extremely high error. Whilst the underlying stereo algorithm is unpublished, it clearly demonstrates the trade-off in accuracy that needs to be made to achieve real-time performance. In this experiment we also outperform the accuracy of Patch-Match: thanks to the ensemble of multiple trees per line our depthmaps we are more robust to noise.</p><p>Qualitative comparisons are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. We also analyzed the noise characteristic of the algorithm computing the standard deviation (jitter) of the depthmaps over multiple frames. Results show <ref type="figure" target="#fig_2">(Fig. 4</ref>, bottom) that our method exhibits noise level very similar to the KinectV1, which is expected. Again, the RealSense cameras poorly performed with respect to HyperDepth, KinectV1 and PatchMatch. The latter seems to have higher noise at the end of the range. We further investigate the level of quantization in KinectV1 by designing a qualitative experiment where we placed some objects at 2.5m distance from the camera and we compute the depth maps with both our method and KinectV1. We show the point clouds in <ref type="figure">Fig. 6</ref>: notice how KinectV1 depth maps are heavily quantized, whereas our method produces smooth and quantization free disparities. This is the main advantage of the regression approach, which does not explicitly test subpixel disparities but automatically recovers the output disparity with high precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Scanning Results</head><p>We evaluated the precision of the algorithm for object scanning. We generated groundtruth 3D models for multiple objects with different shape, texture and material. The groundtruth is generated via ATOS, an industrial 3D scanning technology <ref type="bibr" target="#b0">[1]</ref>. The precision of the ATOS scanner is up to 0.001mm. We then generated 360 • 3D models using our method and multiple state of the art depth acquisition technologies: KinectV1, KinectV2 (Time of Flight), Patch-Match <ref type="bibr" target="#b6">[7]</ref>, Intel RealSense F200 and RealSense R200. To  this end, we placed each object on a turntable and captured hundreds of depth maps from all viewpoints from a distance of 50cm (an exception was Intel RealSense R200 where we used the minimum supported distance of 65cm). We then feed the depth maps into KinectFusion [21] to obtain the 3D mesh. We used the same KinectFusion parameters for generating results for all methods. We then carefully aligned each generated mesh with the groundtruth scans and computed the Hausdorff distance to measure the error between the two meshes. In <ref type="figure" target="#fig_4">Fig. 7</ref> we report the reconstructed objects and their Root Mean Square Error (RMSE) from the groundtruth. Our HyperDepth consistently outperforms KinectV1 on all the objects, especially areas with high level of details are better reconstructed by our method. This is mainly due to the absence of quantization and the ability to produce higher resolution depth maps. KinectV2 is sensitive to multipath effects, causing errors in those areas where multiple reflec- <ref type="figure">Figure 6</ref>. Quantization Experiment. We show point clouds generated with KinectV1 (middle) and our HyperDepth algorithm (right). Notice the heavy quantization in the KinectV1 results, whereas our method infers precise depth. tions occur. As a result, objects are substantially deformed. Our method provides results on par with, and superior to PatchMatch, but at a fraction of the compute. Note we use PatchMatch for training data, and this shows that Hyper-Depth could be further improved given improvements in training data. Both RealSense sensors failed in capturing most of the details, due to the high noise in the depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">High-Speed Camera Setup</head><p>Our algorithm can be used to create extremely high framerate depth cameras useful for solving tracking related problems. We built a prototype sensor (see <ref type="figure" target="#fig_5">Fig. 8</ref> bottom right) capable of generating depth maps at 375Hz. We combine the Kinect IR projector and a USB3 Lumenera Lt425 camera with an IR bandpass filter. This camera reaches 375Hz with a 640 × 480 central crop of the original 4MP image. Notice that in order to operate at this framerate we use an exposure time of 2.5msec, meaning the SNR of the IR images is lower than in Kinect, making the depth estimation more challenging. We calibrated the system and generated training data for our method following the procedure described in Sec. 2.3. We tested this configuration in different sequences to prove the feasibility of high speed depth maps. In particular we show three sequences: a high speed moving hand, capturing a ping-pong ball hitting a racket, and a cup being smashed with a wooden stick. We show qualitative results on <ref type="figure" target="#fig_5">Fig. 8</ref>. HyperDepth is able to retrieve smooth disparities even in this challenging configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We have reframed the correspondence problem for spatial structured light as a learning-based classification-regression task, instead of a stereo matching task. Our novel formulation uses an ensemble of random forests, one per scan line, to efficiently solve this problem, in a pixel independent manner with minimal operations. Our algorithm is independent of matching window size or disparity levels. We have demonstrated a parallel GPU implementation that infers depth for each pixel independently at framerates over 1KHz, with 2 17 disparity levels, and no sequential propagation step. Finally we have demonstrated, high quality and high resolution, quantization free, depth maps produced by our method, with quality superior to state of the art methods for both single frame prediction, and fused 3D models. Our method can be employed in many new scenarios where high speed and high resolution depth is needed such as hand tracking and 3D scanning applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>HyperDepth Algorithm. Overview of the disparity estimation algorithm using decision trees. For each pixel p = (x, y) in the input IR image (left) we run a Random Forest (middle) that predicts the classĉ by sparsely sampling a 2D neighborhood around p. The forest starts with classification and then switches to regression to predict continuous class labelsĉ that maintain subpixel accuracy (see text for details). The mapping d =ĉ − x gives the actual disparity d (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Quantitative Experiments. (top) Our disparity error to ground truth (PatchMatch stereo) as a function of number of trees and their depth. (middle) Our depth error to ground truth (PatchMatch stereo) for different numbers of trees. (bottom) Run time of HyperDepth on 1.3 Megapixel images for different tree depths (D12, D15) and different tree numbers per scanline (T 1 to T 4). The most accurate configuration with 4 trees and depth 15 takes only 2.5msec per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Error and Noise Analysis. We plot the depth error of HyperDepth and baseline technologies for a planar target at distances between 20cm and 350cm. The average error of single depth maps is shown on the top, whereas the variance within multiple depth maps is shown in the bottom figure. Our method exhibits lower error than all baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Plane Fitting Comparison. We visualize 3D point clouds of a planar target at 1m distance. We compare our results against baseline technologies. Notice the quantization artifacts in KinectV1 and the high noise in the RealSense cameras. Our method and PatchMatch produce smoothest results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>3D Scanning Results. Quantitative comparisons between our method and state of the art depth technologies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>High Speed Camera Results. HyperDepth results recorded at 375Hz. (top) smashing a paper cup, (middle) fast moving hand, (bottom) playing ping-pong.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the tree will learn automatically to select the best offsets. The maximum window size is selected to minimize the number of bits needed to store a tree node at runtime. Currently we encode each node in 32 bits, 20 bits for two offsets u and v and 12 for the threshold θ.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We thank Cristian Canton Ferrer for contributing to the calibration process.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.gom.com/metrology-systems/3d-scanner.html.7" />
		<title level="m">Atos -industrial 3d scanning technology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH and Transaction On Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recent progress in coded structured light as a technique to solve the correspondence problem: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batlle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mouaddib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="963" to="982" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Active, optical range imaging sensors. Machine vision and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="127" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stereo matchingstate-of-the-art and research challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Breiteneder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Topics in Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="143" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple but effective tree structures for dynamic programming-based stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISAPP (2)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PatchMatch Stereo -Stereo Matching with Slanted Support Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Decision Forests for Computer Vision and Medical Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spacetime stereo: A unifying framework for depth from triangulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2003 IEEE Computer Society Conference on</title>
		<meeting>2003 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">359</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linear stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De-Maeztu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Villanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabeza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1708" to="1715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mutual information based semiglobal stereo matching on the gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to be a depth camera for close-range human capture and interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH and Transaction On Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Depth mapping using projected patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shpunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Machline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Arieli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-03" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Patent 8,150,142. 1</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient large-scale stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Structured-light 3d surface imaging: a tutorial. Advances in Optics and Photonics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="128" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Time-of-flight cameras: principles, methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hansard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Horaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toward optimal structured light patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiryati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="97" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-speed 3d image acquisition using coded structured light projection. In Intelligent Robots and Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="925" to="930" />
		</imprint>
	</monogr>
	<note>ACM UIST</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Constant time stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="13" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Projected texture stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2010 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structured light patterns for robot mobility. Robotics and Automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Le Moigne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Waxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="541" to="548" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Patch match filter: Efficient edge-aware filtering meets randomized search for fast correspondence field estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kinect unleashed: getting control over high resolution depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13. IAPR International Conference on Machine Vision Applications, MVA</title>
		<meeting>the 13. IAPR International Conference on Machine Vision Applications, MVA</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="247" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Range sensing by projecting multiple slits with random cuts. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="647" to="651" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stereo vision algorithms for fpgas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="636" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kinectrack: 3d pose estimation using a projected dense dot pattern</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcilroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Realtime stereo vision: Optimizing semi-global matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV), 2013 IEEE</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1197" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A revisit to cost aggregation in stereo matching: How far can we reduce its computational redundancy?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1567" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling kinect sensor noise for improved 3d reconstruction and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DIMPVT</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Prism: A practical real-time imaging stereo matcher mit ai memo no</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nishihara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<pubPlace>Cambridge, Mass., USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An fpga-based real-time system for 3d stereo matching, combining absolute differences and census with aggregation and belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dollas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLSI-SoC: At the Crossroads of Emerging Trends</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Surface measurement by space-encoded projected beam systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Posdamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Altschuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer graphics and image processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Monofusion: Real-time 3d reconstruction of small scenes with a single web camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bathiche</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dodgson. Real-time spatiotemporal stereo matching using the dual-cross-bilateral grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="510" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A state of the art in structured light patterns for surface profilometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pribanic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Llado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2666" to="2680" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pattern codification strategies in structured light systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pages</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batlle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="827" to="849" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Range imaging system utilizing nematic liquid crystal mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Conf. Comp. Vision</title>
		<meeting>1st Int. Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="657" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<date type="published" when="2002-04-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Range image acquisition with a single binary-encoded light pattern. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vuylsteke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oosterlinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="164" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Grid coding: A preprocessing technique for robot and machine vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="319" to="329" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stereo matching using tree filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="846" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A constant-space belief propagation algorithm for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1458" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Žbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4326</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint integral histograms and its application in stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lafruit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lauwereins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="817" to="820" />
		</imprint>
	</monogr>
	<note>Image Processing (ICIP)</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rapid shape acquisition using color structured light and multi-pass dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Data Processing Visualization and Transmission</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="24" to="36" />
		</imprint>
	</monogr>
	<note>Proceedings. First International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spacetime stereo: Shape recovery for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2003 IEEE Computer Society Conference on</title>
		<meeting>2003 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">367</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recent progresses on real-time 3d shape measurement using digital fringe projection techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics and lasers in engineering</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="158" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Real-time non-rigid reconstruction using an rgb-d camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">156</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
