<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Structured Inference Neural Networks with Label Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
							<email>hexiangh@sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<settlement>Burnaby</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Tong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<settlement>Burnaby</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
							<email>zhiweid@sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<settlement>Burnaby</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liao</surname></persName>
							<email>zliao@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
							<email>mori@cs.sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<settlement>Burnaby</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Structured Inference Neural Networks with Label Relations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Images of scenes have various objects as well as abundant attributes, and diverse levels of visual categorization are possible. A natural image could be assigned with finegrained labels that describe major components, coarsegrained labels that depict high level abstraction, or a set of labels that reveal attributes. Such categorization at different concept layers can be modeled with label graphs encoding label information. In this paper, we exploit this rich information with a state-of-art deep learning framework, and propose a generic structured model that leverages diverse label relations to improve image classification performance. Our approach employs a novel stacked label prediction neural network, capturing both inter-level and intra-level label semantics. We evaluate our method on benchmark image datasets, and empirical results illustrate the efficacy of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Standard image classification is a fundamental problem in computer vision -assigning category labels to images. It can serve as a building block for many different computer vision tasks including object detection, visual segmentation, and scene parsing. Recent progress in deep learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> significantly improved classification performance on large scale image datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref>. Approaches typically assume image labels to be semantically independent and adapt either a multi-class or binary classifier to label images. In recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>, deep learning methods that take advantage of label relations have been proposed to improve image classification performance.</p><p>However, in realistic settings, these label relationships could form a complicated graph structure. Take <ref type="figure" target="#fig_0">Figure 1</ref> as an example. Various levels of interpretation could be formed to represent such an image. This image of a baseball scene could be described as an outdoor image at coarse This image example has visual concepts at various levels, from sports field at high level to baseball and person at lower level. Our model leverages label relations and jointly predicts layered visual labels from an image using a structured inference neural network. In the graph, colored nodes correspond to the labels associated with the image, and red edges encode label relations.</p><p>level, or with a more concrete concept such as sports field, or with even more fine-grained labels such as batter's box and objects such as grass, bat, person. Models that incorporate semantic label relationships could be utilized to generate better classification results. The desiderata for these models include the ability to model label-label relations such as positive or negative correlation, respect multiple concept layers obtainable from sources such as WordNet, and to handle partially observed label data -given a subset of accurate labels for this image, infer the remaining missing labels.</p><p>The contribution of this paper is in developing a structured inference neural network that permits modeling complex relations between labels, ranging from hierarchical to within-layer dependencies. We do this by defining a net-work in which a node is activated if its corresponding label is present in an image. We introduce stacked layers among these label nodes. These encode layer-wise connectivity among label classification scores, representing dependency from top-level coarse labels to bottom-level fine-grained labels. Activations are propagated bidirectionally and asynchronously on the label relation graph, passing information about the labels within or across concept layers to refine the labeling for the entire image.</p><p>We have evaluated our method on three image classification datasets (AWA dataset <ref type="bibr" target="#b19">[20]</ref>, NUS-WIDE dataset <ref type="bibr" target="#b0">[1]</ref> and SUN397 dataset <ref type="bibr" target="#b36">[37]</ref>). Experimental results show a consistent and significant performance gain with our structured label relation model compared with baseline and related methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-level labeling of images has been addressed in a number of frameworks. In this section we review relevant work within probabilistic, max-margin, multi-task, and deep learning. Structured label prediction with external knowledge: Structured prediction approaches exist <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>, in which a set of class labels are predicted jointly under a fixed loss function. Traditional approaches learn graph structure as well as associated weights that best explain the training data (e.g., <ref type="bibr" target="#b2">[3]</ref>). When external knowledge of label relations (e.g., a taxonomy) is available, it is beneficial to integrate this knowledge to guide the traditional supervised learning systems. For example, Grauman et al. <ref type="bibr" target="#b7">[8]</ref> and Hwang et al. <ref type="bibr" target="#b10">[11]</ref> took the WordNet category taxonomy to improve visual recognition. Johnson et al. <ref type="bibr" target="#b13">[14]</ref> and McAuley and Leskovec <ref type="bibr" target="#b21">[22]</ref> used metadata from a social network to improve image classification. Ordonez et al. <ref type="bibr" target="#b23">[24]</ref> leveraged associated image captions (words of "naturalness") to estimate entry-level labels of visual objects. Multi-label classification with label relations: Traditional multi-label classification cannot avoid predicting an image as both cat and dog, or an image as carnation but not flower. Using external knowledge of label relations, Deng et al. <ref type="bibr" target="#b1">[2]</ref> proposed a representation, the HEX graph, to express and enforce exclusion, inclusion and overlap relations between labels in multi-label classification. This model was further extended for "soft" label relations using the Ising model by Ding et al. <ref type="bibr" target="#b4">[5]</ref>. Structured model with convolutional neural networks (CNNs): Structured deep models extend traditional CNNs to applications of structured label prediction, for which the CNN model is found insufficient to learn implicit constraints or structures between labels. Structured deep learning therefore jointly learns a structured model with the CNN framework. For example, for human pose estimation, Tompson et al. <ref type="bibr" target="#b32">[33]</ref> take the CNN predictions as unary po-tentials for body parts and feed them to a MRF-like spatial model, which further learns pairwise potentials of part relations. Schwing and Urtasun <ref type="bibr" target="#b27">[28]</ref> proposed a structured deep network by concatenating a densely connected MRF model to a CNN for semantic image segmentation, in which the CNN provides unary potentials as the MRF model imposes smoothness. Deng et. al. <ref type="bibr" target="#b3">[4]</ref> proposed a recurrent network that jointly performs message passing-style inference and learns graph structure for group activity recognition.</p><p>Our work combines these lines of work. We take the WordNet taxonomy as our external knowledge, expressing it as a label relation graph, and learning the structured labels within a deep network framework. Our contribution is in proposing a learning and inference algorithm that facilitates knowledge passing in the deep network based on label relations. Multi-task joint learning: Multi-task learning follows the same spirit of structured label prediction, with the distinction that the outputs of multiple (different but related) tasks are estimated. Common jointly modeled tasks include segmentation and detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref>, segmentation and pose estimation <ref type="bibr" target="#b15">[16]</ref>, or segmentation and object classification <ref type="bibr" target="#b20">[21]</ref>. An emerging topic of joint learning is in image understanding and text generation by leveraging intramodal correspondences between visual and human language <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Our work can be naturally extended to multi-task learning, for which each layer of our model represents one task and the labels do not necessarily form a layered structure. Notably, we can improve existing multi-task learning methods by importing knowledge of intra-task label relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our model jointly classifies images in a layered label space with external label relations. The goal is to leverage the label relations to improve inference over the layered visual concepts. We build our model on top of a state-of-the-art deep learning platform: given an image, we first extract CNN features from Krizhevsky et al. <ref type="bibr" target="#b17">[18]</ref> as visual activations at each concept layer. Concept layers are stacked from finegrained to coarser levels. Label relations are defined between consecutive layers and form a layered graph. Inference over the label relation graph is inspired by the recent success of Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref>, where we treat each concept layer as a timestep of a RNN. We connect neighboring timesteps to reflect the inter-layer label relations, while capturing intra-layer relations within each timestep. The label activations are propagated bidirectionally and asynchronously in the label relation graph to refine labeling for the given image. <ref type="figure">Figure 2</ref> shows an overview of our classification pipeline.</p><p>We denote the collection of training images as  <ref type="figure">Figure 2</ref>. The label prediction pipeline. Given an input image, we extract CNN features at the last fully connected layer as activation (in blue box) at different visual concept layers. We then propagate the activation information in a label (concept) relation graph through our structured inference neural network (in red box). The final label prediction (in green box) is made from the output activations (in yellow box) obtained after the inference process.</p><formula xml:id="formula_0">{I i } N i=1 ,</formula><p>each with ground-truth label in every concept layer. We denote the labels of image I i as {y i t } T t=1 , where T is the total number of concept layers. And each concept layer t has n t labels. The CNN framework of Krizhevsky et al. <ref type="bibr" target="#b17">[18]</ref> transforms each image I i into a 4096-dimensional feature vector, denoted as CN N (I i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Framework</head><p>It is straightforward to build an image classification model, by adding a loss layer on top of the CNN features for each concept layer. Specifically, the activations on concept layer t are computed as</p><formula xml:id="formula_1">x i t = W t · CN N (I i ) + b x,t ,<label>(1)</label></formula><p>where W t ∈ R nt×4096 and b x,t ∈ R nt×1 are linear transformation parameters and biases to classify the n t labels at concept layer t. Note that x i t ∈ R nt×1 provides visual activation depending purely on the image I i . To generate label-specific probabilities, we can simply apply a sigmoid function (i.e., σ(z) = 1 1+e −z ) on the elements of x i t . This classification model does not accommodate label relations within or across concept layers. To leverage the benefit of label relations, we adopt an RNN-like inference framework. In the following, we first describe a top-down inference model, then a bidirectional inference model, and lastly propose our Structured Inference Neural Network, the SINN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Top-Down Inference Neural Network</head><p>Our model is inspired by the recent success of RNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>, which make use of dynamic sequential information in learning. RNNs are called recurrent models because they perform the same computation for every timestep, with the input dependent on the current inputs and previous outputs. We apply a similar idea to our layered label prediction problem: we consider each concept layer as an individual timestep, and model the label relations within and across concept layers in the recurrent learning framework.</p><p>Specifically, at each timestep t, we compute an image I i 's activations a i t ∈ R nt×1 based on two terms: a i t−1 ∈ R nt−1×1 , which are the activations from the last timestep t − 1, and x i t ∈ R nt×1 , which are the activations from Eq. (1). The message passing process is defined as:</p><formula xml:id="formula_2">a i t = V t−1,t · a i t−1 + H t · x i t + b a,t ,<label>(2)</label></formula><p>where V t−1,t ∈ R nt×nt−1 are the inter-layer model parameters capturing the label relations between two consecutive concept layers in top-down order, H t ∈ R nt×nt are the intra-layer model parameters to account for the label relations within each concept layer, and b a,t ∈ R nt×1 are the model biases. A sigmoid function can be applied to a i t to obtain label-specific prediction probabilities for image I i .</p><p>Note that the inference process in Eq. <ref type="formula" target="#formula_2">(2)</ref> is different from the standard RNN learning: Eq. (2) unties V t−1,t and H t in each timestep, while the standard RNNs learn the same V and H parameters over and over on all timesteps.</p><p>To learn the model parameters V 's and H's, we apply a sigmoid function function σ on the activations a i t , and minimize the logistic cross-entropy loss with respect to V 's and H's:</p><formula xml:id="formula_3">E({a i t }) = N i=1 T t=1 nt y=1 1(y i t = y) · log σ(a i t ) + 1(y i t = y) · log 1 − σ(a i t ) ,<label>(3)</label></formula><p>where 1(z) is an indicator function which returns 1 if z is true and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">BINN: Bidirectional Inference Neural Network</head><p>It makes more sense to model bidirectional inferences, as a concept layer is related to the two connected layers equally well. Therefore, we adopt the idea of bidirectional recurrent neural network <ref type="bibr" target="#b26">[27]</ref>, and propose the following bidirectional inference model:</p><formula xml:id="formula_4">− → a i t = − → V t−1,t · − → a i t−1 + − → H t · x i t + − → b t , (4) ← − a i t = ← − V t+1,t · ← − a i t+1 + ← − H t · x i t + ← − b t ,<label>(5)</label></formula><formula xml:id="formula_5">a i t = − → U t · − → a i t + ← − U t · ← − a i t + b a,t .<label>(6)</label></formula><p>where Eqs. <ref type="formula">(4)</ref> and <ref type="formula" target="#formula_4">(5)</ref> proceed as top-down propagation and bottom-up propagation, respectively, and Eq. <ref type="formula" target="#formula_5">(6)</ref> aggregates the top-down and bottom-up messages into final activations for label prediction. Here − → U t ∈ R nt×nt and ← − U t ∈ R nt×nt are aggregation model parameters, and we use the arrows → and ← to indicate the directions of label propagation</p><p>As in the top-down inference model, the bidirectional inference model captures both inter-layer and intra-layer label relations in the model parameters V 's and H's. For interlayer relations, we connect a label in one concept layer to any label in its neighboring concept layers. For intra-layer relations, we model fully-connected relations within each concept layer. The model parameters V 's, H's and U 's are learned by minimizing the cross-entropy loss defined in Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">SINN: Structured Inference Neural Network</head><p>The fully connected bidirectional model is capable of representing all types of label relations. In practice, however, it is hard to train a model on limited data due to the large number of free parameters. To avoid this problem, we use a structured label relation graph to restrict information propagation.</p><p>We use structured label relations of positive correlation and negative correlation as prior knowledge to refine the model. Here is the intuition: since we know that office is an indoor scene, beach is an outdoor scene, and indoor and outdoor are mutually exclusive, a high score on indoor should increase the probability of label office and decrease the probability of label beach. Labels that are not semantically related, e.g. motorcycle and shoebox, should not affect each other. The structured label relations can be obtained from semantic taxonomies, or by parsing WordNet relations <ref type="bibr" target="#b22">[23]</ref>. We describe the details of extracting label relations in Section 4.</p><p>We introduce the notation V + , V − , H + and H − to explicitly capture structured label relations in between and within concept layers, where the superscripts + and − indicate positive and negative correlation, respectively. These model parameters are masked metrics capturing the label relations. Instead of learning full parametrized metrics of V + , V − , H + and H − , we freeze some elements to be zero if there is no semantic relation between the corresponding labels. For example, V + models the positive correlation in between two concept layers: only the label pairs that have positive correlation have learnable model parameters, while the rest are zeroed out to remove potential noise. A similar setting goes to V − , H + and H − . <ref type="figure" target="#fig_2">Figure 3</ref> shows an example positive correlation graph and a negative graph between two layers.  To implement the positive and negative label correlation, we propose the following structured message passing process:</p><formula xml:id="formula_6">− → a i t = γ( − → V + t−1,t · − → a i t−1 ) + γ( − → H + t · x i t ) (7) −γ( − → V − t−1,t · − → a i t−1 ) − γ( − → H − t · x i t ) + − → b t , ← − a i t = γ( ← − V + t+1,t · ← − a i t+1 ) + γ( ← − H + t · x i t ) (8) −γ( ← − V − t+1,t · ← − a i t+1 ) − γ( ← − H − t · x i t ) + ← − b t , a i t = − → U t · − → a i t + ← − U t · ← − a i t + b a,t .<label>(9)</label></formula><p>Here γ(·) stands for a ReLU activation function. It is essential for SINN as it enforces that activations from positive correlation always make positive contribution to output activation and keeps activations from negative correlation as negative contribution (notice the minus signs in Eqs <ref type="formula">(7)</ref> and <ref type="formula">(8)</ref>). To learn the model parameters V 's, H's, and U 's, we optimize the cross-entropy loss in Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Label Prediction</head><p>Now we introduce the method of predicting labels in test images with our model. As the model is trained with multiple concept layers, it is straightforward to recognize a label at each concept layer for the provided test image. This mechanism is called label prediction without observation (the default pipeline shown in <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SINN Prediction with Partial Human Labels</head><p>Activations from partial human labels  <ref type="figure">Figure 4</ref>. The label prediction pipeline with partial observation. The pipeline is similar to <ref type="figure">Figure 2</ref> except that we now have a partial observation that this image is outdoor man-made. The SINN is able to take the observed label into consideration and improve the label predictions in the other concept layers.</p><p>A more interesting application is to make predictions with partial observations -we want to predict labels in one concept layer given labels in another concept layers. <ref type="figure">Figure 4</ref> illustrates the idea. Given an image shown in the left side of <ref type="figure">Figure 4</ref>, we have more confidence to predict it as batter box once we know it is an outdoor image with attribute sports field.</p><p>To make use of the partially observed labels in our SINN framework, we need to transform the observed binary labels into soft activation scores for SINN to improve the label prediction on the target concept layers. Recall that SINN minimizes cross-entropy loss which applies sigmoid functions on activations to generate label confidences. Thus, we reverse this process by applying the inverse sigmoid function on the binary ground-truth labels to obtain activations. Formally, we define the activation a obtained from a groundtruth label y as:</p><formula xml:id="formula_7">g(y) = log 1 1−(y+ǫ) , if y = 0, log 1 1−(y−ǫ) , if y = 1.<label>(10)</label></formula><p>Note that we put a small perturbation ǫ on the ground-truth label y for numerical stability. In our experiments, we set ǫ = 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>To optimize our learning objective, we use stochastic gradient descent with mini-batch size of 50 images and momentum of 0.9. For all training runs, we apply a two-stage policy as follows. In the first stage, we fixed pre-trained CNN networks, and train our SINN with a learning rate of 0.01 with fixed-size decay step. In the second stage, we set the learning rate as 0.0001 and fine-tune the CNN together with our SINN. We set the gradient clipping threshold to be 25 to prevent gradient explosion. The weight decay value for our training procedure is set to 0.0005.</p><p>In the computation of visual activations from the CNN, as different experiment datasets describe different semantic domains, we adopt different pretrained CNN models: Im-ageNet pretrained model <ref type="bibr" target="#b12">[13]</ref> for experiments 4.1 and 4.2, placenet pretrained model <ref type="bibr" target="#b37">[38]</ref> for experiment 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We tested our method on three large-scale benchmark image datasets: the Animals with Attributes dataset (AwA) <ref type="bibr" target="#b19">[20]</ref>, the NUS-WIDE dataset <ref type="bibr" target="#b0">[1]</ref>, and the SUN397 dataset <ref type="bibr" target="#b36">[37]</ref>. Each dataset has different concept layers and label relation graphs. Experimental results show that (1) our method effectively boosts classification performance using the label relation graphs; (2) our SINN model consistently outperforms baseline classifiers and related methods in all experiments; and (3) particularly, the SINN model achieves significant performance gain with partial human labels. Dataset and Label relation generation The AwA dataset contains an 85-attribute layer, a 50-animal-category layer and a 28-taxonomy-term layer. We extract the label relations from the WordNet taxonomy knowledge graph <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. The NUS-WIDE dataset is composed of Flickr images with 81 object category labels, 698 image group labels from image metadata, and 1000 noisy tags collected from users. We parse WordNet to obtain label similarity, and threshold the soft similarity values into positive and negative correlation for the label graph. The SUN397 dataset has a typical hierarchical structure in label space, with 397 fine-grained scene categories on the bottom layer, 16 general scene categories on middle layer, and 3 coarsest categories on the top. Here the label relations are also extracted from WordNet. Baseline. For each experiment, we compare our full method (CNN + SINN) with the baseline method: CNN + logistic regression. With further specifications, we may have extra baseline methods, such as CNN + BINN, CNN + logistic regression + extra tags, etc. We also compare our method with related state-of-the-art methods. Evaluation metrics. We measure classification performance by mean average precision (mAP ) in all comparisons. mAP is a widely used metric for label-based retrieval and ranking. It measures the averaged performance over all label categories. In addition to mAP , we also adopted various metrics for special cases.</p><p>In the case of NUS-WIDE, the task is multi-label classification. We adopt the setting of <ref type="bibr" target="#b13">[14]</ref> and report mAP per label (mAP L ) and mAP per image (mAP I ) for easy comparison. For comparison with related works ( <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref>) on NUS-WIDE, we also compute the per image and per label precisions and recalls. We abbreviate these metrics as P rec L for precision per label, P rec I for precision per image, Rec L for recall per label, and Rec I for precision per image.</p><p>For AwA and SUN397, we also compute the multi-class accuracy (M CAcc) and the intersection-over-union accuracy (IoU Acc). M CAcc is a standard measurement for image classification problems. It averages per class accuracies as the final result. IoU Acc is a common prediction measurement for multi-label classification, based on the hamming distance of predicted labels to ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">AwA: Layered Prediction with Label Relations</head><p>This experiment demonstrates the label prediction capability of our SINN model and the effectiveness of adding structured label relations for label prediction. We run each method five times with five random splits -60% for training and 40% for test. We report the average performance as well as the standard deviation of each performance measure.</p><p>Note that there is very little related work with layered label prediction on AwA. The most relevant one is work by Hwang and Sigal <ref type="bibr" target="#b11">[12]</ref> on unified semantic embedding (USE). The comparison is not strictly fair, as the train/test splits are different. Further, we include our BINN model without specifying the label relation graphs (see Section 3.1.2) as a baseline method in this experiment, as it can verify the performance gain in our model from including structure. The results are in <ref type="table">Table 1</ref>.</p><p>Results. <ref type="table">Table 1</ref> shows that our method outperforms the baseline methods (CNN + Logistics and CNN + BINN variants) as well as the USE method, in terms of each concept layer and each performance metric. It validates the efficacy of our proposed model for image classification. Note that for the results in <ref type="table">Table 1</ref>, we did not finetune the first seven layers of CNN <ref type="bibr" target="#b17">[18]</ref> for fairer comparison with Hwang and Sigal <ref type="bibr" target="#b11">[12]</ref> (which only makes use of DECAF features <ref type="bibr" target="#b5">[6]</ref>). Fine-tuning the first seven CNN layers further improves IoU Acc at each concept layer to 86.06 ± 0.72 (28 taxonomy terms), 69.17 ± 1.00 (50 animal classes), 88.22 ± 0.38 (85 attributes), and mAP L to 94.17 ± 0.55 (28 taxonomy terms), 83.12 ± 0.69 (50 animal classes), 96.72 ± 0.20 (85 attributes), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">NUS-WIDE: Multi-label Classification with Partial Human Labels of Tags and Groups</head><p>This experiment shows our model's capability to use noisy tags and structured tag-label relations to improve multi-label classification. The original NUS-WIDE dataset consists of 269,648 images collected from Flickr with 81 ground-truth concepts. As previous work used various evaluation metrics and experiment settings, and there are no fixed train/test splits, it is hard to make direct comparisons. Also note that a fraction of previously used images are unavailable now due to Flickr copyright.</p><p>In order to make our result as comparable as possible, we tried to set up the experiments according to previous work. We collected all available images and discard images with missing labels as previous work did <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>, and got 168,240 images of the original dataset. To make our result comparable with <ref type="bibr" target="#b13">[14]</ref>, we use 5 random splits with the same train/test ratio as <ref type="bibr" target="#b13">[14]</ref> -there are 132,575 training images and 35,665 test images in each split.</p><p>To compare our method with <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14]</ref>, we also used the tags and metadata groups in our experiment. Different from their settings, instead of augmenting images with 5000 tags, we only used 1000 tags, and augment the image with 698 group labels obtained from image medatada to form a threelayer group-concept-tag graph. Instead of using the tags  as sparse binary input features (as in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14]</ref>), we convert them to observed labels and feed them to our model.</p><p>The baselines for comparison are as follows. As our usual baseline, we extract features from a CNN pretrained on ImageNet <ref type="bibr" target="#b24">[25]</ref> and train a logistic classifier on top of it. In addition, we set up a group of baselines that make use of the groups and tags as binary indicator feature vectors for logistic regression. These baselines serve as the control group to evaluate the quality of metadata we used in SINN. Next, a stronger baseline that uses both CNN output and metadata vector with logistic classifier was evaluated. This method has a similar setting as that of the state-of-art method by Johnson et al. <ref type="bibr" target="#b13">[14]</ref>, with difference in visual feature (CNN on image in our method versus CNN on image neighborhood) and tag feature (1k tag vector versus 5k tag vector).</p><p>We report our results on this dataset with two settings for our SINN, the first using 1k tags as the only observa-tions to a bottom level of the relation graph. This method provides a good comparison to the tag neighborhood + tag vector <ref type="bibr" target="#b13">[14]</ref>, as we did not use extra information other than tags. In the second setting, we make both group and tag levels observable to our SINN, which achieves the best performance. We also compared our results with that of McAuley et al. <ref type="bibr" target="#b21">[22]</ref>, Gong et al. <ref type="bibr" target="#b6">[7]</ref>. The results are summarized in <ref type="table">Table 2</ref>. Note that we did not report our performance with fine-tuning the first seven layers of the CNN in this table, so as to make direct comparison of structured inference on SINN with our baseline method CNN + Logistics. Finetuned CNN with SINN improves mAP L to 70.01 ± 0.40 and mAP I to 83.68 ± 0.13. <ref type="table">Table 2</ref> shows that our proposed method outperforms all baseline methods and existing approaches (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref>) by a large margin. Note that the results are not directly comparable due to different settings in train/test splits. However, the results show that, by modeling la- bel relations between tags, groups and concepts, our model achieves dramatic improvement on visual prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>We visualize some results in <ref type="figure" target="#fig_3">Figure 5</ref> showing exemplars on which our method improves over baseline predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">SUN397: Improving Scene Recognition with and without partially Observed Labels</head><p>We conducted two experiments on the SUN397 dataset. The first experiment is similar to the study on AwA: we applied our model to layered image classification with label relations, and compare our model with CNN + Logistics and CNN + BINN baselines, as well as a state-of-the-art approach <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref>. For fair comparison, we used the same train/test split ratio as <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref>, where we have 50 training and test images in each of the 397 scene categories. To migrate the randomness in sampling, we also repeat the experiment 5 times and report the average performance as well as the standard deviations. The results are summarized in Table 3, showing that our proposed method again achieves a considerable performance gain over all the compared methods.</p><p>In the second experiment, we considered partially observed labels from the top (coarsest) scene layer as input to our inference framework. In other words, we assume we know whether an image is indoor, outdoor man-made, or outdoor natural. We compare the 397 fine-grained scene recognition performance in <ref type="table">Table 4</ref>. We compare to a set of baselines, including CNN + Logistics + Partial Labels,  <ref type="table">Table 4</ref>. Recognition results on the 397 fine-grained scene categories. Note that the last two compared methods make use of partially observed labels from the top (coarsest) scene layer, i.e., indoor, outdoor man-made, and outdoor natural.</p><p>that considers the partial labels as an extra binary indicator feature vector for logistic regression. Results show that our method combined with partial labels (i.e., CNN + SINN + Partial Labels) improves over baselines, exceeding the second best by 4% M CAcc and 6% mAP L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a structured inference neural network (SINN) for layered label prediction. Our model makes use of label relation graphs and concept layers to augment inference of semantic image labels. Beyond this, our model can be flexibly extended to consider partially observed human labels. We borrow the idea of RNNs to implement our SINN model, and combine it organically with an underlying CNN visual output. Experiments on three benchmark image datasets show the effectiveness of the proposed method in standard image classification tasks. Moreover, we also demonstrate empirically that label prediction is further improved once partially observed human labels are fed into the SINN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. This image example has visual concepts at various levels, from sports field at high level to baseball and person at lower level. Our model leverages label relations and jointly predicts layered visual labels from an image using a structured inference neural network. In the graph, colored nodes correspond to the labels associated with the image, and red edges encode label relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An example showing the model parameters V + and V − between the animal layer and the attribute layer. Green edges in the graph represent positive correlation, and red edges represent negative correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visualization results (best viewed in color). We pick up 10 representative images from NUS-WIDE, and visualize the predicted labels of our method compared with CNN + Logistics. Under each image, we provide the ground-truth labels for ease of reference, and list the top-3 highest scoring predicted labels for each compared method. Correct predictions are marked in blue and incorrect predictions are in red. Failure cases are shown in the rightmost column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Layered label prediction results on the AwA dataset.</figDesc><table>Concept Layer 
Method 
M CAcc 
IoU Acc 
mAPL 

28 taxonomy terms 

CNN + Logistics 
-
80.41 ± 0.09 90.16 ± 0.10 
CNN + BINN 
-
79.85 ± 0.13 89.92 ± 0.07 
CNN + SINN 
-
84.47 ± 0.38 
84.47 ± 0.38 
84.47 ± 0.38 93.00 ± 0.29 
93.00 ± 0.29 
93.00 ± 0.29 

50 animal classes 

USE [12] + DECAF [6] 46.42 ± 1.33 
-
-
CNN + Logistics 
78.44 ± 0.27 62.75 ± 0.26 78.35 ± 0.19 
CNN + BINN 
79.00 ± 0.43 62.80 ± 0.25 78.88 ± 0.35 
CNN + SINN 
79.36 ± 0.43 
79.36 ± 0.43 
79.36 ± 0.43 66.60 ± 0.43 
66.60 ± 0.43 
66.60 ± 0.43 81.19 ± 0.14 
81.19 ± 0.14 
81.19 ± 0.14 

85 attributes 

CNN + Logistics 
-
81.29 ± 0.10 93.29 ± 0.12 
CNN + BINN 
-
80.64 ± 0.13 93.04 ± 0.13 
CNN + SINN 
-
86.92 ± 0.18 
86.92 ± 0.18 
86.92 ± 0.18 96.05 ± 0.07 
96.05 ± 0.07 
96.05 ± 0.07 
Table 1. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>5k tags + Logistics<ref type="bibr" target="#b13">[14]</ref> 43.88 ± 0.32 77.06 ± 0.14 47.52 ± 2.59 46.83 ± 0.89 71.34 ± 0.16 51.18 ± 0.16 Tag neighbors + 5k tags<ref type="bibr" target="#b13">[14]</ref> 61.88 ± 0.36 80.27 ± 0.08 57.30 ± 0.44 54.74 ± 0.63 75.10 ± 0.20 53.46 ± 0.09 CNN + Logistics 46.94 ± 0.47 72.25 ± 0.19 45.03 ± 0.44 45.60 ± 0.35 70.77 ± 0.21 51.32 ± 0.14 1k tags + Logistics 50.33 ± 0.37 66.57 ± 0.12 23.97 ± 0.23 47.40 ± 0.07 64.95 ± 0.18 47.40 ± 0.07 1k tags + Groups + Logistics 52.81 ± 0.40 68.04 ± 0.12 25.54 ± 0.24 49.26 ± 0.15 65.99 ± 0.15 48.13 ± 0.05 1k tags + Groups + CNN + Logistics 54.67 ± 0.57 77.81 ± 0.22 50.83 ± 0.53 49.36 ± 0.30 75.38 ± 0.16 54.61 ± 0.09 1k tags + CNN + SINN 67.20 ± 0.60 81.99 ± 0.14 59.82 ± 0.12 57.02 ± 0.57 78.78 ± 0.13 56.84 ± 0.07 1k tags + Groups + CNN + SINNTable 2. Results on NUS-WIDE. We measure precision P reL, P reI and recall RecL, RecI with n = 3 labels for each image.Table 3. Layered label prediction results on the SUN397 dataset.</figDesc><table>Method 
mAPL 
mAPI 
RecL 
P recL 
RecI 
P recI 

Graphical Model [22] 
49.00 
-
-
-
-
-
CNN + WARP [7] 
-
-
35.60 
31.65 
60.49 
48.59 
69.24 ± 0.47 
69.24 ± 0.47 
69.24 ± 0.47 82.53 ± 0.15 
82.53 ± 0.15 
82.53 ± 0.15 60.63 ± 0.67 
60.63 ± 0.67 
60.63 ± 0.67 58.30 ± 0.33 
58.30 ± 0.33 
58.30 ± 0.33 79.12 ± 0.18 
79.12 ± 0.18 
79.12 ± 0.18 57.05 ± 0.09 
57.05 ± 0.09 
57.05 ± 0.09 
Concept Layer 

Method 
M CAcc 
IoU Acc 
mAPL 

3 coarse scene categories 

CNN + Logistics 
-
83.67 ± 0.18 95.19 ± 0.07 
CNN + BINN 
-
83.63 ± 0.24 95.19 ± 0.03 
CNN + SINN 
-
85.95 ± 0.44 
85.95 ± 0.44 
85.95 ± 0.44 96.40 ± 0.18 
96.40 ± 0.18 
96.40 ± 0.18 

16 general scene categories 

CNN + Logistics 
-
64.30 ± 0.27 83.30 ± 0.19 
CNN + BINN 
-
63.40 ± 0.35 82.93 ± 0.14 
CNN + SINN 
-
66.46 ± 1.10 
66.46 ± 1.10 
66.46 ± 1.10 84.97 ± 0.96 
84.97 ± 0.96 
84.97 ± 0.96 

397 fine-grained scene categories 

Image features + SVM [37, 36] 
42.70 
-
-
CNN + Logistics 
57.86 ± 0.38 
57.86 ± 0.38 
57.86 ± 0.38 35.97 ± 0.37 55.31 ± 0.30 
CNN + BINN 
57.52 ± 0.29 35.44 ± 1.02 55.57 ± 0.63 
CNN + SINN 
57.60 ± 0.38 37.71 ± 1.13 
37.71 ± 1.13 
37.71 ± 1.13 58.00 ± 0.33 
58.00 ± 0.33 
58.00 ± 0.33 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>CNN + Logistics + Partial Labels 59.08 ± 0.27 56.88 ± 0.29 CNN + SINN + Partial Labels 63.46 ± 0.18</figDesc><table>Method 
M CAcc 
mAPL 

Image features + SVM [37, 36] 
42.70 
-
CNN + Logistics 
57.86 ± 0.38 55.31 ± 0.30 
CNN + BINN 
57.52 ± 0.29 55.57 ± 0.63 
CNN + SINN 
57.60 ± 0.38 58.00 ± 0.33 

63.46 ± 0.18 
63.46 ± 0.18 64.63 ± 0.28 
64.63 ± 0.28 
64.63 ± 0.28 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by grants from NSERC and Nokia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nus-wide: A real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast and balanced: Efficient label tree learning for large scale object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Probabilistic label relation graphs with ising models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep convolutional ranking for multilabel image annotation. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a tree of metrics with disjoint visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation (NC)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic kernel forests from multiple taxonomies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified semantic embedding: Relating taxonomies and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Love thy neighbors: Image annotation by exploiting image metadata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simultaneous segmentation and pose estimation of humans using dynamic graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="298" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What are you talking about? text-to-image coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Obj cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combined object categorization and segmentation with an implicit shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image labeling on a network: using social-network metadata for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. 2012</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM (CACM)</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From large scale image categorization to entry-level categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing (TSP)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simultaneous object detection and segmentation by boosting local shape feature based classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sun database: Exploring a large collection of scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
