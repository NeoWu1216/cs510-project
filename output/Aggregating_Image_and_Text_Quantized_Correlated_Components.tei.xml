<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aggregating Image and Text Quantized Correlated Components</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><forename type="middle">Quynh</forename><surname>Nhi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIST and CEDRIC-CNAM</orgName>
								<orgName type="laboratory" key="lab2">Hervé Le Borgne CEA, LIST</orgName>
								<orgName type="institution">CEDRIC-CNAM</orgName>
								<address>
									<settlement>Gif-sur-Yvette, Paris</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><surname>Cea</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIST and CEDRIC-CNAM</orgName>
								<orgName type="laboratory" key="lab2">Hervé Le Borgne CEA, LIST</orgName>
								<orgName type="institution">CEDRIC-CNAM</orgName>
								<address>
									<settlement>Gif-sur-Yvette, Paris</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Crucianu</surname></persName>
							<email>michel.crucianu@cnam.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIST and CEDRIC-CNAM</orgName>
								<orgName type="laboratory" key="lab2">Hervé Le Borgne CEA, LIST</orgName>
								<orgName type="institution">CEDRIC-CNAM</orgName>
								<address>
									<settlement>Gif-sur-Yvette, Paris</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Aggregating Image and Text Quantized Correlated Components</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-modal tasks occur naturally for multimedia content that can be described along two or more modalities like visual content and text. Such tasks require to "translate" information from one modality to another. Methods like kernelized canonical correlation analysis (KCCA) attempt to solve such tasks by finding aligned subspaces in the description spaces of different modalities. Since they favor correlations against modality-specific information, these methods have shown some success in both cross-modal and bi-modal tasks. However, we show that a direct use of the subspace alignment obtained by KCCA only leads to coarse translation abilities. To address this problem, we first put forward a new representation method that aggregates information provided by the projections of both modalities on their aligned subspaces. We further suggest a method relying on neighborhoods in these subspaces to complete uni-modal information. Our proposal exhibits state-of-the-art results for bi-modal classification on Pascal VOC07 and improves it by over 60% for cross-modal retrieval on FlickR 8K/30K.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An increasing number of multimedia documents are described along two or more modalities that convey partly common and partly complementary information. This gives the opportunity to devise rich multimedia representations that support both multi-modal and cross-modal tasks. For example, images have a visual content but may also have associated textual data (keywords or sentences). In bimodal image classification, visual and textual content are employed together for solving the task. Cross-modal tasks like text illustration or image annotation require instead to "translate" information from one modality to another.</p><p>Simple fusion approaches such as early fusion (i.e. concatenation of visual and textual features) have been extensively employed, with some success, in bi-modal tasks. But to address cross-modal tasks it was necessary to devise methods that are able to link the two modalities more closely. This is accomplished through the development of a common, latent representation space resulting from a maximization of the relatedness between the different modalities. The methods typically rely on Canonical Correlation Analysis or its kernel extension <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref> and on deep learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Given a set of documents described along two different modalities like image and text, Kernel Canonical Correlation Analysis (KCCA) aims to find maximally correlated manifolds in the feature spaces associated to the two modalities. These manifolds are seen as a common space, where each multimedia document is represented by the two projections of its visual and respectively textual features. This approach was shown to be quite effective <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref> and some recently proposed neural networks for cross-modal tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> also make use of KCCA.</p><p>While mainly considered for cross-modal tasks, a common representation space also has the potential to improve the results obtained in bi-modal tasks. For images described by both a visual and a textual content, bi-modal tasks typically focus on semantics. The common representation space favors inter-related information that usually highlights semantics and discounts modality-specific information. Using features from the common representation space instead of early fusion features can then be seen as a form of regularization that reduces the risk of overfitting.</p><p>However, by observing the distribution of data projected on the common representation space obtained with KCCA, we found that this space only provides a very coarse association between modalities. For any given document, the projections of its visual and respectively textual features fall far apart. A direct use of these projections results in limited quality "translation" between modalities.</p><p>To deal with this problem, we put forward a new representation method for the projections on the common space, called Multimedia Aggregated Correlated Components (MACC). It aims to reduce the gap between the projections of visual and textual features by embedding them in a local context reflecting the data distribution in the common space. Given a database of multimedia documents, we first perform KCCA and build a codebook from all the projections of visual and textual features on the KCCA common space. Subsequently, for each multimedia document, visual and textual features are projected on this common space, then coded using the codebook and eventually aggregated into a single MACC vector that is the multimedia representation of the document (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>When one modality is missing from the initial description of a document, we further propose a method for completing its MACC representation using data from an auxiliary dataset. First, its nearest neighbors in KCCA space, according to the available modality, are found in this auxiliary data. Then, the descriptions of the nearest neighbors according to the other modality are combined and complete the MACC representation of the target document.</p><p>We show that MACC representations allow to reach state-of-the-art performance in a bi-modal task (image classification on Pascal VOC07) and in a cross-modal task (cross-modal retrieval on FlickR 8K and FlickR 30K). We also find that the representation completion method supports an interesting novel usage consisting in training classifiers on data from one modality and testing them on data from the other.</p><p>The remainder of this paper is organized as follows. The related work section reviews the usage of KCCA in the recent literature for addressing either cross-modal or bi-modal tasks. After a brief reminder of KCCA, we focus on the construction of MACC representations, involving an aggregation of the projections of visual and textual content represented on a common vocabulary. The method we devised for completing the MACC representation when data is missing for one of the modalities is also presented. The evaluation in Section 4, conducted on three datasets, concerns both image classification and cross-modal retrieval. We briefly highlight the deficiency of KCCA-based representations that motivated our introduction of MACC representations. Several experiments are then presented, supporting comparisons to the state of the art and to several baselines, but also allowing to explore the impact of some key parameters. We end up with the conclusion and a few directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the recent literature, various (K)CCA-based approaches have been proposed to deal with either crossmodal or bi-modal tasks. CCA was first applied to crossmodal retrieval in <ref type="bibr" target="#b9">[10]</ref>, where its kernel extension KCCA was also introduced in order to allow for more general, nonlinear latent spaces. Since not all the words (or tags) annotating an image have equal importance, <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> proposed a method taking advantage of their importance when building the KCCA representation. The importance of a word for an image is obtained from the order of words in the annotations provided by users for that image. Gong et al. <ref type="bibr" target="#b8">[9]</ref> put forward a multi-view (K)CCA method: a third view, explicitly representing image's high-level semantics, is taken into account when searching for the latent space. This "semantic" view corresponds to ground-truth labels, search keywords or semantic topics obtained by clustering tags. This first group of approaches focus on investigating complete representations of data for building a robust common space. Nevertheless, they directly use the projections of the textual and visual descriptors the KCCA common space in order to perform cross-modal tasks.</p><p>Approaches in a second group aim to build upon these direct projections on the KCCA common space. Specifically, Costa Pereira et al. <ref type="bibr" target="#b4">[5]</ref> proposed semantic correlation matching (SCM), where the projections of image and text features by (K)CCA are first transformed into semantic vectors produced by supervised classifiers with respect to predefined semantic classes. These vectors are then used for cross-modal retrieval. Ahsan et al. <ref type="bibr" target="#b0">[1]</ref> employed the concatenation of textual and visual KCCA-descriptors as inputs of a clustering algorithm to perform a bi-modal task, social event detection. Our proposal follows this second group of approaches. The novelty of our work compared to existing methods is to build a common vocabulary for image and text on the KCCA space and to represent multimedia documents by aggregating their visual and textual descriptors defined on this common vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MACC: Multimedia Aggregated Correlated Components Representation</head><p>We briefly remind in Section 3.1 the theoretical foundations of CCA and its kernelized version KCCA. In Section 3.2, we describe a new representation of multimedia documents relying on an aggregation of the projections of visual and textual content defined on a common vocabulary. Since (K)CCA aims to find a projection space where the correlation between modalities is maximized, we named this new representation "Multimedia Aggregated Correlated Components" (MACC). In Section 3.3 we propose an extension for completing the MACC representations of documents for which only one modality is available. While MACC addresses problems with the representation of bimodal documents, this extension focuses on actual crossmodal cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Kernel Canonical Correlation Analysis</head><p>For data simultaneously represented in two different vector spaces, CCA <ref type="bibr" target="#b9">[10]</ref> finds maximally correlated linear subspaces of these spaces. Let X T and X I be two random variables, taking values in R d T and respectively R d I .</p><formula xml:id="formula_0">Consider N samples {(x T i , x I i )} N i=1 ⊂ R d T × R d I .</formula><p>CCA simultaneously seeks directions w T ∈ R d T and w I ∈ R d I that maximize the correlation between the projections of x T onto w T and of x I onto w I ,</p><formula xml:id="formula_1">w * T , w * I = arg max w T ,w I w T ′ C T I w I w T ′ C T T w T w I ′ C II w I<label>(1)</label></formula><p>where C T T , C II denote the autocovariance matrices of X T and X I respectively, while C T I is the cross-covariance matrix. The solutions w * T and w * I are eigenvectors of</p><formula xml:id="formula_2">C −1 T T C T I C −1 II C IT and respectively C −1 II C IT C −1 T T C T I .</formula><p>The d eigenvectors associated to the d largest eigenvalues define maximally correlated d-dimensional subspaces in R d T and respectively R d I . Even though these are linear subspaces of two different spaces, they are often referred to as "common" representation space.</p><p>Kernel CCA (KCCA, see e.g. <ref type="bibr" target="#b9">[10]</ref>) aims to remove the linearity constraint by using the "kernel trick" to first map the data from each initial space to the reproducing kernel Hilbert space (RKHS) associated to a selected kernel and then looking for correlated subspaces in these RKHS. KCCA seeks vectors of coefficients α T , α I ∈ R N that allow to define these maximally correlated subspaces. α T , α I are solutions of</p><formula xml:id="formula_3">α * T , α * I = arg max α T ,α I α ′ T K T K I α I V (α T , K T ) V (α I , K I )<label>(2)</label></formula><p>where V (α, K) = α t (K 2 + κ K) α, κ ∈ [0, 1] is a regularization parameter and K T ,</p><formula xml:id="formula_4">K I denote the N × N kernel matrices obtained from {x T i } N i=1 and {x I i } N i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Aggregation of textual and visual information in the projection space</head><p>Let us consider a document with a textual and a visual (image) content. A feature vector x T is extracted from its textual content and another feature vector x I from the visual one. In what follows, we assimilate a document to a couple of feature vectors (x T , x I ). A set of such data is a set of couples</p><formula xml:id="formula_5">X = {(x T i , x I i ), i = 1 . . . N }.</formula><p>By applying KCCA to this data, as explained in Section 3.1, we obtain 2N points (vectors) belonging to a "common" vector space where the two modalities are maximally correlated. In this space, a document (x T , x I ) is represented by two points, p T that is the projection of x T and p I the projection of x I . Ideally, since they represent the same document, p T and p I should be closer to each other than to any other point in the projection space. However, in practice, this is far from being the case as shown in Section 4.3. It is thus quite problematic for a given document to be represented by two (very) distinct points for multimedia recognition tasks. We propose to create a unified representation for each document, by the following process:</p><p>1. define a unifying vocabulary in the projection space, 2. describe both p T and p I according to this vocabulary, 3. aggregate both descriptions into a unique representative vector of the document.</p><p>Simply said, the "unified vocabulary" is obtained by quantizing the projection space, then p T and p I are projected to this codebook and sum pooled to get the final representation. Since it is well known that in computer vision devil is in the details <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, this is further explained below. Codebook learning. As for the bag of words (BoW) model, we learn a codebook C = {c 1 , .., c k } of k codewords with k-means directly in the projection space. A crucial point is that all the projected points, coming from both textual and visual modalities, are employed as input to the k-means algorithm. Hence, the clustering potentially results into three types of codewords (that are centers of the clusters). Some are representative of textual data only, others of visual data only, while some clusters contain both textual and visual projection points. The codebook is thus intrinsically cross-modal and can serve as "common vocabulary" for all the points in the projection space, whether they result from the projection of a textual content or of a visual one.</p><p>MACC representation. A bi-modal document (x T , x I ) is projected on the KCCA projection space of dimension d into (p T , p I ). Each of these points is then encoded by its differences with respect to its nearest codewords:</p><formula xml:id="formula_6">v T i = p T − c i ; c i ∈ N N n (p T )<label>(3)</label></formula><formula xml:id="formula_7">v I i = p I − c i ; c i ∈ N N n (p I )<label>(4)</label></formula><p>where i = 1, .., k and N N n (p) denotes the set of the n nearest codewords of p. The modality-specific representations v T and v I result from the concatenation of the ddimensional vectors v T i and respectively v I i . The MACC representation v is then obtained by aggregating the visual and textual descriptors v I , v T by sum pooling, leading to:</p><formula xml:id="formula_8">v = [v 1 , v 2 , . . . , v i , . . . , v k ] s.t. (5) v i = (p T − c i )✶ N N n (p T ) (c i ) + (p I − c i )✶ N N n (p I ) (c i ) where ✶ A (.) is the indicator function.</formula><p>Vector v is subsequently L2-normalized. The projection space obtained with KCCA has dimension d, so the modality-specific encoded vectors v T and v I , as well as the MACC vector v, have a size of D = d × k, where k is the size of the codebook C.</p><p>The vectors v T and v I are component-wise differences of p T and p I with some codewords. When n = 1, such a gradient can be seen as a simplified non-probabilistic version of a Fisher Vector (FV) representation. The FV representation is itself an extension of the BoW model resulting from a Maximum Likelihood estimation of the gradient with respect to the parameters of a Gaussian Mixture that models the log-likelihood of data used to learn the codebook <ref type="bibr" target="#b15">[16]</ref>. However, in our case we show in the experimental Section 4 that choosing n &gt; 1 is advantageous. In some cases, the best results are even obtained with n = k. With respect to the vocabulary of a BoW model <ref type="bibr" target="#b1">[2]</ref>, we could say that <ref type="bibr" target="#b15">[16]</ref> uses a hard coding (n = 1) while we prefer soft coding (n = k) or possibly local soft coding (1 &lt; n &lt; k). The benefits of soft coding are well known in the BoW context <ref type="bibr" target="#b12">[13]</ref> but have not been proven in the context of FVlike signatures (i.e. when one uses component-wise gradients with respect to the codebook).</p><p>There is also another advantage in our context, where some codewords may be representative of "modalityspecific" Voronoi cells, i.e. clusters that contain projected points of only one modality after k-means (see Section 4.3). Therefore, by encoding p T and p I according to several codewords, it is more likely to include information from both modalities. Hence, the "modality vectors" v T and v I are not exactly modality-specific since they benefit from a sort of "modality regularization" with the multimodal codebook. Yet another advantage is that if p T and p I are close enough then they certainly share one or several nearest codewords. These codewords will then be enforced by Eq. (5) in the final vector v.</p><p>All this indicates that the MACC representation is a soft synthesis of the contributions of both modalities that compensates for the imperfection of the KCCA projection space in the context of bi-modal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MACC completion with the missing modality</head><p>The MACC representation proposed in the previous section is defined when the multimedia document it describes has both a visual and a textual content. But this condition does not hold for several important multimedia tasks. In particular, for cross-modal tasks, data in the reference base and/or the query usually come from only one modality. In such a case, we estimate MACC representations by completing uni-modal data with suitable information that concerns the missing modality and is obtained from an auxiliary dataset.</p><p>Modality completion. Consider an auxiliary dataset containing m documents where both visual and textual contents are present. Let A be the set of pairs of KCCA projections of the visual and textual features of these documents on the common space, with A = {(q T , q I )}, q T ∈ A T , q I ∈ A I , |A| = m. In practice, the auxiliary dataset could be the training data used to obtain the KCCA space.</p><p>To explain the completion process, let us consider a document with textual content only, described by a feature vector x T that is projected as p T on KCCA space. The same development could be symmetrically applied to a document having only visual content. A "naive" choice would be to combine p T with a vector obtained from its µ nearest neighbors among the points projected from the other modality (visual modality in this case), N N µ A I (p T ). Preliminary experiments (not reported here) have shown that such a strategy is far from being optimal. We propose instead to find the auxiliary documents having similar projected content in the available modality (textual modality in this case) and to use the projections of the visual content of these documents to complete p T . Formally, the set of contributors to the "modality complement" of p T is defined as</p><formula xml:id="formula_9">M c (p T ) = {q I j } such that q T j ∈ N N µ A T (p T ) (q T j , q I j ) ∈ A<label>(6)</label></formula><p>where the condition (q T j , q I j ) ∈ A means that q T j and q I j are the projections of two feature vectors extracted from the same multimedia document. Note that M c (p T ) = µ. MACC representation with the completed modality. Once the complementary information regarding the missing modality has been collected on the KCCA space as M c (p T ), the MACC representation of the initially textualonly document is obtained as</p><formula xml:id="formula_10">v =[v 1 , v 2 , . . . , v i , . . . , v k ] s.t v i =(p T − c i )✶ N N n (p T ) (c i ) + 1 µ q I j ∈Mc(p T ) (q I j − c i )✶ N N n (q I j ) (c i )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>We first describe the datasets and the visual/textual features employed. Then we highlight the limits of the KCCA projection, justifying the need of MACC representations.</p><p>Our contribution is then evaluated for bi-modal and also cross-modal classification on PascalVOC 07. Finally, we show that MACC establishes a new state of the art in crossmodal retrieval, improving former results on FlickR 8K (+11 pts R@1) and FlickR 30K (+15.4 pts R@1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation metrics</head><p>Pascal VOC07 <ref type="bibr" target="#b14">[15]</ref>. This dataset includes 5011 training and 4952 testing images collected from Flickr without their original user tags. Twenty class labels were defined and each image receives between 1 and 6 positive labels. Using Amazon Mechanical Turk, in <ref type="bibr" target="#b14">[15]</ref> each image also received several tags. The classification results are evaluated using mean Average Precision (mAP), following the literature.</p><p>FlickR 8K <ref type="bibr" target="#b21">[22]</ref> and FlickR30K <ref type="bibr" target="#b28">[29]</ref>. These datasets contain 8000 and 31783 images respectively. Each image was annotated by 5 sentences using Amazon Mechanical Turk. These datasets have the same 1000 images for validation and 1000 images for testing. While the training set of FlickR 8K contains 6000 images, the one of FlickR 30K is much larger containing 29783 images. We employ the evaluation metric proposed in <ref type="bibr" target="#b3">[4]</ref> for image retrieval with textual query: the five sentences annotating a given image are used together to retrieve images and we report the Re-call@K, i.e. the fraction of times the ground-truth image is found among the top K images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature extraction</head><p>To represent visual content we use the 4096-dimensional features of the Oxford VGG-Net <ref type="bibr" target="#b23">[24]</ref>, L2-normalized. This representation was shown to provide very good results in several classification and retrieval tasks.</p><p>To represent texts (sets of tags or sentences, respectively) we employ the features built from Word2Vec <ref type="bibr" target="#b18">[19]</ref>, an efficient method for learning vector representations of words from large amounts of unstructured text data. In our experiments, textual features are 300-dimensional L2-normalized vector representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Limitations of KCCA projections</head><p>As previously mentioned in Section 1, the common representation space obtained with KCCA only provides a coarse association between modalities. Several data analysis results shown here highlight this problem. <ref type="table">Table 1</ref> reports several average distances between KCCA projections of the training data (10022 points in Pascal VOC07 and 12000 points in FlickR 8K). We denote by d intramodality (I) and d intramodality (T ) the average withinmodality distances between image and respectively text projected points. Next, d intermodality (sample) is the average distance between visual projection and associated textual projection on the KCCA space of a training sample, while d intermodality (overall) is the average distance between visual    and textual projections over all training data. The values obtained in <ref type="table">Table 1</ref> show that projected points are closer to their within-modality neighbors than to their corresponding points in the other modality. For a better visualization, we computed the centers of gravity of the visual and respectively textual points, then projected all the points onto the line that joins these two centers. In <ref type="figure" target="#fig_2">Figure 2</ref>, we report the distribution of these projected points. The separation in the KCCA space between data points from the two modalities appears very clearly, for both Pascal VOC07 and FlickR 8K datasets.</p><p>Given this separation between modalities on the common space, the clusters we obtain with k-means contain mostly data from a single modality (image or text). <ref type="table">Table 2</ref> shows the number of clusters associated to each modality in Pascal VOC07 and FlickR 8K. They are qualified as "visual" or "textual" according to the majority of points they contain, but each cluster can have both visual and textual points. The clusters are used for codebooks in the following experiments. The value of k is chosen on a validation set.</p><p>Approach mAP (%) BoVW 54.5 FV <ref type="bibr" target="#b22">[23]</ref> 63.9 improved FV <ref type="bibr" target="#b2">[3]</ref> 68.0 BoMW <ref type="bibr" target="#b29">[30]</ref> 67.8 AGS <ref type="bibr" target="#b5">[6]</ref> 71.1 FV+CNN <ref type="bibr" target="#b20">[21]</ref> 76.2 He et al. <ref type="bibr" target="#b10">[11]</ref> 82.4 Chatfield et al. <ref type="bibr" target="#b2">[3]</ref> 82.4 HCP-2000C <ref type="bibr" target="#b27">[28]</ref> 85.2 VGG NetD&amp;NetE <ref type="bibr" target="#b23">[24]</ref> 89.7 MACC 90.12 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image classification on Pascal VOC07</head><p>The KCCA is learnt on the 5011 training data, with both visual and textual content. We used the seminal KCCA implementation <ref type="bibr" target="#b9">[10]</ref>, with a regularization parameter κ = 0.1 and a Gaussian kernel with standard deviation σ = 0.2. The dimension of the "common" projected space is set to d = 150. All 5011 training data are then projected on this common space and a codebook C is learnt with k-means from this set (2×5011 = 10022 points) for k ∈ {8, 16, 32}.</p><p>Classification of bi-modal documents. The first evaluation considers the classification of documents having both a visual and a textual content, such that a MACC representation (of size d × k) of each document is directly obtained from Eq. 5, using the previously built codebook. The parameter n in Eq. 5 varies in {1, 2, 5, 16, 32}. For each category, we learn a SVM classifier with linear kernel, following a one-versus-all strategy.</p><p>With such settings, the best result we obtain on the testing set is a mAP of 90.37, with (k = 16, n = 5), resulting into a 2400-dimensional MACC representation. However, when a full cross-validation is conducted on the training set, we obtain a mAP of 90.12 with (n = 5, k = 32). <ref type="table" target="#tab_2">Table 3</ref> compares this performance to other results in the literature. We report superior performance with respect to methods that use only the original (visual) data of the Pascal VOC07 challenge, such as BoVW and Fisher Vectors (FV) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3]</ref>. Our approach also outperforms methods employing additional information sources for training, such as text <ref type="bibr" target="#b29">[30]</ref>, ground-truth bounding box information <ref type="bibr" target="#b5">[6]</ref>, or based on deep learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>We also compare our image classification result to several baselines that uses the same features as MACC in <ref type="table">Table 4</ref>. For the VGG-Net (respectively Word2Vec) baseline, classifiers are trained and tested on VGG-Net (respectively Word2Vec) features only, i.e. using the visual (respectively textual) content alone. For the VGG-Net+Word2Vec baseline, representations for both training and testing data are For the KCCA img (respectively KCCA txt ) baseline, the visual (respectively textual) features are first projected on the KCCA common space for both training and testing data and then used for classifiers learning. We consider two different sizes of the KCCA common space, 150 and 2400, so that the results can be compared to our 2400-dimensional MACC representation (built from a 150-dimensional common space, with 16 codewords). The results in <ref type="table">Table 4</ref> show that the MACC approach outperforms all the mentioned baselines.</p><p>We report in <ref type="table">Table 5</ref> the results obtained with the MACC approach for different values of k and n (for d = 150). We note that the results are quite stable and consistently above the performance of the previously mentioned baselines for this entire range of parameters. Furthermore, these results show that (local) soft coding (n &gt; 1) is more effective than hard coding (n = 1) to build the MACC representations.</p><p>Classification in a cross-modal context. Let us now consider a scenario where a global resource is available, consisting of a projection space obtained by KCCA and a codebook built on this space. One may wish to train classifiers on new classes, using new data for which only one modality is available, and then run these classifiers on other data that may also have only one modality available (and maybe not the same as the one used for training). Thanks to the completion mechanism (Eq. 7), the MACC representation addresses not only classical cross-modal tasks but also such a scenario, that is tested in the following.</p><p>Specifically, we use the same 150-dimensional projection space obtained by KCCA from the bi-modal training data of Pascal VOC07 and the codebook learnt on this space (k = 16). We then define two new symmetric tasks. In the Text-Image task, the SVM classifiers are trained with documents from the training set of Pascal VOC07 but the visual content was removed. Each document, originally described by its textual content alone, has its MACC representation completed with a visual part following the procedure described in Section 3.3, with the training set of PascalVOC 07 chosen as auxiliary dataset A. Hence, the visual part of the signature is not computed from the original visual content of that document but results from combining the contributions of the visual parts of its nearest neighbors according to the textual modality (the document itself is not considered among its µ nearest neighbors). The resulting classifiers are then evaluated on the testing documents of Pascal VOC07 but where the textual content was removed and the MACC representations completed following the procedure in Section 3.3. The Image-Text task is symmetric to the Text-Image task: classifiers are trained with documents without textual content and tested on documents without visual content, all being completed according to Eq. 7.</p><p>The results obtained on these two novel tasks are shown in <ref type="table">Table 6</ref> for several values of the parameter µ and compared to two baselines. All representations are 2400dimensional vectors. For the "Random" baseline, the MACC representation is completed with randomly selected data point along the missing modality. For the KCCA inc baseline, classifiers are trained with the projections of one modality on the common KCCA space and tested with the projections of the other modality on this space.</p><p>Without completion (µ = 0), the performance of MACC representations is very low. However, as soon as the completion is considered, the performance is significantly above that of the baseline. To our knowledge, no previous work has investigated this type of cross-modal classification scenario on Pascal VOC07, thus there is no other comparison in <ref type="table">Table 6</ref>. It is not surprising that the results obtained in this cross-modal scenario are not as good as those obtained in the bi-modal task (90.12%, see <ref type="table" target="#tab_2">Table 3</ref>). However, the difference is not so large and the improvement with respect to the baselines is significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Image retrieval on FlickR 8K and FlickR 30K</head><p>KCCA is learnt on the 6000 training documents with both visual and textual content. To select the parameters, a grid search is performed employing the validation set of 1000 documents. This leads to use a Gaussian kernel with a standard deviation σ = 2, a regularization parameter κ = 1 and only d = 50 dimensions for the projected space. The visual and textual features of the training documents are then all projected on this common space and a codebook is learnt from this set of 12000 (= 2 × 6000) points.</p><p>FlickR 8K image retrieval. For the text-to-image re- trieval task the training dataset of FlickR 8K is used as auxiliary dataset A. Parameters being cross-validated on the training data, we get R@1=27.6% for k = n = 32; µ = 64. As shown in <ref type="table">Table 7</ref>, the proposed approach has higher R@1, R@5 and R@10 than the other image retrieval methods in the recent literature on the FlickR 8K dataset. Hodosh et al. <ref type="bibr" target="#b11">[12]</ref> work was also based on cross retrieval in the KCCA space but their visual and textual representations are simply described by several specific kernels on classical features such as color, texture or GIST descriptors for images, and bag of words for texts. In the KCCA (V GG+W 2V ) baseline, we apply the image retrieval method of in <ref type="bibr" target="#b11">[12]</ref> with our KCCA space built from VGG-Net and Word2Vec features, leading to much better performance (26.1%) than <ref type="bibr" target="#b11">[12]</ref>. Our method also significantly outperforms several recent deep learning approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4</ref>] that use content representation similar to ours. Furthermore, the MACC representation achieves better results than <ref type="bibr" target="#b3">[4]</ref>, the current state-of-the-art on both FlickR 8K and FlickR 30K image retrieval, in which the VGG-Net features are also employed. We studied the impact of different coding parameters on the effectiveness of MACC representations. Codebook size being fixed to k = 64, <ref type="figure" target="#fig_4">Figure 3</ref> reports the performance with hard coding (n = 1), local soft coding (1 &lt; n &lt; k) and soft coding (n = k). Since soft coding provides a better location of data points in feature space (with respect to all k codewords, not only to one or to a few of them), it usually performs better for retrieval. The most important result is nevertheless that our method achieves better per-  formance than the state-of-the-art <ref type="bibr" target="#b3">[4]</ref> as soon as µ &gt; 10.</p><p>On the FlickR 8K benchmark, the performances are quite stable with any given coding scheme for µ &gt; 20.</p><p>In a third experiment we study the stability of our approach with regard to k, n and µ. <ref type="figure" target="#fig_6">Figure 4</ref> reports performance on FlickR 8K while varying these parameters. Following the conclusion of the second experiment, soft coding (n = k) is employed in this experiment for its effectiveness. The results firstly show that even when the size of codebook and resulting MACC representations is very small, we consistently achieve better performance than the other methods in <ref type="table">Table 7</ref>. For instance, our approach has a first rank recall (R@1) of 18.5% with k as low as 8 (the corresponding MACC representation is only 400-dimensional). Besides, an interesting observation is that with a sufficiently large number µ of contributors to MACC completion, the proposed approach yields superior performance over the robust KCCA (V GG+W 2V ) baseline regardless of the size of the codebook. These results show the stability of our approach over a large range of parameters.</p><p>FlickR 30K: benefit of auxiliary dataset. To study the impact of the auxiliary dataset A used for MACC completion in cross-modal tasks, we conducted an experiment on FlickR 30K that has the same validation and testing sets as FlickR 8K but a larger training set. The experimental protocol was the same as for FlickR 8K (same KCCA space and codebook) except for the choice of A, where we used the full training set of FlickR 30K (29783 images) instead of the training set of FlickR 8K (6000 images). The results in <ref type="table">Table 8</ref> show a significant improvement of 5 points for the MACC approach, which is thus due to the larger auxiliary dataset. This improvement is higher that those obtained in previous publications. It increases when the parameters are cross-validated on FlickR30k training set. While the improvement in the previous state-of-the-art <ref type="bibr" target="#b3">[4]</ref> is from 17.3% on FlickR 8k to 18.5% on FlickR 30K, in our case it is from   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>We proposed a new representation of a multimedia document that aggregates information provided by the projections of both modalities on their aligned subspaces. We also suggested a method to complete uni-modal information relying on neighborhoods in these subspaces. The interest of our approach was demonstrated in bi-modal classification, cross-modal classification and cross-modal retrieval, where our method provides state-of-the-art performance.</p><p>We believe that this approach should also be relevant for other types of joint text-image representations built using e.g. Latent Dirichlet Allocation, Partial Least Squares or deep neural networks. However, the scheme we propose already relies on very effective methods (VGG and Word2Vec) to produce uni-modal representations from raw content. The choice of an algorithm to compute the joint representation should be made in compliance with the characteristics of the uni-modal representations employed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visual and textual contents of a document are projected onto a common space that has been previously quantized. Both points, corresponding to the same document, are encoded according to a common vocabulary before their aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Separation between modalities on the KCCA space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Coding methods comparison for MACC representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>FlickR 8K image retrieval: stability of MACC representations over a large range of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Pascal VOC07: comparison with published results.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Table 6. Pascal VOC07: classification in a cross-modal context using the completion mechanism for MACC representations.Table 7. Image retrieval results on FlickR 8K.</figDesc><table>mAP (%) 
mAP (%) 
Text-Image Image-Text 
Random 
7.76 
7.33 
KCCA inc 
71.21 
51.20 
MACC(µ = 0) 
12.03 
10.04 
MACC(µ = 1) 
79.00 
76.88 
MACC(µ = 3) 
81.72 
79.18 
MACC(µ = 5) 
82.17 
78.82 
MACC(µ = 8) 
82.18 
78.65 
MACC(µ = 10) 
82.09 
77.97 

Approach 
R@1 R@5 R@10 
Socher et al. [25] 
6.1 
18.5 
29 
Hodosh et al. [12] 
7.6 
20.7 
30.1 
Karpathy et al. [17] 
11.8 
32.1 
44.7 
Chen et al. [4] 
17.3 
42.5 
57.4 
KCCA(VGG+W2V) 26.1 
53.7 
65.6 
MACC 
27.6 
55.6 
69.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Table 8. Image retrieval results on FlickR 30K. MACC parameters are cross-validated on FlickR 8k (F8k) or FlickR 30k (F30k)28.3% to 33.9%, showing a better use of the extended training dataset, at a limited cost (KCCA and the codebook are always computed on FlickR 8K).</figDesc><table>Approach 
R@1 R@5 R@10 
Socher et al. [25] 
8.9 
29.8 
41.1 
Karpathy et al. [17] 15.2 
37.7 
50.5 
Chen et al. [4] 
18.5 
45.7 
58.1 
MACC (F8k) 
33.9 
65.6 
77.5 
MACC (F30k) 
35.3 
66.0 
78.2 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clustering social event images using kernel canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW &apos;14</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW &apos;14<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="814" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the role of correlation and abstraction in cross-modal multimedia retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Costa</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="521" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Subcategory-aware object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="827" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval with correspondence autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Intl. Conf. on Multimedia, MM &apos;14</title>
		<meeting>of ACM Intl. Conf. on Multimedia, MM &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multi-view embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="233" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature coding in image classification: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="506" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning the relative importance of objects from tagged images for retrieval and crossmodal search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="134" to="153" />
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reading between the lines: Object localization using implicit cues from image tags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1145" to="1158" />
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1310.4546</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fisher vectors meet neural networks: A hybrid classification architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collecting image annotations using amazon&apos;s mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk, CSLDAMT &apos;10</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk, CSLDAMT &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="222" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On deep multi-view representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">CNN: single-label to multi-label. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5726</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bag-of-multimedia-words for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Le</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1509" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
