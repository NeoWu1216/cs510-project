<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Learning of Single-image and Cross-image Representations for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff2">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
							<email>cslzhang@comp.polyu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Learning of Single-image and Cross-image Representations for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification has been usually solved as either the matching of single-image representation (SIR) or the classification of cross-image representation (CIR). In this work, we exploit the connection between these two categories of methods, and propose a joint learning framework to unify SIR and CIR using convolutional neural network (CNN). Specifically, our deep architecture contains one shared sub-network together with two sub-networks that extract the SIRs of given images and the CIRs of given image pairs, respectively. The SIR sub-network is required to be computed once for each image (in both the probe and gallery sets), and the depth of the CIR sub-network is required to be minimal to reduce computational burden. Therefore, the two types of representation can be jointly optimized for pursuing better matching accuracy with moderate computational cost. Furthermore, the representations learned with pairwise comparison and triplet comparison objectives can be combined to improve matching performance. Experiments on the CUHK03, CUHK01 and VIPeR datasets show that the proposed method can achieve favorable accuracy while compared with state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification is the task of matching two pedestrian images from different viewpoints <ref type="bibr" target="#b10">[11]</ref>. It has attracted increasing interests and encouraged considerable efforts in recent years due to its broad applications in video surveillance <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33]</ref>. This problem, however, is still very challenging and deserves further studies, because of the large variations in illumination, poses, viewpoints and background of pedestrian images.</p><p>The task of person re-identification can be accomplished by two categories of methods: (i) distance or similarity measures on single-image representation, which is the rep- resentation of a given image <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7]</ref> and (ii) classification on cross-image representation, which is the representation of an image pair <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24]</ref>. For the first category of methods, single-image representation (SIR) is first obtained using either hand-crafted feature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> or deep convolutional neural network (CNN) approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, and then a distance measure together with a threshold is utilized to predict whether two pedestrian images are matched or not. For the second category of methods, after obtaining the cross-image representation (CIR), person re-identification can be regarded as an ordinary binary classification task <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>These two categories of methods have their own advantages. The SIR has some outstanding advantages in terms of efficiency. Given a gallery set of N images, one can precompute their SIRs in advance. In the matching stage, we only need to extract the SIR of the probe image and compute its distances to the SIRs of the gallery images, while for CIR classification method we need to extract the CIR between the probe image and each gallery image (i.e., N times). On the other hand, compared with SIR, CIR is effective in capturing the relationships between the two images, and several approaches have been suggested to address horizonal displacement by local patch matching. Therefore, the SIR and CIR have their respective advantages and this finding inspires us to investigate a comprehensive way of combining these two representations in terms of both effectiveness and efficiency.</p><p>In this work, we study the connection between SIR and CIR, and propose a joint learning framework with deep C-NN to exploit the advantages of these two categories of representation methods. Denote by x i and x j two pedestrian images. We adopt the following classifier based on the cross-image representation g(x i , x j ):</p><formula xml:id="formula_0">S CIR (x i , x j ) = w T g (x i , x j ) − b<label>(1)</label></formula><p>and use the Euclidean distance to measure the dissimilarity between the SIRs of x i and x j :</p><formula xml:id="formula_1">S SIR (x i , x j ) = f (x i ) − f (x j ) 2 2<label>(2)</label></formula><p>where (w, b) is the parameter of the classifier, f (x i ) and f (x j ) are the SIRs of x i and x j , respectively, and 2 denotes the L 2 norm. With S SIR (x i , x j ), a threshold t S is introduced to predict whether the two pedestrian images are from the same person.</p><p>We show that classification on CIR is the generalization of conventional similarity measures based on SIR. Denote by [ ] vec the vector form of a matrix. Using the Euclidean distance in (2) as an example, it is obvious to see that</p><formula xml:id="formula_2">S SIR (x i , x j ) is a special case of S CIR (x i , x j ) with w = [I] vec , b = t S , and g (x i , x j ) = [(f (x i ) − f (x j )) (f (x i ) − f (x j )) T ] vec ,</formula><p>where I is the identity matrix. As illustrated in Sect. 3.1, other distance or similarity measures <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3]</ref> are also special cases of CIR.</p><p>By using the deep CNN architecture, we propose a framework to jointly learn SIR and CIR for improving matching performance with the least increase of the computational cost. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, our network consists of three sub-networks, i.e. first one shared sub-network and followed by two sub-networks for extracting SIR and CIR features, respectively. To save the computational cost, we can store the CNN feature maps from the shared subnetwork of the gallery images in advance, and reduce the depth of the CIR sub-network to include only one convolutional layer and one fully-connected layer. In the test stage, the shared feature maps and SIR of each probe image are required to be computed one time, and only the CIR subnetwork is used to compute the CIR between the probe image and each gallery image. Thus we can exploit the CIR to improve the matching accuracy, while exploiting the SIR and shared sub-network to reduce the computational cost.</p><p>Furthermore, we extend our model by utilizing two different deep CNNs for joint SIR and CIR learning based on either pairwise comparison objective or triplet comparison objective, respectively. For the pairwise comparison based network, we learn the CIR by standard support vector machine (SVM) <ref type="bibr" target="#b31">[32]</ref>. For the triplet comparison based network, we learn the CIR by ranking SVM (RankSVM) <ref type="bibr" target="#b29">[30]</ref>. Finally we combine the matching scores of these two networks together as the similarity of the image pair.</p><p>Experiments have been conducted on several public datasets for person re-identification, i.e. CUHK03 <ref type="bibr" target="#b18">[19]</ref>, CUHK01 <ref type="bibr" target="#b17">[18]</ref> and VIPeR <ref type="bibr" target="#b11">[12]</ref>. The results show that, joint SIR and CIR learning is effective in improving the person re-identification performance, and the matching accuracy can be further boosted by combining the learned models based on pairwise and triplet comparison objectives. Compared with the state-of-the-art approaches, the proposed methods perform favorably in person re-identification.</p><p>The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 describes the proposed model. Section 4 presents the deep network architecture. Section 5 reports the experimental results, and Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The existing person re-identification methods can be divided into two categories depending on whether they use the hand-crafted or deep CNN features. There have been many kinds of hand-crafted features used for person re-identification, including local binary patterns (LBP) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16]</ref>, color histogram <ref type="bibr" target="#b15">[16]</ref> and local maximal occurrence (LOMO) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. For the methods based on handcrafted features, they usually focus on learning an effective distance/similarity metric to compare the features. For the methods based on deep CNN features, feature representation and classifier can be jointly optimized for learning either SIR or CIR features. This section will provide a brief review on these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Metric Learning for Person Re-identification</head><p>Many distance metric learning methods have been developed for person re-identification. They aim to learn a distance metric to reduce the distance of the matched images, and enlarge the distance of the mismatched images. Among the existing distance metric learning methods, some of them are based on pairwise comparison constraint. Guillaumin et al. proposed a logistic discriminant metric learning (LDM-L) model by modeling the probability of a given sample pair (x i , x j ) and used the maximum log-likelihood as the objective function <ref type="bibr" target="#b12">[13]</ref>. Following the keep-it-simple-andstraight forward (KISS) principle, Köstinger et al. proposed a KISS metric learning (KISSME) method to address the scalability issue of metric learning from equivalence constraints <ref type="bibr" target="#b15">[16]</ref>. Li et al. developed a generalized similarity metric for person re-identification by introducing an adaptive threshold into Mahalanobis distance <ref type="bibr" target="#b19">[20]</ref>. Li and Wang introduced the locally aligned feature transform to match the person images across camera views <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr">Liao et al.</ref> improved the KISSME method by learning a discriminant low dimensional subspace <ref type="bibr" target="#b20">[21]</ref> based on the LOMO features. They also improved the LDML model by enforcing the positive semidefinite constraint and the asymmetric sample weighting strategy <ref type="bibr" target="#b21">[22]</ref>. Some other works, including pairwise constrained component analysis (PCCA) <ref type="bibr" target="#b27">[28]</ref>, local Fisher discriminant analysis (LFDA) <ref type="bibr" target="#b28">[29]</ref> and information-theoretic metric learning (ITML) <ref type="bibr" target="#b4">[5]</ref>, are also based on the pairwise comparison constraints.</p><p>Apart from the methods based on pairwise comparison constraints, some other methods are based on the triplet comparison constraints. Weinberger et al. proposed a large margin nearest neighbor (LMNN) model <ref type="bibr" target="#b34">[35]</ref>, where the distance metric is learned to separate the matched neighbors from the mismatched ones by a large margin. Dikmen et al. improved LMNN by adding the option of rejection <ref type="bibr" target="#b5">[6]</ref>. Zheng et al. developed a person re-identification model by maximizing the likelihood that each sample is more closed to its matched sample than its mismatched sample <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Learning for Person Re-identification</head><p>Due to the power of deep CNNs in learning discriminative features from large-scale image data, many methods have adopted the deep architecture to jointly learn the representation and the classifier <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31]</ref>. Some of them focus on learning the SIR together with the similarity function. Schroff et al. proposed a FaceNet model for face verification <ref type="bibr" target="#b30">[31]</ref>, which adopts a deep CNN to learn the Euclidean embedding per image by using the triplet comparison loss. Online triplet generation is also developed to gradually increase the difficulty of the triplets in training. Ding et al. proposed a deep SIR learning model based on relative distance comparison for person re-identification <ref type="bibr" target="#b6">[7]</ref>. It first presents an effective triplet generation strategy to construct triplets, which contains one image with a matched image and a mismatched image. For each triplet, this model learns the SIR by maximizing the relative distance between the matched pair and the mismatched pair.</p><p>Despite learning SIR, some other methods are suggested to perform person re-identification based on CIR. Li et al. proposed a filter pairing neural network (FPNN) <ref type="bibr" target="#b18">[19]</ref>, which learns the CIRs by a patch matching layer followed by a maxout-grouping layer. In FPNN, the patch matching layer is used to model the displacement of each horizontal stripe in the images across views, the maxout-grouping layer improves the robustness of patch matching, and finally a softmax classifier is imposed on the learned CIR for person re-identification. The work in <ref type="bibr" target="#b0">[1]</ref> shares the similar idea, but introduces a new layer to learn the cross-image representation by computing the neighborhood difference between two input images. The work in <ref type="bibr" target="#b3">[4]</ref> learns the CIR by formulating the person re-identification task as a learning-torank problem. For each image pair, this model first stitchs its two images horizontally to form a holistic image, then feeds these images to a CNN to learn their representations. Finally the ranking loss is used to ensure that each sample is more similar to its positive matched image than its negative matched image. Liu et al. proposed a Matching C-NN (M-CNN) architecture for human parsing <ref type="bibr" target="#b23">[24]</ref>, which learns the CIR of the image and a semantic region by a multi-layer cross image convolutional path to predict their matching confidence and displacements.</p><p>There are considerable differences between the proposed deep architecture and the previous networks. First, both SIR and CIR can be jointly learned with the proposed deep architecture, while only SIR is learned in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7]</ref> and only CIR is learned in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref>. Second, to improve the computational efficiency, we restrict the depth of the CIR sub-network with only a convolutional layer and a fullyconnected layer. In contrast, multiple convolutional and fully-connected layers are adopted in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref> for CIR learning. Besides, we present two deep CNN architectures for joint SIR and CIR learning based on pairwise and triplet comparison objectives, respectively, and the matching scores of these two networks can be combined to improve the matching accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint SIR and CIR Learning</head><p>In this section, we first discuss the connections between SIR and CIR, then propose two formulations (i.e. pairwise comparison formulation and triplet comparison formulation) for joint SIR and CIR learning, and finally introduce the matching scores for person re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Connection between SIR and CIR</head><p>With the SIR features, there are four commonly used distance/similarity measures for person re-identification, i.e. Euclidean distance, Mahalanobis distance, joint Bayesian <ref type="bibr" target="#b2">[3]</ref>, and LADF <ref type="bibr" target="#b19">[20]</ref>. As explained in Sect. 1, Euclidean distance on SIRs can be regarded as a special case of CIRbased classification. In the following, we will show that the other measures are also special cases of CIR-based classification.</p><p>The Mahalanobis distance based on the SIR</p><formula xml:id="formula_3">z i = f (x i ) can be formulated as s (x i , x j ) = (z i − z j ) T M (z i − z j ),</formula><p>where M is positive semi-definite. This formulation is equivalent to (1) when w = [M] vec and g (</p><formula xml:id="formula_4">x i , x j ) = (z i − z j ) (z i − z j ) T vec .</formula><p>The joint Bayesian formulation <ref type="bibr" target="#b2">[3]</ref> is defined as follows,</p><formula xml:id="formula_5">s (x i , x j ) = z T i Az i + z T j Az j − 2z T i Gz j (3)</formula><p>which is the generalization of Mahalanobis distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By setting w = [A]</head><p>T vec <ref type="formula" target="#formula_0">(1)</ref>, joint Bayesian can be regarded as a classifier w on the CIR g(x i , x j ).</p><formula xml:id="formula_6">[G] T vec T and g (x i , x j ) = z i z T i + z j z T j T vec −2z j z T i T vec T in</formula><p>The LADF <ref type="bibr" target="#b19">[20]</ref> is defined as follows,</p><formula xml:id="formula_7">s (x i , x j ) = 1 2 z T i Az i + 1 2 z T j Az j + z T i Bz j + c T (z i + z j ) + b<label>(4)</label></formula><p>which is the generalization of Mahalanobis distance and joint Bayesian. It can also be viewed as a special case of (1) when w = (</p><formula xml:id="formula_8">[A] T vec [B] T vec c T b) T and g(x i , x j ) = 1 2 z i z T i + z j z T j T vec z j z T i T vec (z i + z j ) T 1 T .</formula><p>Despite the connections between SIR and CIR, they do have their own advantages and can be combined to improve the matching performance. For the SIR-based method, the SIR features of the gallery set can be precomputed in advance. For each probe image, we only require extract its SIR and compute its distance/similarity measure to the precomputed SIRs from the gallery images, making SIR computationally efficient for person re-identification. The CIRbased method can effectively model the complex relationships between the gallery and probe images, and is robust to spatial displacement and changed views. In the following, we will investigate the loss for joint SIR and CIR learning and design proper network architecture by considering both accuracy and efficiency factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pairwise Comparison Formulation</head><p>Denote by {((x i , x j ) , h ij )} the doublet training set, where x i and x j are the ith and jth training samples, respectively. h ij is the label assigned to the doublet (x i , x j ). If x i and x j are from the same class, then h ij = 1, otherwise h ij = −1. Let f (x i ) be the SIR of x i and b SIR be a distance threshold. In the pairwise comparison formulation, the similarity of the positive pair is expected to be higher than a given threshold, while the similarity of the negative pairs is expected to be lower than the threshold. The Euclidean distance of the SIRs for any doublet (x i , x j ) should satisfy the constraints as follows:</p><formula xml:id="formula_9">f (x i ) − f (x j ) 2 2 ≤ b SIR − 1 + ξ P ij if h ij = 1 f (x i ) − f (x j ) 2 2 ≥ b SIR + 1 − ξ P ij if h ij = −1<label>(5)</label></formula><p>where ξ P ij is a nonnegative slack variable. Then the loss function of SIR learning is</p><formula xml:id="formula_10">L P SIR = i,j 1 + h ij f (x i ) − f (x j ) 2 2 − b SIR +<label>(6)</label></formula><p>where [z] + = max (z, 0). The CIR learning can be formulated as a binary classification problem, where the CIR for any doublet (x i , x j ) should satisfy the constraints:</p><formula xml:id="formula_11">w T g (x i , x j ) ≤ b CIR − 1 + ζ P ij if h ij = 1 w T g (x i , x j ) ≥ b CIR + 1 − ζ P ij if h ij = −1<label>(7)</label></formula><p>where b CIR is the threshold and ζ P ij is a nonnegative slack variable. We use the loss function of the standard SVM <ref type="bibr" target="#b31">[32]</ref> to learn CIR:</p><formula xml:id="formula_12">L P CIR = α P 2 w 2 2 + i,j 1 + h ij w T g (x i , x j ) − b CIR + .</formula><p>(8) where α P is a trade-off parameter, and we set α P = 0.0005 in the experiments.</p><p>The overall loss function of pairwise comparison based representation learning method is the combination of <ref type="formula" target="#formula_10">(6)</ref> and <ref type="formula">(8)</ref>:</p><formula xml:id="formula_13">L P = L P SIR + η P L P CIR<label>(9)</label></formula><p>where η P is a trade-off parameter and we set η P = 1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Triplet Comparison Formulation</head><p>The triplet comparison formulation is trained on a series of triplets (x i , x j , x k ), where x i and x j are from the same class, while x i and x k are from different classes. To make the distance between x i and x j smaller than the one between x i and x k , for any triplet (x i , x j , x k ) the SIR should satisfy the following constraint:</p><formula xml:id="formula_14">f (x i ) − f (x k ) 2 2 − f (x i ) − f (x j ) 2 2 ≥ 1 − ξ T ijk (10)</formula><p>where ξ T ijk is a nonnegative slack variable. Then the loss function of SIR learning is</p><formula xml:id="formula_15">L T SIR = i,j,k 1 − f (x i ) − f (x k ) 2 2 + f (x i ) − f (x j ) 2 2 +<label>(11)</label></formula><p>The CIR learning can be formulated as a learning-to-rank problem, where the CIRs should satisfy the following constraint: <ref type="bibr" target="#b11">(12)</ref> where ζ T ijk is a nonnegative slack variable. We use the loss function of the RankSVM <ref type="bibr" target="#b29">[30]</ref> to learn CIR:</p><formula xml:id="formula_16">w T g (x i , x k ) − w T g (x i , x j ) ≥ 1 − ζ T ijk</formula><formula xml:id="formula_17">L T CIR = α T 2 w 2 2 + i,j,k 1 + w T g (x i , x k ) − w T g (x i , x j ) +<label>(13)</label></formula><p>where α T is a trade-off parameter, and we set α T = 0.0005 in the experiments.</p><p>The overall loss function of triplet comparison based learning method is the combination of (11) and <ref type="formula" target="#formula_0">(13)</ref>:</p><formula xml:id="formula_18">L T = L T SIR + η T L T CIR<label>(14)</label></formula><p>where η T is a trade-off parameter and we set η T = 1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Prediction</head><p>We use both of SIR and CIR for matching. For the given image pair (x i , x j ), we take the Euclidean distance</p><formula xml:id="formula_19">f (x i ) − f (x j ) 2 2</formula><p>as the indicator of the SIRs, and take w T g (x i , x j ) as the indicator of the CIR. In this view, we use the combination of these indicators, which is as follows,</p><formula xml:id="formula_20">S (x i , x j ) = f (x i ) − f (x j ) 2 2 + λw T g (x i , x j ) (15)</formula><p>where λ is the trade-off parameter. This parameter can be selected by cross validation. In the experiments, we set it as λ = 0.7 in the pairwise comparison model, and λ = 1 in the triplet comparison model. We compare S (x i , x j ) with a threshold t to decide whether these two images x i and x j are matched or not. If S (x i , x j ) &lt; t, then x i and x j are matched, otherwise they are not matched.</p><p>We also combine the matching scores of the learning models based on pairwise and triplet comparison formulations, which are denoted by S P (x i , x j ) and S T (x i , x j ), respectively. The combined matching score is</p><formula xml:id="formula_21">S P &amp;T (x i , x j ) = S P (x i , x j ) + µS T (x i , x j ),</formula><p>where µ is a trade-off parameter and we set it as µ = 0.5 in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep Convolutional Neural Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Architecture</head><p>Instead of using the hand-crafted image features, we jointly learn the SIRs and CIRs using a deep CNN. For the pairwise comparison formulation, we learn the SIRs (f (x i ) and f (x j )) and CIR g (x i , x j ) for the image pair (x i , x j ). For the triplet comparison formulation, we learn the SIRs (f (x i ), f (x j ) and f (x k )) and the CIRs (g (x i , x j ) and g (x i , x k )) for the image triplet (x i , x j , x k ). The deep architectures of the pairwise and triplet comparison models are illustrated in <ref type="figure" target="#fig_1">Fig. 2 and Fig. 3</ref>, respectively. Each of these two networks consists of a SIR learning sub-network (green part), a CIR learning sub-network (red part), and a sub-network shared by SIR and CIR learning (blue part). For each of the probe and gallery images, its CNN feature maps (yellow part) from the shared sub-network and the SIR feature are computed once. Only the CIR learning sub-network is used to extract the CIR features for each image pair of probe image and gallery image.</p><p>Shared sub-network. The sub-network in the blue part of Figs. 2 and 3 is shared by SIR learning and CIR learning. It consists of two convolutional layers with rectified linear unit (ReLU) activation. Each of them is followed by a pooling layer. The kernel sizes of the first and second convolutional layers are 5×5 and 3×3, respectively. The stride of the convolutional layers is 1 pixel. The kernel sizes of the first and second pooling layers are set to 3 × 3 and 2 × 2, respectively.</p><p>SIR sub-network. We use the sub-network in the green part of <ref type="figure" target="#fig_1">Figs. 2 and 3</ref> to learn the SIR f (x i ) for the input image x i . This sub-network contains one convolutional layers with ReLU activation, a pooling layer and two fullyconnected layers. The kernel sizes of the convolutional layer and the pooling layer are 3 × 3 and 2 × 2. The output dimensions of these two fully-connected layers are 1000 and 500, respectively. For the pairwise and triplet comparison model, there are two and three sub-networks, which share the same parameter, to learn the SIR, respectively.</p><p>CIR sub-network. We use the sub-network in the red part of Figs. 2 and 3 to learn the CIR g (x i , x j ) for the input image pair (x i , x j ). This sub-network contains one convolutional layer with ReLU activation followed by one pooling layer and one fully-connected layer. The kernel sizes of the convolutional layer and the pooling layer are 3 × 3 and 2 × 2. The output dimension of the fully-connected layer is 1000. Denote by φ p (x i ) the pth channel of the CNN feature map of x i from the shared sub-network. When we extract the CIR of (x i , x j ), the CIR sub-network is feeded by the CNN feature maps of x i and x j from the shared subnetwork. The first convolutional layer of CIR sub-network is used to compute the cross-image feature map as follows</p><formula xml:id="formula_22">ϕ r (x i , x j ) = max 0, b r + q k q,r * φ q (x i ) + l q,r * φ q (x j ) ,<label>(16)</label></formula><p>where ϕ r (x i , x j ) is the rth channel of cross-image feature map, k q,r and l q,r are different convolutional kernels of the qth channel of the shared sub-network feature map and the rth channel of cross-image feature map. The similar operation has also been used in <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Training</head><p>There are three main steps in the training process, including data preprocessing, doublet/triplet generation and network training. Like most of the deep models, back propagation (BP) is utilized to train the proposed network. The details of the first two steps are described as follows.</p><p>Data preprocessing. To make the model robust to the image translation variance, we randomly crop the input images before the training process. The original image size in our experiment is 180 × 80 pixels. We randomly select the cropped image center from [80, 100] × <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">50]</ref> and crop the original image to 160 × 60 pixels. We also enlarge the   <ref type="table">Table 2</ref>. The training times of the proposed pairwise and triplet comparison models and the automatically cropped bounding box with a pedestrian detector <ref type="bibr" target="#b8">[9]</ref>. Here we use the images cropped by the pedestrian detector in our experiments. Following the testing protocol in <ref type="bibr" target="#b18">[19]</ref>, the identities in this dataset are randomly divided into non-overlapping training and test set. The training set consists of 1,367 persons and the test set consists of 100 persons. By this strategy, 20 partitions of training and test set are constructed. The reported cumulative matching characteristic (CMC) curve and accuracy are averaged by these 20 groups. For each person in the test set, we randomly select one camera view to construct the probe set, and use one image from another camera view as the gallery set. By this way we construct 10 pairs of probe and gallery sets for testing. The result is averaged by these 10 groups. The reported results of CUHK03 dataset are based on single-shot setting. First, we report the accuracies of different settings of the proposed pairwise and triplet comparison models in <ref type="table">Table  1</ref>. For each of the pairwise and triplet comparison models, we report the matching accuracies by SIR and CIR, respectively. The CMC curves of these settings are in the supplementary material. From the results, we can see that the SIR and CIR based matching have comparable results. However, their combination achieves a higher accuracy than either of them. The accuracy of triplet comparison model is higher than pairwise comparison model, and their combination also outperforms either of them. We also report the training time of the proposed model in <ref type="table">Table 2</ref>. Compared with SIR learning, the proposed joint SIR and CIR learning model can achieve substantial improvement of matching accuracy with slight increase of training time.</p><p>Second, we investigate the sensitivity of rank-1 accuracy to the trade-off parameter λ in <ref type="bibr" target="#b14">(15)</ref>. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the curves of rank-1 accuracy on the test set versus λ. It can be observed that the pairwise and triplet comparison models reach the highest accuracies when λ = 0.7 and 1, respectively.</p><p>We also compare the performances of the proposed method and some other state-of-the-art methods, including Euclidean distance, ITML <ref type="bibr" target="#b4">[5]</ref>, LMNN <ref type="bibr" target="#b34">[35]</ref>, metric learning to rank (RANK) <ref type="bibr" target="#b26">[27]</ref>, LDML <ref type="bibr" target="#b12">[13]</ref>, symmetry-driven    <ref type="figure">Figure 5</ref>. The rank-1 accuracies and CMC curves of different methods on the CUHK03 dataset <ref type="bibr" target="#b0">[1]</ref> (best viewed in color) accumulation of local features (SDALF) <ref type="bibr" target="#b7">[8]</ref>, eSDC <ref type="bibr" target="#b40">[41]</ref>, KISSME <ref type="bibr" target="#b15">[16]</ref>, FPNN <ref type="bibr" target="#b18">[19]</ref>, the work by Ahmed et al. <ref type="bibr" target="#b0">[1]</ref>, and LOMO+XQDA <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure">Fig. 5</ref> illustrates the CMC curves and the rank-1 accuracies of these methods. We can see that the rank-1 accuracy of the proposed method can reach 52.17%, which is 5.92% higher than the second best performance method (LOMO+XQDA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">CUHK01 Dataset</head><p>The CUHK01 dataset consists of 3,884 pedestrian images taken by two surveillance cameras from 971 persons. Each person has 4 images. This dataset has been randomly divided into 10 partitions of training and test sets, and the reported CMC curves and rank-1 accuracies are averaged on these 10 groups.</p><p>Following the protocol in <ref type="bibr" target="#b0">[1]</ref>, we use 871 persons for training and 100 persons for testing. We pretrain the deep network using CUHK03 dataset for 100,000 iterations, and fine-tune the CNN using the training set of CUHK01 for 50,000 iterations. On the basis of the singleshot setting, we report the CMC curves and rank-1 ac-  <ref type="figure">Figure 6</ref>. The rank-1 accuracies and CMC curves of different methods on the CUHK01 dataset <ref type="bibr" target="#b0">[1]</ref> (best viewed in color) curacies of the proposed model (marked as "Ours (Pairwise/Triplet/Combined, Pretrain)") and the other state-ofthe-art person re-identification methods, including FPNN <ref type="bibr" target="#b18">[19]</ref>, Euclidean distance, ITML <ref type="bibr" target="#b4">[5]</ref>, LMNN <ref type="bibr" target="#b34">[35]</ref>, RANK <ref type="bibr" target="#b26">[27]</ref>, LDML <ref type="bibr" target="#b12">[13]</ref>, SDALF <ref type="bibr" target="#b7">[8]</ref>, eSDC <ref type="bibr" target="#b40">[41]</ref>, KISSME <ref type="bibr" target="#b15">[16]</ref>, and the work by Ahmed et al. <ref type="bibr" target="#b0">[1]</ref> in <ref type="figure">Fig. 6</ref>. The rank-1 accuracy of the proposed method is much higher than the other competing methods. We also report the result using the same setting in <ref type="bibr" target="#b0">[1]</ref> without pre-training (marked as "Ours (Pairwise/Triplet/Combined)"). In this setting, the rank-1 accuracy of the proposed method is much higher than most of the competing methods and is comparable to <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">VIPeR Dataset</head><p>The VIPeR dataset consists of 1,264 images from 632 persons <ref type="bibr" target="#b11">[12]</ref>. These images are taken by two camera views. We randomly select 316 persons for training, and use the rest 316 persons for testing. For each person in the test set, we randomly select one camera view as the probe set, and use the other camera view as the gallery set. Following the testing protocol in <ref type="bibr" target="#b0">[1]</ref>, we pretrain the CNN using CUHK03 and CUHK01 datasets, and fine-tune the network on the training set of VIPeR. We report the CMC curves and rank-1 accuracies of local Fisher discriminant analysis (LF) <ref type="bibr" target="#b28">[29]</ref>, pairwise constrained component analysis (PC-CA) <ref type="bibr" target="#b27">[28]</ref>, aPRDC <ref type="bibr" target="#b22">[23]</ref>, PRDC <ref type="bibr" target="#b42">[43]</ref>, enriched BiCov (eBi-Cov) <ref type="bibr" target="#b24">[25]</ref>, PRSVM <ref type="bibr" target="#b1">[2]</ref>, and ELF <ref type="bibr" target="#b9">[10]</ref>, saliency matching (SalMatch) <ref type="bibr" target="#b39">[40]</ref>, patch matching (PatMatch) <ref type="bibr" target="#b39">[40]</ref>, locallyadaptive decision function (LADF) <ref type="bibr" target="#b19">[20]</ref>, mid-level filters (mFilter) <ref type="bibr" target="#b41">[42]</ref>, visWord <ref type="bibr" target="#b38">[39]</ref>, the work by Ahmed et al. <ref type="bibr" target="#b0">[1]</ref>, the proposed model, etc. The proposed method performs better than most of the other competing methods except mFilter <ref type="bibr" target="#b41">[42]</ref>+LADF <ref type="bibr" target="#b19">[20]</ref>, which is the combination of two methods.   <ref type="figure">Figure 7</ref>. The rank-1 accuracies and CMC curves of different methods on the VIPeR dataset <ref type="bibr" target="#b0">[1]</ref> (best viewed in color)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we propose an approach for person reidentification by joint SIR and CIR learning. Since SIR is efficient in matching, while CIR is effective in modeling the relationship between probe and gallery images, we fuse their losses together to utilize the advantages of both these representations. We present a pairwise comparison formulation and a triplet comparison formulation for joint SIR and CIR learning. For each of these two models, we formulate a deep neural network to jointly learn the SIR and CIR. Experimental results validate the efficacy of joint SIR and CIR learning, and the proposed method outperforms most of the state-of-the-art models in the CUHK03, CUHK01 and VIPeR datasets. In the future, we will investigate other ways to integrate SIR and CIR learning (e.g., explicit modeling on patch correspondence), and study model-level fusion from pairwise and triplet comparisons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The sketch of the network for learning the single-image and cross-image representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The proposed deep architecture of the pairwise comparison model (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Rank-1 accuracy versus λ in the CUHK03 dataset (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>68% eSDC 11.70% KISSME 44.96% Ahmed et al. 46.25% LOMO+XQDA 43.36% Ours(Pairwise) 51.33% Ours(Triplet) 52.17% Ours(Combined)</figDesc><table>1 
5 
9 
13 
17 
21 
25 
30 
0 

0.2 

0.4 

0.6 

0.8 

1 

Rank 

Matching accuracy 

19.89% FPNN 
4.94% Euclidean 
5.14% ITML 
6.25% LMNN 
8.52% RANK 
10.92% LDML 
4.87% SDALF 
7.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>00% Ahmed et al. 58.93% Ours(Pairwise) 65.21% Ours(Triplet) 65.71% Ours(Combined) 61.20% Ours(Pairwise, Pretrain) 71.30% Ours(Triplet, Pretrain) 71.80% Ours(Combined, Pretrain)</figDesc><table>1 

5 
9 
13 
17 
21 
25 
29 
0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Rank 

Matching accuracy 

27.87% FPNN 
10.52% Euclidean 
17.10% ITML 
21.17%LMNN 
20.61% RANK 
26.45% LDML 
9.90% SDALF 
22.83% eSDC 
29.40% KISSME 
65.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Matching accuracy<ref type="bibr" target="#b42">43</ref>.39% mFilter + LADF 29.11% mFilter 30.16% SalMatch 26.90% PatMatch 29.34% LADF 24.18% LF 19.60% KISSME 19.27% PCCA 16.14% aPRDC 15.66% PRDC 26.31% eSDC 20.66% eBiCov 19.87% SDALF 30.70% visWord 14.00% PRSVM 12.00% ELF 34.81% Ahmed et al. 29.75% Ours (Pairwise) 35.13% Ours (Triplet) 35.76% Ours (Combined)</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://vision.soe.ucsc.edu/projects</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by NSFC grant (61271093), the program of ministry of education for new century excellent talents (NCET-12-0150), and the Hong Kong RGC General Research Fund (PolyU 152212/14E). This work is also supported in part by Special Program for Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (the second phase). The authors would like to thank NVIDI-A Corporation for the donation of Tesla K40 GPU.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>training set by creating the horizontal mirror of each training images. Doublet/triplet generation based on mini-batch strategy. Since the training set may be too large to be loaded into the memory, we divide the training set into multiple minibatches. Following the strategy in <ref type="bibr" target="#b6">[7]</ref>, for each iteration, we randomly select 80 classes from the training set, and construct 60 doublets or triplets for each class. Using this strategy, we can generate 4,800 doublets or triplets in each round of training. For the SIR learning, we use all of the 4,800 doublets or triplets in training. For the CIR learning, we randomly select 100 doublets or triplets for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the proposed method using three person re-identification datasets, i.e. CUHK03 <ref type="bibr" target="#b18">[19]</ref>   <ref type="bibr" target="#b14">[15]</ref>. We set the momentum as γ = 0.5 and set the weight decay as µ = 0.0005. We train the network for 150,000 iterations. It takes about 28-34 hours in training with a NVIDIA Tesla K40 GPU. The learning rates of pairwise and triplet comparison models are 1 × 10 −3 and 3 × 10 −4 before the 100,000th iteration, respectively. After that their learning rates reduce to 1 × 10 −4 and 3 × 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">CUHK03 Dataset</head><p>The CUHK03 dataset contains 14,096 pedestrian images, which were taken from 1,467 persons by two surveillance cameras <ref type="bibr" target="#b18">[19]</ref>. Each person has 4.8 images on average. All of the images are collected from five video clips. The dataset provides both the manually cropped bounding box</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multipleshot person re-identification by chromatic and epitomic analyses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="898" to="903" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep ranking for person re-identification via joint representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.0682</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Informationtheoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian recognition with a learned metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Is that you? metric learning approaches for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relaxed pairwise learned metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human re-identification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepReID: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient PSD constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Person reidentification: What features are important?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops and Demonstrations</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching-CNN meets KNN: Quasiparametric human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BiCov: a novel image representation for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Saliency weighted features for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-CV Workshop on Visual Surveillance and Re-identification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICM-L</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PCCA: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person reidentification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">People reidentification in surveillance and forensics: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intelligent multi-camera video surveillance: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bitscalable deep hashing with regularized similarity learning for image retrieval and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4766" to="4779" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A novel visual word co-occurrence model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-CV Workshop on Visual Surveillance and Re-identification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
