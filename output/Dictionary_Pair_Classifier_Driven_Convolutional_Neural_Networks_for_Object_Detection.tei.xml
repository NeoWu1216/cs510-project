<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dictionary Pair Classifier Driven Convolutional Neural Networks for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
							<email>kezewang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>cslzhang@comp.polyu.edu.hk.</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dictionary Pair Classifier Driven Convolutional Neural Networks for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature representation and object category classification are two key components of most object detection methods. While significant improvements have been achieved for deep feature representation learning, traditional SVM/softmax classifiers remain the dominant methods for the final object category classification. However, SVM/softmax classifiers lack the capacity of explicitly exploiting the complex structure of deep features, as they are purely discriminative methods. The recently proposed discriminative dictionary pair learning (DPL) model involves a fidelity term to minimize the reconstruction loss and a discrimination term to enhance the discriminative capability of the learned dictionary pair, and thus is appropriate for balancing the representation and discrimination to boost object detection performance. In this paper, we propose a novel object detection system by unifying DPL with the convolutional feature learning. Specifically, we incorporate DPL as a Dictionary Pair Classifier Layer (DPCL) into the deep architecture, and develop an end-to-end learning algorithm for optimizing the dictionary pairs and the neural networks simultaneously. Moreover, we design a multi-task loss for guiding our model to accomplish the three correlated tasks: objectness estimation, categoryness computation, and bounding box regression. From the extensive experiments on PASCAL VOC 2007/2012 benchmarks, our approach demonstrates the effectiveness to substantially improve the performances over the popular existing object detection frameworks (e.g., and FRCN [12]), and achieves new state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aiming at finding instances of real-world objects from images or video sequences, object detection has been at- * Corresponding author is Liang Lin (Email: linliang@ieee.org).</p><p>tracting great interests in computer vision community. Although its performance has been improved substantially in the past decade <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>, object detection remains a challenge problem under complex and unconstrained environments.</p><p>Recently, ground breaking progress on object detection has been made due to the advances in deep convolutional neural networks (CNNs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref> and the increasing size of training dataset <ref type="bibr" target="#b4">[5]</ref>. The state-of-the-art object detection methods generally adopt the region-based CNN framework which includes three components: region proposal, feature extraction and object category classification. By far, many region proposal methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref> and deep CNN architectures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed, but not too many methods have been proposed for object category classification, where the SVM/softmax classifiers are dominantly used. Several complex classifiers, such as network on convolutional feature maps (NoCs) <ref type="bibr" target="#b24">[25]</ref> and structured SVM <ref type="bibr" target="#b37">[38]</ref>, have been developed to improve the accuracy and robustness of object detection. These classifiers, however, are fully discriminative methods which directly learn an optimal mapping from the CNN features to the desired classification output.</p><p>Combining of discriminative learning with representation or generative modeling is beneficial to exploit the complex structure of CNN features for improving object detection. As an extension of the reconstructive dictionary learning proposed in image and signal modeling, discriminative dictionary learning (DDL) has achieved great success in the last decade <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18]</ref>. DDL aims to learn a dictionary by considering both its representation accuracy and discriminative capability, and thus it is more suitable to act as a classifier for object category classification. However, the existing DDL methods cannot achieve state-of-the-art performance for large scale image classification and object detection, partially due to that the DDL models have only been evaluated with conventional handcrafted features (e.g., SIFT and HOG). Therefore, it is interesting to investigate whether we can significantly boost the object detection performance of DDL by utilizing more powerful deep CNN features.</p><p>Computational burden is another obstacle which restricts the application of DDL to large scale scenarios. Most DDL models involve costly ℓ 0 -or ℓ 1 -norm regularization to generate sparse coding vectors, limiting their use to the scenario with high feature dimension and large volumes of data. Fortunately, Gu et al. <ref type="bibr" target="#b14">[15]</ref> suggested a projective dictionary pair learning (DPL) method, which improves greatly the computational efficiency. To avoid costly sparse coding, DPL adopts an analysis dictionary to generate coding vector via linear projection and a synthesis dictionary for classspecific discriminative reconstruction, respectively. In this work, we propose to design a dictionary pair classifier layer (DPCL) at the end of the CNN for object detection. For readability, some main abbreviations of this paper are listed in Tab. 1.</p><p>Rather than learning the CNN and the dictionary pair separately, we adopt a joint training mechanism for simultaneous feature learning and classifier learning. A dictionary pair back propagation (DPBP) algorithm is proposed to jointly update the parameters of CNN and DPCL in an end-to-end learning manner. With DPBP, we can fine-tune the trained CNN to extract discriminative features specialized to DPCL. Meanwhile, DPCL is tailored to the learned CNN features and better detection results can be expected.</p><p>Furthermore, we present a sample weighting scheme in DPCL to improve the localization accuracy. As analyzed in <ref type="bibr" target="#b12">[13]</ref>, poor localization remains the major type of detection errors. One major cause of inaccurate localization is that the objective of classifier is to correctly predict the category label of the object, while the objective of detection is to accurately estimate the location. To make classification conformable with localization, careful selection of thresholds of the intersection-over-union (IoU) with the ground truth is important to define positive and negative samples <ref type="bibr" target="#b12">[13]</ref>. To alleviate the inaccurate localization, Zhang et al. <ref type="bibr" target="#b37">[38]</ref> adopted the structured SVM classifier to simultaneously predict category and location, while Girshick <ref type="bibr" target="#b11">[12]</ref> suggested a multi-task loss to balance between classification and localization. Different from <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12]</ref>, we introduce a predefined weight to each training sample based on its IoU overlapping with the ground truth bounding box, encouraging the samples with higher IoUs (i.e., better localization) to have lower reconstruction residual (i.e., higher score). Experimental results show that the weighting scheme in DP-CLs can further improve the detection performance.</p><p>Motivated by the success of multi-task learning <ref type="bibr" target="#b3">[4]</ref> in object detection <ref type="bibr" target="#b11">[12]</ref>, we present a novel multi-task loss for joint training of the DPCL and bounding-box regressor. In <ref type="bibr" target="#b11">[12]</ref>, Girshick considered two learning tasks, where the classification task loss is on the probability distribution over K + 1 categories (K object categories and one background category), and the location task loss is on the bounding box regression offsets. In this work, we divide the classification task into two related ones, i.e., an objectness task to distinguish object from background and a categoryness task to recognize the category of the object. Although the objectness <ref type="bibr" target="#b1">[2]</ref> can be used as a pre-filtering process in object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>, its potential remains untapped and not fully released. First, most objectness measures are based on hand-crafted features, while the learned objectness on deep CNN features can further benefit object detection. Second, the incorporation of objectness and categoryness allows us to use the coarse-to-fine strategy for object category classification. Third, our objectness detection task is not aimed to learn a general objectness measure but to learn a classifier to distinguish background from objects of interest. To this end, we employ two separate DPCLs to accomplish the two correlated tasks, i.e., objectness learning and categoryness learning, and our multi-task loss includes three tasks: objectness, categoryness, and localization. Compared with <ref type="bibr" target="#b11">[12]</ref>, we adopt a hybrid fusion strategy, where the product rule is used to fuse objectness score and categoryness score into classification score, and the sum rule is then utilized to combine classification score and localization loss. Moreover, DPBP can also be extended to minimize the multi-task loss in an end-to-end manner.</p><p>By integrating DPCL classifier training with CNN feature learning, the proposed method achieves about 3% / 2% mAP gain over the popular existing object detection frameworks (e.g., R-CNN <ref type="bibr" target="#b12">[13]</ref>, FRCN <ref type="bibr" target="#b11">[12]</ref>) on PASCAL VOC 2007/2012 benchmarks, respectively. This establishes the significance of the joint learning framework as well as the proposed multi-task loss. In summary, the contributions of this work are three-fold. i) A novel deep architecture is developed by integrating DPCL with CNN for objection detection, and a DPBP algorithm is suggested for the endto-end learning of CNN and DPCL parameters. ii) Based on the R-CNN <ref type="bibr" target="#b12">[13]</ref>/FRCN <ref type="bibr" target="#b11">[12]</ref> framework, we propose a novel multi-task loss by combining objectness estimation, categoryness computation and bounding box regression to improve the detection performance. iii) A sample weighting scheme is introduced to assign larger weight to the training samples with higher IoU with the ground truth, which can further improve the location accuracy of object detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Convolutional Neural Networks. By directly learning features from raw images, deep convolutional neural networks (CNNs) have made impressive progresses on image classification, object detection, semantic segmentation and many other recognition tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17]</ref>. Motivated by the success of CNNs <ref type="bibr" target="#b18">[19]</ref> on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" target="#b4">[5]</ref>, a variety of CNN-based object detection methods have been proposed. Szegedy et al. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b5">6]</ref> treated object detection as a regression problem, and trained CNNs to predict object bounding boxes (MultiBox) or bounding box masks (Detec-torNet). Overfeat <ref type="bibr" target="#b25">[26]</ref> suggested by Sermanet et al. adopts the sliding window scheme, and uses two CNNs to predict the objectness and the true bounding box location, respectively. Deformable parts models (DPMs) can also be explained from the CNN perspective, and the integration of DPMs and CNNs has been investigated in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Most recent object detection methods are based on the R-CNN framework <ref type="bibr" target="#b12">[13]</ref>, which includes three main components: region proposal, feature extraction and object category classification. To improve the efficiency of region proposal generation, Szegedy et al. <ref type="bibr" target="#b28">[29]</ref> improved Multi-Box by using the Inception-style network, contextual features and robust loss, while Ren et al. <ref type="bibr" target="#b23">[24]</ref> suggested a region proposal network (RPN). To improve the efficiency of detection network and avoid region proposal resizing, spatial pyramid pooling networks (SPPnets) <ref type="bibr" target="#b15">[16]</ref> and fast R-CNN <ref type="bibr" target="#b11">[12]</ref> proposed to introduce a RoI-pooling layer to extract fixed-size proposal features from shared convolutional feature maps of the entire image. For better classification and localization, Girshick adopted a multi-task loss, and Zhang et al. <ref type="bibr" target="#b37">[38]</ref> used a fine-grained Bayesian search algorithm for region proposal refining and a structured SVM classifier for simultaneous classification and localization. Besides, contextual information, e.g., background, parts, and segmentation, can also be utilized to improve the detection performance <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Discriminative Dictionary Learning. Discriminative dictionary learning (DDL) plays an important role in sparse representation or collaborative representation based classifier <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>, and has been intensively studied in computer vision community. Generally, there are two approaches to enhance the discriminative capability of the learned dictionary. First, the discrimination can be imposed on the coding vectors to have a better classification performance. Jiang et al. <ref type="bibr" target="#b17">[18]</ref> introduced a binary class label sparse code matrix to encourage samples from the same class to have similar sparse codes. Mairal et al. <ref type="bibr" target="#b21">[22]</ref> proposed a task driven dictionary learning (TDDL) framework, which minimizes different risk functions of the coding coefficients for different tasks. Yang et al. <ref type="bibr" target="#b35">[36]</ref> proposed a Fisher discrimination dictionary learning (FDDL) method which applies the Fisher criterion to representation coefficients.</p><p>Second, the discrimination can also be obtained by learning structured dictionary, i.e., learning a sub-dictionary for each class and minimizing the class-specific residual <ref type="bibr" target="#b35">[36]</ref>. Ramirez et al. <ref type="bibr" target="#b22">[23]</ref> used a structured incoherence term to enforce the independence of the sub-dictionaries. Besides the sub-dictionaries, Gao et al. <ref type="bibr" target="#b9">[10]</ref> learned an extra shared dictionary to encode common features shared by all classes. To improve the efficiency of DDL, Gu et al. <ref type="bibr" target="#b14">[15]</ref> proposed a projective projective dictionary pair learning (DPL) model by utilizing an analytic dictionary to estimate the representation coefficients efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Integration of DPCL and CNN</head><p>3.1. The Dictionary Pair Classifier Layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Layer Description</head><p>Let X = [X 0 , ..., X k , ..., X K ] (X k ∈ R d×n k , n k is the number of training samples for the k-th category) denote a set of previous layer's d-dimensional outputs for the input image regions I from K + 1 categories. The DPCL aims to find a class-specific analysis dictio-</p><formula xml:id="formula_0">nary P = [P 0 , ..., P k , ..., P K ] ∈ R m(K+1)×d (P k ∈ R m×d ) and a class-specific synthesis dictionary D = [D 0 , ..., D k , ..., D K ] ∈ R d×m(K+1) (D k ∈ R d×m )</formula><p>to analytically encode and reconstruct the feature X, where m is the number of dictionary atoms. The sub-dictionaries P k and D k form a dictionary pair for the k-th category. Given P k and D k , the encoding coefficients A k of the k-th category training samples X k over synthesis D k can be analytically obtained as A k = P k X k . Compared to the costly l 0 -norm or l 1 -norm non-linear sparse coding operation in most of the existing DDL methods, it is quite efficient to resolve the code A k for the representation of X k in DPL. To learn such an analysis dictionary P together with the synthesis dictionary D, the DPL model <ref type="bibr" target="#b14">[15]</ref> is formulated as:</p><formula xml:id="formula_1">{P * , D * } = arg min P,D K k=0 X k − D k P k X k 2 F +Φ{P, D, X, Y},<label>(1)</label></formula><p>where Y represents the category label matrix of samples in X, and Φ{P, D, X, Y} is some discrimination term to promote the discriminative power of D and P.</p><p>In the original DPL <ref type="bibr" target="#b14">[15]</ref>, the sub-dictionary P k is enforced to project the samples X k from another category i, i = k, to a nearly null space, i.e., P k X i ≈ 0, ∀k = i. With this constraint, the coefficient matrix A k is nearly block diagonal. However, the original DPL does not consider the fact that different training samples may play different importance in training a discriminative model. In this work, we introduce a diagonal importance weight matrix W k to the k-th category of training samples, and the proposed DPCL is then defined as: </p><formula xml:id="formula_2">{P * , D * } = arg min P,D K k=0 (X k − D k P k X k )W k 2 F +λ P k X k 2 F + κ D k 2 F .<label>(2)</label></formula><p>where λ &gt; 0 and κ &gt; 0 are scalar constants, X k denotes the complementary data matrix of X k from the whole training samples X. To avoid the trivial solution of P k = 0, an extra constraint term D k 2 F is added. The introduction of W k is to improve the localization performance. To this end, we assign higher weights to the samples with better localization. By this way, lower reconstruction residual will be expected for sample with higher weight, and thus the reconstruction residual can be adopted to find the proposal with better localization. Therefore, we use the IoU with the ground truth bounding box of the k-th object category to define W k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Learning of the Dictionary Pair</head><p>To exploit the alternating minimization algorithm for dictionary pair learning, we relax Eqn. (2) by introducing a coding coefficient matrix A:</p><formula xml:id="formula_3">{P * , A * , D * } = arg min P,A,D K k=0 (X k − D k A k )W k 2 F +τ (P k X k − A k )W k 2 F + λ P k X k 2 F + κ D k 2 F ,<label>(3)</label></formula><p>where τ is a scalar constant. All terms in the above objective function are characterized by the squared Frobenius norm, and thus Eqn. (3) can be efficiently solved by alternating minimization.</p><p>By initializing P and D with random matrices with unit Frobenius norm, the minimization for Eqn. (3) can be performed by alternating between the following three steps:</p><p>(i) Fix {D, P, X}, and update A via:</p><formula xml:id="formula_4">A * k = (D T k D k + τ I) −1 (τ P k X k + D T k X k ).<label>(4)</label></formula><p>(ii) Fix {D, A, X}, and update P via:</p><formula xml:id="formula_5">P * k = τ A k W k W T k X T k (τ X k W k W T k X T k +λX k X T k + γI) −1 ,<label>(5)</label></formula><p>where the constant γ is empirically set as 0.0001 according to the validation set.</p><p>(iii) Fix {A, P, X}, update D via:</p><formula xml:id="formula_6">D * k = X k W k W T k A T k (A k W k W T k A T k + κI) −1 . (6)</formula><p>Since all steps have closed-form solutions for {A, P, D}, the 3-step minimization is quite efficient. We stop the iteration when the difference between the energy in two adjacent iterations is less than a threshold (e.g., 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dictionary Pair Back Propagation</head><p>In this section, we propose a dictionary pair back propagation (DPBP) algorithm for joint learning of DPCL and CNN parameters in an end-to-end manner.</p><p>The dictionary pair (D k , P k ) of the DPCL model can be optimized separately, and thus Eqn. (2) can be decomposed into the following K + 1 sub-problems:</p><formula xml:id="formula_7">arg min P k ,D k L k (P k , D k ) = arg min P k ,D k (X k − D k P k X k )W k 2 F +λ P k X k 2 F + κ D k 2 F .<label>(7)</label></formula><p>In DPBP, the partial derivatives with respect to {P k , D k } are defined as:</p><formula xml:id="formula_8">∂L k (P k , D k ) ∂P k = −2D k (I − D k P k )X k W k W T k X T k +2λP k X k X T k ∂L k (P k , D k ) ∂D k = −2X k W k (I − D k P k )W T k X T k P T k +2κD k<label>(8)</label></formula><p>With L = K k=0 L k , the partial derivatives with respect to X k is then defined as:</p><formula xml:id="formula_9">∂L ∂X k = 2(I − P T k D T k )(X k − D k P k X k )W k W T k + k ′ =k 2λP T k ′ P k ′ X k<label>(9)</label></formula><p>Once all ∂L ∂X k are obtained, we can perform the standard back propagation <ref type="bibr" target="#b19">[20]</ref> to update the CNN parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object Detection on Test Image</head><p>Given a proposal I from the test image, we first extract the CNN feature x from I, and then define the reconstruction residual for the k-th category as:</p><formula xml:id="formula_10">L(x; D k , P k ) = x − D k P k x 2 F .<label>(10)</label></formula><p>The classification rule of the DPCL is</p><formula xml:id="formula_11">y = arg min i L(x; D i , P i ).<label>(11)</label></formula><p>When y = 0, we further use bounding box regression to refine the location of the object location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimization with Multi-Task Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-Task Loss</head><p>The proposed DPCL is a category classification method and is not conformable with localization task. To improve localization, Girshick [12] adopted a multi-task loss to balance classification and localization. In this method, each proposal is classified into either background or one of the object categories, which may not work well in distinguishing background from object categories. To address this issue, we further decompose the classification task into two related ones. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the feature extracted by the CNN layers is duplicated and simultaneously fed into two DPCLs: the objectness DPCL layer and the categoryness DPCL layer. The former estimates the score for being an object, while the latter computes the scores for being a specific object category.</p><p>Objectness Dictionary Pair Classifier Layer. The objectness usually is defined as the score of covering objects of any category. For the purpose of measuring the objectness of the input region, the proposed Objectness Dictionary Pair (ODP) layer applies two dictionary pairs {D o , P o } and {D b , P b } to represent objects of any category and background, respectively. If a region feature x can be better represented by the background dictionary pairs {D b , P b }, it is very unlikely to have objects inside. Rather than directly identify the background according to Eqn. <ref type="bibr" target="#b10">(11)</ref>, ODP calculates objectness of the input feature x for further object detection in a soft way: a threshold T is used to distinguish the region with large background. With the reconstruction residual defined in Eqn. <ref type="bibr" target="#b9">(10)</ref>, the objectness score Q(x) for the feature x of the input region is defined as:</p><formula xml:id="formula_12">Q(x) = 1 − L(x;Do,Po) i∈{o,b} L(x;Di,Pi) ; L(x;Do,Po) L(x,D b ,P b ) &lt; T ; 0, otherwise,<label>(12)</label></formula><p>where T controls the precision and recall rate of detecting background (larger T results in higher precision and lower recall rate), and is empirically set as 0.5 according to the validation set. Thus, our model is able to identify the background based on whether Q(x) is 0 or not.</p><p>Categoryness Dictionary Pair Classifier Layer. The categoryness score S(x, k) denotes the likeliness that the feature x belongs to the k-th category. In order to compute the categoryness for object detection, the proposed Categoryness Dictionary Pair (CDP) layer consists of K dictionary pairs, where K is the number of object categories. Once the feature x of the input region is fed, CDP will encode x over the K category-specific dictionary pairs {D k , P k } and output the reconstruction residual for each dictionary pair. We define the categoryness S(x, k) using the reconstruction residual as:</p><formula xml:id="formula_13">S(x, k) = 1 − L(x; D k , P k ) · e βL(x;D k ,P k ) K i=1 L(x; D i , P i ) · e βL(x;Di,Pi) ,<label>(13)</label></formula><p>where the constant β is empirically set as 0.003 according to the validation set. Then, the product rule is used to fuse objectness score and categoryness score, and the classification score F k that x belongs to the k-th category is defined as:</p><formula xml:id="formula_14">F k (x) = S(x, k) * Q(x).<label>(14)</label></formula><p>Let φ denote the function of the CNN layers and I i denote the input region with the category label y i , we have the feature x = φ(I, ω). With the classification score F k , the final classification loss is defined as:</p><formula xml:id="formula_15">L cls (I) = K k=0 1(y = k) log F k (φ(I, ω)) +(1 − 1(y = k)) log(1 − F k (φ(I, ω))) +R{ω, D, P},<label>(15)</label></formula><p>where 1 ∈ {0, 1} is the indicator function, and R{ω, D, P} denotes the regularization term on the parameters of CNN and two DPCLs. Bounding Box Regression Loss. Our defined multi-task loss can easily append other correlated loss, e.g., the robust loss in <ref type="bibr" target="#b11">[12]</ref>. Let t k (I) = (t k x , t k y , t k w , t k h ) and t * (I) = (t * x , t * y , t * w , t * h ) be the predicted and ground truth bounding boxes of the proposal I, where k denotes that the proposal I belongs to the k-th object category. Then, the bounding box regression loss is defined as:</p><p>L loc (t k (I), t * (I)) = i∈x,y,w,h where H 1 (z) is the Huber loss</p><formula xml:id="formula_16">H 1 (t k i − t * i ),<label>(16)</label></formula><formula xml:id="formula_17">H 1 (z) = 0.5x 2 , if |z| &lt; 1 |z| − 0.5, otherwise ,<label>(17)</label></formula><p>which is robust to outliers. We can adopt the sum rule in <ref type="bibr" target="#b11">[12]</ref> to combine L cls and L loc , and the multi-task loss is defined as:</p><formula xml:id="formula_18">L mt = − 1 N N i=1 L cls (I i ) + p * i L loc (t k (I i ), t * (I i )) ,<label>(18)</label></formula><p>where p * i is an indicator to denote whether the proposal I i is an object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimization</head><p>After obtaining the partial derivatives of L mt with respect to D b , P b , D o , P o , D k , P k , X k , we can extend DPBP to fine-tune CNN+DPCL to update the dictionary pairs, CNN parameters and bounding box regressors. To optimize L mt , we initialize the CNN parameters with some pre-trained network, e.g., AlexNet <ref type="bibr" target="#b18">[19]</ref> or VGG <ref type="bibr" target="#b26">[27]</ref>, and initialize the dictionary pairs using the dictionary pair learning algorithm in Sect. 3.1.2. Then the DPBP algorithm is adopted to further optimize CNN+DPCL in an end-to-end manner. We summarize the whole learning procedure as Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference</head><p>The inference task is to predict the detection scores and bounding box to a given image region I. Formally, we perform forward propagation to output the CNN feature φ(I, ω) of the region, and then feed it into the ODP layer and the CDP layer, simultaneously. With the learned dictionary pairs, we calculate reconstruction residuals of the feature via Eqn. <ref type="bibr" target="#b9">(10)</ref>, and obtain the objectness Q(φ(I, ω)) via Eqn. <ref type="bibr" target="#b11">(12)</ref> as well as the categoryness S(φ(I, ω)) for each category via Eqn. <ref type="bibr" target="#b12">(13)</ref>. Finally, our model outputs the final object detection score via Eqn. <ref type="bibr" target="#b13">(14)</ref> for each object category. If Q(φ(I, ω)) &gt; 0, we further use the bounding box regressors to predict the object location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We demonstrate the performance of our proposed joint feature and DPCL learning framework on serveral object detection benchmarks. The experiments are conducted on the commonly used Pascal VOC 2007/2012 datasets <ref type="bibr" target="#b6">[7]</ref>. During evaluation, we adopt the PASCAL Challenge protocol: a correct detection should has more than 0.5 IoU with the ground truth bounding-box. The performance is evaluated by mean Average Precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Parameter Setting</head><p>In all experiments, we set {τ, λ, κ, β, γ, T , m} as {0.01, 0.01, 0.001, 0.003, 0.0001, 0.5, 64}. We consider R-CNN <ref type="bibr" target="#b12">[13]</ref> with AlexNet <ref type="bibr" target="#b18">[19]</ref> / VGG <ref type="bibr" target="#b26">[27]</ref> and FRCN with VGG <ref type="bibr" target="#b26">[27]</ref> as the baseline model. Following the same experiment settings in <ref type="bibr" target="#b12">[13]</ref>, the employed CNN parameters are firstly pretrained on ImageNet, and then fine-tuned on the corresponding VOC training and validation sets by stochastic gradient descent (SGD) with a 21-way softmax loss (20 object categories plus one background). Then we replace the softmax classification layer with our proposed model, and fine-tune the network via DPBP with learning rate starting at 0.00001 and momentum beginning at 0.9. During the fine-tuning, all regions with &lt; 0.5 IoU overlap with a ground-truth bounding box are treated as background, while those with ≥ 0.5 IoU are considered as positives for the corresponding object category. The weight of these positives is defined as the IoU with the ground truth bounding box. For instance, if a region has 0.6 IoU with the ground-truth bounding box from the cat category, then it is a positive sample with the weight 0.6 for the further dictionary pair learning of the cat category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Comparisons</head><p>We denote by R-CNN(Alex/VGG) <ref type="bibr" target="#b12">[13]</ref> and FRCN <ref type="bibr" target="#b11">[12]</ref> the used CNN frameworks, by ODP and CDP the proposed objectiveness and categoryness dictionary pair layers, and by BB the Bounding Box regression in the R-CNN framework. From Tab. 2, BB regression can consistently achieve 3% ∼ 4% performance gain by mAP. Therefore, we have included BB regression for all the results listed in the Tab. 3∼5, and the comparison is fair. Our full implemented model with the proposed DPBP in AlexNet <ref type="bibr" target="#b18">[19]</ref> is then denoted as "R-CNN(Alex) + CDP + ODP". Other variants of our implementation are represented similarly.</p><p>In <ref type="table">Tab</ref>    <ref type="figure" target="#fig_2">Fig. 2</ref> demonstrates some object detection examples obtained by the proposed method and FRCN. Thanks to the use of reconstruction residual in both objectness estimation and categoryness calculation, the selected bounding box with optimal detection score by our method has fewer background, as shown in <ref type="figure" target="#fig_2">Fig. 2 (a)</ref>. When one bounding box covers more background, its reconstruction residual over the objectness dictionary pairs will be larger, resulting in a lower detection score. Thanks to the divide-and-conquer manner brought by ODP and CDP, our model can recognize more objects <ref type="figure" target="#fig_2">(Fig. 2 (b)</ref>) in the image with better accuracy <ref type="figure" target="#fig_2">(Fig. 2 (c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis</head><p>For further evaluation, we conduct the following three empirical analysis under different settings. To show the advantages of the proposed model, we directly employ the pre- trained CNN models (VGG <ref type="bibr" target="#b26">[27]</ref>) under the FRCN framework, and perform component analysis on the VOC 2007 dataset.</p><p>(I) We demonstrate the effectiveness of incorporating objectness estimation into our model for object detection. That is, we discard the ODP in our model, and train it only with the CDP via DPBP. Note that, the number of dictionary pairs is now 21 (20 object categories plus background). We denote the model without ODP as "FRCN+CDP". Similarly, we introduce ODP into FRCN and keep its softmax layer and denote this scheme as "FRCN+softmax+ODP". In FRCN+softmax+ODP, we directly adopt the FRCN model fine-tuned on PASCAL VOC 07+12, which is provided by the authors. Based on its feature representation, we train an extra ODP classifier, and use the original softmax classifier to replace CDP for object detection. As Tab. 5 reports, FRCN+softmax+ODP achieves 2.3% performance gain. "FRCN+CDP" drops by 1.0% the performance. This is because there are too many background samples to achieve fine level representation of objects. Hence, owe to detecting objects in a divide-and-conquer strategy, ODP makes great contributions to improve the detection accuracy.</p><p>(II) To clarify the significance of the proposed DPBP for network fine-tuning, we directly replace the softmax layer of FRCN by the proposed ODP and CDP layers. We denote these models as "FRCN+ODP+CDP (w/o FT)" and "FRCN+CDP (w/o FT)". The results demonstrate that finetuning can obtain about 1.0% performance gain.</p><p>(III) To demonstrate the effectiveness of predefined weights for training samples, we set all weights to 1 in our model. That is, the training samples have the same weights during the dictionary pair training inside ODP and CDP. We denote this model as "Ours (w/o weights)", and compare it with the original version "Ours (full)". As illustrated in <ref type="figure" target="#fig_4">Fig. 3</ref>, the test error rate of "Ours (w/o weights)" is much higher than "Ours (full)" after 10,000 iterations. The reason is that the category-specific dictionary pair is introduced to represent the parts of other category or background inside the training examples. By means of regarding the IoU as the predefined weights of training samples, this phenomenon can be suppressed. <ref type="figure" target="#fig_4">From Fig. 3</ref>, one can see that the introduced weights can make the training phase stable and achieve a lower error rate. Tab. 5 also demonstrates that  "FRCN+ODP+CDP" with weights can improve about 2% mAP, compared with "FRCN+ODP+CDP (w/o weights)".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The flowchart of our multi-loss CNN+DPCL model. Our model is stacked up by convolutional layers, fully connected layers, objectness DPCL and categoryness DPCL. The four values inside 2 × 2 grids are corresponding to the four input regions. The final score for each image region is the combination of objectness and categoryness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Training samples I = [I 1 , I 2 , ..., I K , I b ] for K + 1 classes (I b denotes background), pre-trained CNN layers' parameters. Output: Dictionary pairs {D, P} = {{D 1 , P 1 }, {D 2 , P 2 }, ..., {D K , P K }, {Do, Po}, {D b , P b }}, fine-tuned CNN parameter ω, bounding box regressors ωr. Initialization: 1. Initialize CNN parameters ω with pre-trained network; 2. Obtain output features x i = φ(I i ; ω) for all training samples; 3. Regard φ(I b ; ω) as background samples and the other φ(Io; ω) = [φ(I 1 ; ω), φ(I 2 ; ω), ..., φ(I K ; ω)] as object samples; 5. Optimize dictionary pair {D, P} as described in Sect. 3.1.2: i. Given φ(I b ; ω) and φ(Io; ω), train {Do, Po}, {D b , P b }; ii. Given φ(I k ; ω), train {D k , P k }, k = 1, ..., K. repeat 6. Fine-tune {D, P, ω, ωr} via mini-batch based back propagation on Lmt; until Eqn.(18) converges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Detection examples from PASCAL VOC 2007 dataset. The boxes and category labels obtained by baseline FRCN is in green, and those obtained by the proposed structure model is in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Test error rates with/without weighted training samples in the deep model. The solid curve represents our full model, and the dashed curve represents our model without using weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Method data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv</figDesc><table>. 2, we report in detail the accuracy on all ob-
ject categories of VOC 2007, compared with the meth-
DPM [8] 
07 33.7 33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 
43.2 12.0 21.1 36.1 46.0 43.5 
SS [31] 
07 33.7 43.5 46.5 10.4 12.0 9.3 49.4 53.7 39.4 12.5 36.9 42.2 26.4 47.0 52.4 
23.5 12.1 29.9 36.3 42.2 48.8 
Regionlet [34] 
07 41.7 54.2 52.0 20.3 24.0 20.1 55.5 68.7 42.6 19.2 44.2 49.1 26.6 57.0 54.5 
43.4 16.4 36.6 37.7 59.4 52.3 
DetNet [30] 
07 30.5 29.2 35.2 19.4 16.7 3.7 53.2 50.2 27.2 10.2 34.8 30.2 28.2 46.6 41.7 
26.2 10.3 32.8 26.8 39.8 47.0 
R-CNN(Alex) [13] 
07 54.2 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 
54.2 31.5 52.8 48.9 57.9 64.7 
SPP(Alex) [16] 
07 55.2 65.5 65.9 51.7 38.4 32.7 62.6 68.6 69.7 33.1 66.6 53.1 58.2 63.6 68.8 
50.4 27.4 53.7 48.2 61.7 64.7 
Best approach of [32] 
07 46.9 49.3 69.5 31.9 28.7 40.4 61.5 61.5 41.5 25.5 44.5 47.8 32.0 67.5 61.8 
46.7 25.9 40.5 46.0 57.1 58.2 
R-CNN(Alex)+ODP+CDP 
07 57.5 64.8 71.5 54.6 46.1 50.7 68.9 78.2 56.9 36.2 58.3 47.1 51.2 67.5 67.8 66.0 34.7 61.5 42.8 58.7 66.7 
R-CNN(VGG) [13] 
07 62.2 71.6 73.5 58.1 42.2 39.4 70.7 76.0 74.5 38.7 71.0 56.9 74.5 67.9 69.6 
59.3 35.7 62.1 64.0 66.5 71.2 
R-CNN(VGG)+BB [13] 
07 66.0 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 
64.2 35.6 66.8 67.2 70.4 71.1 
Best approach of [38] with BB 07 68.5 74.1 83.2 67.0 50.8 51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4 75.1 
70.1 41.4 69.6 60.8 70.2 73.7 
R-CNN(VGG)+ODP+CDP 
07 65.7 71.0 76.5 62.8 49.5 58.3 76.9 81.1 74.0 43.5 72.5 58.1 71.8 75.2 74.0 
70.2 42.2 65.2 56.9 64.9 70.0 
R-CNN(VGG)+ODP+CDP+BB 07 68.6 75.0 79.3 65.3 52.8 60.9 80.2 81.7 77.0 45.2 75.5 62.5 76.1 80.3 74.8 
71.7 42.1 68.1 59.4 72.3 71.7 

Table 2. Test set mAP for VOC 2007. The entries with the best APs for each object category are bold-faced. Training data key: "07": 
VOC07 trainval. 

Method 
data mAP areo bike bird boat bottle bus car 
cat chair cow table dog horse mbike person plant sheep sofa train tv 
FRCN 
07 
66.9 74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 
73.0 
69.0 
30.1 65.4 70.2 75.8 65.8 
FRCN+CDP+ODP 
07 
71.1 78.6 78.9 70.4 57.8 47.7 83.1 82.5 84.2 51.8 75.7 69.1 80.4 82.1 
77.7 
76.3 
42.4 69.0 68.6 77.6 67.3 
FRCN 
07+12 70.0 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 
76.6 
69.9 
31.8 70.1 74.8 80.4 70.4 
FRCN+CDP+ODP 07+12 73.4 79.6 80.0 70.6 65.1 50.0 86.1 85.4 84.1 54.2 79.5 71.5 82.0 83.9 
79.3 
77.1 
44.6 69.2 74.1 83.3 69.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Test set mAP for VOC 2007. The entries with the best APs for each object category are bold-faced. Training data key: "07": 
VOC07 trainval, "07+12": VOC07 trainval union with VOC12 trainval, "07++12": VOC07 trainval and test with VOC12 trainval. 

Method 
data 
mAP areo bike bird boat bottle bus car 
cat chair cow table dog horse mbike person plant sheep sofa train tv 
FRCN 
12 
65.7 80.3 74.7 66.9 46.9 37.7 73.9 68.6 87.7 41.7 71.1 51.1 86.0 77.8 
79.8 
69.8 
32.1 65.5 63.8 76.4 61.7 
FRCN+CDP+ODP 
12 
66.9 81.0 75.1 69.8 50.0 43.6 73.4 71.0 87.7 44.4 69.6 54.4 85.5 77.2 
77.8 
72.0 
37.1 66.4 58.8 77.0 65.5 
FRCN 
07++12 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 
80.8 
72.0 
35.1 68.3 65.7 80.4 64.2 
FRCN+CDP+ODP 07++12 69.7 82.4 78.1 72.1 55.7 44.3 77.6 73.3 89.4 49.1 73.4 56.3 87.0 80.2 
79.5 
74.4 
40.9 67.0 66.9 78.9 67.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Test set mAP for VOC 2012. The entries with the best APs for each object category are bold-faced. Training data key: "12": VOC07 trainval, "07++12": VOC07 trainval and test with VOC12 trainval. ods based on hand-crafted feature engineering<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31]</ref> and deep CNNs<ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b31">32]</ref>. Given the same CNN and region proposals (AlexNet), our proposed model, R-CNN(Alex)+ODP+CDP, achieves the mAP of 57.5%, distinctly superior to the other six competing methods, i.e., 33.7%<ref type="bibr" target="#b7">[8]</ref>, 33.7%<ref type="bibr" target="#b30">[31]</ref>, 41.7%<ref type="bibr" target="#b33">[34]</ref>, 46.9%<ref type="bibr" target="#b31">[32]</ref>, 30.5%<ref type="bibr" target="#b29">[30]</ref>, 54.2%<ref type="bibr" target="#b12">[13]</ref> and 55.2%<ref type="bibr" target="#b15">[16]</ref>. Given another network architecture VGG, our R-CNN(VGG)+ODP+CDP model obtains 65.7%/68.6% mAP with/without bounding box regression, and is comparable with the recently published state-of-the-art method<ref type="bibr" target="#b37">[38]</ref>.Tab. 3 and Tab. 4 demonstrate that our proposed model FRCN+CDP+ODP can consistently achieve 3% and 2% performance gain over FRCN on VOC 2007 and VOC 2012 datasets, and also justify that our model is robust to different CNN frameworks.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Method data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv FRCN+CDP+ODP (w/o weights) 07+12 71.1 78.9 79.0 70.5 59.0 47.2 83.7 82.7 84.7 51.5 75.3 69.1 80.3 82.Table 5. Test set mAP for VOC 2007. The entries with the best APs for each object category are bold-faced. "07+12": VOC07 trainval union with VOC12 trainval.</figDesc><table>FRCN 
07+12 70.0 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 
69.9 31.8 70.1 74.8 80.4 70.4 
FRCN+softmax+ODP 
07+12 72.3 81.3 79.7 71.6 65.4 47.9 86.6 84.0 85.6 48.4 78.6 70.2 80.4 82.9 77.6 
70.8 43.7 69.8 71.3 81.8 69.6 
FRCN+CDP (w/o FT) 
07+12 70.2 74.7 76.1 68.3 60.2 43.6 79.8 79.1 82.7 50.5 77.3 69.9 83.9 81.0 72.0 
68.8 37.4 73.3 71.8 77.2 76.7 
FRCN+CDP 
07+12 70.9 78.1 78.8 70.1 57.8 47.8 84.1 82.9 84.1 51.7 75.2 67.5 79.7 82.3 77.0 
76.2 42.5 68.2 68.4 77.6 67.7 
FRCN+CDP+ODP (w/o FT) 
07+12 72.4 83.0 83.4 77.1 56.1 42.7 83.5 72.8 90.5 52.5 73.3 62.0 90.0 81.8 85.1 
69.1 44.0 71.2 73.6 85.1 72.0 
4 77.6 
76.2 41.8 67.8 69.4 77.3 67.7 
FRCN+CDP+ODP 
07+12 73.4 79.6 80.0 70.6 65.1 50.0 86.1 85.4 84.1 54.2 79.5 71.5 82.0 83.9 79.3 
77.1 44.6 69.2 74.1 83.3 69.2 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported in part by the Hong Kong Polyutechnic University's Joint Supervision Scheme with the Chinese Mainland, Taiwan and Macao Universities (Grant no. G-SB20). This work was also supported in part by the Guangdong Natural Science Foundation under Grant S2013050014548 and 2014A030313201, in part by Special Program for Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (the second phase), and in part by the Fundamental Research Funds for the Central Universities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we presented dictionary pair classifierdriven CNNs for object detection, where dictionary pair back propagation (DPBP) is proposed for the end-to-end learning of dictionary pair classifiers and CNN representation, and sample weighting is adopted to improve the localization performance. Furthermore, a multi-task loss is suggested for joint training of the DPCLs and bounding-box regressor. Experiments demonstrated the superiority of the proposed framework. In the future, we will apply our model with other powerful CNNs to improve detection accuracy.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge 2007 (voc2007) results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint discriminative dimensionality reduction and dictionary learning for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2134" to="2143" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning category-specific dictionary and shared dictionary for fine-grained image categorization. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="623" to="634" />
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion &amp; semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Projective dictionary pair learning for pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lsda: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3536" to="3544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Label consistent k-svd: learning a discriminative dictionary for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representing and recognizing objects with massive local image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Task-driven dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classification and clustering via dictionary learning with structured incoherence and shared features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Object detection networks on convolutional feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06066</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scalable, high-quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno>abs/1412.1441</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end integration of a convolution network, deformable parts model and nonmaximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Incorporating structural alternatives and sharing into hierarchy for multiclass object recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse representation based fisher discrimination dictionary learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sparse representation or collaborative representation: Which helps face recognition? In ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">segdeepm: Exploiting segmentation and context in deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
