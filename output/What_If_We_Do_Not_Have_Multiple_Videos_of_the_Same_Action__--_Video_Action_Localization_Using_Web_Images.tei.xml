<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What if we do not have multiple videos of the same action? - Video Action Localization Using Web Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
							<email>waqassultani@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="institution">University of Central Florida (UCF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="institution">University of Central Florida (UCF)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What if we do not have multiple videos of the same action? - Video Action Localization Using Web Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper tackles the problem of spatio-temporal action localization in a video, without assuming the availability of multiple videos or any prior annotations. Action is localized by employing images downloaded from internet using action name. Given web images, we first dampen image noise using random walk and evade distracting backgrounds within images using image action proposals. Then, given a video, we generate multiple spatio-temporal action proposals. We suppress camera and background generated proposals by exploiting optical flow gradients within proposals. To obtain the most action representative proposals, we propose to reconstruct action proposals in the video by leveraging the action proposals in images. Moreover, we preserve the temporal smoothness of the video and reconstruct all proposal bounding boxes jointly using the constraints that push the coefficients for each bounding box toward a common consensus, thus enforcing the coefficient similarity across multiple frames. We solve this optimization problem using variant of two-metric projection algorithm. Finally, the video proposal that has the lowest reconstruction cost and is motion salient is used to localize the action. Our method is not only applicable to the trimmed videos, but it can also be used for action localization in untrimmed videos, which is a very challenging problem. We present extensive experiments on trimmed as well as untrimmed datasets to validate the effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Bounding box annotations have played a crucial role in development of several computer vision applications, such as: object/action recognition, detection, tracking and segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6]</ref>. However, these annotations are cumbersome to obtain, require hundreds of hours and are subject to human biases.</p><p>To mitigate this annotation challenge, several weakly-  our key idea of action localization in a video using images. We first download images of an action of interest from internet. After removing noisy images, we co-localize all the images jointly to obtain action proposals in each of the image. Then, given the candidate action locations in a video, we leverage image proposals to discover the most action representative proposal in a video.</p><p>supervised approaches have been introduced recently <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>, particularly in the object domain. In general, all these approaches assume presence of dominant centered objects in multiple images. For instance, the method proposed in <ref type="bibr" target="#b8">[9]</ref> annotate objects from previously annotated images, <ref type="bibr" target="#b20">[21]</ref> obtain bounding boxes by involving human eye tracking, and <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3]</ref> achieve object annotation using multiple images, where most of these images contain the object of interest. As compared to object annotation, spatio-temporal action annotations in videos are far more challenging and, therefore, it is not surprising that most of recent action datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14]</ref> contain only a few or no spatio-temporal annotations. The straightforward approach to obtain spatiotemporal action annotations in a video would be to extend any of the previously mentioned methods from image domain to video domain. However, temporal extension has many challenges due to large search space and critical differences between spatial and temporal dimensions <ref type="bibr" target="#b32">[33]</ref>.</p><p>More importantly, what if we do not have available multiple videos of the same action?</p><p>To tackle the challenge of action localization in a single video, we propose to leverage images downloaded from the internet using text-based queries. In contrast to previous works in object annotations, we neither assume availability of bounding box annotations nor the presence of multiple videos of the same class. Furthermore, we do not assume the availability of clean images either.</p><p>Images are usually taken to capture key poses, descriptive viewpoints and important instances of an action or event <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Our key idea is to exploit this useful information to obtain precise spatio-temporal action localization in videos. To operationalize our intuition (see <ref type="figure" target="#fig_1">Fig. 1</ref>) , we first download several images of the action of interest using the action label as a query from Google. These images contain human performing actions in different locations (not necessarily at the center), backgrounds and include many irrelevant and noisy images. To circumvent these issues, we remove irrelevant noisy images using random walk. To handle the challenge of variable locations and backgrounds, we co-localize the action in multiple images using a recently proposed unsupervised localization method <ref type="bibr" target="#b2">[3]</ref>. The output of these steps is the candidate action localization in the images.</p><p>Our ultimate goal is to obtain spatio-temporal annotations in a video. Therefore, given a video clip, we first obtain action proposals <ref type="bibr" target="#b19">[20]</ref>. These proposals represent candidate spatio-temporal action locations in the video. However, not all proposals are truly action representative as many are due to camera motion and cluttered backgrounds. Therefore, we remove highly overlapping action proposals using non-maximal suppression by exploiting optical flow gradient within the proposals. To obtain the most action representative proposal, we propose to reconstruct action proposals in the video by leveraging the action proposals in images. Furthermore, we preserve the temporal smoothness of the video by introducing consensus regularization. Consensus regularization enforces consistency among coefficients vectors of multiple frames within the proposal. The proposal with the lowest reconstruction error and a high motion saliency is selected as a final action localization.</p><p>Our experimental results reveal that it is possible to automatically annotate an action in a video by employing web images of the same action through mitigating the effect of distracting backgrounds within images and by preserving the temporal structure of video during reconstruction.</p><p>Most of the previous works demonstrate action localization accuracy either on trimmed videos or carefully staged clean untrimmed videos. However, these videos do not represent the real-world videos, which are long, have variable scenes and backgrounds and contain multiple or no instance of the action of interest. Since proposed approach does not require multiple videos and prior annotations, it can easily be applied to more realistic untrimmed videos. We have evaluated our approach on trimmed <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref> as well as on the part of untrimmed <ref type="bibr" target="#b13">[14]</ref> datasets and have obtained encouraging results.</p><p>In summary, 1) We demonstrate the feasibility of using images to achieve spatio-temporal action localization in videos, 2) By utilizing video proposal sparse reconstruction error with motion saliency, we achieve impressive localization results on popular trimmed action datasets, 3) We are the first to report spatio-temporal action localization results on (the part of) challenging untrimmed action dataset <ref type="bibr" target="#b13">[14]</ref>. Furthermore, we will release spatio-temporal annotations of 35, 000 frames of <ref type="bibr" target="#b13">[14]</ref> to facilitate further research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>With the exponential increase in the size of object/action datasets, obtaining annotations is becoming increasingly daunting task. Moreover, it is subject to human biases in terms of start and end of the activity and the sizes of the exact spatial boxes around an actor.</p><p>One way to avoid these time consuming annotations altogether is to use weakly supervised object/action detector approaches such as <ref type="bibr" target="#b24">[25]</ref>. This type of approaches only use image/video level labels and learn the object/action detector without requiring bounding box annotations. Although impressive, their accuracy is still far behind that of detectors trained on hundreds of bounding box annotations. Another interesting area of research relates weakly supervised annotations methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref>. Tang et al. <ref type="bibr" target="#b29">[30]</ref> introduced co-localization method where the objective is to obtain bounding boxes around common objects among multiple images. Their joint image and box formulation can also handle the presence of noisy images to some extent. Joulin et al. <ref type="bibr" target="#b14">[15]</ref> extended <ref type="bibr" target="#b29">[30]</ref> to videos and co-localize objects in several frames using multiple videos. Both methods require image or video level labels only. Recently, Cho et al. <ref type="bibr" target="#b2">[3]</ref> introduced part based matching approach to localize common objects across multiple images, without requiring images level labels. Given several images of different object classes, this method efficiently localize objects which are common in multiple images. Although encouraging results have been obtained, these methods require multiple images of the object of interest and cannot localize the objects if multiple images containing the same object are not available.</p><p>Recently, <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b25">[26]</ref> respectively introduced weakly supervised methods to obtain object segmentation and bounding box annotations in a single video. Similar to our approach, these methods do not require multiple images of object/action of interest. However, in contrast to our method, they rely on negative data (the videos that do not contain the object of interest). These methods robustly segment and locate object/action in a single video. We compare our approach with both of these methods and show superior performance.</p><p>There is an increasing interest in leveraging images and videos to improve the performance of either domain or both. The method in <ref type="bibr" target="#b21">[22]</ref> used YouTube videos to gather more examples for training object detectors. Chen et al. <ref type="bibr" target="#b1">[2]</ref> used unlabeled video to learn action detector for images. Kevin et al. <ref type="bibr" target="#b30">[31]</ref> proposed to adapt object detector from images to video. The approach presented in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref> use images to produce effective video summarization. The authors in <ref type="bibr" target="#b16">[17]</ref> presented an efficient framework to produce a joint summary of a video and Flicker images. Recently, Jain et al. <ref type="bibr" target="#b11">[12]</ref> demonstrates that object classifiers can be used to improve action recognition accuracy.</p><p>However, we are not aware of any previous work that uses images to localize an action in a video. In what follows, we first describe our approach in detail for trimmed videos and then present its extension to untrimmed videos (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weakly Supervised Action localization in Images</head><p>The first step of our approach is to obtain candidate action proposals in downloaded images. For this purpose, we download images from internet and obtain candidate action locations in each image. The details of each step are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Web Image Collection</head><p>Using the action name such as tennis swing, golf swing etc., as a text query we download images from Google Image search engine. Although, Google image search quality has been improved significantly over last few years, the retrieved images still contain outliers and irrelevant images due to in-accurate query text and polysemy.</p><p>We perform random walk over these images to get rid of image noise. The key benefit of using random walk is that it can discover both small cluster of outliers as well as the images far away from all other images (in feature space) <ref type="bibr" target="#b18">[19]</ref>. We define a fully connected graph Z(N, E), where N is the set of all images and E represents set of edges between them. The weight between any two nodes i and j on the graph is measured by Euclidean distance between <ref type="figure">Figure 2</ref>: Noisy golf swing images removed by random walk. These images include cartoons, people in unusal backgrounds and clipart. Last image (bottom right) represents the failure case, which random walk is unable to remove (perhaps due to its similarity to golf swing in the feature space).</p><p>their features φ(i) and φ(j), where φ represents deep learning features <ref type="bibr" target="#b34">[35]</ref> computed over the whole image. Finally, the transition probability between any two nodes i and j is given by</p><formula xml:id="formula_0">p(i, j) = e −γ φ(i)−φ(j) 2 k m=1 e −γ φ(i)−φ(m) 2 .<label>(1)</label></formula><p>The random walk over the graph is then formulated as:</p><formula xml:id="formula_1">r k (j) = β i r k−1 (i)p ij + (1 − β)v j ,<label>(2)</label></formula><p>where r k (j) represents relevance score of the image j at k th iteration, v j is its initial probabilistic score and β controls the contribution of both terms to the final score. Due to the absence of any prior knowledge about images, we assign the same initial probabilistic score to all the images. The relevance score r k (j) is iteratively updated for all nodes until fixed number of iterations are achieved. The images with low relevance score can be considered as outliers and subsequently removed. In our experiments, we removed 30% of the originally downloaded images. When removing images more than 30%, we start losing good quality images. In experiments, we use β=0.99. Some of the typical images removed by random walk are shown in <ref type="figure">Figure.</ref> 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action Proposals in Images</head><p>Although images downloaded using the text query belong to the same overall concept; they are mostly captured in different scenes and contain distracting backgrounds. Using these images naively is detrimental to video proposals ranking (see <ref type="table" target="#tab_1">Table 1</ref>). Therefore, to get rid of unnecessary backgrounds, we propose to localize the action in images.</p><p>To localize the action in the downloaded images, we use recently proposed state-of-art unsupervised localization method <ref type="bibr" target="#b2">[3]</ref>. We use this method because of its excellent performance on many complex datasets <ref type="bibr" target="#b5">[6]</ref>.</p><p>Following <ref type="bibr" target="#b2">[3]</ref>, we extract hundreds of candidate action proposals <ref type="bibr" target="#b17">[18]</ref> from each image. The objective is to obtain <ref type="figure">Figure 3</ref>: Automatically generated action proposals in images. In bottom row, last two images (from right) show the failure cases due to very small size of actor and cluttered background.</p><p>the proposals which represent the most common concept (the action in our case) across all the images. To achieve this, we efficiently match action proposals across all the images using Probabilistic Hough Matching (PHM) <ref type="bibr" target="#b2">[3]</ref>. The PHM matching is performed on local regions within proposals by carefully considering their scale and localization variations. The score of a local region in proposal p m with respect to p ′ m is given as:</p><formula xml:id="formula_2">ψ(p) = max r ′ c((r, r ′ )|(p m , p ′ m )),<label>(3)</label></formula><p>where c represents Hough matching confidence of local region r in p m with respect to p ′ m . The high region score represents the highest matched proposal across images. However, it does not provide the explicit action localization as the background regions can also have good matches. Therefore, we use both the standout score <ref type="bibr" target="#b2">[3]</ref> of each proposal and the PHM based region matching to obtain the final action localization in images. In our experiments, we use only top two action proposals from each image. Selecting more than two proposals increases the computational time of next steps, while not always helping the performance. <ref type="figure">Figure 3</ref> shows automatically generated images' proposals for actions of THUMOS14. The right most images in the second row show the failure cases, where we are unable to localize the action of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Action Proposals in videos</head><p>Our end goal is to obtain spatio-temporal action localization in a video using image action proposals generated in the previous section. Therefore, we first estimate action locations in the video and try to remove the majority of camera and background generated proposals.</p><p>In order to obtain spatio-temporal action localization in a video, we first need to obtain candidate action locations in a video. Traditional ways to achieve this is to use 3D (spatiotemporal volume) sliding window approach. However, this approach has two main limitations. Firstly, it produces extremely large number of candidate locations. Secondly, 3D cuboids contain a large amount of background particularly in case of dynamic actions. To circumvent these problems, recently, action proposals have been presented <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>. Compared to sliding window, these techniques provide far less number of high quality action proposals.</p><p>In this work, we employ supervoxel segmentation based approach to generate action proposals <ref type="bibr" target="#b19">[20]</ref>. However, our method does not depend on specific action proposal methods and any action proposals method can be used <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11]</ref>. We compute fixed number of superpixels from each video frame and estimate mean color, color histogram and optical flow histogram within each superpixel. Given n number of superpixels, we build a graph G(V, E), where V is a set of superpixels and E represents a set of edges between them. We use discontinuity preserving first and second order spatial edge weights between superpixels m and n, where the first order edge weight is given by:</p><formula xml:id="formula_3">e nm,s = α 1 d 1 (n, m) + α 2 d 2 (n, m) + α 3 d 3 (n, m) +α 4 d 4 (n, m) + α 5 d 5 (n, m),<label>(4)</label></formula><p>where d 1 corresponds to distance between color means, d 2 and d 3 represent distance between color and flow histograms and d 4 and d 5 represent geodesic distance between superpixel centroids computed through motion and color boundaries.</p><p>In addition to spatial edges, we also build temporal edges given as e nm,t = α 7 d 1 (n, m) + α 8 d 2 (n, m) + α 9 d 3 (n, m), <ref type="bibr" target="#b4">(5)</ref> where d 1 , d 2 and d 3 are the same as described before and m and n represents temporal neighbors. Hierarchical clustering on this graph results into supervoxels segmentations. Finally, action proposals are built by merging supervoxels using randomized Prim's maximum span tree algorithm <ref type="bibr" target="#b17">[18]</ref>, extended to videos. During proposals generation, appearance, motion and size similarities of superpixels are taken in account. Typical examples of few action proposals for UCF Sports videos are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Although, the above method generates significantly less number of action proposals (approx. 2000 in each video), their number is still huge for our application, since we want to obtain only the most action representative proposal in each video clip.</p><p>Human actions are mainly characterize by motion. We use this important cue for two purposes. First, we use it to discard camera and background generated proposal (as they would have small optical flow gradients). Secondly, we use it to facilitate action proposal ranking. To this end, we use optical flow gradients within each video proposal. We first compute Frobenius norm of optical flow within each proposal, defined as: where U = (u, v) represents forward optical flow and u x , v x , u y and v y are optical flow gradients. The motion score, η p , of each video proposal, p v , is then defined as weighted summation of Frobenius norm, namely,</p><formula xml:id="formula_4">U X F = u x u y v x v y F ,<label>(6)</label></formula><formula xml:id="formula_5">η p = G l (x c , y c ) × G s (h, w) × U X F ,<label>(7)</label></formula><p>where x c , y c , h, w represent center coordinates, height and width of the proposal respectively. Gaussians G l and G s encourage proposals that are in the center of the video and are in vertical shapes since humans in these action videos are mostly in the center and are in upright position. Assuming η p , as a detection score, we perform Non-Maximal Suppression (NMS) to obtain a few proposals, which have high optical flow gradients and have small overlap with each other. In our experiments, we keep at most fifty proposals from each video. This results in a huge decrease in computation for further steps. Finally, we normalize motion score η p of all proposal within a video between zero and one. We use this normalize score to represent motion saliency of each proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ranking Video Action Proposals using Image Action Proposals</head><p>In this section, we present our key idea of ranking video action proposals, P v , using image action proposals, P m . We propose to achieve this by reconstructing video action proposals as a linear combination of image action proposals. The main idea is that video action proposals which can easily be reconstructed using image action proposals (i.e., have low reconstruction error) can be considered to be capturing the key poses and viewpoints of the specific action and therefore represents the action of interest.</p><p>Suppose a video contains k number of video action proposals,</p><formula xml:id="formula_6">P v = [p 1 v , p 2 v , . . . , p k v ].</formula><p>Within each proposal, we extract visual features <ref type="bibr" target="#b34">[35]</ref> from each of the key frame (bounding box). Let Π f ∈ R d×n represents the matrix obtained by vertical concatenation of all key frames features within a proposal, where d is the dimension of visual feature, and n is the number key-frames within proposal.</p><p>Similarly, Υ f ∈ R d×m , represents vertical concatenation of visual features from all image proposals, where m represents the total number of image proposals.</p><p>The straightforward approach would be to reconstruct each of the video proposal bounding box independently using image proposals and aggregate the reconstruction error for all the bounding boxes to obtain overall proposal action score. Although appealing, it ignores the underlying temporal structure of the video. Videos are not just the collection of frames but the sequence of frames and hence contain temporal information. Therefore, we propose to reconstruct all proposal bounding boxes jointly using the constraints that push the coefficients for each bounding box towards a common consensus, thus enforcing the coefficient similarity across multiple frames. Moreover, we introduce sparsity constraint to take care of noise in image data. Consensus regularization has been introduced recently for different applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>To achieve above goal, we minimize following the objective function:</p><formula xml:id="formula_7">Z = min C Π f − Υ f C 2 F + λ 1 C −C 2 F + λ 2 C 1 ,<label>(8)</label></formula><p>where the first term minimizes reconstruction error and second and third term enforce consistency (across columns) and sparsity in coefficient matrix C, respectively. The consensus matrixC is obtained by columns-wise concatenation of mean of coefficient matrix C.</p><p>We solve the optimization mentioned in Eq. 8 using variant of two-metric projection algorithm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>. We divide the optimization variables c i into two sets: active set and working set. Active set, A, contains the variables that have positive partial derivative and are close to zero.</p><formula xml:id="formula_8">A = {i|c i &lt; ǫ, ∇ i Z(C)}<label>(9)</label></formula><p>Similarly, variables that have negative partial derivative or that are sufficiently non-zero belong to working set, W. We compute a diagonally-scaled projected pseudo-gradient step for active set variables and a projection of Newton step along working set, namely,</p><formula xml:id="formula_9">C W ← P[C W − σH −1 W ∇ W Z(C)] C A ← P[C A − σD A ∇ A Z(C)],<label>(10)</label></formula><p>where P is orthant projection and H is Hessian matrix. Note that, given positive diagonal scaling matrix D A , combined gradient direction is descent, unless C is optimal. We iteratively solve the above equations until we obtain the optimal solution or the maximum number of iterations are met. We optimize Eq. 8 for every proposal in the video clip and estimate the reconstruction error. We normalize reconstruction errors of all proposals within a video between zero and one. The final action score Λ p of each proposal, p v , is simply given as:</p><formula xml:id="formula_10">Λ p = (1 − R p ) + η p ,<label>(11)</label></formula><p>where R p and η p represent reconstruction error and motion saliency (calculated in Section 4) of proposal, p v .</p><p>Note that we have experimented with several state-of-art domain adaptation methods such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, however either they do not help at all or have diminishing effect on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Action localization in Untrimmed videos</head><p>The proposed approach is generic in nature and can be applied to any action dataset including recently introduced extremely challenging untrimmed action datasets such as THUMOS14 <ref type="bibr" target="#b13">[14]</ref>. This dataset contains long YouTube sports videos, mostly gathered from news and documentaries. The general trend in these videos is that they contain: newscaster or reporter, clips showing the crowd and stadium, people talking about the specific sport and finally the actual action clips somewhere in between these irrelevant clips.</p><p>To use our approach on untrimmed videos, we first divide long videos into shots <ref type="bibr" target="#b0">[1]</ref>. We start with the assumption that each shot contains an action. By considering each shot as a trimmed video, we compute top ranked action proposal from each video using exactly the same procedure as described in Section 3, 4 and 5. After computing the most representative action proposal in each shot (Section 5), we compare the action score (Eq. 11) of these top ranked proposals across the shots. Intuitively, the shots that contain an action would have top ranked proposals with high action score as compared to the shots that do not contain action. We max-normalize the reconstruction error of shots across the video. By sweeping the threshold of reconstruction error, we generate ROC curve as shown in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Results</head><p>The main goal of our experiments is to quantitatively evaluate the performance of proposed approach, verify that each component contributes to its final accuracy and demonstrate the generality of our approach. To this end, we performed extensive experiments on trimmed as well untrimmed action datasets.</p><p>For shot detection, we computed RGB histogram of frames as a feature representation. For all other experiments, we used CNN features <ref type="bibr" target="#b34">[35]</ref>, computed within image/video proposals bounding boxes. We set the parameters in Equation 8 as λ 1 =0.06 and λ 2 =0.16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Experiments on Trimmed Action Dataset</head><p>For experiment on trimmed dataset, we have chosen UCF-Sports <ref type="bibr" target="#b22">[23]</ref> and THUMOS13 <ref type="bibr" target="#b12">[13]</ref> because of their complexity and that several recent works have used in their experiments <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33]</ref>. In these datasets, an action spans the complete video clip. These broadcast videos contain large camera motion, cluttered background, variable viewpoints and occlusion. UCF-Sports dataset contains 150 videos and include 10 actions including: diving, golf swing, kicking, lifting, horse riding, running, etc. THUMOS13 is a subset of UCF101 <ref type="bibr" target="#b27">[28]</ref> and contains 24 human actions that have spatio-temporal annotations. These actions include: cricket bowling, biking, salsa spin, etc. This dataset has 3207 videos. We used all videos of both datasets for evaluation (except Walk-Front-005 in UCF-Sports since it is actually a running action).</p><p>To evaluate localization accuracy, we use the standard intersection over union metric at 20% threshold <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>. The localization accuracy of our complete method for UCF-Sports is given in <ref type="table" target="#tab_3">Table 2</ref>. We compare our method with two strong baselines: CRANE <ref type="bibr" target="#b31">[32]</ref> and Negative Mining <ref type="bibr" target="#b25">[26]</ref>. Similar to the proposed approach, both of these techniques are weakly supervised annotation methods, i.e., they only assume video level labels. The comparison shown in <ref type="table" target="#tab_3">Table  2</ref> indicates the significantly improved localization accuracy of our method. Note that we use the same features <ref type="bibr" target="#b34">[35]</ref> for all three methods.</p><p>In <ref type="figure">Figure.</ref> 5, we show qualitative examples of localization. We show four frames for a video from each action. It can be seen that our method performs quite well despite large camera motion (diving, kicking), scale changes (walking), cluttered background (horse riding, skateboarding), small actor size (running, golf swing) and abrupt motion (swinging).</p><p>Our method contains several components. We evaluate the contribution of each component towards final localization accuracy in <ref type="table" target="#tab_1">Table 1</ref>. First row indicates localization accuracy, where we use all of the downloaded images (without removing noisy ones) in our reconstruction framework. Removing noisy images gives 3% improvement in localization accuracy (second row). Reducing the effect of images background noise through proposal, we achieve further 15% improvement. By enforcing consistency and sparsity in coefficient vectors among multiple frames of the proposal, we obtain 5% improvement. Finally, by adding motion score, we achieve further 5% improvement. Our results demonstrate that each component of our approach is necessary and contributes towards final localization accuracy. Moreover, our results reinforce that the web images do have the ability to make a significant impact on action localization in videos. <ref type="table" target="#tab_4">Table 3</ref> shows localization accuracy of top ranked proposals in UCF-Sports and UCF-101 across different overlap thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Experiments on Un-Trimmed Action Dataset</head><p>To demonstrate the effectiveness of the proposed approach, we have evaluated it on a part of recently released un-trimmed action dataset <ref type="bibr" target="#b13">[14]</ref>. This dataset was released in 2014 in THUMOS challenge workshop. In addition to having cluttered background, severe occlusion and huge camera motion, these extremely challenging real-world videos   contain several irrelevant frames such as non-action frames and multiple instance of the same action.</p><p>THUMOS14 test-set contains 20 actions, where only temporal annotations are provided without any spatial annotations. To evaluate spatio-temporal localization accuracy of our method on this dataset, we manually anno-    shots or clips. We, then, compute video action proposals within each clip by assuming that each clip contains the action. We compute the action scores of all proposals within the shot and obtain the most action representative proposal in every shot. We consider the action score as a action detection score and evaluate localization accuracy using intersection over union metric at 10% threshold. The mean ROC curve for all four actions is shown in <ref type="figure">Figure 7</ref>. Again, we compare our results with weakly supervised annotation methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26]</ref> and obtain better results. Improved results as compared to strong baseline methods signify the effectiveness of the proposed approach. We use lower threshold criterion due to extreme difficulty of the dataset. Even though the results of all three methods are lower as compared to state-of-art results on similar actions in trimmed datasets, we consider these results encouraging, due to the complexity of dataset. The qualitative results for all four actions are shown in <ref type="figure">Figure 6</ref>. <ref type="figure">Figure. 8</ref> shows some of typical failure cases on THU-MOS14 dataset. The figure on the top-left shows a frame from golf swing video. In this video of more than 5000 frames, the complete golf swing action happens only for 500 frames. In the rest of the video, the person is teaching golf swing techniques and performing in-complete golf swing action several times. Although, we achieve good localization over the actor, our method has problem in distinguishing complete action from the in-complete ones. Other failures occurred due to actor's occlusion and blurred video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We present a new approach to spatio-temporally localize an action in a single video. As compared to previous similar works, we don't assume availability of multiple videos, prior annotations or clean images. Our experimental results show that impressive action localization can be achieved by reconstructing candidate action locations by leveraging freely available internet images. Our framework tackles noisy images through random walk and sparse representation, removes background and camera generated video proposals through optical flow gradients and preserves temporal smoothness of video by enforcing consistency of coefficient vectors across multiple frames. Our extensive experiments on trimmed as well as un-trimmed action datasets validate the effectiveness of proposed ideas and the framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>This figure illustrates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Video action proposals. Colors in the figures are randomly assigned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Localization results (Top ranked proposal) from UCF-Sports. We show four frames of each action video. Red box indicates ground truth and green box shows localization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Localization results (Top ranked proposal) from four actions of THUMOS14. We show four frames of each action video. Red box indicates ground truth and green box shows localization results. Mean ROC curves for four actions of THU-MOS14: Tennis swing, Golf swing, Throw Discus, and Baseball pitch. The results are shown for Negative Mining approach<ref type="bibr" target="#b25">[26]</ref> (green), CRANE<ref type="bibr" target="#b31">[32]</ref> (yellow) and Proposed method (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results for UCF-Sports. Top row shows localization accuracy of reconstructing video proposals using all images (including noisy ones). The second row shows the same after noise removal using random walk. The third row shows localization accuracy of reconstructing video proposals from image proposals without enforcing sparsity and consensus constraints. Localization accuracy of complete reconstruction model (Eq.8) is shown in fourth row. Finally, fifth row shows accuracy of complete method. The results indicate that noise removal, image proposals, regularization and motion saliency; all contribute to overall localization accuracy.</figDesc><table>Diving 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>A comparison of our approach with related weakly supervised annotation methods on UCF-Sports</figDesc><table>Threshold 0.1 0.2 0.3 0.4 0.5 0.6 
UCF-Sports 93.9 92.7 82.1 61.0 40.7 18.5 
UCF-101 78.0 62.7 47.8 28.8 13.8 4.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Localization accuracy of UCF-Sports and THU-MOS13 (24 classes) at various thresholds. tated four actions: baseball pitch, golf swing, tennis swing and throw discus. Specifically, we annotated around 35, 000 video frames (these annotations will be made publicly available). Baseball pitch, golf swing, tennis swing and throw discus contain 40, 141, 80, and 28 number of action instances, respectively. Given a video, we first divide it into</figDesc><table>Baseball Pitch 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The group fused lasso for multiple change-point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bleakley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1106.4199</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Watching unlabeled video helps learn new human actions from very few labeled snapshots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving semantic concept detection through the dictionary of visually-distinct elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two-metric projection methods for constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale knowledge transfer for object localization in imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with frank-wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Largescale video summarization using web-image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint summarization of largescale collections of web images and videos for storyline reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prime object proposals with randomized prim&apos;s algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Outlier detection using random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moonesinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-N</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools with Artificial Intelligence, 2006. ICTAI &apos;06. 18th IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Spatio-Temporal Object Detection Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training object class detectors from eye tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D F</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action MACH: A spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graphical model structure learning with l1-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Similarity constrained latent support vector machine: An application to weakly supervised action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shapovalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cannons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tvsum: Summarizing web videos using titles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Co-localization in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1464" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Co-localization in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shifting weights: Adapting object detectors from image to video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative segment annotation in weakly labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable part models for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Apt: Action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of visionbased methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relaxed collaborative representation for pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
