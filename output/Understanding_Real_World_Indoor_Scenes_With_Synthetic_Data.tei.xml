<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Real World Indoor Scenes With Synthetic Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
							<email>handa.ankur@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorica</forename><surname>Pȃtrȃucean</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Real World Indoor Scenes With Synthetic Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments. Recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised dataperformance increases in proportion to the amount of data used. However, this quickly becomes prohibitive when considering the manual labour needed to collect such data. In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes. By carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-theart RGBD systems on NYUv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on SUN RGB-D dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many high level tasks in real world require some knowledge of objects present in the scene, their physical locations and the underlying physics involved as a means to understanding the scene. Autonomously navigating robots equipped with cameras can build upon the awareness and understanding of their environment to perform a range of simple to complex tasks in real world. In this work, we choose to focus on per-pixel semantic labelling as an intermediate step towards scene understanding. Indeed, not only can per-pixel labelling lead to object recognition, but it can also facilitate estimating volume and actual physical extent of the objects e.g. an indoor robot must understand how the floor bends or curves so as to adjust its path while navigating, a robot operating in a kitchen may need to have an idea of the volume of the objects to appropriately arrange them in the cabinet -knowledge of the physical extent of the supporting surface can provide a rough estimate of where different objects can be placed. In 3D modelling, it is sometimes required to have precise knowledge of where different objects can be fitted or inserted into others. It is exactly in these high level tasks where the role of semantic segmentation is emphasized more than pure object detection. Furthermore, it is more likely to benefit from the context present in the scene to segment objects unlike any 'blanket' detection module that is generally run independently on a sliding window on the image. Motivated by its recent success, we use deep learning as our computational framework for semantic segmentation. Inspired by the human brain <ref type="bibr" target="#b3">[5]</ref>, deep learning models have superseded many traditional approaches that relied on hand engineered features -past few years have seen a rapid proliferation of deep learning based approaches in many domains in AI. However, a major limitation of modern day deep learning models is the requirement of enormous supervised training data. Collecting big datasets can quickly become labour intensive and may not be a viable option in the end. In this work, we focus on the challenges of obtaining the desired training data for scene understanding.</p><p>Many existing datasets do not have the volume of data needed to make significant advances in scene understanding that we aim in this work. For instance, indoor scene datasets like, NYUv2 <ref type="bibr" target="#b23">[25]</ref> contains only 795 training images for as many as 894 object classes. SUN RGB-D <ref type="bibr" target="#b25">[27]</ref>, on the other hand contains 5,285 training images for 37 classes. These are the only two indoor depth datasets with per-pixel labels and are limited in size considering the enormity of data needed to achieve good performance on unseen data. This limitation results from the difficulty of the labelling process. Indeed, per-pixel segmentation is a tedious, expensive, and even error-prone process, when done manually by human labellers. We believe scene understanding can greatly benefit from the computer graphics community that has long had the tradition of CAD model repos-itories. Synthetic data is already used for many computer vision problems <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b26">28]</ref> and <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b16">18]</ref> in the context of robotics. We believe that the role of synthetic data and gaming environments <ref type="bibr" target="#b21">[23]</ref> will continue to grow in providing training data with further advances in machine learning and data driven understanding.</p><p>Our main contribution in this work is to show the potential of synthesised ground truth depth data generated from annotated 3D scenes <ref type="bibr" target="#b17">[19]</ref> in improving the performance of per-pixel labelling with a thorough and extensive evaluation on challenging real world indoor datasets. Consequently, we also highlight the usefulness of depth-only data, generally overlooked in favour of RGB, in segmenting functional categories of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Prior work on per-pixel indoor image labelling has been due to NYUv2 and SUN RGB-D datasets. The work of <ref type="bibr" target="#b6">[8]</ref> was one of the first in the direction and built on top of a deep learning framework trained on NYUv2. However, it achieved only modest performance on the test data. Subsequently, <ref type="bibr" target="#b20">[22]</ref> and <ref type="bibr" target="#b8">[10]</ref> have improved the performance, again with deep learning inspired methods. We think that the potential of these new methods is yet to be explored fully and that the lack of training data is the primary hindrance. Both NYUv2 and SUN RGB-D are limited in their sizes and only provide per-pixel labels for low quality raw depth-maps and corresponding RGB frames -missing data and noise in raw sensor measurements exacerbate the problem even more. Also, since most of the labelling relies on human labour, missing labels and mislabelled data are very common, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. This is inevitable as labelling for humans can be a tiring process and sometimes comes with a considerable monetary cost.</p><p>Ideally, one would like to have a fully labelled 3D model for every scene to generate annotations from arbitrary viewpoints but this is clearly missing in both the datasets. SUN3D <ref type="bibr" target="#b28">[30]</ref> goes in the direction of providing annotated video sequences together with 3D point clouds obtained with SfM. However, they only provide 8 such sequences. The NYUv2 dataset provides a large number of videos, but only provide a couple of annotated frames per video. Additionally, the videos are captured without 3D reconstruction in mind and therefore not suitable for generating accurate 3D reconstructions or annotations, as observed from our own experiments. Furthermore, fusion of raw depth images within a temporal window can provide smooth depth-map to aid the segmentation as opposed to noisy raw depth-maps. SUN3D <ref type="bibr" target="#b28">[30]</ref> also fuse the raw depth maps but again they are limited in the number of annotated sequences. Although, NYUv2 and SUN RGB-D do not provide fused depth measurements, fortunately, both datasets provide an in-painted version of the raw depth maps which has been used in <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b8">10]</ref> on per-pixel scene understanding. In this work, we build upon SceneNet <ref type="bibr" target="#b17">[19]</ref>, a repository of annotated synthetic 3D scenes, to collect potentially unlimited labelled training data with perfect ground truth for per-pixel labelling. The idea of using synthetic scenes has existed in the past, in particular, <ref type="bibr" target="#b10">[12]</ref>, who released a number of scenes targetted towards the application of scene retrieval, which could potentially be used in the problem we are interested in. However, those scenes are small scale of the order of 4m×3m×3m, and contain only one or two instances of characteristic objects that define the scene e.g. only one desk and monitor in the office room. On the other hand, object repositories have existed for a long time now particularly the popular Trimble Warehouse 1 , ModelNet <ref type="bibr" target="#b27">[29]</ref> and ShapeNet 2 . Unfortunately, object repositories are not directly useful for the problem we are targetting. SceneNet contains sufficiently large and variegated scenes to provide the desired data we need in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Synthesizing Training Data</head><p>SceneNet 3 <ref type="bibr" target="#b17">[19]</ref> is inspired by the efforts developed in the graphics community to build large scale repositories of CAD models. It is an open-source repository of annotated synthetic indoor scenes -the SceneNet Basis Scenes (SN-BS) -containing a significant number of manually labelled 3D models. Having a labelled scene gives a lot more flexibility in obtaining the desired training data -annotations from arbitrary view points are easily available, saving the expensive human labour required to label each image independently. Since our goal here is to show improvements on existing static-image datasets we do not render video trajectories and instead generate synthetic data from random poses.</p><p>SceneNet contains 3D models from five different scene categories: bedrooms, living rooms, offices, bathrooms, and kitchens, with at least 10 annotated scenes for each category. All the 3D models are metrically accurate and are available in standard .obj format. We use OpenGL to place virtual cameras in the synthetic scenes to generate ground  truth data from random viewpoints. <ref type="figure" target="#fig_2">Fig. 2</ref>(b) shows samples of rendered annotated views of a living room. In this work, we have used purely depth-based scene understanding. We also considered raytracing for RGB rendering but found it to be very time consuming when modelling complicated light transport. However, depth-only experiments allow us to study the effect of geometry in isolation.</p><p>Additionally, we generate new physically realistic scenes from object models downloaded from various online object repositories <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b4">6]</ref> (see <ref type="table">Table 1</ref>), using object cooccurrence statistics from SceneNet and optimising on the object positions within the scene with simulated annealing <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b17">19]</ref>. Most importantly, scene generation is fully automated and sampling objects means that these scenes already come with annotations. We again render from random viewpoints to obtain the desired ground truth data and augment it to the SN-BS ground truth. In total, we use 10,030 rendered images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repository</head><p>Objects ModelNet <ref type="bibr" target="#b27">[29]</ref> 127,915 Archive3D 45,000 Stanford Database <ref type="bibr" target="#b10">[12]</ref> 1,723 <ref type="table">Table 1</ref>. Potential 3D object repositories for scene generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Adding noise to ground truth data</head><p>We realise the possible mismatch of the distribution of the noise characteristics in real world datasets and our synthetic depth-maps and therefore add noise to the perfect rendered depth-maps according to the simulated kinect noise model in <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b18">20]</ref> (see <ref type="figure" target="#fig_3">Fig. 3</ref> for visualisations). This is to ensure that any model trained on synthetic datasets can have a significant effect on real world depth data either directly or via fine-tuning <ref type="bibr" target="#b9">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Set-up</head><p>Our primary goal in this work is to show how synthetic data can enable improvements in per-pixel semantic segmentation on real world indoor scenes. For all our experiments, we choose the state-of-the-art segmentation algorithm <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b2">4]</ref> with encoder-decoder architecture built on top of the popular VGG network <ref type="bibr" target="#b24">[26]</ref>. Both algorithms have only been used for RGB image based segmentation, therefore, we adapted them to work on depth based three channel input, DHA, 4 namely depth, height from ground plane and angle with gravity vector. Since we already know the camera poses when rendering, it is straightforward to obtain height and angle with gravity vector for synthetic data but we implemented a C++ GPU version of the otherwise slow MATLAB code of <ref type="bibr" target="#b13">[15]</ref> to align the NYUv2 depth-maps to intertial frame to obtain the corresponding heights and angles. SUN RGB-D already provide the tilt angle and rotation matrices so we use them to obtain our DHA features. We intially report results on only 11 different categories (see <ref type="table">Table 3</ref>). This is because, to generate new scenes, we sample objects from axis aligned ModelNet10 <ref type="bibr" target="#b27">[29]</ref> which does not contain painting and books that in total add up to the 13 classes used in <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b20">22]</ref>. However, we take special care in doing comparisons with <ref type="bibr" target="#b8">[10]</ref> who made their per-pixel semantic predictions for the NYUv2 test data publicly available. We later, also report results on 13 classes by directly finetuning on the standard 13 class datasets.</p><p>Also, we inpainted the noisy depth-maps from our simulator with the MATLAB colorization code provided in the NYUv2 dataset. This was to ensure that our depthmaps qualitatively match with the inpainted depth-maps in NYUv2 (see <ref type="figure" target="#fig_3">Fig.3</ref>). We also ran the colorization with different kernel sizes and empirically found 3×3 to be good enough -our final results did not change much with 7×7 and bigger kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We collect a wide variety of random images sampled from SceneNet and convert each depth image into three We denote training performed on noise injected synthetic data from SceneNet, as SceneNet-DHA, and finetuning on training data in NYUv2 by SceneNet-FT-NYU-DHA. When using dropout we denote them by SceneNet-DO-DHA and SceneNet-FT-NYU-DO-DHA respectively. Similarly, the networks trained on NYU are denoted by NYU-DHA and NYU-DO-DHA. We also tried dropout at test time as mentioned in <ref type="bibr" target="#b11">[13]</ref> but this comes at the expense of increased inference time -dropout at test time is shown to have similar effect of multiple model averaging and at the same time provides uncertainty in the final predictions. First, we quantify the performance of all these variants on the NYUv2 test data and later on a bigger real world dataset, SUN RGB-D, based on standard global and class accuracy metrics.</p><p>We use DHA images of size 224×224 as input to the network and intialise the weights with pretrained VGG network. At first, it may seem that the network trained on images cannot be useful for training on depth data, however, since the first layer filters often look for edges and orientations <ref type="bibr" target="#b30">[32]</ref>, it is sensible to use them for relatively similar modalities like depth -edges in depth almost always align with the RGB edges -and deeper layers adapt accordingly. Therefore, in all our experiments the network always converged with accuracies in high nineties on the training set. Also, it is worth noting that FCN <ref type="bibr" target="#b20">[22]</ref> quantise the HHA to [0,255] for an equivalent RGB image and use pretrained VGG network to initialise. We do not quantise and maintain the floating point precision of the DHA images.</p><p>We render 10,030 depth images from random view points ensuring that a minimum number of objects is visible -avoiding camera looking at only walls or floorand perturb the depth values according to Section 3.1 to generate depth maps qualitatively similar to NYUv2. Comparison of different proportions of objects in these rendered images and NYUv2 training data is shown <ref type="figure" target="#fig_4">Fig. 5</ref>. All the models are trained with stochastic gradient descent with a starting learning rate of 0.01 which is multiplied by 0.9 every 3-4 epochs, a weight decay of 0.0005, and a momentum of 0.9. We characterise the experiments into comparisons with real training data in NYUv2 and the state-of-the-art, Eigen &amp; Fergus <ref type="bibr" target="#b8">[10]</ref>, for both 11 and 13 class segmentations. We also perform experiments on SUN RGB-D for 13 classes and set a new benchmark for pure depth based segmentation.</p><p>Comparisons with NYUv2 training Training purely on synthetic data, SceneNet-DHA, results in only modest performance on the test data (see <ref type="table">Table 2</ref>). Comparison with NYU-DHA reveals that fine-tuning is needed to obtain further improvements in the results. As a result, we see that the performance jump from NYU-DHA to SceneNet-FT-NYU-DHA is very clear -an increase of 5.4% in the class and 3.6% in the global accuracy showing the usefulness of synthetic data for real world scene segmentation. Importantly, convergence was twice as fast for SceneNet-FT-NYU-DHA compared to NYU-DHA. Specifically, prior to fine-tuning we trained SceneNet-DHA for 21K iterations (14 epochs). Fine-tuning on NYUv2 took another 4K iter- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with Eigen &amp; Fergus [10]</head><p>We also compare our results to the state-of-the-art system of Eigen and Fergus <ref type="bibr" target="#b8">[10]</ref> who use data augmentation and a combination of RGBD and normals. Since we use only depth, our system is not directly comparable to <ref type="bibr" target="#b8">[10]</ref> but we obtain competitive performance. SceneNet-FT-NYU-DHA although performs better than NYU-DHA, it still falls short of the performance of the state-of-the-art method <ref type="bibr" target="#b8">[10]</ref>. However, careful examination in class accuracies (see <ref type="table">Table 3</ref>) reveals that we perform comparably only compromising on tv and windowswe expect RGB to play a bigger role here -emphasizing that for functional categories of objects geometry is a strong cue for segmentation. We used the publicly available annotations of <ref type="bibr" target="#b8">[10]</ref> with AlexNet 5 , to re-evaluate the accuracies for the 11 classes we use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with Dropout</head><p>We also used dropout ratio of 0.5 in the fully connected layers to gauge the effect of regularisation in the parameters to prevent over-fitting. We see 3.2% improvement in class and 1.9% global accuracy in the results with real training data used in NYU-DO-DHA compared to NYU-DHA. However, we only observed minor improvements with both SceneNet-DO-DHA and SceneNet-FT-NYU-DO-DHA against SceneNet-DHA and SceneNet-FT-NYU-DHA respectively suggesting that increase in data acts as an implicit regulariser. Further improvements depend largely on the amount of training data <ref type="bibr" target="#b3">5</ref>   <ref type="figure" target="#fig_0">1 78.3 72.1 96.5 55.1 52.1 45.8 45.0 41.9</ref> 88.7 57.7 <ref type="table">Table 3</ref>. Different training variants on the NYUv2 test set. The benefits of adding synthetic data are clear. For fair comparison, it should be noted that <ref type="bibr" target="#b8">[10]</ref> use RGBD+Normals than just depth as used in this work. We would like to stress here that our network is trained end-to-end as compared to multi-stage training done in Eigen et al. <ref type="bibr" target="#b8">[10]</ref> and does not use any RGB data or data augmentation. We outperform <ref type="bibr" target="#b8">[10]</ref> on sofas and beds but fall behind on chairs, tv and windows. We expect tv and windows are likely to be segmented better with RGB data. However, we obtain comparable performance on rest of the classes further emphasising that for functional categories of objects shape is a strong cue. Poor performance of SceneNet-DHA and SceneNet-DO-DHA on tv and windows is mainly due to limited training data for these classes in SceneNet. Also note that results of Hermans et al. <ref type="bibr" target="#b19">[21]</ref> are not directly comparable for 11 classes because they do not make their predicted annotations publicly available.  <ref type="table">Table 2</ref>. Different variants of training data that we use in our experiments. The performance jump from NYU-DHA to SceneNet-FT-NYU-DHA is clear. Adding dropout helps most in the NYU-DO-DHA but SceneNet-FT-NYU-DO-DHA shows only a marginal improvement. Overall increase in performance from NYU-DO-DHA to SceneNet-FT-NYU-DO-DHA is by 2.3% and 3% in global and class accuracy respectively. Note that we recomputed the accuracies of <ref type="bibr" target="#b8">[10]</ref> using their publicly available annotations of 320×240 and resizing them to 224×224. Note that Hermans et al. <ref type="bibr" target="#b19">[21]</ref> predictions are not publicly available. As a result, we cannot evaluate their performance on these 11 classes.</p><p>used [1]. Although there is no limit to the amount of training data we can render we are only limited by GPU speed and memory -training time for a batch size of 6 images takes about 0.8s for forward pass and 1.2s for backward pass, a combined total of 2s per iteration on NVidia Tesla K80. We also anticipate that training can be made efficient by either completely forgoing the fully connected layers <ref type="bibr" target="#b2">[4]</ref> or reducing their number of features <ref type="bibr" target="#b5">[7]</ref> -fully connected layers in VGG contain nearly 204800 246912 ∽ 83% of the parameters. Training on data of the order of ImageNet <ref type="bibr" target="#b7">[9]</ref> remains an exciting opportunity for the future.</p><p>We also used dropout at test time <ref type="bibr" target="#b11">[13]</ref> but observed very similar performance gain without it. However, dropout at test time <ref type="bibr" target="#b11">[13]</ref> makes the network robust to out-of-domain data. We leave it as an interesting future direction to explore. Overall, we have SceneNet-FT-NYU-DO-DHA as a clear winner against NYU-DO-DHA as shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on 13 Class Semantic Segmentation: NYUv2</head><p>We also performed our experiments on the 13 class semantic segmentation task. It is worth remembering that the two extra classes we add to the 11 class experiment are painting and books. Although, we are limited by the number of labels for painting and books in SceneNet, we fine tune directly on 13 class semantic segmentation task on NYUv2. The performance gain from NYU-DHA to SceneNet-FT-NYU-DHA is evident, highlighting the role of synthetic data. As seen in <ref type="table">Table 4</ref>, SceneNet-FT-NYU-DHA performs consistently better than NYU-DHA for all the classes. We observe similar trend for the comparison between NYU-DO-DHA and SceneNet-FT-NYU-DO-DHA. It is worth remembering that Eigen &amp; Fergus <ref type="bibr" target="#b8">[10]</ref> use RGB and normals together with the depth channel and hence maintain superior overall performance over our methods that use only depth data. Again, careful examination reveals that we only compromise on books, painting, tv and windows -we expect RGB to play a bigger role here in segmenting these classes. This is also reflected in the overall mean global and class accuracies as shown in <ref type="table">Table 5</ref>.  <ref type="table">Table 4</ref>. Results on NYUv2 test data for 13 semantic classes. We see a similar pattern here -adding synthetic data helps immensely in improving the performance of nearly all functional categories of objects using DHA as input channels. As expected, accuracy on books, painting, tv, and windows, is compromised highlighting that the role of depth as a modality to segment these objects is limited. Note that we recomputed the accuracies of <ref type="bibr" target="#b8">[10]</ref> using their publicly available annotations of 320×240 and resizing them to 224×224. Hermans et al. <ref type="bibr" target="#b19">[21]</ref> use "Decoration" and "Bookshelf " instead of painting and books as the other two classes. Therefore, they are not directly comparable. Also, their annotations are not publicly available but we have still added their results in the table. Note that they use 640×480.</p><p>Poor performance of SceneNet-DHA and SceneNet-DO-DHA on tv and windows is mainly due to limited training data for these classes in SceneNet.   <ref type="table">Table 6</ref>. Results on SUNRGBD test data for 13 semantic classes. We see a similar pattern here -adding synthetic data helps immensely in improving the performance of nearly all functional categories of objects using DHA as input channels. As expected, accuracy on books, painting, tv, and windows, is compromised highlighting that the role of depth as a modality to segment these objects is limited. magnitude bigger in size than NYUv2. As shown in Table 7, we observe similar trends i.e. training on synthetic data and finetuning on real dataset helps in improving the accuracies by 1% and 3.5% in global and class accuracy respectively when comparing SUNRGBD-DHA and SceneNet-FT-SUNRGBD-DHA. However, we quickly see diminshing returns when using dropout i.e. SceneNet-FT-SUNRGBD-DHA and SceneNet-FT-SUNRGBD-DO-DHA perform nearly the same. Furthermore, SceneNet-FT-SUNRGBD-DO-DHA performs only 0.8% and 0.9% better in global and class accuracy respectively, compared to SUNRGBD-DO-DHA. It is worth remembering that when experimenting with NYUv2, the proportion of synthetic data was 10 times the real training data while a bigger SUN RGB-D dataset means that this proportion is only 2 times ( 10,030 5,825 ∽ 2 ) the real training data, suggesting that further improvements can be possible either through another order of magnitude increase in data or a possible change in the architecture. Nonetheless, we set a benchmark on pure depth based segmentation on SUN RGB-D dataset. Breakdown of class accuracies for different training variants is shown in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Class Semantic Segmentation: SUNRGBD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented an effective solution to the problem of indoor scene understanding -system trained with large number of rendered synthetic depth frames is able to achieve near state-of-the-art performance on per-pixel image labelling despite using only depth data. We specifically show that adding synthetic data improves the performance on NYUv2 and SUN RGB-D for depth-only experiments and offers a promising route in further improvements in the state-of-the-art. We hope to continue indoor scene segmentation on real world scenes with a reconstruction system  <ref type="bibr" target="#b8">[10]</ref> 68.0 60.8 <ref type="table">Table 5</ref>. The performance jump from NYU-DHA to SceneNet-FT-NYU-DHA follows similar trend -the role of synthetic data is evident. Adding dropout shows only a marginal improvement. Overall increase in performance from NYU-DO-DHA to SceneNet-FT-NYU-DO-DHA is by 2.2% and 3.9% in global and class accuracy respectively. Note that we recomputed the accuracies of <ref type="bibr" target="#b8">[10]</ref> using their publicly available annotations of 320×240 and resizing them to 224×224. Hermans et al. <ref type="bibr" target="#b19">[21]</ref> annotations are not publicly available but we have still added their results in the table. Moreover, they use 640×480. <ref type="figure">Figure 6</ref>. Results for 13 class labels: First row is the RGB image. Second row shows the results obtained with SceneNet-FT-NYU-DHA while the third row shows the results with NYU-DHA. Fourth row has the predictions returned by Eigen and Fergus <ref type="bibr" target="#b8">[10]</ref>. Last row shows the ground truth. Notice that when the camera is viewing at cluttered scenes or far away objects, our results have more blob-like segments in the output. Also, note that monitors tend to get confused with chairs for all algorithms.</p><p>running in the loop to bring real-time semantic segmentation using fused depth-maps.  <ref type="table">Table 7</ref>. Global and class accuracies for 13 class experiments on SUN RGB-D. We see improvements of 1% and 3.5% in global and class accuacy comparing SUNRGB-DHA and SceneNet-FT-SUNRGBD-DHA. However, when using dropout, SUNRGBD-DO-DHA, SceneNet-FT-SUNRGBD-DHA and SceneNet-FT-SUNRGBD-DO-DHA perform nearly the same suggesting that increase in data is only helpful up to a point and that further improvements can be possible either through another order of magnitude of data as seen in NYUv2 experiments or a possible change in the architecture. <ref type="figure">Figure 7</ref>. Results for 13 class labels: First row is the RGB image. Second row shows the results obtained with SceneNet-FT-NYU-DHA while the third row shows the results with NYU-DHA. Fourth row has the predictions returned by Eigen and Fergus <ref type="bibr" target="#b8">[10]</ref>. Last row shows the ground truth. Looking at the results, it is evident that the predictions by <ref type="bibr" target="#b8">[10]</ref> tend to be smoother and contained unlike NYU-DHA as well as SceneNet-FT-NYU-DHA. We believe this difference is a result of rich input channels in RGB, normals and depth used by <ref type="bibr" target="#b8">[10]</ref> while we use only depth in our experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Missing labels (a) and mislabelled frames (b) are very common in many real datasets. In (b) the toilet and sink have the same ground truth label. Both images are from SUN RGB-D<ref type="bibr" target="#b25">[27]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>of per-pixel semantically labelled views from the scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Annotated 3D models can conveniently allow for generation of potentially unlimited ground truth labelled data from different view-points. (a) shows one of the office scenes in SceneNet and (b) shows annotated images rendered from different viewpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Left image in (a) shows the perfect rendered depth map and right image shows the corresponding noisy image provided by our simulator. In (b) we show a side by side comparison of the inpainted depth maps of one of the bedroom scenes in SceneNet with a similar looking bedroom in NYUv2 test data and (c) shows the comparison of angle with gravity vector images. Importantly, left image in (c) highlights the view point invariance of the angle with gravity. channel DHA input. Our network is first trained on purely synthetic data and then fine tuned on the training images provided in the NYUv2 and SUN RGB-D datasets. Finally, NYUv2 and SUN RGB-D test data is used to compare the results with different variants of training data used in experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Side by side comparison of proportions of objects in the 9K images rendered with SceneNet and 795 training images in NYUv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Results for 11 classes on NYUv2 test data obtained with SceneNet-FT-NYU-DHA and NYU-DHA. First row shows the RGB images, predictions returned by SceneNet-FT-NYU-DHA and NYU-DHA are shown in second and third row respectively. Last row shows the ground truth. ations (30 epochs) while NYU-DHA took about 7.5K iterations (56 epochs) to converge. Qualitative results of the segmentation are shown in Fig. 4 which highlights the impact of synthetic data in the final results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figures 6 and 7 compare predictions returned by SceneNet-FT-NYU-DHA, NYU-DHA and Eigen &amp; Fergus [10] on a variety of test images in NYUv2 dataset. Results on 13 Class Semantic Segmentation: SUN RGB-D We perform similar experiments on 13 classes on SUN RGB-D. The dataset provides 5,825 training images and 5,050 images for testing in total. This is one order et al. (rgbd+normals) [10] 61.1 49.7 78.3 72.1 96.0 55.1 40.7 58.7 45.8 44.9 41.9 88.7 57.7 Hermans et al.(rgbd+crf)<ref type="bibr" target="#b19">[21]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>FT-SUNRGBD-DO-DHA 75.6 13.5 69.2 73.6 93.8 52.0 37.1 16.8 57.2 62.7 9.5 88.8 36.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>at the time of publication only AlexNet annotations were available</figDesc><table>11 Class Semantic Segmentation: NYUv2 
Training 
bed 
ceil. chair floor furn objs. sofa table tv 
wall window 
NYU-DHA 
64.5 68.2 51.0 95.0 51.0 48.2 49.7 41.9 12.8 84.2 24.5 
SceneNet-DHA 
60.8 43.4 68.5 90.0 26.5 24.3 21.2 42.1 0 
92.1 0.3 
SceneNet-FT-NYU-DHA 
70.3 75.9 59.8 96.0 60.7 49.7 59.9 49.7 24.3 84.8 27.9 
NYU-DO-DHA 
69.0 74.6 54.0 95.6 57.1 48.7 55.7 42.5 18.5 84.7 25.5 
SceneNet-DO-DHA 
67.7 40.9 67.5 87.8 38.6 22.6 15.8 44.2 0 
89.0 0.8 
SceneNet-FT-NYU-DO-DHA 
72.5 74.1 61.0 96.2 60.4 50.0 62.8 43.8 19.4 85.3 30.0 
Eigen et al. (rgbd+normals) [10] 61.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>SceneNet-FT-SUNRGBD-DO-DHA 75.0 53.1</figDesc><table>13 Class Semantic Segmentation: SUNRGBD 
Training 
global 
acc. 

class 
acc. 
SceneNet-DHA 
56.9 
30.2 
SUNRGBD-DHA 
73.7 
49.2 
SceneNet-FT-SUNRGBD-DHA 
74.7 
52.7 
SceneNet-DO-DHA 
54.7 
31.6 
SUNRGBD-DO-DHA 
74.2 
52.2 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.sketchup.com 2 http://shapenet.cs.stanford.edu/ 3 robotvault.bitbucket.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A similar HHA encoding has been used in<ref type="bibr" target="#b14">[16]</ref> but like<ref type="bibr" target="#b8">[10]</ref> we did not observe much difference.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>We thank Andrew Davison, Daniel Cremers, and Zoubin Ghahramani, and colleagues at CUED, Cambridge for their encouragement and valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding deep features with computer-generated imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Seg-Net: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L K</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Majaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository. CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FlowNet: Learning Optical Flow with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazrbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Example-based synthesis of 3d object arrangements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Asia, SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02142</idno>
		<title level="m">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BlenSor: blender sensor simulation toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gschwandtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aligning 3D models to RGB-D images of cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Rich Features from RGB-D Images for Object Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Davison. Real-Time Camera Tracking: When is High Frame-Rate Best?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SceneNet: an Annotated Model Generator for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pȃtrȃucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Benchmark for RGB-D Visual Odometry, 3D Reconstruction and SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dense 3d semantic mapping of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04366</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perceiving physical object properties by integrating a physics engine with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Make It Home: automatic optimization of furniture arrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
