<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Proximal Riemannian Pursuit for Large-scale Trace-norm Minimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Computer Science</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OmniVision Technologies Singapore Pte. Ltd</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Business School</orgName>
								<orgName type="institution" key="instit1">The University of Sydney</orgName>
								<orgName type="institution" key="instit2">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Computer Science</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Computer Science</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Proximal Riemannian Pursuit for Large-scale Trace-norm Minimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Trace-norm regularization plays an important role in many areas such as computer vision and machine learning. When solving general large-scale trace-norm regularized problems, existing methods may be computationally expensive due to many high-dimensional truncated singular value decompositions (SVDs) or the unawareness of matrix ranks. In this paper, we propose a proximal Riemannian pursuit (PRP) paradigm which addresses a sequence of trace-norm regularized subproblems defined on nonlinear matrix varieties. To address the subproblem, we extend the proximal gradient method on vector space to nonlinear matrix varieties, in which the SVDs of intermediate solutions are maintained by cheap low-rank QR decompositions, therefore making the proposed method more scalable. Empirical studies on several tasks, such as matrix completion and low-rank representation based subspace clustering, demonstrate the competitive performance of the proposed paradigms over existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Trace-norm regularization has widely appeared in many problems, such as matrix recovery (MR) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>, robust principal component analysis (RPCA) <ref type="bibr" target="#b6">[7]</ref>, low-rank representation (LRR) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, and robust multi-task learning <ref type="bibr" target="#b3">[4]</ref>. Most of the trace-norm based problems can be formulated into the following general form <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref>:</p><formula xml:id="formula_0">min X,E ||X|| * + λΥ(E), s.t. A(X) + B(E) = D,<label>(1)</label></formula><p>where λ is a regularization parameter, ||X|| * is the tracenorm (or the nuclear-norm) of a matrix X ∈ R m×n , A and B are linear operators depending on specific applications <ref type="bibr" target="#b28">[29]</ref>, D represents data or observations, and E represents the fitting error. The minimization of problem <ref type="bibr" target="#b0">(1)</ref> would encourage X to be low-rank <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>. In general, Υ(E) is a non-smooth regularizer on E, such as the ℓ 1 -norm regularization (i.e., ||E|| 1 ) or ℓ 2,1 -norm regularization (i.e., ||E|| 2,1 ) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. Problem <ref type="bibr" target="#b0">(1)</ref> has been involved in many computer vision tasks recently, such as image restoration <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref>, multi-label image classification problems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>, video segmentation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b59">60]</ref>, and so on. Note that by removing the term Υ(E), problem (1) is reduced to a simple form:</p><formula xml:id="formula_1">min X X * s.t. A(X) = D,<label>(2)</label></formula><p>or a slightly relaxed matrix lasso problem <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">50]</ref>:</p><formula xml:id="formula_2">min X X * + γ 2 A(X) − D 2 2 ,<label>(3)</label></formula><p>where γ is a regularization parameter.</p><p>In the last decade, many algorithms have been proposed to solve problem <ref type="bibr" target="#b1">(2)</ref> or <ref type="bibr" target="#b2">(3)</ref>  <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b57">58]</ref>, for example, the singular value thresholding (SVT) method <ref type="bibr" target="#b5">[6]</ref>, augmented Lagrangian method (ALM) and alternating direction method (ADM) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>, proximal gradient (PG) <ref type="bibr" target="#b17">[18]</ref>, and accelerated proximal gradient (APG) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">50]</ref>. Due to the non-smoothness of Υ(E), the optimization of problem (1) is more challenging. Recently, aforementioned methods (e.g., ADM and APG) have been extended to solve this problem by minimizing X and E in an alternative way <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>These methods have shown great success in practice. However, their optimization usually requires many singular value decompositions (SVDs), which can be very expensive for large-scale problems. With continuation strategy and rank prediction techniques, the convergence can be accelerated by applying truncated SVDs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b49">50]</ref>. However, the truncated SVDs are often cold-started, that is, when updating X, one has to compute the SVD of a new intermediate matrix in order to compute the thresholding operations involved in the optimizations <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b32">33]</ref>. As a result, the computation cost can be very high on large-scale problems with large ranks <ref type="bibr" target="#b51">[52]</ref>. Since variables between iterations may be very close to each other, a warm-start SVD could be applied to accelerate the speed <ref type="bibr" target="#b51">[52]</ref>, but this is not very stable and may incur convergence issues.</p><p>In this paper, we make the following contributions.</p><p>• We propose a proximal Riemannian gradient (PRG) method to address trace-norm regularized problems defined on M ≤r = {X ∈ R m×n : rank(X) ≤ r}, and provide the convergence result of the algorithm. By exploiting geometries on M ≤r , the SVDs of intermediate solutions are maintained by cheap low-rank QR decompositions, making the method very scalable.</p><p>• To address general trace-norm regularized problems in (1), we present a simple proximal Riemannian pursuit (PRP) scheme, which addresses a sequence of subproblems defined on M ≤r , where r increases monotonically with simple update rules. Therefore, unlike existing fixed-rank methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26]</ref>, this paradigm does not require the knowledge of matrix ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Notations and Preliminaries</head><p>Let the superscript T denote the transpose of a vector/matrix, 0 be a vector/matrix with all zeros, diag(v) be a diagonal matrix with diagonal elements equal to v, A, B = tr(AB T ) be the inner product of A and B, and v p be the ℓ p -norm of a vector v. Let A be a linear operator with A * being its adjoint operator. The operator max(σ, v) operates on each dimension of σ. Let X = Udiag(σ)V T be the SVD of X ∈ R m×n . The nuclear norm of X is defined as X * = σ 1 = i |σ i | and the Frobenius norm of X is defined as X F = σ 2 . Lastly, for any convex function Ω(X), let ∂Ω(X) denote its subdifferential at X.</p><p>We also need some basics of the Geometries of fixedrank matrices and matrix varieties. Due to page limitation, more details are presented in the supplementary file.</p><p>Riemannian manifold fixed-rank matrices. The fixed rank-s matrices lie on a smooth submanifold defined below</p><formula xml:id="formula_3">M s = {X ∈ R m×n : rank(X) = s} = {Udiag(σ)V T : U ∈ St m s , V ∈ St n s , ||σ|| 0 = s},</formula><p>where St m s = {U ∈ R m×s : U T U = I} denotes the Stiefel manifold of m × s real and orthonormal matrices, and the entries in σ are in descending order <ref type="bibr" target="#b50">[51]</ref>. The tangent space T X M s at X is given by</p><formula xml:id="formula_4">T X M s = {UMV T + U p V T + UV T p : M ∈ R s×s ,U p ∈ R m×s , U T p U = 0,V p ∈ R n×s ,V T p V = 0}</formula><p>. Given X ∈ M s and A, B ∈ T X M s , by defining a metric g X (A, B) = A, B , M s is a Riemannian manifold by restricting A, B to the tangent bundle <ref type="bibr" target="#b1">[2]</ref>. Here, the tangent bundle is defined as the disjoint union of all tangent spaces T M s = X∈Ms {X} × T X M s . The norm of a tangent vector ζ X ∈ T X M s at X is defined as ||ζ X || = ζ X , ζ X . As M s is embedded in R m×n , the Riemannian gradient of f at X = Udiag(σ)V T , denoted by gradf (X), is given as the orthogonal projection of the gradient of ∇f (X) onto the tangent space T X M s .</p><formula xml:id="formula_5">PT X Ms (Z) : Z → PU ZPV + P ⊥ U ZPV + PU ZP ⊥ V .<label>(4)</label></formula><p>where P U = UU T and P ⊥ U = I − UU T . Letting G = ∇f (X) be the gradient of f (X), it follows that gradf (X) = P T X Ms (G).</p><p>Moreover, define P T0Ms (Z) = 0 when X = 0.</p><p>Varieties of low-rank matrices <ref type="bibr" target="#b43">[44]</ref>. Now we consider the closure of M r , which is defined by</p><formula xml:id="formula_7">M ≤r = {X ∈ R m×n : rank(X) ≤ r},<label>(6)</label></formula><p>which is a real-algebraic variety. Let ran(X) be the column space of X. In the singular points where rank(X) = s &lt; r, we will construct search directions in the tangent cone <ref type="bibr" target="#b43">[44]</ref> (instead of the tangent space)</p><formula xml:id="formula_8">T X M ≤r = T X M s ⊕ {Ξ r−s ∈ U ⊥ ⊗ V ⊥ },<label>(7)</label></formula><p>where U = ran(X) and V = ran(X T ), and Ξ r−s is a best rank-(r −s) approximation of G − P T X Ms (G) which can be cheaply computed with truncated SVD of rank (r − s). This implies that a tangent vector on T X M ≤r can be represented by</p><formula xml:id="formula_9">ξ = UMV T +U p V T +UV T p + U s diag(σ s )V T s ,<label>(8)</label></formula><p>where Ξ r−s = U s diag(σ s )V T s . Let gradf (X) ∈ T X M ≤r be the projection of G on T X M ≤r . It can be computed by gradf (X) = P T X Ms (G) + Ξ r−s .</p><p>Retraction. Given a search direction ξ ∈ T X M ≤r , the retraction finds the best approximation by a matrix with rank at most r as measured in terms of the Frobenius norm,</p><formula xml:id="formula_11">R ≤r X (ξ) = arg min Y∈M ≤r ||Y − (X + ξ)|| F .<label>(10)</label></formula><p>Algorithm 1: Computation of R ≤r X (ξ).</p><p>Require: X = Udiag(σ)V T ∈ M ≤r , the tangent vector ξ = UMV T +UpV T +UV T p + Usdiag(σs)V T s . 1: Compute (Qu, Ru)=qr(Up, 0), (Qv,Rv)=qr(Vp,0). In general, problem (10) can be addressed by performing truncated SVD on X + ξ, which, however, can be very expensive for high-dimensional matrices when r is large. Fortunately, as summarized in the following, by exploiting geometries over M ≤r , R ≤r X (ξ) can be efficiently computed by slightly modifying the retraction on fixed-rank manifold M r (see details in <ref type="bibr" target="#b50">[51]</ref>). Remark 1. R ≤r X (ξ) can be efficiently computed in Algorithm 1 with efficient QR decompositions on low rank matrices U p and V p . The corresponding time complexity is 14(m+n)r 2 +C SV D r 3 , where r ≪ min(m, n) and C SV D is a moderate constant (say less than 200) <ref type="bibr" target="#b50">[51]</ref>.</p><p>Most Riemannian-optimization based algorithms focus on fixed-rank manifolds M r <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26]</ref>. In the tracenorm minimization, the rank degeneration (i.e. rank(X) &lt; r) may happen and even inevitable. Therefore, the introduction of M ≤r is important and necessary in the trace-norm minimization of our paper. Note that the fixed-rank manifold M r is open, thus the manifold properties break down at the boundary where rank(X) &lt; r, making the convergence analysis of the algorithm difficult accordingly <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proximal Riemannian pursuit</head><p>In this paper, similar to <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b27">28]</ref>, we solve a slightly relaxed form of (1) with the equality constraint replaced with a penalty term:</p><formula xml:id="formula_12">min X,E ||X|| * + λΥ(E) + γ 2 ||A(X) + B(E) − D|| 2 F ,<label>(11)</label></formula><p>where γ is often a large penalty parameter. For convenience, let us define</p><formula xml:id="formula_13">Ψ(X, E) = ||X|| * + λΥ(E) + γ 2 ||A(X) + B(E) − D|| 2 F .</formula><p>Problem <ref type="bibr" target="#b10">(11)</ref> can be addressed by proximal gradient (PG) method and accelerated proximal gradient (APG) <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b27">28]</ref>. However, they are not computationally efficient due to many truncated SVDs. To address this, in this work, we propose to address problem (11) by iteratively solving a series of subproblems (indexed by t = 1, . . . , T ) with progressively relaxed rank constraint on X. Specifically, each of the subproblems is in the form of</p><formula xml:id="formula_14">min X,E Ψ(X, E), s.t. rank(X) ≤ r,<label>(12)</label></formula><p>where the rank constraint is progressively relaxed with r := tκ when t increases. The parameter κ is an integer that is several times smaller than the true rank, and a simple and reasonable setting of this parameter will be discussed later.</p><p>The proposed paradigm is referred to as the proximal Riemannian pursuit (PRP) method and is illustrated in Algorithm 2, where a continuation strategy is applied for γ to accelerate the convergence as in <ref type="bibr" target="#b53">[54]</ref>. The key step of the proposed approach is to address the subproblem in <ref type="bibr" target="#b11">(12)</ref> which is defined on M ≤r . In this paper, we solve problem (12) by a proximal Riemannian gradient method.</p><p>Algorithm 2: General PRP scheme for solving problem (1).</p><p>Input: Parameters κ, λ, γ 0 , γ tar , and χ ∈ (1, +∞). 1 Initialize X 0 = 0 and E 0 = 0; 2 for t = 1 : T do <ref type="bibr" target="#b2">3</ref> Let r := r + κ and γ t ← max(γ t−1 χ, γ tar ). <ref type="bibr" target="#b3">4</ref> Update (X t , E t ) by addressing <ref type="bibr" target="#b11">(12)</ref> with r and γ t ; <ref type="bibr" target="#b4">5</ref> Terminate if the stopping condition is achieved.</p><p>Before continuing, we discuss the determination of κ and stopping conditions.</p><p>Determination of κ. Let σ be the singular vector of A * (D) in descending order. We choose κ such that</p><formula xml:id="formula_15">σ i ≥ η σ 1 , ∀i ≤ κ,<label>(13)</label></formula><p>where η ∈ (0.5, 1]. In other words, κ denotes the number of sufficiently large singular values of A * (D).</p><p>Besides the progressive update of r, one may choose a large κ such that κ &gt; rank(X * ), where X * is the optimal solution of (1). However, this strategy has two drawbacks. First, the X * is unknown, thus the setting of r is difficult. Second, if r is too large, it may incur severely illconditioning issues <ref type="bibr" target="#b48">[49]</ref> and the computational complexity will increase dramatically.</p><p>Stopping condition: Since PRP increase the rank by κ iteratively, anyway it will be stopped in limited steps due to limited size of X, but we may stop it earlier in practice. Basically, we can stop the algorithm if the objective value cannot be decreased significantly at some iteration.</p><formula xml:id="formula_16">Remark 2. Let {X t , E t } be the sequence generated by Al- gorithm 2. Then we have Ψ(X t , E t ) ≤ Ψ(X t−1 , E t−1 ) − β||Ξ t−1 κ || 2 F ,<label>(14)</label></formula><p>where β is some number, Ξ t−1 κ can be computed according to equation <ref type="bibr" target="#b6">(7)</ref>.</p><p>The proof relies on some results from later sections, and can be found in the supplementary file. According to Remark 2, if ||Ξ t−1 κ || 2 F is very small, then there is no need to proceed. We therefore set the convergence condition as follows:</p><formula xml:id="formula_17">Ψ(X t−1 ,E t−1 )−Ψ(X t ,E t ) κΨ(X t−1 ,E t−1 ) ≤ ε,<label>(15)</label></formula><p>where ε denotes a tolerance value. In practice, a large γ may be required to prevent from the solution bias incurred by the regularization. However, in this case, the optimal solution X * may not be an exact low-rank matrix. In this case, the early stopping will help to obtain a low-rank solution, thus it is very important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proximal Riemannian gradient methods</head><p>In this section, we present the proximal Riemannian gradient (PRG) method to address problem (12), 1 which extends the classical proximal method over vector space to M ≤r <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50]</ref>. For simplicity, we first consider a simpler case where Υ(E) is not considered. <ref type="formula" target="#formula_0">(12)</ref> is equivalent to the following problem</p><formula xml:id="formula_18">4.1. Case 1: λ = 0 When Υ(E) is not considered, problem</formula><formula xml:id="formula_19">min X∈M ≤r ||X|| * + γ 2 ||A(X) − D|| 2 F ,<label>(16)</label></formula><p>which is known as the matrix lasso problem <ref type="bibr" target="#b49">[50]</ref>. For convenience, let us define</p><formula xml:id="formula_20">f (X) := γ 2 ||A(X) − D|| 2 F .</formula><p>To introduce the proximal method on M ≤r , similarly as in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50]</ref>, we introduce a local model of Ψ(X) on the tangent cone T X M ≤r around Y ∈ M ≤r but keeping ||X|| * intact as follows:</p><formula xml:id="formula_21">m L (Y; X, ξ) := ||X|| * + Q(X),<label>(17)</label></formula><formula xml:id="formula_22">Q(X) := f (Y) + gradf (Y), ξ + L 2 ξ, ξ ,<label>(18)</label></formula><p>where ξ ∈ T Y M ≤r and X := Y+ξ, and L is the Lipschitz constant. Note that the above local model is very different from that in classical proximal gradient methods (e.g., <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50]</ref>) in the sense that the operation X := Y + ξ is restricted on the tangent cone T Y M ≤r . On the tangent cone, it is valid for the operation Y + ξ for any ξ ∈ T Y M ≤r . With the introduction of the local model, the proximal Rimannian gradient method addresses problem <ref type="bibr" target="#b15">(16)</ref> in an iterative way (similar to <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50]</ref>), which updates X k in the kth iteration by minimizing m L (X k−1 ; X, ξ) on M ≤r , i.e.,</p><formula xml:id="formula_23">X k = arg min X∈M ≤r m L (Y; X, ξ),<label>(19)</label></formula><p>where Y is set to X k−1 and L is set to L k . Let T L (Y) be the minimizer of problem <ref type="bibr" target="#b18">(19)</ref>, i.e. T L (Y) := arg min X∈M ≤r m L (Y; X, ξ). Then it can be computed by first computing the solution to arg min X∈M ≤r Q(X), and then performing a singular value thresholding operation on the solution. Let R Y (ξ) = arg min X∈M ≤r Q(X). The computation of R Y (ξ) and T L (Y) is shown in the following lemma whose proof is available in the supplementary file.</p><p>Lemma 1. Given Y and the parameter L, R Y (ξ) can be computed by equation <ref type="bibr" target="#b9">(10)</ref> </p><formula xml:id="formula_24">with ξ = −gradf (Y)/L. De- noting the SVD of R Y (ξ) as R Y (ξ)=U + diag(σ + )V T + , we have T L (Y) = U + diag(max(σ + − 1/L, 0))V T + .</formula><p>From Lemma 1, R Y (ξ) is exactly the Retraction R ≤r X (ξ), and it can be cheaply computed according to Algorithm 1 whose complexity is 14(m + n)r 2 + C SV D r 3 , where r ≪ min(m, n). Since the decomposition R Y (ξ) = U + diag(σ + )V T + can be maintained by QR decompositions on low-rank matrices, we do not have to compute the truncated SVDs as in the classical proximal gradient methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50]</ref>. After obtaining R Y (ξ), T L (Y) can be easily computed. The general scheme of PRG is shown in Algorithm 3.</p><p>Algorithm 3: Proximal Riemannian gradient method for solving problem <ref type="bibr" target="#b15">(16)</ref>.</p><p>Input: X 0 , parameter γ and r, stopping tolerance ǫ.</p><formula xml:id="formula_25">1 for k = 1, ..., K do 2 Compute gradf (X k−1 ); 3 Let ξ k = −gradf (X k−1 )/L k where L k is determined by Armijo line search in (20); 4 Set X k = T L k (X k−1 ); 5</formula><p>Terminate if stopping conditions are achieved;</p><p>6 Return X k .</p><p>Determining L k : In practice, it is critical to find a good parameter L k to make a sufficient decrease of the objective. Let Ψ(X) = ||X|| * + γ 2 ||A(X) − D|| 2 F be the objective function. In the kth iteration, given a descent direction ζ k ∈ T X k M ≤r , and L k can be determined using Armijo line search to satisfy</p><formula xml:id="formula_26">Ψ(T L k (X k ))≤Ψ(X k ) + β gradf (X k ), ζ k /L k ,<label>(20)</label></formula><p>where β ∈ (0, 1). Here, 1/L k can be considered as the step size. The existence of L k is guaranteed by the following lemma, whose proof is available in the supplementary file.</p><p>Lemma 2. Let X k ∈ M ≤r , and ζ k ∈ T X M ≤r be a descent direction. Then there exists an L k that satisfies the condition in <ref type="bibr" target="#b19">(20)</ref>.</p><p>In the following, we discuss the convergence and stopping conditions of the algorithm.</p><p>Convergence and optimality: Optimization methods on Riemannian manifolds are often locally convergent. Whereas, for PRG, the limit point X * will be a global solution if rank(X * ) &lt; r, as shown in the following proposition whose proof can be found in the supplementary file. Proposition 1. Let {X k } be an infinite sequence of iterates generated by Algorithm 3. Then every accumulation point of {X k } is a critical point of f over M ≤r . Furthermore, lim k→∞ ||gradf (X k ) + ζ|| F = 0, where ζ is the subdifferential of ||X|| * at X <ref type="bibr" target="#b17">[18]</ref>. If rank(X * ) &lt; r, then we have ∇f (X * ) + ζ = 0, i.e., X * is a global optimum to <ref type="bibr" target="#b15">(16)</ref>.</p><p>Stopping conditions. A natural stopping condition would be ||gradf (X k ) + ζ|| F ≤ ǫ with some predefined ǫ. In practice, we do not have to solve the subproblem exactly. For simplicity, we stop PRG early if the following condition is achieved:</p><formula xml:id="formula_27">(Ψ(X k−1 )−Ψ(X k )) Ψ(X k−1 ) ≤ ǫ,<label>(21)</label></formula><p>where ǫ denotes a tolerance value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Case 2: λ = 0</head><p>Now, we are ready to extend PRG to minimize problem <ref type="bibr" target="#b11">(12)</ref> in which λ = 0. Following <ref type="bibr" target="#b28">[29]</ref>, we optimize the two variables X and E using an alternating approach. Let the pair (X k , E k ) denote the variables obtained from the kiteration. At the (k + 1)th iteration, we update X and E as below.</p><p>To update X, we fix E = E k and define a local model of Ψ(X, E) on the tangent cone T X M ≤r around X k ∈ M ≤r :</p><formula xml:id="formula_28">m L (X; X k , E k , ξ) := ||X|| * + f (X k , E k ) + gradf (X k , E k ), ξ + L/2 ξ, ξ , where ξ ∈ T X k M ≤r and L is the Lipschitz constant. S- ince ξ ∈ T X k M ≤r , it is valid to have X = X k + ξ. Let T L (X k , E k ) be the minimizer of m L (X; X k , E k , ξ) sub- jected to rank(X) ≤ r. Similar to Lemma 1, T L (X k , E k )</formula><p>can be computed with two steps, where L can be determined by Armijo line search to make a sufficient decrease of the objective.</p><p>To update E, we fix X = X k+1 and solve a problem:</p><formula xml:id="formula_29">min E λΥ(E) + γ 2 ||A(X k+1 ) + B(E) − D|| 2 F .<label>(22)</label></formula><p>Solving problem <ref type="formula" target="#formula_1">(22)</ref>  The detailed algorithm, which is referred to as robust PRG (PRG(R)), is shown in Algorithm 4. Due to the possible ill-conditioning issues, 2 we again apply a homotopy continuation technique for λ to accelerate the convergence speed. Starting from an initial guess λ 0 , we set λ k = min(λ 0 ρ k−1 , λ) and compute E k = S λ k (X k , E k−1 ), where ρ is chosen from (0, 1). Clearly, λ k is non-increasing w.r.t. the iteration index k.</p><p>We discuss the convergence as follows.</p><p>Algorithm 4: Robust PRG for solving problem <ref type="bibr" target="#b11">(12)</ref>. Input: Initial (X 0 , E 0 ), parameter λ, γ and r, initial λ 0 &gt; λ, ρ ∈ (0, 1), tolerance ǫ. 1 for k = 1, ..., K do 2 Let λ k = max(λ k−1 ρ, λ); <ref type="bibr" target="#b2">3</ref> Compute gradf (X k−1 , E k−1 ) by (5) or (9); <ref type="bibr" target="#b3">4</ref> Choose L k by Armijo line search. Set</p><formula xml:id="formula_30">X k = T L k (X k−1 , E k−1 ); 5 Compute E k = S λ k (B k−1 ) with B k−1 = D − A(X k ); 6</formula><p>Terminate if stopping conditions are achieved;</p><formula xml:id="formula_31">7 Return (X k , E k ). Proposition 2. Let Ψ(X k , E k ) = ||X k || * + λ k Υ(E k ) + f (X k , E k ), and {(X k , E k )} be</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>an infinite sequence of iterates generated by Algorithm 4. It follows that</head><formula xml:id="formula_32">Ψ(X k+1 , E k+1 ) ≤ Ψ(X k , E k ), and {(X k , E k )} converges to a limit point (X * , E * ).</formula><p>The proof of Proposition 2 can be found in the supplementary file. The stopping condition in <ref type="bibr" target="#b20">(21)</ref> can be extended to PRG(R) by replacing Ψ(X) with Ψ(X, E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Complexity analysis</head><p>The complexity of PRP includes two main folds, i.e., the computation of Ξ t κ which can be done by truncated SVD of rank κ and the subproblem optimization by PRG or PRG(R). Here, we focus on the complexity of PRP on M-R. At the tth iteration of PRP, the complexity of PRG or PRG(R) is O((m + n)(κt) 2 + lκt), where κt ≤ r + κ. For sufficiently sparse matrices like in MR, the truncated SVD of rank κ in PRP can be completed in O((m + n)κ) using PROPACK <ref type="bibr" target="#b22">[23]</ref>; while the truncated SVD in existing proximal gradient based methods takes O((m + n)r), where κ is several times smaller than r. The complexity comparison on LRR and RPCA can be found in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Parameter settings</head><p>For convenience of parameter setting, we suggest choosing the penalty parameter γ in (12) according to γ = 1/(νσ 1 ), where ν is a scaling factor and σ 1 denotes the largest singular value of A * (D). Note that this setting is consistent with the regularization parameter setting in matrix lasso in <ref type="bibr" target="#b49">[50]</ref>. For robust cases, the parameter λ in (12) is chosen by λ = δd m , where d m denotes the mean of |D|. Without loss of generality, we suggest setting ν ∈ (0.0001, 0.01) and δ ∈ (0.01, 1]. One may also apply the cross-validation to choose ν and δ, but it is not considered in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related studies</head><p>The proposed PRG methods over M ≤r is closely related to fixed-rank methods defined on nonlinear fixed-rank manifolds <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26]</ref>, which have shown great advantages in computation for solving large-scale matrix completion problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35]</ref>, such as the low-rank geometric conjugate gradient method (LRGeomCG) <ref type="bibr" target="#b50">[51]</ref>, the quotient geometric matrix completion method (qGeomMC) <ref type="bibr" target="#b36">[37]</ref>, scaled gradients on Grassmann manifolds for matrix completion (ScGrassMC) method <ref type="bibr" target="#b39">[40]</ref>. However, these methods can only deal with smooth objectives, and cannot handle tracenorm regularized problems. Some researchers proposed to solve a variational form of trace-norm regularized problem <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref>:</p><formula xml:id="formula_33">min G,H G 2 F + H 2 F , s.t. A(GH ⊤ ) = D,</formula><p>where G ∈ R m×r and H ∈ R n×r . This problem can be addressed by either gradient based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref> or stochastic gradient methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b52">53]</ref>. However, these methods may suffer from slow convergence speeds <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Recently, the authors in <ref type="bibr" target="#b37">[38]</ref> exploited Riemannian structures and presented a trust-region algorithm to address trace-norm minimizations. The proposed method, denoted by MMBS, alternates between fixed-rank optimization and rank-one updates. However, empirically this method shows slower speed even than APG on large-scale problems <ref type="bibr" target="#b37">[38]</ref>. The authors in <ref type="bibr" target="#b31">[32]</ref> proposed a Grassmann manifold method based on a fixed-rank manifold. In general, this method has similar complexity to ScGrassMC that also operates on Grassmann manifold <ref type="bibr" target="#b39">[40]</ref>. More recently, a new Retraction for accelerating the Riemannian three-factor low-rank matrix completion problem <ref type="bibr" target="#b25">[26]</ref>.</p><p>Active subspace methods or greedy methods, which increase the rank by one per iteration, have gained great attention in recent years <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b23">24]</ref>. However, these methods usually involve expensive subproblems, and might be very expensive when the true rank is high. For example, Laue's method <ref type="bibr" target="#b23">[24]</ref> needs to solve nonlinear master problems using the BFGS method, which is not scalable for large-scale problems. More recently, <ref type="bibr" target="#b15">[16]</ref> proposed a novel active subspace selection method for solving trace-norm regularized problems, but this method may suffer from slow convergence speed due to the approximated SVDs and inefficient solvers for the subproblem optimization. The authors in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b56">57]</ref> proposed a Riemanian pursuit algorithm which increases the rank more than one, but this method cannot deal with trace-norm regularized problems. In <ref type="bibr" target="#b58">[59]</ref>, Zhou et al. proposed an algorithm which also operated on M ≤r and adjusted the rank iteratively. However, our method is different from Zhou's method. Specifically, we address tracenorm regularized problems which are non-smooth; while Zhou's method focuses on smooth objectives. The tracenorm regularizer naturally encourages low-rank solution; while Zhou's method resorts to heuristics to reduce ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>For convenience, we refer PRP with PRG(R) and PRG to as PRP(R) and PRP, respectively. We evaluate proposed methods on two classical tasks, namely matrix completion and LRR based clustering. All the experiments are conducted in Matlab (R2012b) on a PC installed a 64-bit operating system with an Intel(R) Core(TM) i7 CPU (3.2GHz with single-thread mode) and 64GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experiments on Matrix Completion</head><p>We compare the proposed methods, i.e., PRG, PRG(R), PRP and PRP(R), on several matrix completion tasks. Three state-of-the-art trace-norm based methods, e.g. APG <ref type="bibr" target="#b49">[50]</ref>, MMBS <ref type="bibr" target="#b37">[38]</ref>, and Active ALT <ref type="bibr" target="#b15">[16]</ref>, are adopted as baselines. We also compare with several efficient fixed-rank methods operating on manifolds, such as LRGeomCG <ref type="bibr" target="#b50">[51]</ref>, RP <ref type="bibr" target="#b48">[49]</ref>, LMaFit <ref type="bibr" target="#b52">[53]</ref>, ScGrassMC <ref type="bibr" target="#b39">[40]</ref>, A-R3MC1 <ref type="bibr" target="#b25">[26]</ref>, and qGeomMC <ref type="bibr" target="#b36">[37]</ref>. We do not report the results of some methods (e.g., IALM <ref type="bibr" target="#b26">[27]</ref> and the method in <ref type="bibr" target="#b31">[32]</ref>), since they are either slower than other compared methods or the sources are not available. We adopt the root-mean-square error (RMSE) as a major evaluation metric: RMSE = P Ω (D − X * ) F / (|Ω|), where X * denotes the recovered matrix, and Ω denotes the index set for testing, and P Ω denotes the orthogonal projection onto Ω [51].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Synthetic Experiments</head><p>Following <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49]</ref>, we generate synthetic low-rank matrices D = Udiag(σ)V T ∈ R m×m of rank r, where U ∈ St m r , V ∈ St m r , m = 5000, r = 50 and σ is a 50dimensional vector with its entries sampled from a uniform distribution [0, 1000]. We sample l = ωr(2m − r) entries from D uniformly as the observations stored in d ∈ R l , where ω is an oversampling factor <ref type="bibr" target="#b26">[27]</ref>. Here, we set ω = 2.5. We study two toy data sets whose observations are perturbed by two kind of noises: In the first toy data set TOY1, each entry of d is perturbed by additive Gaussian noise of magnitude 0.01 d 2 / n 2 , where n ∈ R l is a Gaussian random vector sampled from N (0, 1); The second toy data TOY2 is constructed based on TOY1, by further perturbing 5% of the observations with outliers uniformly sampled from [−10, 10]. In synthetic experiments, we set ν = 0.005, δ = 0.1 and η = 0.65 (see equation <ref type="bibr" target="#b12">(13)</ref>). The trace-norm based methods APG, MMBS and Active ALT, are adopted as the baselines. The Relative objective difference and Testing RMSE w.r.t. time on TOY1 and TOY2 are reported in <ref type="figure" target="#fig_3">Figure 1</ref>.</p><p>According to <ref type="figure" target="#fig_3">Figure 1</ref>(a), our proposed PRG, PRG(R), PRP and PRP(R) converge much faster than the comparators, and PRP and PRP(R) improve upon their counterparts (i.e., PRG and PRG(R)) significantly. From <ref type="figure" target="#fig_3">Figure 1(d)</ref>, the testing RMSE shows similar trends to the objective values.  Note that, our methods thus achieve low RMSE values in very short time. In general, the Active ALT method is slower than others, which may be due to the approximated SVDs and inefficient solvers for the subproblem optimization.</p><p>From <ref type="figure" target="#fig_3">Figure 1</ref>(b), on TOY2 which is disturbed by outliers, our proposed PRG(R) and PRP(R) converge faster than the baselines. From <ref type="figure" target="#fig_3">Figure 1</ref>(e), only the proposed PRG(R) and PRP(R) achieve promising testing RMSE values; while other methods over-fit the data after several iterations due to outliers. Note that PRP(R) converges faster than its counterpart PRG(R). These observations demonstrate the effectiveness and efficiency of our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Experiments on Real-world Data</head><p>We study the performance of PRP and SR-PRG on three collaborative filtering data sets: MovieLens with 10M ratings (denoted by Movie-10M) <ref type="bibr" target="#b14">[15]</ref>, Netflix Prize dataset <ref type="bibr" target="#b21">[22]</ref> and and Yahoo! Music Track 1 data set <ref type="bibr" target="#b10">[11]</ref>. The statistics of these data sets are recorded in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In the first experiment, we only compare with the three trace-norm based methods, e.g. APG <ref type="bibr" target="#b49">[50]</ref>, MMBS <ref type="bibr" target="#b37">[38]</ref> and Active ALT <ref type="bibr" target="#b15">[16]</ref> on Movie-10M. We report the change of Relative objective difference and Testing RMSE w.r.t. time in <ref type="figure" target="#fig_3">Figure 1</ref>. Here, we randomly choose 80% of the ratings as the training set and the remainder as the testing set.</p><p>From <ref type="figure" target="#fig_3">Figures 1(c)</ref> and 1(f), our proposed methods show For fixed-rank methods, the rank parameter must be provided. In this paper, the ranks returned by PRP are used as the rank estimations for the fixed-rank methods, i.e., Sc-GrassMC, qGeomMC, LMaFit and A-R3MC1. Specifically, the ranks returned by PRP for the three data sets are 14, 16, 28, respectively, and PRP(R) returns the same ranks. Other parameters for the comparison methods are kept default. We set ν = 0.001 η = 0.65, and δ = 0.7 in our methods. Following <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b16">17]</ref>, we report the testing RMSE of different methods over 10 random 80/20 training/testing partitions.</p><p>Comparison results are shown in <ref type="table" target="#tab_1">Table 2</ref>. According to the table, PRP and PRP(R) generally perform the best among all the compared methods in terms of testing RMSE and time. The proposed PRP and PRP(R) methods also achieve slightly better testing RMSE than RP with comparable time. Note that RP relies on carefully designed stopping conditions to induce low-rank solutions, and cannot deal with outliers <ref type="bibr" target="#b48">[49]</ref>. Maybe due to this reason, PRP and PRP(R) achieve significant improvements in terms of testing RMSE on the Yahoo data set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experiments on LRR Subspace Clustering</head><p>We compare our PRP(R) method with existing LRR solvers in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. In <ref type="bibr" target="#b54">[55]</ref>, the authors proposed a LRR solver which is based on factorizing the data matrix with its truncated SVD. However, the computation of skinny SVD for large data matrix is usually too expensive. We thus exclude it for the comparison.</p><p>We conduct experiments regarding the clustering task on the Extended Yale Face Database B (ExtYaleB) and the Human Activity Recognition Using Smartphones dataset (HARUS) <ref type="bibr" target="#b2">[3]</ref> We have D ∈ R 2016×640 for the ExtYaleB data set and D ∈ R 561×10299 for the HARUS data set. The ExtYaleB contains 2, 414 frontal face images of 38 subjects with different lighting, poses and illumination conditions, where each subject has round 64 faces. Following <ref type="bibr" target="#b30">[31]</ref>, we use 640 faces from the first 10 subjects. Each face image is resized to 48 × 42 pixels and then reshaped as a 2016dimensional gray-level intensity feature. The HARUS is a large-scale data set (containing 10,299 signals w.r.t. 6 activities) with data collected using embedded sensors on the smartphones carried by volunteers on their waists, when they are conducting daily activities (e.g., walking, sitting, laying). The captured sensor signals are pre-processed to filter noise and post-processed.</p><p>Following <ref type="bibr" target="#b29">[30]</ref>, we measure the clustering performance by clustering accuracy. The best clustering accuracies and corresponding running times are reported in <ref type="figure" target="#fig_0">Figure 2</ref>. From the figure, our PRP(R) method outperforms the two existing LRR solvers in terms of efficiency, since our algorithm does not frequently involve SVDs w.r.t. large matrices. Moreover, our algorithm achieves comparable clustering performance with <ref type="bibr" target="#b28">[29]</ref>. In contrast, the LRR solver in <ref type="bibr" target="#b29">[30]</ref> achieves lower clustering accuracy on HARUS, possibly because that algorithm is not guaranteed to obtain a globally optimal solution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Classical proximal gradient methods for addressing trace-norm regularized problems may suffer from high computational cost on large-scale problems <ref type="bibr" target="#b51">[52]</ref>. To reduce the computational complexity, we have proposed in this paper a Proximal Riemannian Pursuit (PRP) strategy which addresses general trace-norm regularized problems by progressively activating a number of active subspaces. Moreover, we have proposed a Proximal Riemannian Gradient (PRG) method for addressing the trace-norm regularized subproblems defined over a matrix variety M ≤r , where r is adjusted automatically by PRP. By exploiting geometries on M ≤r , PRG maintains the SVDs of the intermediate solutions via cheaper low-rank QR decompositions, without solving truncated SVDs of large-ranks explicitly at high computational cost. Extensive experiments on multiple data sets have demonstrated the superior efficiency of the proposed methods over other methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 :</head><label>2</label><figDesc>Let Z = [diag(σ) + M R T v ; Ru 0]. 3: Compute (Uz, σz, Vz) = svd(Z). 4: Letσ = [σz; σs]. 5: LetŪ=[[U Qu]Uz Us],V=[[V Qv]Vz Vs]. 6: Arrangeσ in descending order, andŪ andV accordingly. 7: Let U+ =Ū(:, 1 : r), V+ =V(:, 1 : r) and σ+ =σ(1 : r). 8: Output R ≤r X (ξ) = U+diag(σ+)V T + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>with general B may be difficult. Fortunately, for MR and LRR, where B(E) = E and Υ(E) is either E 1 or E 2,1 , problem (22) has a closed-form solution. Let us define B k = D − A(X k+1 ). Then B k is a vector for MR and a matrix in the form of B k = [b k 1 , . . . , b k n ] for LRR. The closed-form solution, denoted by S λ (B k ), can be found in supplementary file. In cases where the problem (22) cannot be solved in closed-form, one may adopt iterative procedures to solve it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 .</head><label>1</label><figDesc>Performance of various methods on TOY1, TOY2 and Movie-10M, in terms of relative objective difference vs computational time (see (a)(b)(c)) and testing RMSE values vs computational time (see (d)(e)(f)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Clustering accuracies and Running time of different LRR solvers on two data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Statistics of datasets.much faster convergence speed as well as faster decreasing of testing RMSE values. In the second experiment, the baseline methods include APG<ref type="bibr" target="#b49">[50]</ref>,3 MMBS [38], L-RGeomCG<ref type="bibr" target="#b50">[51]</ref>, qGeomMC<ref type="bibr" target="#b36">[37]</ref>, LMaFit<ref type="bibr" target="#b52">[53]</ref>, ScGrassM-C<ref type="bibr" target="#b39">[40]</ref>,<ref type="bibr" target="#b3">4</ref> Active ALT<ref type="bibr" target="#b15">[16]</ref>, 5 RP [49] 6 and A-R3MC1<ref type="bibr" target="#b25">[26]</ref>.<ref type="bibr" target="#b6">7</ref> Among them, A-R3MC1 is a recently developed method which applies a new retraction technique for solving fixedrank problems. It is considered as the state-of-the-art method.</figDesc><table>Data set 
m 
n 
|Ω| 
Movie-10M 
71,567 
10,677 
10,000,054 
Netflix 
48,089 
17,770 
100,480,507 
Yahoo 
1,000,990 624,961 252,800,275 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Experimental results on real-world datasets, where time is recorded in seconds. Some results on Netflix and Yahoo are left blank, since expensive computation cost makes them unavailable.</figDesc><table>Method 
Movie-10M 
Netflix 
Yahoo 
RMSE 
Time 
RMSE 
Time 
RMSE 
Time 
APG 
1.094 
810.01 
1.038 
2883.80 
-
-
LRGeomCG 
0.823 
57.67 
0.860 
2356.86 
25.228 
18319 
QgeomMC 
0.836 
96.41 
0.897 
9794.75 
24.167 
82419 
LMaFit 
0.838 
133.86 
0.876 
2683.73 
24.368 
24349 
ALT 
0.855 
917.17 
-
-
-
-
MMBS 
0.821 
441.10 
-
-
-
-
ScGrassMC 
0.845 
216.07 
0.892 
4522.68 
24.954 
37705 
RP 
0.818 
46.56 
0.858 
1143.02 
23.451 
12456 
A-R3MC1(50) 
0.8327 
93.27 
0.9003 
3340.08 
23.243 
13157 
A-R3MC1(100) 
0.8276 
175.87 
0.8868 
4999.45 
22.497 
21185 
PRP 
0.817 
53.42 
0.855 
1057.35 
22.644 
15972 
PRP(R) 
0.815 
67.73 
0.857 
1245.15 
22.537 
17263 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In fact, M ≤r is a closure of the Riemannian submanifold Mr. We abuse "Riemannian" here for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">When λ is very small, E 1 can be very large at the beginning, making the initial point far from the optimum.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">APG method is from http://www.math.nus.edu.sg/ mattohkc/NNLS.html. 4 MMBS, LRGeomCG, qGeomMC, LMaFit, and ScGrassMC are available from http://www.montefiore.ulg.ac.be/˜mishra/ fixedrank/fixedrank.html. 5 http://www.cs.utexas.edu/˜cjhsieh/. 6 http://www.tanmingkui.com/rp.html. 7 https://github.com/innerlee/Publications.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two Newton methods on the manifold of fixed-rank matrices endowed with Riemannian quotient geometries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="569" to="590" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optimization Algorithms on Matrix Manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Reyes-Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ambient assisted living and home care</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convex multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="272" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RTRMC: A Riemannian trustregion method for low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boumal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Optim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tight oracle bounds for low-rank matrix recovery from a minimal number of random measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Plan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2342" to="2359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix completion for resolving label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
		<title level="m">The Yahoo! Music Dataset and KDD-Cup&apos;11. JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Matrix rank minimization with applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transduction with matrix completion: Three birds with one stone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sparse approximate solutions to semidefinite programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="306" to="316" />
		</imprint>
		<respStmt>
			<orgName>LATIN</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An algorithmic framework for performing collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nuclear norm minimization via active subspace selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple algorithm for nuclear norm regularized problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sulovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An accelerated gradient method for trace norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the minimal problems of low-rank matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oskarsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Astrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised low-rank mapping learning for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Acm</forename><surname>Kddcup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigkdd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netflix</surname></persName>
		</author>
		<title level="m">Proceedings of KDD Cup and Workshop</title>
		<meeting>KDD Cup and Workshop</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Propack-software for large and sparse svd calculations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Larsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A hybrid algorithm for convex semidefinite optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sold: Sub-optimal low-rank decomposition for efficient video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new retraction for accelerating the riemannian three-factor low-rank matrix completion algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>UIUC</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAMSAP</title>
		<imprint>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Linearized alternating direction method with adaptive penalty for low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1109.0367</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<title level="m">Robust recovery of subspace structures by low-rank representation. T-PAMI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="171" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nuclear norm regularized least squares optimization on grassmannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized nonconvex nonsmooth low-rank minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Depth enhancement via low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linear regression under fixed-rank constraints: A Riemannian approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bonnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">R3mc: A riemannian threefactor algorithm for low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">53rd IEEE Conference on Decision and Control</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Riemannian geometry for low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Apuroop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lowrank optimization with trace norm penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2124" to="2149" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gradient methods for minimizing composite objective function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Center for Operations Research and Econometrics (CORE), Catholic University of Louvain (UCL</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scaled gradients on Grassmann manifolds for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parallel stochastic gradient algorithms for large-scale matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programm. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="226" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Collaborative filtering in a non-uniform world: Learning with the weighted trace norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Convergence results for projected line-search methods on varieties of low-rank matrices via Lojasiewicz inequality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uschmajew</surname></persName>
		</author>
		<idno>arX- iv:1402.5284</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Augmented lagrangian alternating direction method for matrix separation based on lowrank factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Method. and Softw</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ahead-ofprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust orthonormal subspace learning: Efficient recovery of corrupted low-rank matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large-scale convex minimization with a low-rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Optimization for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Riemannian pursuit for big matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vandereycken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pac. J. Optim</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="615" to="640" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Low-rank matrix completion by Riemannian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vandereycken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1214" to="1236" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Accelerating iterations involving eigenvalue or singular value decomposition by block lanczos with warm start</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno>MSR-TR- 2010-162</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Solving a low-rank factorization model for matrix completion by a non-linear successive over-relaxation algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="361" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Falrr: A fast low rank representation solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automatic face naming by learning discriminative affinity matrices from weakly labeled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2440" to="2452" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scalable maximum margin matrix factorization by active riemannian subspace search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Accelerated training for matrix-norm regularization: A boosting approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rank-constrainted optimization: A riemannian manifold approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Gallivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Dooren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN 2015 proceedings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Godec:randomized low-rank &amp; sparse matrix decomposition in noisy case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
