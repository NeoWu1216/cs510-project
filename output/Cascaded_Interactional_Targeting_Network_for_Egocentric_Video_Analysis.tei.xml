<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Interactional Targeting Network for Egocentric Video Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at San Antonio</orgName>
								<address>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
							<email>nibingbing@sjtu.edu.cnhongrc@hfut.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">HeFei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
							<email>xkyang@sjtu.edu.cnqi.tian@utsa.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at San Antonio</orgName>
								<address>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Interactional Targeting Network for Egocentric Video Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowing how hands move and what object is being manipulated are two key sub-tasks for analyzing first-person (egocentric) action. However, lack of fully annotated hand data as well as imprecise foreground segmentation make either sub-task challenging. This work aims to explicitly address these two issues via introducing a cascaded interactional targeting (i.e., infer both hand and active object regions) deep neural network. Firstly, a novel EM-like learning framework is proposed to train the pixel-level deep convolutional neural network (DCNN) by seamlessly integrating weakly supervised data (i.e., massive bounding box annotations) with a small set of strongly supervised data (i.e., fully annotated hand segmentation maps) to achieve stateof-the-art hand segmentation performance. Secondly, the resulting high-quality hand segmentation maps are further paired with the corresponding motion maps and object feature maps, in order to explore the contextual information among object, motion and hand to generate interactional foreground regions (operated objects). The resulting interactional target maps (hand + active object) from our cascaded DCNN are further utilized to form discriminative action representation. Experiments show that our framework has achieved the state-of-the-art egocentric action recognition performance on the benchmark dataset Activities of Daily Living (ADL).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed the emergence of firstperson (egocentric) action analysis due to its various applications in assisted daily living, medical surveillance and smart home <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38]</ref>. Daily living egocentric videos involve a large amount of manipulation actions. The key challenges are irrelevant objects-of-interest and the noisy background motions. Therefore, the key of addressing these issues is to successfully segment out hand region and active region (i.e., the objects-of-interest region).</p><p>Prior art <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b24">25]</ref> in egocentric action analysis have paid the most attention to these sub-tasks. Ren et al. <ref type="bibr" target="#b33">[34]</ref> quantitatively analyzed the feasibilities and challenges of the egocentric recognition of handled objects. It was pointed out that hand and motion information are the keys to solve the egocentric video recognition problem. Pirsiavash et al. <ref type="bibr" target="#b30">[31]</ref> used a temporal pyramid for both passive and active objects as the action representation, and they suggested that the daily living egocentric video understanding are "all about the objects being interacted with". McCandless et al. <ref type="bibr" target="#b24">[25]</ref> applied an "object-centric" scheme to automatically select some representative spatio-temporal partitions from a pool of pre-detected partitions.</p><p>However, there are two major difficulties in hand and foreground object segmentation. Firstly, in egocentric videos, the segmentation map for a non-rigid object like hand is more helpful in later processing (e.g., motion feature pooling) than object bounding box information. Unfortunately, previous methods for hand detection in egocentric video analysis mainly adopt the hand detection method (i.e., bounding box detection) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15]</ref>, which is not precise enough to model hand movements. This is mainly due to the lack of pixel-level hand annotations for training good hand segmentation models. In fact, it is not feasible to get a large scale per-pixel annotated hand dataset because it requires intensive human labor. Moreover, pixel-level hand detection/segmentation is challenging due to large illumination changes and hand deformations. Secondly, most of previous works separately model the hands, foreground objects and motion information. Due to the noisy background motion, highly frequent occlusion, and object deformation, jointly detecting/segmenting these objects is hard. We find the rich contextual information could be explored to enhance the detection/segmentation. Two important observations are that the hand information are helpful for localizing the handmanipulated objects, and the motion information are useful for detecting the foreground objects. Motivated by these observations, we propose a hybrid/cascaded end-to-end deep convolutional neural network to jointly infer the hand maps and manipulated foreground object maps.</p><p>On one hand, we propose a novel end-to-end trainable semantic parsing network for hand segmentation. In order to tackle the problem of insufficient fully annotated (i.e., per-pixel annotated) hand maps, we develop an EM-like training method to augment the semi-supervised data, i.e., a small set of fully annotated hand segmentation data and a large number of hand bounding box data. In particular, in the E-step, we firstly generate a number of hand map proposals by the traditional hand segmentation method such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>. Then we use our trained deep semantic parsing neural network <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b3">4]</ref> to select the best hand candidate, (i.e., the hand candidate has the largest overlap with the predicted hand segmentation map). In the M-step, the selected hand candidates are considered as new ground truth which are utilized to further fine-tune the hand segmentation neural network. The converged network model parameters are used as the ultimate hand segmentation model. We evaluate our approach on the Georgia Tech Egocentric Activity (GTEA) <ref type="bibr" target="#b8">[9]</ref> dataset. In the experiments, we show that our proposed pixel-level hand detection could handle some difficult cases such as large illumination changes, or hand deformations in egocentric videos.</p><p>On the other hand, we propose a second end-to-end deep convolutional network to maximally utilize the contextual information among hand, foreground object, and motion for interactional foreground object detection. More specifically, the network inputs are the strongest object feature maps from the convolutional layer of AlexNet <ref type="bibr" target="#b17">[18]</ref>, the hand segmentation maps detected by our proposed end-toend semantic parsing network, and the optical flow motion maps. The convolutional filters connected to each pair of the three types of maps are learned to explore the contextual information and generate the interactional foreground object maps. Finally, based on the detected foreground object maps, we pack both object-centric features and the locally pooled motion features into one unified action representation, which is used for the action classification task. The action recognition framework is extensively evaluated on the egocentric video benchmark dataset Activities of Daily Living (ADL) <ref type="bibr" target="#b30">[31]</ref>. We show that our proposed framework significantly outperforms the state-of-the-art algorithms in terms of action classification performance.</p><p>We summarize our contributions as two-fold: 1) we propose a novel iterative training scheme for a pixel-to-pixel hand segmentation DCNN. Our work transforms the weakly supervised hand bounding boxes into the strongly supervised hand segmentations, which saves a large amount of human labor for per-pixel annotation; 2) contextual information among the corresponding hand segmentation map, object feature map, and motion map are jointly explored in a DCNN architecture to generate accurate interactional foreground regions, i.e., the area of manipulated objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pixel-level Hand Detection</head><p>The approaches to generate pixel-level detections of hand regions mainly fall into the following three categories: (1) the appearance based methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13]</ref> use the appearance features such as color or texture to detect static and dynamic appearance of skin; (2) the global appearancebased models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29]</ref> detect hands by matching against the global hand templates under different configurations; (3) the motion-based approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref> explicitly take into account the ego-motion of the camera by assuming that hands (foreground) and the background have different motion or appearance statistics. The focus of our work is different, i.e., to design an effective training scheme that can utilize weakly supervised hand bounding box data (easily obtainable) with a small set of strongly supervised hand segmentation map data (expensive) to facilitate the pixel-topixel hand segmentation DCNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Egocentric Action Recognition</head><p>Currently, there are mainly four types of methods focusing on the egocentric action recognition. First of all, the objects manipulated by human hands in egocentric videos are modeled <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25]</ref>. Secondly, it is suggested that gaze location is an important cue for egocentric activity recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, but this fine-grained information is difficult for detection. Thirdly, human-human interaction or human-object-human interaction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref> is important for egocentric action recognition. For example, Ryoo et al. <ref type="bibr" target="#b35">[36]</ref> integrated global and local motion information to model interaction-level human activities. Finally, motion features also play an important role in egocentric action analysis <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>, which is consistent with the general thirdperson action recognition scenario <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b29">30]</ref>. Ying et al. <ref type="bibr" target="#b21">[22]</ref> combines multiple cues including object, motion, head movement, hand and gaze information to achieve the state-of-the-art, which is a sound work for egocentric information fusion. However, these information are not always feasible from pure wearable camera. We focus on the problem of recognizing single-human activities of daily living, where there is a large amount of manipulation actions. In such case, neither gaze information nor human-human interaction modeling could well solve the problem. We argue that it is necessary to detect foreground interactional (manipulated) objects, and combine both object cue and motion cue for action representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cascaded Interactional Targeting Network</head><p>Two key difficulties that prevent the egocentric action recognition from higher accuracy are the deficiency in segmenting/identifying hand and interactional foreground object. Therefore, the key idea of this work is two cascaded end-to-end DCNNs which identify both hand regions  and foreground (active) object regions to facilitate egocentric action analysis. The cascaded DCNN infrastructure is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, where the resulting hand segmentation maps from the first DCNN (called pixel-to-pixel hand segmentation sub-network, i.e., CNN1 in <ref type="figure" target="#fig_0">Figure 1</ref>) are input to the second DCNN (called active/interactional object detection sub-network, i.e., CNN2 in <ref type="figure" target="#fig_0">Figure 1</ref>) to infer the interactional foreground region. Outputs of both sub-networks are further utilized to form the active object histogram and locally pooled motion histogram (on interactional foreground regions) for action representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pixel-level Hand Segmentation</head><p>Detecting and segmenting hand regions up to pixel-level precision is challenging <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref>. This is partly due to the fact that ground-truth images with pixel-level hand segmentations are rare. However, bounding box based hand annotations are easily obtainable, and rich in public available datasets such as GTEA <ref type="bibr" target="#b8">[9]</ref>. Inspired by these observations, we propose an expectation-maximization style algorithm to train a pixel-to-pixel hand segmentation network by fully utilizing the weakly supervised data (i.e., hand bounding boxes). This algorithm starts from a small set of fully annotated hand segmentation maps, and iteratively selects and adds good hypothesized hand maps (augmented from the weakly supervised hand data) to gradually refine the end-to-end hand segmentation network, in an expectationmaximization manner.</p><p>Network Architecture. To generate hand segmentation maps based on image input, we adopt the DeconvNet deep network <ref type="bibr" target="#b27">[28]</ref> for semantic parsing. This model improves the prior FCN model <ref type="bibr" target="#b23">[24]</ref> (i.e., using coarse bilinear interpolation as deconvolution procedure) with more sophisticated deconvolution and un-pooling layers. Specifically, we employ the VGG-16 net <ref type="bibr" target="#b40">[41]</ref> as our baseline convolutional network, and we initialize this network with 1500 fully annotated hand images (pixel-level annotations) from the GTEA dataset <ref type="bibr" target="#b8">[9]</ref>. The network parameter set is denoted by θ. The network is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. This network contains two symmetric parts of the VGG-16 net, i.e., local-to-global convolutional network and the mirroring global-to-local deconvolutional network. More details of the network can be seen in <ref type="bibr" target="#b27">[28]</ref>. Our algorithm to learn the above network is as follows: firstly, we use the fully annotated hand image samples to initialize the DCNN, we fine-tune the weights in the convolutional layers using VGG-16 model pre-trained on ILSVRC dataset, while the weights in the de-convolutional network are initialized with zero-mean Gaussians. Then we employ an expectation-maximization style training procedure, to alternate the following two steps: 1) with the network parameters θ fixed, we seek the best hand map proposals for the weakly supervised data; and 2) we augment the fully annotated dataset with the newly identified hand maps from the weakly supervised data, and refine the model parameter set θ. Each step is detailed as follows. Our iterative network training procedure is also elaborated in Algorithm 1.</p><p>E-Step: Seek the Best Hand Map Proposal. With the network parameters θ fixed, we generate a set of hand map/mask hypotheses from each weakly supervised hand image (i.e., with bounding box), then search the best hypothesis/proposal for next iteration of hand segmentation network training. In particular, we firstly apply super-pixel segmentation using SLIC <ref type="bibr" target="#b0">[1]</ref> onto each bounding box image I n . We then represent each super-pixel with a concatenated feature vector consisting of HSV color histogram and Gabor filter texture histogram. A linear support vector machine classifier is trained and applied to calculate the hand detection scores for each super-pixel. We build our training object patch (super-pixel) dataset by randomly annotating 50000 hand super-pixels and 80000 background superpixels. After that, we generate different versions of hand map proposals by applying the thresholded Grabcut <ref type="bibr" target="#b34">[35]</ref> algorithm. Namely, we apply the Grabcut onto the hand detection score maps with different parameter settings w.r.t. to sure foreground, sure background, possible foreground, and possible background. A set of N s (N s = 24) proposals (denoted as a proposal set P n , |P n | = N s ) are generated for each weakly supervised image I n .</p><p>To seek the best hand segmentation hypothesis s n for each weakly supervised hand bounding box image I n , we describe the measurement criteria (indication of a good hypothesis) as:</p><formula xml:id="formula_0">ε s (θ, s n ) = 1 N N n=1</formula><p>(1 − κ (h(I n |θ), l (I n |s n ))), s n ∈ P n ,</p><p>(1) where h(I n |θ) is the pixel-level prediction for image I n from the hand segmentation DCNN h with parameters θ, l (I n |s n ) is the pixel-level hand segmentation map from the hand map proposal s n , s n ∈ P n . To compute the overlap ratio between h(I n |θ) and l (I n |s n ), we define κ (h(I n |θ), l(I n |s n )) as:</p><p>κ (h(I n |θ), l(I n |s n )) = h(I n |θ) l(I n |s n ) h(I n |θ) (I n |s n ) ,</p><p>which is the intersection over union ratio of the predicted and ground-truth hand regions. For the predicted map, we threshold it into a binary map by the resulting hand probability scores, the threshold value is 0.5. Equation 1 is normalized by the number of images N . Intuitively, we might pick up the best proposal s n for each hand bounding box image I n , i.e., to achieve the largest κ (h(I n |θ), l(I n |s n )). However, this greedy solution may be trapped to a bad local optima, by always picking up the same candidate from the hand map proposal set P n . To overcome this issue, we randomly select one of the best K M-Step: Refine the Hand Segmentation Network. With the augmented training set, i.e., the weakly supervised hand bounding boxes and the inferred best hand map hypotheses, we fine-tune the hand segmentation network h(I n |θ). We formulate the objective of the network training as a per-pixel regression problem to the selected hand map proposals {s n } from E-step. More formally, the objective function is written as: ε θ (θ, s n ) = e (h(I n |θ), l(I n |s n )),</p><p>where e (h(I n |θ), l(I n |s n )) is the cross-entropy error function.</p><p>The ultimate objective function is concluded as the combination of Equation 1 and Equation 3:</p><formula xml:id="formula_3">min θ,{sn} N n=1 ε s (θ, s n ) + λ N n=1 ε θ (θ, s n ) ,<label>(4)</label></formula><p>where λ is the weight parameter, which we fix it as 3 by cross validation. To minimize the objective function, the network parameter θ and the best hand map hypotheses {s n } are alternatively optimized. The procedure of the hand segmentation network training can be solved by the back propagation and stochastic gradient descent (SGD). In the SGD training, we use a mini-batch size of 8. The learning rate and momentum are initialized to be 0.001 and 0.9, learning rate is divided by 10 after every 4K iterations, the weight decay is set as 0.0005. The training procedure lasts for 12K SGD iterations. This is the training parameters for one epoch of our iterative training. We keep the training for N max (N max = 10) epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Foreground/Active Object Region Localization</head><p>On one hand, we note that precise detection of hands could help localize the foreground manipulated objects in some actions. For example, in the actions such as "operating the TV monitor", "pouring water with bottle", "turning on the tap", "drink water/bottle", etc., hands are always very close to the objects-of-interest. Therefore, to find the hands directly secures the object locations. On the other hand, we also note that sometimes motion is also a good indication of the location of the foreground object, i.e., foreground object movements are different from background motions. For example, the movements of washing hands are significantly more consistent and stronger than the background motions. We note that this is the first time that hand map, motion map and object map are combined together to generate foreground interactional object regions, in an end-to-end network. The advantage of the network is that rich mutual contextual information could be explored.  Network Architecture. Inspired by these observations, we employ an end-to-end DCNN to take the corresponding hand map with, object feature map, and motion map to detect the active object region, i.e., interactional foreground. Firstly, the hand maps (i.e., binary masks) are generated from the above introduced pixel-level hand detection/segmentation network. Secondly, we calculate the optical flow motion map of the original image, from two sequential video frames. Thirdly, we input the training object bounding box images to the AlexNet <ref type="bibr" target="#b17">[18]</ref> image classification deep network. For each image, we extract the top-5 strongest object feature maps (the ones with strongest responses) of the fifth convolutional layer (Conv 5 ) as the object map. The three maps with the raw image are fed into a bounding box regression network based on the VGG-16 net, as illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. The deep network mainly contains 16 weights layers, 13 convolutional layers, 3 fully connected layers, softmax layer and bounding box regression layer. The bounding box regressor is used to predict the object location and the softmax classifier outputs the object class probability. This regression network prototype can be found in the work <ref type="bibr" target="#b9">[10]</ref>. For each image, the outputs from the network are the tuples of (x, y, w, h, p), where (x, y, w, h) is the object location for the detected bounding box, and p is the corresponding object class probability.</p><p>Network Training. The two objectives of network training are to predict object bounding boxes and their confidence scores for each training image, such that the highest scoring boxes well match the ground-truth bounding boxes for the image, both objectives can be jointly modeled as in <ref type="bibr" target="#b9">[10]</ref>. The ground truth for passive and active bounding boxes are provided by the dataset ADL <ref type="bibr" target="#b30">[31]</ref>. To train the network, we firstly fine-tune all the convolutional layer weights by the pre-trained VGG-16 model on ImageNet. We initialize learning rate as 0.001 and run SGD for 30k mini-batch iterations, then lower the learning rate to 0.0001 and train for another 10k iterations. A momentum term with weight 0.9 and weight decay factor of 0.0005 are used in the experiments. We regard the trained model as the active object model. In our work, we also train the passive object model by feeding the bounding box regression network <ref type="bibr" target="#b9">[10]</ref> with only passive object bounding boxes, without using the active object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Action Representation</head><p>We include both key object information and motion information in our action representation framework. On one hand, we follow <ref type="bibr" target="#b30">[31]</ref> to compute the object features from the detected object locations and confidence scores, then we represent the object features in a temporal pyramid manner <ref type="bibr" target="#b30">[31]</ref>, and obtain a global object representation for each action. In section 3.2, we train both active object model (i.e., to detect manipulated foreground objects) and passive object model (i.e., without considering active objects), thus each action is formed into the active object histogram and passive object histogram. On the other hand, we also propose to utilize state-of-the-art improved dense trajectories <ref type="bibr" target="#b43">[44]</ref> to represent motion characteristics in egocentric actions. For each trajectory, we extract the motion features including HOG (96-dim), and MBH (96-dim for MBHx, 96-dim for MBHy), they are reduced to 64-dim by PCA. We train Gaussian mixture models with 64 components, and encode each action with the improved Fisher vectors <ref type="bibr" target="#b29">[30]</ref> (8192-dim, 64 × 64 × 2), we regard the trained model as global motion pooling. To apply local motion pooling, we perform the feature encoding and pooling from the trajectories that are going through the hand regions or the interactional foreground objects, note that we regard the top-scored bounding box as the active object for each image.</p><p>To this end, we obtain both object model (passive+active object model) and motion model (global+local motion pooling). To combine the representations from each feature channel, we adopt a multi-channel approach <ref type="bibr" target="#b42">[43]</ref>. Based on the combined feature channel mapping, we train a nonlinear SVM classifier. We fix the regularization parameter C=10 by cross validation. We use the LibSVM <ref type="bibr" target="#b2">[3]</ref> as our SVM solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluate our hand segmentation method on the Georgia Tech Egocentric Activity (GTEA) dataset <ref type="bibr" target="#b8">[9]</ref>, and we evaluate the action recognition framework on the egocentric video benchmark Activities of Daily Living (ADL) <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hand Segmentation Performance</head><p>Dataset. The GTEA dataset <ref type="bibr" target="#b8">[9]</ref> contains 7 types of daily activities, each is performed by 4 different subjects. The tested frames are taken from the actions of subject one, who is making tea, making a peanut butter sandwich or making coffee. Follow the same settings in <ref type="bibr" target="#b8">[9]</ref>, we use the coffee sequence as training when testing on the tea and peanut sequence, and we use the tea sequence as training when testing on the coffee sequence. We use the F-score (i.e., harmonic mean of the precision and recall) to quantitatively evaluate the segmentation performance. The scores are computed by comparing the predictions (i.e., segmentation) to the ground truth from the project site.  <ref type="bibr" target="#b12">[13]</ref> 0.730 0.837 0.804 Superpixel + CRF <ref type="bibr" target="#b8">[9]</ref> 0.727 0.713 0.812 Global scene <ref type="bibr" target="#b19">[20]</ref> 0.883 0.933 0.943 Proposed 0.912 0.954 0.962 <ref type="table" target="#tab_1">Table 1</ref> shows the comparative hand segmentation results on the GTEA dataset. Firstly, the super-pixel [9] + CRF and Single pixel color <ref type="bibr" target="#b12">[13]</ref> methods are based on the effective low-level features in the hand segmentation task. Secondly, the Global scene method <ref type="bibr" target="#b19">[20]</ref> integrates different low-level features (e.g., color feature or texture feature), and models the global background scenes (i.e., to mitigate the large illumination changes), to achieve better performance than the previous methods. However, the performance is sensitive to the number of scene categories, and it is extremely difficult to model the scenes for the large-scale dataset in real life. Instead, our method can directly learn rich features from the neural network by feeding the network with various training data that are under different scenes. We observe that our performance outperforms all the above methods.</p><p>In <ref type="figure" target="#fig_5">Figure 4</ref>, we show that both of the Global scene method <ref type="bibr" target="#b19">[20]</ref> and our proposed method can perform well under the controlled environment. However, it is indeed difficult to model all the background scenes, we show such difficulties for Global scene method in row one of <ref type="figure" target="#fig_6">Figure 5</ref>,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Acton Recognition Performance</head><p>Dataset. The ADL <ref type="bibr" target="#b30">[31]</ref> dataset consists of 20 egocentric videos which are collected by 20 persons. Both action annotations (i.e., start time, end time, and action label for each video sequence) and object annotations (i.e., object class, object bounding box, passive/active status) are provided. A total of 18 action categories and 44 objects are annotated. During object detection, the actions of the first 6 videos are used as training and the rest are used for testing. To evaluate the action classification performance, we perform the leaveone-person-out cross validation, We report the per-class average precision (mAP) score, with equal weight for each action class.</p><p>First of all, we define the following methodology terms: 1) Passive object trains the object detection DCNN by only using the passive object bounding box data, without active object detection; 2) Passive+Active object augments the passive object model by combining the passive object histogram with the active object histogram from the active object detection; 3) Global motion pooling uses the recent  <ref type="table" target="#tab_2">Table 2</ref>.</p><p>popular improved dense trajectories <ref type="bibr" target="#b43">[44]</ref>; 4) Global+Local motion pooling combines the Global motion pooling with Local motion pooling on the active foreground regions; 5) Object+Motion is the model to combine Passive+Active object and Global+Local motion pooling. In <ref type="table" target="#tab_2">Table 2</ref>, we compare our approach with the state-ofthe-art methods on ADL dataset. Firstly, we show that the motion-based Global motion pooling is very useful in egocentric action representation. Secondly, the object-centric methods, such as Bag-of-objects, Boost-RSTP Boost-RSTP + OCC, and Bag-of-objects+Active model, combine the passive and active object features with temporal pyramid, and can achieve comparative performance to the Global motion pooling method. However, none of the above methods can well solve the active object detection problem. We improve these methods by jointly modeling the object, hand and motion information from the deep neural network. We show that our Passive+Active object model has outperformed the previous object-centric methods. We also augment the Global motion pooling with Local motion pooling, by highlighting active foreground regions (objects-ofinterest) and suppressing the background noise. Finally, we combine the complementary object model and motion model (Object + Motion In <ref type="table" target="#tab_2">Table 2</ref>), to achieve the aggregated performance, we also show its confusion matrix in <ref type="figure" target="#fig_8">Figure 7</ref>. Particularly, we find our method can have good accuracy for those manipulation actions such as watching tv (operating tv remote), using computer (typing keyboard), washing hands (manipulating hands, tap), etc. In <ref type="table" target="#tab_3">Table 3</ref>, we show the correlation between hand segmentation accuracy and action recognition performance. Firstly, we show the largely degenerated performance by replacing the hand segmentation maps with hand bounding boxes (first method in <ref type="table" target="#tab_3">Table 3</ref>). Secondly, the other hand segmentation methods such as Superpixel + CRF <ref type="bibr" target="#b8">[9]</ref> and Global scene <ref type="bibr" target="#b19">[20]</ref> intuitively result in worse action recognition performance because their hand segmentation accuracy cannot compare to ours. We observe that the Global scene method achieves even worse performance than the bounding box input, as we find it is extremely difficult to model all the scene categories by using the simple scene clustering algorithm. In contrast, our hand segmentation method can directly learn various background scenes from the deep neural network, and results in the best action recognition performance in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>In <ref type="figure" target="#fig_10">Figure 9</ref>, we show that the presence of hand, object and motion maps can influence training of the active object detection network CNN2, therefore affecting the ultimate action recognition performance. We use the Pas-sive+Active object method for performance evaluation. We show that all types of maps are necessary for performance improvement, specifically, the hand map and object map are most significant of all. We also expect that motion map cannot be compared to the other two maps because of the background noise.</p><p>We list some exemplars of active foreground regions from the active object detection neural network CNN2, in <ref type="figure" target="#fig_9">Figure 8</ref>. These detected objects are either manipulated by hands (e.g., keyboard, kettle, tv remote) or nearby (e.g., For each image, we visualize the active object as the bounding box to achieve the topest confidence score. monitor, laptop, detergent). They are easier for detection because finding the hands can co-localize the active objects.</p><p>All the experiments are conducted on a computing server with two Intel Xeon E5450 Quad Core processors (3.00GHz) and 32 GB memory, the computational platform is equipped with one Nvidia Tesla K40 GPU. The deep semantic parsing neural network CNN1 is based on the De-convNet package <ref type="bibr" target="#b27">[28]</ref>, total of 24,569 hand bounding box images are used for the training. The network training speed is 5 seconds per iteration, it takes approximately 14 hours for each epoch of our iterative EM-like training algorithm, and 6 days to finish the training procedure. The testing speed is 1.35 seconds/image. The active object detection neural network CNN2 is based on the Fast-rcnn <ref type="bibr" target="#b9">[10]</ref> package. We sample one video frame every second for object detection. The training time is 13 hours in total for 11,643 images (i.e., 6 out of 20 subjects are used for training). The prediction time is even faster, i.e., 0.2 second per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Firstly, we propose a novel pixel-to-pixel deep convolutional neural network to achieve decent hand segmentation performance. Secondly, the resulting hand maps are further paired with motion maps and object maps via another object detection DCNN, which explores the contexts among object, motion and hand to generate foreground interactional objects. Experiments show that our framework has achieved the state-of-the-art egocentric action recognition performance on the benchmark dataset ADL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The cascaded neural network to collaboratively infer the hand segmentation maps and manipulated foreground objects. It includes the hand segmentation network CNN1 and the active object detection network CNN2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>We use the end-to-end DeconvNet<ref type="bibr" target="#b27">[28]</ref> as our baseline semantic parsing network for hand segmentation. The network outputs the hand probability score map, the brightness indicates the likelihood of hand region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Hand bounding box images {I n } n=1:N , initialized network h(I n |θ) with the parameter set θ. Output: h(I n |θ) (1) Apply super-pixel based image segmentation on each hand bounding box image I n to generate hand probability map I ′ n . (2) Apply the thresholded Grabcut on each I ′ n to generate a set of hand segmentation proposals P n , each hand map hypothesis is denoted as l(I ′ n |s n ). (3) EM-training algorithm: i = 1, N max = 10 while i ≤ N max do E-step: fix θ, optimize s n in Equation 4. i.e., for each image, select candidate s n ∈ P n to achieve the largest κ, in Equation 1. M-step: fix the set {s n }, optimize θ in Equation 4. i.e., apply SGD training to update θ, in Equation 3. i = i + 1 end while (4) Infer each hand bounding box image I n with the network h(I n |θ) (K = 3) hand segmentation hypotheses, instead of always picking up the best one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The deep bounding box regression neural network for manipulated foreground object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Successful hand segmentation examples of Global scene method<ref type="bibr" target="#b19">[20]</ref> (row one) and our method (row two).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Hand segmentation comparison between Global scene method<ref type="bibr" target="#b19">[20]</ref> (row one) and our method (row two) under background scene changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Some failure examples of our method under extreme darkness or extreme brightness. and it is expected that these failures can lead to rather unstable performance. We show that our method is still effective compared to the Global scene method, in row two ofFigure 5. However, it is intuitive to expect rather bad performance under extreme darkness or extreme brightness. We show such failure cases of our proposed method inFigure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Confusion matrix resulted from the combined Object + Motion model in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>The active foreground regions detected from the active object detection neural network CNN2. The green bounding boxes consist of the hands and co-localized/detected manipulated objects, for different action categories under various scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Correlation between action recognition performance and presence of hand, object and motion map in training the active object detection network CNN2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Hand segmentation performance comparison on GTEA dataset.</figDesc><table>Method 
peanut coffee tea 
Trajectory projection [39] 0.255 0.275 0.239 
Single pixel color </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Action classification performance comparison on ADL dataset.</figDesc><table>Method 
mAP (%) 
BoW 
16.5 
Boost-RSTP [25] 
33.7 
Boost-RSTP + OCC [25] 
38.7 
Bag-of-objects [31] 
32.7 
Bag-of-objects + Active model [31] 
36.9 
Passive object 
35.2 
Passive+Active object 
43.8 
Global motion pooling 
36.7 
Global+Local motion pooling 
42.5 
Object + Motion 
55.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Correlation between different hand segmentation accuracy and action recognition performance (mAP).</figDesc><table>mAP (%) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This work was partially supported by National Natural Science Foundation of China (61502301 and 61429201). This work was also partially supported to Dr. Qi Tian by ARO grants W911NF-15-1-0290 and Faculty Research Gift Awards by NEC Laboratories of America and Blippar.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time tracking of multiple skin-colored objects with a possibly moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Lourakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="368" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive skin color model for hand segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Dawod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Alam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Applications and Industrial Electronics (ICCAIE), 2010 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="486" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social interactions: A first-person perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1226" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="314" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Fast r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling instrumental activities of daily living in egocentric vision as sequences of active objects and context for alzheimer disease research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Buso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bourmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Megret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM international workshop on Multimedia indexing and information retrieval for healthcare</title>
		<meeting>the 1st ACM international workshop on Multimedia indexing and information retrieval for healthcare</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical background subtraction for a mobile observer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical color models with application to skin detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="96" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of skin-color modeling and detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kakumanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makrogiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bourbakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1106" to="1122" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analysis of rotational robustness of hand detection with a viola-jones detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kölsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on</title>
		<meeting>the 17th International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="107" to="110" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust hand detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kölsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="614" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hand tracking with flocks of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kölsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1187</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model recommendation with virtual probes for egocentric hand detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2624" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pixel-level hand detection in egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to predict gaze in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3216" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1996" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object-centric spatiotemporal pyramids for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mccandless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC. Citeseer</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hand detection using multiple proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action and interaction recognition in first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="526" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04366</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Markerless and efficient 26-dof hand pose recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="744" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal segmentation of egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2537" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual tracking of high dof articulated structures: an application to human hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="35" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Egocentric recognition of handled objects: Benchmark and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">First-person activity recognition: What are they doing to me</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2730" to="2737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6505</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Do life-logging technologies support memory for the past?: an experimental study using sensecam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Sellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Background subtraction for freely moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1219" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Skin color-based video segmentation under time-varying illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="862" to="877" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Model-based 3d tracking of an articulated hand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>II-310. IEEE</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interaction part mining: A mid-level approach for fine-grained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3323" to="3331" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
