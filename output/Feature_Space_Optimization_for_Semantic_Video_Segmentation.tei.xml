<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Space Optimization for Semantic Video Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intel Labs Vladlen Koltun Intel Labs</orgName>
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intel Labs Vladlen Koltun Intel Labs</orgName>
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Space Optimization for Semantic Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Joint first authors</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Semantic video segmentation on the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>. Input frame on the left, semantic segmentation computed by our approach on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present an approach to long-range spatio-temporal regularization in semantic video segmentation. Temporal regularization in video is challenging because both the camera and the scene may be in motion. Thus Euclidean distance in the space-time volume is not a good proxy for correspondence. We optimize the mapping of pixels to a Euclidean feature space so as to minimize distances between corresponding points. Structured prediction is performed by a dense CRF that operates on the optimized features. Experimental results demonstrate that the presented approach increases the accuracy and temporal consistency of semantic video segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Structured prediction has become a standard means of achieving maximal accuracy in semantic segmentation. In structured prediction, all pixels are labeled jointly and labeling coherence is explicitly enforced. This alleviates the noise and inconsistency that can arise when pixels are classified independently. In particular, the fully-connected CRF <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> -also known as the dense CRF -often yields significant improvements in semantic segmentation accuracy. For example, after the recent breakthrough of Long et al. <ref type="bibr" target="#b17">[18]</ref>, who developed a new model for semantic image segmentation, an application of the dense CRF over the new model yielded substantial accuracy gains <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>The natural form of input for vision systems that operate in the physical world is video. For this reason, we consider semantic segmentation of video sequences, rather than individual images. In a typical video sequence, each frame depicts a different view of the scene. Thus structured prediction can be used not only for spatial regularization within individual frames but also for temporal consistency across frames. In this paper, we address the challenges brought up by such spatio-temporal regularization.</p><p>Long-range temporal regularization in video is complicated by the fact that both the camera and the scene may be in motion. In particular, camera motion can induce significant optical flow across the visual field. For example, when the camera rotates, a point in the scene can quickly translate across the image plane. For this reason, simply appending the time dimension to the feature space used for regularization can lead to incorrect associations and cause misprediction in the presence of significant camera and object motion. The underlying problem is that Euclidean distance in the space-time video volume is not a good proxy for correspondence.</p><p>Our solution is to optimize the feature space used by the dense CRF so that distances between features associated with corresponding points in the scene are minimized. The dense CRF operates on an embedding of the pixels into a Euclidean feature space <ref type="bibr" target="#b9">[10]</ref>. The Euclidean norm in this space is used to define a continuous measure of correspondence. All pairs of pixels are connected and all pairs of pixels are regularized. In our setting, the regularization is performed over a fully-connected graph over the video volume. The strength of the connection between a pair of pixels is a function of their distance in the feature space. Our approach optimizes the feature space embedding such that Euclidean distance in feature space is a more accurate measure of correspondence in the underlying scene.</p><p>Specifically, we establish temporal correspondences via optical flow and long-term tracks and optimize the feature space embedding to minimize distances between corresponding points, subject to second-order regularization constraints. We express the embedding objective as a linear least-squares problem and show that feature space optimization can be performed efficiently over high-resolution video volumes. The resulting embedding is used by a fullyconnected space-time CRF that performs direct long-range regularization across the video volume, while operating at full resolution and producing sharp pixel-level boundaries.</p><p>We evaluate the proposed semantic video segmentation approach through extensive experiments on the CamVid and Cityscapes datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. Experimental results demonstrate that feature space optimization increases the accuracy of semantic video segmentation. Our approach yields a 66.1% mean IoU on CamVid and a 70.3% mean IoU on the Cityscapes validation set. Both results are the highest reported to date. In addition, the presented approach substantially increases the temporal consistency of the labeling. This is evaluated quantitatively in our experiments and is also evident in the supplementary video. <ref type="figure">Figure 1</ref> shows results produced by the presented approach on two frames from the Cityscapes dataset. <ref type="figure">Figure 2</ref>. The temporal structure of the model. The video is covered by overlapping blocks. A dense CRF is defined over each block and feature space optimization is performed within blocks. Structured prediction is performed over multiple blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>Our model is a set of cliques that cover overlapping blocks in the video volume. We cover the video by overlapping temporal blocks, define a dense CRF over each block, and build in provisions for temporally smooth prediction across block boundaries. The temporal structure of the model is illustrated in <ref type="figure">Figure 2</ref>. The block construction is described in Section 5.</p><p>Each pixel in the video is identified by a vector p = (b, t, i) ∈ R 3 , where b is the block number, t is the frame number within block b, and i is the index of the pixel within the frame. The color of pixel p is denoted by I p ∈ R 3 and the coordinates of pixel p in its frame are denoted bys p ∈ R 2 . Let P be the set of pixels in the video.</p><p>Given pixel p, let X p be a random variable with the domain L = {l 1 , ..., l L }. The states l i will be referred to as labels. Let X be a random field over P and let x : P → L be a label assignment. The random field X is characterized by a Gibbs distribution P (x|P) and the corresponding Gibbs energy E(x|P) associated with each label assignment:</p><formula xml:id="formula_0">P (x|P) = 1 Z(P) exp − E(x|P) , E(x|P) = p ψ u p (x p ) + (p,q)∈E ψ p p,q (x p , x q ).<label>(1)</label></formula><p>Here Z(P) = x exp − E(x|P) is the partition function and E is a neighborhood structure defined on pairs of variables. The neighborhood structure is a union of cliques: each block is covered by a clique, each pixel is covered by two blocks, and each variable is correspondingly covered by two fully-connected subgraphs in the random field. Our goal is to find a label assignment x * that minimizes the Gibbs energy. The unary term ψ u p (x p ) specifies the cost of assigning label x p to pixel p. Pairwise terms ψ p p,q (x p , x q ) couple pairs of variables and penalize inconsistent labeling. These terms are defined using Gaussian kernels <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_1">ψ p p,q (x p , x q ) = µ(x p , x q ) M m=1 w m κ m (f p , f q ), (2)</formula><p>where µ(x p , x q ) is a label compatibility term and w m are the mixture weights. f p and f q are features associated with x p and x q , respectively. Each kernel has the following form:</p><formula xml:id="formula_2">κ m (f p , f q ) = exp − f p − f q 2 σ 2 m .<label>(3)</label></formula><p>Given point p, the feature f p ∈ R D is a vector in a D-dimensional feature space. In semantic image segmentation, the canonical feature space is five-dimensional and combines image position and color <ref type="bibr" target="#b9">[10]</ref>. A natural feature space for semantic video segmentation is six-dimensional and combines time, color, and position: f p = (t p , I p ,s p ). We will use this feature space as a starting point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Space Optimization</head><p>Feature-sensitive models of the kind described in Section 2 have been very successful in semantic image segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>. However, applying such models to spacetime video volumes is not straightforward. A key difficulty is that both the camera and the objects may be in motion and can carry corresponding pixels apart. Thus the natural six-dimensional feature space yields a distance measure that does not appropriately model spatio-temporal correspondence.</p><p>A hypothetical solution that would address this issue is to obtain a dense metric 3D reconstruction of the scene through time, associate each pixel with the true 3D position of the corresponding surface element in the environment, and use this 3D position along with time as a feature. This would enforce a coherence assumption on surfaces that are truly proximate in space-time. However, dense monocular 3D reconstruction of dynamic scenes is an open problem. We therefore develop an alternative approach that does not require understanding the three-dimensional layout of the scene.</p><p>Our approach involves optimizing a subspace of the feature space to reduce Euclidean distance between corresponding points while adhering to regularization terms that aim to preserve object shapes. Specifically, for all points {p}, we optimize position features {s p }. (The time and color dimensions are fixed.) Thus the feature mapping (t p , I p ,s p ) is replaced by (t p , I p , s p ).</p><p>Consider a block b that consists of T×N points, where T is the number of frames in the block and N is the number of pixels in each frame. The optimization objective is defined as follows:</p><formula xml:id="formula_3">s * = arg min s E(s), E(s) = E u (s) + γ 1 E s (s) + γ 2 E t (s).<label>(4)</label></formula><p>Here s are the position features for all pixels in the block and s * are the optimal features. The objective E(s) comprises a data term E u (s), a spatial regularizer E s (s), and a temporal regularizer E t (s). We now explain each of these three terms. We will use p and (b, t, i) interchangeably to denote a point in the block.</p><p>Data term E(s). The data term prevents the feature space embedding from drifting or collapsing under the strength of the regularization terms. The middle frame in the block is used as an anchor. Let a = ⌊T /2⌋ be the frame number of the anchor frame and let P a be the set of pixels in frame a. Let {s p : p ∈ P a } be the unoptimized natural feature space for P a . The data term ensures that points in the anchor frame do not drift far from their natural positions:</p><formula xml:id="formula_4">E u = p∈P a s p −s p 2 .<label>(5)</label></formula><p>Spatial regularization term E s (s). The spatial regularizer preserves shapes within color boundaries and detected contours. We use anisotropic second-order regularization over the 4-connected pixel grid <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>:</p><formula xml:id="formula_5">E s (s) = T t=1 N i=1 s (b,t,i) − j∈Ni w ij s (b,t,j) 2 .<label>(6)</label></formula><p>Here N i is the set of neighbors of point (b, t, i). The weight w ij attenuates the regularization at object boundaries:</p><formula xml:id="formula_6">w ij = exp − I (b,t,i) − I (b,t,j) 2 σ 1 exp − c 2 p σ 2 .<label>(7)</label></formula><p>The first factor in <ref type="formula" target="#formula_6">(7)</ref> is based on the color difference between the two pixels and the second factor is based on the contour strength at pixel p. We use structured forests to compute contour strength c p <ref type="bibr" target="#b6">[7]</ref>, such that c p ∈ [0, 1] and c p = 1 indicates the presence of a boundary.</p><p>Temporal regularization term E t (s). The temporal regularizer pulls corresponding points in different frames to assume similar positions in feature space:</p><formula xml:id="formula_7">E t (s) = (p,q)∈K s p − s q 2 .<label>(8)</label></formula><p>This is the term that minimizes distances between corresponding points. K is a collection of correspondence pairs (p, q), where p and q are in different frames. Correspondences are established via optical flow and long-term tracks, as described in Section 5.</p><p>Optimization. Objective (4) is a large-scale linear leastsquares problem with second-order regularization. We optimize the objective using the biconjugate gradient stabilized method <ref type="bibr" target="#b28">[29]</ref> with algebraic multigrid preconditioning <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inference</head><p>Efficient inference in the model specified by Equation <ref type="formula" target="#formula_0">(1)</ref> can be performed by an extension of the mean-field inference algorithm introduced by Krähenbühl and Koltun <ref type="bibr" target="#b9">[10]</ref>. Note that our model is a collection of overlapping cliques and is thus different from the fully-connected model considered by Krähenbühl and Koltun.</p><p>Define a distribution Q that approximates the true distribution P , where similarity between distributions is measured by the KL-divergence. Assume that Q factorizes over the individual variables: Q(x) = p Q p (x p ), where Q p is a distribution over the random variable X p . The mean-field updates have the following form:</p><formula xml:id="formula_8">Q p (l) = 1 Z p exp − ψ u p (l) − S 1 (l) − S 2 (l) , S 1 (l) = − l ′ ∈L q∈N 1 p Q q (l ′ )ψ p p,q (l, l ′ ), S 2 (l) = − l ′ ∈L q∈N 2 p Q q (l ′ )ψ p p,q (l, l ′ ), Z p = l exp − ψ u p (l) − S 1 (l) − S 2 (l) ,<label>(9)</label></formula><p>where N 1 p and N 2 p are sets of neighbors of p in the two blocks that cover p. The updates can be performed efficiently using Gaussian filtering in feature space <ref type="bibr" target="#b9">[10]</ref>. Given the Q distribution at the end of the final iteration, a labeling can be obtained by assigning x * p = arg max l Q p (l). We now consider what happens when the video volume is too large to fit in memory. We can partition the video into chunks of consecutive blocks, such that inference in each chunk is performed separately. To align the predicted distributions across blocks, we could use a distributed optimization strategy such as dual decomposition <ref type="bibr" target="#b8">[9]</ref>. However, the convergence of such schemes can be quite slow. We therefore opt for a simple heuristic that has the added benefit that chunks can be processed in a streaming fashion.</p><p>Consider two overlapping blocks b 1 and b 2 , such that b 1 is the last block in one chunk and b 2 is the first block in the next chunk. Let Q 1 and Q 2 be the distributions produced by mean-field inference for these blocks in their respective chunks. Let [t 1 , t 2 ] be the overlap region. Let Q t be the sought-after distribution for frame t ∈ [t 1 , t 2 ] and let Q 1,t and Q 2,t be the corresponding slices of Q 1 and Q 2 . We transition between chunks via simple linear interpolation:</p><formula xml:id="formula_9">Q t = Q 1,t + t − t 1 t 2 − t 1 Q 2,t .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation</head><p>We use two sets of unary potentials in our experiments. The first is the classical TextonBoost classifier of Shotton et al. <ref type="bibr" target="#b22">[23]</ref>, as implemented by Ladicky et al. <ref type="bibr" target="#b12">[13]</ref>. This classifier was used in a number of prior semantic video segmentation systems and enables a fair comparison to prior work. Second, we use a convolutional network based on the work of Yu and Koltun <ref type="bibr" target="#b29">[30]</ref>, which we refer to as the Dilation unary. This network consists of a front-end prediction module and a context aggregation module. The frontend module is an adaptation of the VGG-16 network based on dilated convolutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>. The context module uses dilated convolutions to systematically expand the receptive field and aggregate contextual information <ref type="bibr" target="#b29">[30]</ref>. In combination, the two modules form a high-performing convolutional network for dense prediction. In particular, the Dilation network yielded the highest semantic segmentation accuracy among all models evaluated by Cordts et al. <ref type="bibr" target="#b5">[6]</ref>, without using structured prediction.</p><p>In all experiments, we use optical flow computed by LDOF <ref type="bibr" target="#b2">[3]</ref>. To evaluate the influence of the input flow, we also conduct a controlled experiment with Discrete Flow <ref type="bibr" target="#b18">[19]</ref>. Long-term tracks are computed using the approach of <ref type="bibr">Sundaram et al. [26]</ref>. CRF parameters are optimized using grid search on a subset of the validation set.</p><p>The decomposition into blocks can be performed using a fixed block size, such as 100 frames. Our implementation uses a different approach that adapts block boundaries to the content of the video. Specifically, we consider long-term tracks [26] and spawn a new block when more than half of the tracks in the frame were not present at the beginning of the block. This increases the internal coherence of each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluate the presented approach on two datasets for road scene understanding: the CamVid dataset <ref type="bibr" target="#b1">[2]</ref> and the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>. Both datasets provide video input along with pixel-level semantic annotations of selected frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">CamVid dataset</head><p>We begin by performing experiments on the CamVid dataset. We use the split of Sturgess et al. <ref type="bibr" target="#b24">[25]</ref>, which has been adopted in a number of prior works. This split partitions the dataset into 367 training images, 100 validation images, and 233 test images. 11 semantic classes are used.</p><p>The primary accuracy measure we use is mean IoU (intersection over union). The IoU score for a particular class is defined as TP TP+FP+FN , where TP, FP, and FN is the number of true positives, false positives, and false negatives for this class, respectively <ref type="bibr" target="#b7">[8]</ref>.</p><p>We have also evaluated global pixel accuracy, defined as the total fraction of correctly classified pixels. We have ascertained that our approach outperforms the prior work in terms of pixel accuracy. However, this measure is severely biased in favor of large classes, such as "sky" and "road", and discounts small but important classes such as "pole" or "sign". We therefore do not report it and discourage other researchers from using it.</p><p>In addition to accuracy evaluation on frames that have ground-truth label maps, we have also evaluated the temporal consistency of the labeling produced by each technique. To this end, we have defined a consistency measure in terms of long-term tracks <ref type="bibr">[26]</ref>. A track is said to be consistently labeled if all pixels along the track are assigned the same label. The consistency of a labeling is defined to be the fraction of tracks that are consistently labeled. Note that perfect consistency can be achieved trivially at the expense of accuracy: all pixels in all frames in the video can simply be assigned the same label. However, a combination of high accuracy and high consistency is not easy to achieve and we have found that high consistency does correspond to qualitative stability.</p><p>Ablation study. We first perform a controlled study to isolate the effect of feature space optimization on labeling accuracy. The results of this experiment are provided in <ref type="table">Table 1</ref>. We use the TextonBoost unary <ref type="bibr" target="#b22">[23]</ref>. Applying a dense 2D CRF within each frame independently improves both mean IoU and consistency. Applying a dense 3D CRF over the video volume improves both metrics further. Performing feature space optimization as proposed in this paper improves both metrics further still. Comparison to prior work. We now compare the presented approach against state-of-the-art methods for semantic video segmentation. The first set of baseline methods -SuperParsing <ref type="bibr" target="#b26">[27]</ref>   <ref type="bibr" target="#b19">[20]</ref> is provided separately in supplementary material, since Miksik et al. only provided the results of their approach on a subset of the CamVid test set.) Using the classical TextonBoost unary, our approach achieves an accuracy gain of 8 percentage points over the recent method of Liu and He <ref type="bibr" target="#b15">[16]</ref> and an improvement of 2 percentage points over Tripathi et al. <ref type="bibr" target="#b27">[28]</ref>.</p><p>The Dilation network outperforms SegNet by 19 percentage points. Feature space optimization and structured prediction yield a further accuracy gain and a 9 percentage point boost in consistency over the Dilation unary. To assess the sensitivity of feature space optimization to the input optical flow, we provide the results of feature space optimization when the input flow fields are computed by LDOF <ref type="bibr" target="#b2">[3]</ref> and Discrete Flow <ref type="bibr" target="#b18">[19]</ref>, respectively. As shown in <ref type="table">Table 2</ref>, the performance of the approach is virtually identical in the two conditions.</p><p>Qualitative results are provided in <ref type="figure">Figure 3</ref> and in the supplementary video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Cityscapes dataset</head><p>Cityscapes is a new dataset for scene understanding in urban environments <ref type="bibr" target="#b5">[6]</ref>. The dataset contains 2975 training images, 500 validation images, and 1525 test images. 19 semantic classes are used. We report results on the validation set. Results on the test set will be provided in supplementary material.</p><p>The results are reported in <ref type="table">Table 3</ref>. We compare to the recent Adelaide model, a comprehensive system that integrates convolutional networks and conditional random fields <ref type="bibr" target="#b14">[15]</ref>. The Dilation network yields slightly higher accuracy than the Adelaide model. Using the Dilation unary, our approach yields a further gain in accuracy and an improvement of more than 6 percentage points in consistency.</p><p>mean IoU Consistency Adelaide <ref type="bibr" target="#b14">[15]</ref> 68.6 -Dilation unary <ref type="bibr" target="#b29">[30]</ref> 68.65 88.14 Dilation + Our approach 70.30 94.71 <ref type="table">Table 3</ref>. Quantitative results on the Cityscapes validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed feature space optimization for spatiotemporal regularization. The key observation is that naive regularization over the video volume does not take camera and object motion into account. To support efficient longrange temporal regularization, we optimize the positions of points in the space so that distances between corresponding points are minimized. Applying a dense random field over this optimized feature space yields state-of-the-art semantic video segmentation accuracy. The presented approach can directly benefit from more accurate optical flow and more stable and temporally extended point trajectories. We encourage further development of these basic building blocks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. More broadly, the presented feature space optimization formulation has significant limitations and more flexible approaches should be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 2. Quantitative results on the CamVid dataset<ref type="bibr" target="#b1">[2]</ref>. This table reports per-class IoU, mean IoU, and temporal consistency. Top: comparison to prior work that did not use convolutional networks. Using the classical TextonBoost classifier<ref type="bibr" target="#b22">[23]</ref>, our approach outperforms the prior work. Bottom: comparison to prior work that used convolutional networks and evaluated on the CamVid dataset. Using the Dilation network<ref type="bibr" target="#b29">[30]</ref>, our approach (FSO) yields the highest accuracy reported on the CamVid dataset to date. The performance of the presented approach is virtually identical when two different optical flow algorithms -LDOF and Discrete Flow -are used to compute the input flow fields.</figDesc><table>Building 

Tree 
Sky 
Car 
Sign 
Road 
Pedestrian 
Fence 
Pole 
Sidewalk 
Bicyclist 
mean IoU 
Consistency 

Without ConvNet 
ALE [13] 
73.4 70.2 91.1 64.24 24.4 91.1 29.1 
31 
13.6 72.4 28.6 
53.59 72.2 
SuperParsing [27] 
70.4 54.8 83.5 
43.3 
25.4 83.4 11.6 18.3 
5.2 
57.4 
8.9 
42.03 88.8 
Tripathi et al. [28] 
74.2 67.9 
91 
66.5 
23.6 90.7 26.2 28.5 16.3 71.9 28.2 
53.18 76.8 
Liu and He [16] 
66.8 66.6 90.1 
62.9 
21.4 85.8 
28 
17.8 
8.3 
63.5 
8.5 
47.2 
77.6 
TextonBoost + FSO 
74.4 71.8 91.6 
64.9 
27.7 91.0 33.8 34.1 16.8 73.9 27.6 
55.2 
87.3 
With ConvNet 
SegNet [1] 
68.7 
52 
87 
58.5 
13.4 86.2 25.3 17.9 16.0 60.5 24.8 
46.4 
62.5 
Dilation [30] 
82.6 76.2 89.9 
84.0 
46.9 92.2 56.3 35.8 23.4 75.3 55.5 
65.29 79.0 
Dilation + FSO -LDOF 
84.0 77.2 91.3 
85.7 
49.8 92.6 59.3 37.6 16.9 76.2 56.8 
66.11 88.3 
Dilation + FSO -DiscreteFlow 84.0 77.2 91.3 
85.6 
49.9 92.5 59.1 37.6 16.9 76.0 57.2 
66.12 88.3 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Seg-Net: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>2015. 3</idno>
		<imprint>
			<publisher>PAMI</publisher>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">MRF energy minimization and beyond via dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tziritas</surname></persName>
		</author>
		<idno>2011. 4</idno>
		<imprint>
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient preconditioning of Laplacian matrices for computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Associative hierarchical CRFs for object class image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2009. 4, 6 results on the CamVid dataset</title>
		<imprint/>
	</monogr>
	<note>From top to bottom: input frame. Miksik et al. [20], Tripathi et al. [28], Liu and He [16], SegNet [1], semantic segmentation produced by the presented approach, and ground truth</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Dan Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiclass semantic video segmentation with object-level active inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-class semantic video segmentation with exemplar-based object reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From top to bottom: input frame, output from the Dilation unary [30], semantic segmentation produced by the presented approach, and ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, results on the Cityscapes dataset</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Fully convolutional networks for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discrete optimization for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient temporal consistency for streaming video scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards longer long-range motion trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Algebraic multigrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ruge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stüben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multigrid Methods. SIAM</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">TextonBoost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining appearance and structure from motion features for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dense point trajectories by GPU-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Superparsing -scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic video segmentation: Exploring inference efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISOCC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vorst</surname></persName>
		</author>
		<idno>1992. 4</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific and Statistical Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
