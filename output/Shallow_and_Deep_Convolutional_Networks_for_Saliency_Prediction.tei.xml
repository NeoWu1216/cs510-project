<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shallow and Deep Convolutional Networks for Saliency Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insight Center for Data Analytics</orgName>
								<orgName type="laboratory">Image Processing Group Universitat Politecnica de Catalunya</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Barcelona, Dublin</settlement>
									<region>Catalonia</region>
									<country>Spain, Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Sayrol</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insight Center for Data Analytics</orgName>
								<orgName type="laboratory">Image Processing Group Universitat Politecnica de Catalunya</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Barcelona, Dublin</settlement>
									<region>Catalonia</region>
									<country>Spain, Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I-Nieto</surname></persName>
							<email>xavier.giro@upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Insight Center for Data Analytics</orgName>
								<orgName type="laboratory">Image Processing Group Universitat Politecnica de Catalunya</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Barcelona, Dublin</settlement>
									<region>Catalonia</region>
									<country>Spain, Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
							<email>kevin.mcguinness@insight-centre.org</email>
							<affiliation key="aff0">
								<orgName type="department">Insight Center for Data Analytics</orgName>
								<orgName type="laboratory">Image Processing Group Universitat Politecnica de Catalunya</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Barcelona, Dublin</settlement>
									<region>Catalonia</region>
									<country>Spain, Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insight Center for Data Analytics</orgName>
								<orgName type="laboratory">Image Processing Group Universitat Politecnica de Catalunya</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Barcelona, Dublin</settlement>
									<region>Catalonia</region>
									<country>Spain, Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shallow and Deep Convolutional Networks for Saliency Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The prediction of salient areas in images has been traditionally addressed with hand-crafted features based on neuroscience principles. This paper, however, addresses the problem with a completely data-driven approach by training a convolutional neural network (convnet). The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train end-to-end architectures that are both fast and accurate. Two designs are proposed: a shallow convnet trained from scratch, and a another deeper solution whose first three layers are adapted from another network trained for classification. To the authors' knowledge, these are the first end-to-end CNNs trained and tested for the purpose of saliency prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This work presents two approaches for end-to-end convolutional neural networks (convnets or CNNs) for saliency prediction. Our objective is to compute saliency maps that represent the probability of visual attention on an image, defined as the eye gaze fixation points. This problem has been traditionally addressed with hand-crafted features inspired by neurology studies. In our case we have adopted a completely data-driven approach, using a large amount of annotated data for saliency prediction. <ref type="figure" target="#fig_0">Figure 1</ref> provides an example of an image together with its ground truth saliency map and the two saliency maps predicted by the proposed convnets: a shallow one and a deep one.</p><p>Convnets are popular architectures in the field of deep learning and have been widely explored for visual pattern recognition, ranging from a global scale image classification <ref type="bibr" target="#b15">[16]</ref> to a more local object detection <ref type="bibr" target="#b6">[7]</ref> or semantic segmen- * Equal contribution. tation <ref type="bibr" target="#b20">[21]</ref>. The hierarchy of layers in convnets are inspired by biological models, and some works have pointed to a relation between the activity of certain areas in the brain and the hierarchy of layers in the convnets <ref type="bibr" target="#b3">[4]</ref>. Provided with enough training data, convnets have shown impressive results, often outperforming other hand-crafted methods. The rise of convnets originated with image classification <ref type="bibr" target="#b22">[23]</ref> in the context of increasing availability of annotated data. Large datasets like ImageNet <ref type="bibr" target="#b4">[5]</ref> or Places <ref type="bibr" target="#b32">[33]</ref> have provided enough visual examples to train the millions of parameters that most popular convnets contain. These datasets provide thousands of images for each discrete label typically associated to a semantic class.</p><p>The saliency prediction problem, however, poses two specific challenges that differentiate it from classic image classification. First, collecting large amount of training data is much more costly because it requires capturing the fixation points of human observers instead of a textual label for each image. Our work has benefited from recent publications of two large datasets containing images and an annotation of their salient points for humans <ref type="bibr" target="#b13">[14]</ref>. Collecting this level of of data has been possible thanks to crowdsourcing approaches, the same strategy used to annotate the ImageNet and Places datasets.</p><p>The second challenge to address when using convnets for saliency prediction is that a saliency score must be estimated for each pixel in the input image, instead of a global-scale label for the whole image. The saliency map at the output must present a spatial coherence and a smooth transition between neighbouring pixels.</p><p>The main contribution of this work is addressing the saliency prediction problem from an end-to-end perspective, by using convnets for regression rather than classification. We apply this strategy with two different architectures trained with two different approaches: a shallow convnet trained from scratch, and a deep convnet that reuses parameters from the bottom three layer of a network previously trained for classification. To the authors' knowledge, these were the first convnets that formulate saliency prediction as an end-to-end regression problem. This paper is structured as follows. Section 2 presents the previous and recent works using convolutional networks for saliency prediction and detection. Section 3 introduces the shallow convnet, while Section 4 presents the deep network. Section 5 compares both networks in terms of memory requirements. It also shows, prediction performance in the MIT Saliency Benchmark and LSUN Saliency Prediction Challenge 2015 and they are compared with other models. Conclusions and future directions are outlined in Section 6.</p><p>Our results can be reproduced with the source code and trained models available at https://github.com/ imatge-upc/saliency-2016-cvpr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The proposed networks presents the next natural step to two main trends in deep learning: using convolutional neural networks for saliency prediction and training these networks by formulating saliency prediction as an end-toend regression problem. This section reviews related work in these directions.</p><p>An early attempt of predicting saliency with a convnet was the ensembles of Deep Networks (eDN) <ref type="bibr" target="#b26">[27]</ref>, which proposed an optimal blend of feature maps from three different convnet layers, that were finally combined with a simple linear classifier trained with positive (salient) or negative (non-salient) local regions. This approach inspired DeepGaze <ref type="bibr" target="#b16">[17]</ref> to adopt a deeper network. In particular, DeepGaze used the existing AlexNet network <ref type="bibr" target="#b15">[16]</ref>, where the fully connected layers were removed to keep the feature maps from the convolutional layers. The response of each layer were fed into a linear model and its weights learned. DeepGaze would be the first case of transfer learning from a convnet for classification used for saliency, as we propose in our deeper architecture. However, we do not train a linear model to combine feature maps but directly train a stack of new convolutional layers on top of the transferred ones. Other recent works have explored the combination of different convnets working at different resolutions to capture both global and local saliency. Liu et al. <ref type="bibr" target="#b19">[20]</ref> proposed an architecture with three convnets working in parallel where the three final fully connected layers are combined in a single layer to obtain the saliency map. Unlike our work the network is trained with image regions centered on fixation and non-fixation eye locations.</p><p>On the other hand, DeepFix model (unpublished) captures information at different scales by using very deep networks, inspired by the VGG network architecture proposed by Simonyan and Zisserman <ref type="bibr" target="#b25">[26]</ref>.</p><p>Other approaches introduce new architectures and improvements in salient object detection. Zhao et al. <ref type="bibr" target="#b31">[32]</ref> use also two parallel networks to obtain local and global context modeling. The input image consists of a superpixel-centered window that is preprocessed differently to feed each of the two convnets. Fully connected layers are combined at the end to obtain the salient objects. The work by Li and Yu <ref type="bibr" target="#b17">[18]</ref> proposes three nested windows as inputs to three different convnet at different scales that are fused together to obtain an aggregated saliency map. Wang et al. proposed a different pipeline <ref type="bibr" target="#b27">[28]</ref>: local estimation is carried out and the resulting information is used as input to obtain a global search. That is, first, to detect local saliency, a deep neural network (DNN-L) learns local patch features to determine the saliency value of each pixel. Second, the local saliency map together with global contrast and geometric information are used as global features to obtain object candidate regions. A deep neural network (DNN-G) is then trained to predict the saliency score of each object region based on global features.</p><p>Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b20">[21]</ref> addressed the semantic segmentation task to predict the semantic label of every individual pixel in the image. This approach dramatically improved previous results on the challenging PASCAL VOC segmentation benchmark <ref type="bibr" target="#b5">[6]</ref>.</p><p>Finally, the SALICON model <ref type="bibr" target="#b10">[11]</ref> uses the same saliency evaluation metrics as loss function. The proposed architecture is very similar to our Deep Convnet, but in their work multiple scales are incorporated to consider selective attention at different resolutions.</p><p>In our work we are interested in finding saliency maps rather than salient object detection by training convnets endto-end. We also focus on novel databases that are annotated for the purpose of saliency prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Shallow Convnet</head><p>This section presents the first of our proposed convnets, which is based on a lightweight architecture whose parameters are trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>The network consists of five layers with learned weights: three convolutional layers and two fully connected layers. Each of the three convolutional layers is followed by a rectified linear unit non-lineraity (ReLU) and a max pooling layers. <ref type="figure" target="#fig_1">Figure 2</ref> shows a detailed description of each layer. The network has to a total of 64.4 million free parameters.</p><p>The network was designed considering the amount of available saliency maps for training it from scratch. Different strategies were considered to avoid overfitting the model. First, we used three convolutional layers rather than the five used in the classic AlexNet architecture <ref type="bibr" target="#b15">[16]</ref> (and far less than very deep networks used recently such as the thirteen used in VGG-16 <ref type="bibr" target="#b25">[26]</ref>). Second, the input images are resized to [96 × 96], a much smaller dimension that the [227 × 227] used in AlexNet <ref type="bibr" target="#b15">[16]</ref>. The three max pooling layers reduce the initial [96 × 96] feature maps down to <ref type="bibr">[10 × 10]</ref> by the last of the three poolings.</p><p>Even with the above constraints, the network still overfits significantly. We found that norm constraint regularization for the maxout layers <ref type="bibr" target="#b7">[8]</ref>, which computes the max between pairs of of the previous layers output, was essential to mitigate against this overfitting. We also tested using dropout after the first fully connected layer, with a dropout ratio of 0.5 (50% of probability to set a neuron's output value to zero), but this did not improve overfitting much, and so was not included in the final model.</p><p>Notice that the 2,304-dimensional vector at the output is mapped into a 2D array of [48 × 48], which corresponds to the saliency map. This decrease in resolution is compensated at test time by resizing the dimensions of the output to match the input image and posterior filtering using a Gaussian kernel with a standard deviation of 2.0.</p><p>This shallow convnet was implemented using Python, NumPy, and the deep learning library Theano <ref type="bibr" target="#b0">[1]</ref>. Processing was performed on an NVIDIA GTX 980 GPU with 2048 CUDA cores and 4GB of RAM. It took between 6 and 7 hours to train for the SALICON dataset, and 5 to 6 hours for the iSUN dataset. Saliency prediction requires 200 ms per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>This shallow network was trained from scratch twice, each time from a different dataset. A first model was built using the 10, 000 saliency maps from the SALICON dataset <ref type="bibr" target="#b13">[14]</ref>, and a second model using the 6, 000 saliency maps from the iSUN dataset. Both datasets are described in detail in Section 5.2. Given the smaller amount of images available in the iSUN dataset, a slight modification was introduced in this second model: the depth of the third convolutional network was of 64 instead of 128, as depicted in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The weights in all layers were initialized from a normal Gaussian distribution with zero mean and a standard deviation of 0.01, with biases initialized to 0.1. The network was trained with stochastic gradient descent (SGD) and the Nesterov momentum method, which we found helps convergence. The learning rate changed over time, starting with a higher learning rate 0.03 and decreased during training to 0.0001. We trained the network for 1,000 epochs. For validation purposes, we split the training data into 80% for training and the rest for periodic validation. A data augmentation technique was used by mirroring all images. All considered saliency maps were normalized to [0, 1].</p><p>We used regularized L2 (Euclidean) loss for this network as well the deep one presented in Section 4, with the standard L2 norm regularizer on the weights. We experimented with several other loss functions while developing our algorithm (including L1 loss and sigmoid cross entropy loss), but found that these often resulted in vanishing gradients, significantly slowing convergence. We considered designing a loss function that approximated one of the evaluation metrics directly. Unfortunately, many of these metrics are complex, and difficult to approximate with an easily differentiable function.</p><p>The filters learned in the first convolutional layer are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. They present a similar pattern to other similar filters learned for classification convnets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b29">30]</ref>, where edge detectors can be identified. It is noticeable how these type of filters arise also when training our network on saliency maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep Convnet</head><p>The second approach explored in this paper is the adaptation of an existing very deep convnet trained for image classification for the task of saliency prediction. Previous    <ref type="bibr" target="#b29">[30]</ref> has noted how, in image classification tasks, the model parameters from the lowest levels in the convnets converge in a few epochs. This observation, together with visualization of the filters learned at these layers <ref type="bibr" target="#b24">[25]</ref>, suggest that these layers perform low-level visual task in vision, such as the detection of colors or textures. Our hypothesis is that these lower layers trained for classification can also be transferred for the task of saliency prediction. We propose a second convnet which adapts these pre-trained filters and combines them with new layers specifically trained for saliency. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the layer architecture of the network, composed of 10 weight layers and a total of 25.8 million parameters. The architecture of the first 3 weight layers is compatible with that of the VGG network from <ref type="bibr" target="#b2">[3]</ref>. Each convolutional layer is followed by a rectified linear unit nonlinearity (ReLU). Pooling layers follow the first two convolutional layers, effectively reducing the width and height of the feature maps in the intermediate layers by a factor of four. A deconvolution layer follows the final convolution to produce a saliency map that matches the input width and height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture</head><p>To choose the final network architectures, we experimented with many different different variants, testing each on a held-out validation set of 1,000 images. In general we found that: 1) adding more layers improves accuracy; 2) adding more feature maps per layer usually improves accuracy too; and 3) using dropout regularization did not significantly improve accuracy but did increase training time. The final network design was primarily constrained in resolution, number of layers, and layer depth by the amount of available GPU memory.</p><p>We used transfer learning to initialize the weights for the first three convolutional layers with the pre-trained weights from the VGG CNN M network from <ref type="bibr" target="#b2">[3]</ref>. This acts as a regularizer and improves the final network result. The remaining weights were initialized randomly using the strategy from <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>We trained our network on 9,000 of the 10,000 training images in the SALICON dataset, setting aside 1,000 images for validation (ground truth for the SALICON validation set had not yet been released when this network was first trained). We used several standard pre-processing techniques on both the input images and the target saliency maps. We subtracted the mean pixel value of the training set from the image pixels to zero center them and rescaled the resulting values linearly to be in the interval [−1, 1]. We similarly preprocessed the saliency maps by subtracting the mean and scaling to [−1, 1]. Both the input images and the saliency maps were downsampled by half to 320 × 240 prior to training.</p><p>The network was trained using stochastic gradient descent with Euclidean loss using a batch size of 2 images for 24,000 iterations. During training, the network was validated against the validation set after every 100 iterations to monitor convergence and overfitting. We used the standard L 2 weight regularizer (weight decay), and halved the learning rate every 100 iterations. The network took approximately 15 hours to train on a NVIDIA GTX Titan GPU running the Caffe framework <ref type="bibr" target="#b12">[13]</ref>. We normalized the base learning rate by the number of predictions per image, to give a learning rate of 0.01/(320 × 240) ≈ 1.3 × 10 −7 . Using a larger learning rate causes the learning to diverge.</p><p>The network was trained on inputs of size 320 × 240, but in principle, it can handle images of any size, since it only consists of convolutional and pooling layers. In practice, the input size is constrained by the amount of GPU memory (or RAM) needed to store the outputs of the intermediary layers. Nevertheless, the network has the advantage that it can be sized to match the aspect ratio of any image, and indeed use this approach for the images in the MIT300 benchmark in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Memory requirements</head><p>The architectures of the two networks present different requirements in terms of memory resources. These resources are dedicated to two different tasks: the parameters that de- fine the network, and the blob data that characterizes network response at the different processing stages. The parameters that define the network are fit during training, and, together with the architecture layout, correspond to the actual characterization of the network. These parameters characterize the output of each neuron in the net, which can be defined as f (w T x + b), where w describes the filter parameters in the convolutional layers, b corresponds to the biases and f is the non-linearity. Each neuron, therefore, has parameters w and b, which are fit during backpropagation.</p><p>The data associated to the input image is the second source of memory requirements. The input image is hierarchically process in the convnet, creating multiple intermediate feature maps (or data blobs) after each processing stage. <ref type="table">Table 1</ref> presents the complementary memory requirements for each of the two convnets. These values have been obtained from the architectures of the shallow and very deep networks described in <ref type="figure" target="#fig_1">Figures 2 and 4</ref>, respectively. The estimation assumes 32-bit floating points to store parameters and layer output (4 bytes per value). The memory estimate for blob data assumes test time (forward pass only): at train time this value is doubled to account for the error signal during backpropagation.</p><p>The number of parameters for both networks are much lower than the very deep networks used in classification. For example, the 19 layers version of VGG net requires 144 million parameters <ref type="bibr" target="#b25">[26]</ref>.</p><p>Our shallow network requires far less memory for the layer outputs, but has significantly more parameters (due to the fully connected layers). This explains why our deep network does not overfit, whereas stronger regularization is necessary to fit the shallow one. Since the shallow network needs less memory for the layer outputs, it is possible to make batch size on this network very large at test time, allowing it to process many more images at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets</head><p>We used three datasets, presented in <ref type="table">Table 2</ref>: SALICON <ref type="bibr" target="#b13">[14]</ref>: This is the largest dataset available for saliency prediction and was used to train our models. It was built from images of the Microsoft CoCo: Common Objects in Context <ref type="bibr" target="#b18">[19]</ref> dataset, which inspired the SALICON naming: SALIency in CONtext. However, the saliency maps in SALICON were not collected with eyetrackers as in most popular datasets for saliency prediction, but with mouse clicks captured in a crowdsourcing campaign. iSUN The iSUN dataset has been built with an online game using webcams to track player eye gaze. The dataset uses natural scene images from the SUN database <ref type="bibr" target="#b28">[29]</ref>, a large dataset organized in 397 scene categories. MIT1003 and MIT300 <ref type="bibr" target="#b14">[15]</ref> This dataset is the most well-known among saliency prediction researchers. It is accompanied by an online benchmark maintained by its authors. The MIT1003 dataset consists of both images and fixation points that can be used for training. The fixation points for the MIT300 dataset are not public: the dataset can only be used for benchmarking. The stimuli images in these datasets consist of indoor and outdoor natural scenes from the Flickr Creative Commons and LabelMe <ref type="bibr" target="#b23">[24]</ref> datasets.</p><p>Our two models were tested on a total of 7,300 images coming from three different datasets. The dataset sizes and diversity in terms of observers and nature (eye gaze and clicks) provide a higher statistical significance than previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>. Although larger in size, SALICON and iSUN datasets are more exposed to quality degradation because they were built via crowdsourcing. On the other hand, MIT1003 and MIT300 are considered cleaner because fixations points were captured in a controlled environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results</head><p>The evaluation of saliency prediction has received the attention of several researchers, resulting in various proposed approaches. Our experiments consider several of these, in a similar way to the MIT saliency benchmark <ref type="bibr" target="#b1">[2]</ref>. Some of these metrics compare the predicted saliency maps with the maps generated from the fixation points of the ground truth, while some other metrics directly compare with the fixation points. In the result tables that follow, we have sorted the different techniques based on the AUC Judd metric.</p><p>Where not otherwise stated, our convnets were trained with images from the SALICON <ref type="bibr" target="#b13">[14]</ref> dataset and tested on images from iSUN and MIT300 datasets to avoid overfitting. The one exception to this is our submission for the LSUN 2015 challenge, where our shallow network was trained with training and validation data from iSUN, and assessed on the test partition. <ref type="figure" target="#fig_5">Figure 5</ref> presents a qualitative comparison of the two networks, showing the predicted saliency maps alongside the ground truth fixation maps. These examples show a different behaviour between the two networks, with the shallow one presenting a bias towards the central part of the image. The deep network, on the other hand, offers a higher spatial resolution thanks to its architecture with larger feature maps.</p><p>Our shallow convnet was the winner of the 2015 LSUN saliency prediction challenge. This challenge required participants to evaluate their algorithms on the test partitions Dataset Description</p><p>Capture device Observers Train Validation Test SALICON <ref type="bibr" target="#b13">[14]</ref> Microsoft CoCo <ref type="bibr" target="#b18">[19]</ref> Mouse clicks Crowd 10,000 5,000 5,000 iSUN SUN <ref type="bibr" target="#b28">[29]</ref> Eyetracker Crowd 6,000 926 2,000 MIT300 <ref type="bibr" target="#b1">[2]</ref> Flickr and LabelMe <ref type="bibr" target="#b23">[24]</ref> Eyetracker 39 --300 <ref type="table">Table 2</ref>. Description of the three datasets used in our experiments. of the iSUN and the SALICON datasets. Our network was trained only with images from the training and validation partitions of each dataset separately, so images from different datasets were never mixed for these experiments. <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> include the results provided by the organizers of the challenge for the iSUN and SALICON datasets. The scores obtained for every measure considered demonstrate the superior performance of our shallow network compared with the other participants. The presented shallow and deep convnets are compared quantitatively in <ref type="table">Tables 5 and 6</ref>. The deep convnet usually outperforms shallow in all cases but on iSUN validation. We hypothesize this better results are because: 1) it retains the aspect ratio of the input image (there are no fully connected layers), and 2) that it produces a higher resolution output, which can often better match more complex patterns in the saliency map. <ref type="table">Table 6</ref> also compares our results with some other top performers in the MIT300 benchmark. Our deep convnet achieves similar results to the ones obtained by Deep Gaze 1 <ref type="bibr" target="#b16">[17]</ref> at the upper part of the table. The shallow convnet performs worse but still in the upper part of a table which, in its full version, compares 47 different models. SALICON <ref type="bibr" target="#b10">[11]</ref> and DeepFix obtain better scores than our deep convnet in MIT300, but their complexity is also higher. DeepFix requires 22 layers and SALICON uses 16 layers and multiple scales of the image, while our simpler models are defined with 10 (deep) or 5 (shallow) layers on a single scale.</p><p>Results also indicate a robustness of our models across datasets. A detailed analysis of <ref type="table">Table 6</ref>  performing results on the MIT benchmark when compared with other models trained on MIT1003 Deep Gaze 1 <ref type="bibr" target="#b16">[17]</ref>, eDN <ref type="bibr" target="#b26">[27]</ref>, and Judd <ref type="bibr" target="#b14">[15]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We propose a novel end-to-end approach for training convnets in the task of saliency prediction. The excellent results of both architectures in state-of-the-art benchmarks demonstrate the superior performance of our convnets with respect to hand-crafted solutions and highlight the importance of an end-to-end formulation of saliency prediction.</p><p>The comparison between our shallow and deep networks trained on SALICON data has provided similar results for the iSUN dataset, but a better result for the deep network on MIT300. On the other hand, the shallow network requires less memory at train time and generates saliency maps much faster because it has fewer layers. Both networks rank highly in the MIT300 benchmark despite not being trained on this dataset. This clearly demonstrates the generalization performance of the networks and robustness to dataset biases.  <ref type="table">Table 6</ref>. Results of the MIT300 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Input Image (top left) and saliency maps from the ground truth (top right), our shallow convnet (bottom left) and our deep convnet (bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of the shallow convolutional network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Filters learned for the first convolutional layer of the shallow convnet (best viewed from a distance).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of the deep convolutional network work</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Saliency maps generated by our shallow and deep network on the SALICON and iSUN validation data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>suggests a robust behaviour of our convnets across datasets. Our networks were trained purely on SALICON data but still obtained top Similarity CC AUC shuffled AUC Borji AUC JuddTable 3. Results for the iSUN test set, according to the LSUN Challenge 2015.Table 4. Results for the SALICON test set, according to the LSUN Challenge 2015.iSUN (validation) CC AUC Shuffled AUC BorjiTable 5. Comparison of our shallow and deep convnets.</figDesc><table>Shallow Convnet (iSUN) 
0.6833 
0.8230 
0.6650 
0.8463 
0.8693 
Xidian 
0.5713 
0.6167 
0.6484 
0.7949 
0.8207 
WHU IIP 
0.5593 
0.6263 
0.6307 
0.7960 
0.8197 
LCYLab 
0.5474 
0.5699 
0.6259 
0.7921 
0.8133 
Rare 2012 Improved [22] 
0.5199 
0.5199 
0.6283 
0.7582 
0.7846 

Baseline: BMS [31] 
0.5026 
0.3465 
0.5885 
0.6560 
0.6914 
Baseline: GBVS [9] 
0.4798 
0.5087 
0.6208 
0.7913 
0.8115 
Baseline: Itti [12] 
0.4251 
0.3728 
0.6024 
0.7262 
0.7489 

Similarity 
CC 
AUC shuffled AUC Borji AUC Judd 

Shallow Convnet 
0.5198 
0.5957 
0.6698 
0.8291 
0.8364 
WHU IIP 
0.4908 
0.4569 
0.6064 
0.7759 
0.7923 
Rare 2012 Improved [22] 
0.5017 
0.5108 
0.6644 
0.8047 
0.8148 
Xidian 
0.4617 
0.4811 
0.6809 
0.7990 
0.8051 

Baseline: BMS [31] 
0.4542 
0.4268 
0.6935 
0.7699 
0.7899 
Baseline: GBVS [9] 
0.4460 
0.4212 
0.6303 
0.7816 
0.7899 
Baseline: Itti [12] 
0.3777 
0.2046 
0.6101 
0.6603 
0.6669 

Shallow Convnet 
0.59 
0.64 
0.79 
Deep Convnet 
0.53 
0.63 
0.80 

SALICON (val.) 

Shallow Convnet 
0.58 
0.67 
0.83 
Deep Convnet 
0.61 
0.73 
0.86 

SALICON (test) 

Shallow Convnet 
0.60 
0.67 
0.83 
Deep Convnet 
0.62 
0.72 
0.86 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under grant number SFI/12/RC/2289, project Big-Graph TEC2013-43935-R, funded by the Spanish Ministerio de Economía y Competitividad and the European Regional Development Fund (ERDF), and SGR14 Consolidated Research Group sponsored by the Catalan Government (Generalitat de Catalunya) through its AGAUR office. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GeForce GTX Titan Z used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theano: a cpu and gpu math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Python for Scientific Computing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://saliency.mit.edu/" />
		<title level="m">Mit saliency benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mapping human visual representations in space and time by neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pantazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="376" to="376" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting eye fixations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rare2012: A multi-scale raritybased saliency detection with its comparative statistical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Duvinage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mibulumukini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dutoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="642" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Best of both worlds: human-machine collaboration for object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2121" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2798" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Saliency detection: A boolean map approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
