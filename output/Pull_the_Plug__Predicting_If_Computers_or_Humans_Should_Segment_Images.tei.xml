<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pull the Plug? Predicting If Computers or Humans Should Segment Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Gurari</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Dutt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jain</forename><forename type="middle">Margrit</forename><surname>Betke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
						</author>
						<title level="a" type="main">Pull the Plug? Predicting If Computers or Humans Should Segment Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Foreground object segmentation is a critical step for many image analysis tasks. While automated methods can produce high-quality results, their failures disappoint users in need of practical solutions. We propose a resource allocation framework for predicting how best to allocate a fixed budget of human annotation effort in order to collect higher quality segmentations for a given batch of images and automated methods. The framework is based on a proposed prediction module that estimates the quality of given algorithm-drawn segmentations. We demonstrate the value of the framework for two novel tasks related to "pulling the plug" on computer and human annotators. Specifically, we implement two systems that automatically decide, for a batch of images, when to replace 1) humans with computers to create coarse segmentations required to initialize segmentation tools and 2) computers with humans to create final, fine-grained segmentations. Experiments demonstrate the advantage of relying on a mix of human and computer efforts over relying on either resource alone for segmenting objects in three diverse datasets representing visible, phase contrast microscopy, and fluorescence microscopy images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A common question people ask when needing to annotate images is whether automated options are sufficient for their images or they should instead bring humans in the loop to create accurate annotations. We explore this question for the task of demarcating object regions, i.e., creating foreground object segmentations. Foreground object segmentation is important for many downstream tasks including collecting measurements (features), differentiating between types of objects (classification), and finding similar images in a database (image retrieval). Our goal is to intelligently distribute segmentation work between humans and computers when human effort is only available for K% of images.</p><p>Our work is partially inspired by the observation that fully-automated algorithms can produce high-quality foreground object segmentations when they are successful, yet their performance often is inconsistent on diverse datasets <ref type="bibr">Figure 1</ref>. Use a human-drawn or computer-drawn segmentation? We propose a task of automatically deciding when to "pull the plug" on human annotators and use computers instead to create the initial foreground segmentations (rows 1, 2) that segmentation tools refine. We also propose a task of automatically deciding when to "pull the plug" on computers (row 3) and use humans instead to create high quality segmentations.</p><p>( <ref type="figure">Figure 1</ref>). This is because algorithms embed assumptions about how to separate an object from the background that are relevant for specific object and background appearances, yet restrict their widespread applicability <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. Consequently, the knowledge of when segmentation algorithms will succeed is currently a highly-specialized skill often resigned to computer vision experts or applications specialists who spent years studying the algorithms. Moreover, many researchers agree that there is not a one-sizefits-all segmentation solution. Thus, lay persons needing consistently high quality segmentations currently face a brute force approach of reviewing all images with available algorithm-drawn segmentations to identify images that should be re-annotated by humans.</p><p>Our work is also inspired by the observation that widelyused segmentation tools that rely on initialization are often inefficient because of their exclusive reliance on human input <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. Specifically, humans create initial bounding boxes or coarse segmentations to localize the object of interest in every image. A motivation for leveraging human guidance per image is that a segmentation tool can only succeed when initializations are sufficiently close to the true object boundary <ref type="bibr" target="#b22">[23]</ref>. A weakness of relying on humans is that for numerous methods, including level set based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>, humans typically have to wait for minutes or more per image to validate whether the tool successfully converts their coarse input to high quality segmentations. Intuitively, one may expect that computers at times can create good enough segmentations to replace human initialization effort (e.g., <ref type="figure">Figure 1</ref>, rows 1 &amp; 2) and so minimize human effort both for initialization and validation of the results. Still, lay persons typically lack the expertise to decide which images to distribute to computers.</p><p>To the best of our knowledge, this work is the first to predict when to "pull the plug" on humans or computers for segmenting images. We address two novel tasks. First, we propose a system that intelligently allocates computer effort to replace human effort to create initial coarse object segmentations for refinement by segmentation tools. Second, we propose a system that automatically identifies images to have humans re-annotate from scratch by predicting which images the automated methods segmented poorly. Both systems are designed to empower users to consistently collect higher quality object segmentations with segmentation tools while using considerably less human involvement. More broadly, our systems could be exploited to efficiently create segmentations as input for downstream tasks (e.g., object recognition, tracking).</p><p>Interactive co-segmentation methods address the issue of relying on human input to initialize segmentation tools for every image in a batch <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>. However, unlike our approach, these methods require that all images in the batch show related content (e.g., dogs). Moreover, interactive cosegmentation involves continual back-and-forth with an annotator to incrementally refine the segmentation. Avoiding a continual back-and-forth is particularly important for segmentation tools such as level set methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref> that take on the order of minutes or more per image to compute a segmentation from the initialization. We instead recruit human input at most once per image and consider the more general problem of annotating unrelated, unknown objects in a batch.</p><p>Our aim to minimize human involvement while collecting accurate image annotations is shared by active learning <ref type="bibr" target="#b35">[36]</ref>. Specifically, active learners try to identify the most impactful, yet least expensive information necessary to train accurate prediction models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. For example, some methods iteratively supplement a training dataset with images predicted to require little human annotation time to label <ref type="bibr" target="#b36">[37]</ref>. Other methods actively solicit human feedback to identify features with stronger predictive power than those currently available <ref type="bibr" target="#b6">[7]</ref>. Unlike active learners, which leverage human input at training-time to improve the utility of a single algorithm, our method leverages human effort at test-time to recover from failures by different algorithms.</p><p>Our novel tasks rely on a module to estimate the quality of computer-generated segmentations. Related methods find top "object-like" region proposals for a given im-age <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref>. However, most of these methods are inadequate for ranking "object-like" proposals across a batch of images because they only return relative rankings of proposals per image <ref type="bibr" target="#b14">[15]</ref>. Another method proposes an absolute segmentation difficulty measure based on the image content alone <ref type="bibr" target="#b29">[30]</ref>. However, this method does not account for differences in segmentation tools and that they perform differently when applied to segment the same image.</p><p>Our prediction framework most closely aligns with methods that predict the error/quality of a given algorithmdrawn segmentation in absolute terms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. In particular, we also perform supervised learning to train a regression model. Unlike prior work, which was proposed independently in the medical <ref type="bibr" target="#b23">[24]</ref> and computer vision <ref type="bibr" target="#b9">[10]</ref> communities, we aim to develop a single prediction model that is applicable across domains. Consequently, we populate our training data with segmentations resulting from a variety of algorithms on images from three imaging modalities (visible, phase contrast microscopy, fluorescence microscopy). Our approach consistently predicts well, outperforming a widely-used method <ref type="bibr" target="#b9">[10]</ref>, on three diverse datasets.</p><p>More broadly, our work is a contribution to the emerging research field at the intersection of human computation and computer vision to build hybrid systems that outperform relying on humans or computers alone. For example, hybrid systems combine non-expert and algorithm strengths to perform the challenging fine-grained bird classification task typically performed by experts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref>. While our hybrid system design complements existing work by also demonstrating the advantages of combining human and computer efforts, our work differs by addressing the image segmentation task rather than the class labeling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Segmentations by Humans or Computers?</head><p>We first describe two prediction systems for creating different levels of segmentations detail (Section 2.1). Then, we describe the module used by both systems to predict the quality of algorithm-generated segmentations (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Batch Allocation of Humans &amp; Computers</head><p>We call our resource allocation framework PTP which reflects that the system, for each image in a batch, predicts whether to "Pull The Plug" on humans or computers. In other words, our framework involves predicting for each image whether the annotation should come from a human or computer. We implement two PTP systems to create coarse and fine-grained foreground object segmentations respectively. We examine the value of our systems with segmentation tools that require initialization. These tools are well-suited for studying both systems because they require coarse object segmentation input and aim to output high quality, fine-grained object segmentations. <ref type="figure">Figure 2</ref>. We propose a system to predict when to delegate the task of creating coarse segmentations to an algorithm or a human. The system decides based on a predicted similarity of each algorithm-generated segmentation (i.e., last eight segmentations per row) to the unobserved ground truth (i.e., first segmentation per row). Our system is designed for use across domains, to demarcate the foreground object in fluorescence microscopy (row 1), phase contrast microscopy (row 2), and everyday (row 3) images.</p><p>Like existing interactive segmentation methods, we assume the user is interested in a primary foreground object <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. That is, there is a primary object of interest that the user wishes to isolate from the background. Foreground object segmentation is therefore distinct from natural scene segmentation, where methods aim to segment all objects present in the image or delineate their boundaries or primary contours <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Coarse Segmentation: Computer or Human? Our first system automatically decides when to delegate the task of creating coarse segmentations refined by segmentation tools to computers in an effort to improve upon today's status quo of relying exclusively on human input <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>. The motivation of the system design is to remain agnostic to the particular segmentation tool. Since some segmentation tools require minutes or more to refine a single initialization, we limit our system to run a segmentation tool exactly once per image with one input. Consequently, in the interest of increasing the chance of computer success, our system deploys the best predicted algorithm from a larger list of eight options for each image.</p><p>This system involves six key steps to segment a given batch of images. First, eight algorithm-drawn foreground segmentations 1 are collected per image ( <ref type="figure">Figure 2</ref>). Our <ref type="bibr" target="#b0">1</ref> The system applies algorithms used in current literature for foreground segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref>: Otsu thresholding <ref type="bibr" target="#b33">[34]</ref>, adaptive thresholding, and Hough Transform with circles <ref type="bibr" target="#b3">[4]</ref> . The system applies Otsu thresholding and its complement. The system also applies adaptive thresholding using the local median from a window size of 45 pixels and its complement as well as a third variant using the local mean from a window size of 45 pixels. Finally, the system applies three variants of Hough Transforms using a circle radius of 3, 5, and 10. Our system then post-processes each binary mask by filling all holes and keeping only the largest object.</p><p>While other algorithms could easily be integrated into our system, we found our choices create similar quality for initial segmentations. Specifically, across the three datasets in our experiments, our choices yield an average quality (Jaccard index) of 0.59 using the best option per image com-motivation is to employ fully-automated algorithms applicable across the image modalities investigated in this paper (visible, phase contrast microscopy, fluorescence microscopy). Then, for each image, the quality of each candidate segmentation is predicted using our proposed prediction system discussed in Section 2.2. Third, the top-scoring segmentation per image is selected as the computer choice. Next, all images are sorted based on the selected computer choices, from highest to lowest predicted quality scores. Fifth, the system allocates the available human budget to create coarse segmentations for the allotted number of images with the lowest predicted quality scores. Finally, all coarse segmentations created by humans and computers are fed to the segmentation tool of interest for refinement.</p><p>Fine-Grained Segmentation: Computer or Human? A related yet more challenging task is predicting whether a computer-generated segmentation captures the fine-grained details describing a true object region or whether humans should instead segment images from scratch. Whereas the previous system elicits coarse human input to initialize a segmentation tool, we now propose a system that elicits fine-grained human input to replace segmentation tools when they segment images poorly. The motivation of the system design is to offer a better solution than today's status quo of humans reviewing all images with associated segmentations to spot algorithm failures.</p><p>This system consists of five key steps to segment a given batch of images. First, a coarse segmentation is automatically generated for every image. Then, each coarse segmentation is refined by a segmentation tool. Next the prediction framework is applied to all resulting segmentations from the segmentation tool to estimate the quality of each pared to 0.57 using MCG's best option from 8 top-ranked candidates <ref type="bibr" target="#b2">[3]</ref>, 0.59 using CPMC's best option from 8 top-ranked candidates <ref type="bibr" target="#b9">[10]</ref>, and 0.17 using <ref type="bibr" target="#b30">[31]</ref>. result. Then, the system sorts all images from highest to lowest predicted quality scores for the resulting segmentations. Finally, the system allocates the available human budget to create fine-grained segmentations for the allotted number of images with the lowest predicted quality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Predicting Segmentation Quality</head><p>Embedded in both the Coarse and Fine-Grained segmentation systems is a module which automatically predicts the similarity of a given segmentation to an unseen ground truth segmentation. We propose as our prediction framework a regression model in order to capture that algorithmdrawn segmentations can range in quality from complete failures to nearly perfect <ref type="figure">(Figures 1, 2)</ref>. Our key design decisions lie in how to generate training data and choose predictive features.</p><p>Training Instances. We aim to populate our training data with segmentation masks that reflect the transition of segmentation quality from perfect (i.e., ground truth), to reasonable human mistakes, to a variety of failure behaviors. Towards this goal, our system collects 11 binary segmentation masks per training image.</p><p>We first derive a variety of binary masks using the same fully-automated algorithms leveraged in our Coarse segmentation system. Specifically, our system produces eight segmentations per training image using multiple implementations of the algorithms Hough Transform with Circles [4], Otsu Thresholding <ref type="bibr" target="#b33">[34]</ref>, and adaptive thresholding. An important distinction of our chosen segmentation algorithms compared to alternative tools <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref> is that they do not incorporate regularizer terms that can conceal typical failure behaviors, e.g., smoothing highly-jagged edges. Consequently, the different algorithms capture a variety of types of failure behaviors ( <ref type="figure">Figure 2)</ref>.</p><p>Given that the training data may be insufficiently populated with higher-scoring segmentations (if all eight algorithm implementations consistently fail), our system augments three binary masks based on the ground truth segmentations. The system uses the ground truth directly. Our system also dilates and erodes the ground truth binary mask by three pixels to simulate a slightly under-segmented and over-segmented segmentation respectively where fine details may get smoothed out or chopped off.</p><p>Training Data -Labels. To create each output label, the system computes a score indicating the quality of each training instance segmentation. We use the standard Jaccard index which indicates the fraction of pixels that are in common to both the training instance and ground truth segmentation (i.e., |A∩G| |A∪G| ). Training Data -Features. Next, our motivation is to use knowledge about algorithm behavior on everyday and biomedical images to choose predictive features. We take advantage of the observation that the chosen algorithms fail big when they fail, manifesting appearances unlike what one would expect from widely meaningful object shapes ( <ref type="figure">Figure 2)</ref>. We propose nine features derived from the binary segmentation mask to capture the failure behaviors. We hypothesize that, in aggregation, these features may account for objects of different shapes and sizes. In results, we will examine their advantages over an off-the-shelf state of the art image descriptor, i.e., based on CNNs.</p><p>Segmentation Boundary. When algorithms fail, resulting segmentations often have boundaries characterized by an abnormally large proportion of highly-jagged edges. We implement two boundary-based features to capture this observation. We compute the mean and standard deviation of the Euclidean distance of every point on the segmentation boundary to the centroid. The boundary is defined as all pixels on the exterior of the object in a binary mask using an 8-connected neighborhood. The centroid is defined as the center of mass of the segmentation in the binary mask.</p><p>Segmentation Compactness. When algorithms fail, segmentations often are not compact. We implement three features to capture this observation. Two measures compute the coverage of segmentation pixels within a bounding region. Extent is defined as the ratio of the number of pixels in the segmentation to the number of pixels in the area of the bounding box. Solidity is defined as the ratio of the number of pixels in the segmentation to the number of pixels in the area of the convex hull. We also compute the shape factor to capture the circularity of the segmentation since a pure circle is a good measure to indicate highly compact objects. It is defined as the ratio of region area A to a circle with the same perimeter P : 4πA P 2 . Location of Segmentation in Image. When algorithms fail, resulting segmentation regions often lie closer to the edges of images. We compute the normalized x and y centroid coordinates of the segmentation centroid in the image to capture this observation. Specifically, we compute the x value of the center of mass divided by the image width and y value of the center of mass divided by the image height.</p><p>Coverage of Segmentation in Image. When algorithms fail, resulting segmentations often cover abnormally large and small areas in the image. We implement two features to capture this observation. First, we compute the fraction of pixels in the image that belong to the segmentation. Second, we compute the fraction of pixels in the image that belong to the bounding box of the segmentation.</p><p>See Section 3 for an analysis of the variability of these cues measured for objects observed within diverse datasets.</p><p>Regression Model. We train a multiple linear regression model with the aforementioned training data. This model leads to easy to interpret, intuitive systems as it indicates how to predict the segmentation quality from a weighted combination of predictive features. Formally, the model is represented as y = Xβ + e where y denotes an n-dimensional vector of segmentation quality scores, X denotes a matrix containing feature vectors that characterize every training instance, β denotes the model parameters to be learned, and e denotes errors measured between actual quality scores (y) and predicted quality scores (Xβ). The objective is to learn β so that e is minimized. We train models with WEKA <ref type="bibr" target="#b21">[22]</ref> using M5 feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Results</head><p>We conduct studies to analyze the reliability of our prediction framework and its value for deciding when to intelligently target computers versus humans to segment images.</p><p>Datasets. We evaluate our methods on three datasets that represent three imaging modalities: Boston University Biomedical Image Library (BU-BIL:1-5) <ref type="bibr" target="#b20">[21]</ref> includes 271 gray-scale images coming from three fluorescence microscopy image sets and two phase contrast microscopy image sets, Weizmann <ref type="bibr" target="#b0">[1]</ref> consists of 100 grayscale images showing a variety of everyday objects, and Interactive Image Segmentation <ref type="bibr" target="#b18">[19]</ref> (IIS) includes 151 RGB images showing a variety of everyday objects. Each dataset includes human-drawn segmentations that serve as pixelaccurate ground truth segmentations for evaluation.</p><p>Together, the three datasets exhibit large variability with respect to object and image properties ( <ref type="table">Table 1)</ref>. The datasets depict objects that vary greatly in size (e.g., BU-BIL vs IIS), coverage of the image (e.g., BU-BIL vs Weizmann), shape (i.e., large Shape σ σ σ for all datasets), and texture (i.e., large FG Var σ σ σ for all datasets). Furthermore, our analysis suggests that image backgrounds can be complicated and/or cluttered (i.e., large BG Var µ µ µ and σ σ σ). This diversity is important to ensure our method is challenged to learn generic cues predictive of segmentation failure. <ref type="table">Table 1</ref>. Characterization of studied datasets to reveal the diversity of image content with respect to object area (# pixels), centroid location (X Loc, Y Loc), shape (Sec. 2.2; shape factor), and coverage in image ( FG Area Image Area ) as well as image texture (FG Var, BG Var = variance of Laplacian values for object and background pixels respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BU-BIL</head><p>Weizmann IIS µ µ µ σ σ σ µ µ µ σ σ σ µ µ µ σ σ σ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quality Prediction for Algorithm Set</head><p>We first analyze the predictive power of our proposed framework (Section 2.2) to automatically estimate the quality of foreground object segmentations.</p><p>Baselines. We compare our method to the CPMC <ref type="bibr" target="#b9">[10]</ref> approach that also predicts a Jaccard score indicating the quality of a given object segmentation. This baseline stresses generality by learning statistics typical for real world objects. The method learns to predict Jaccard scores on everyday images using a combination of shape and intensity-based features. We use publicly-available code.</p><p>Given the recent rise of CNN features as standard baselines for learning, we also examine the value of a CNN baseline for making predictions. We employ the same training instances using features extracted from the last fully connected layer of AlexNet <ref type="bibr" target="#b24">[25]</ref> to train linear regression models. Consequently, each training instance is characterized with a 4096-dimensional vector that is extracted from the image patch created by using the bounding box of the automatically generated segmentation.</p><p>Evaluation Metrics. We evaluate each prediction model using Pearson's correlation coefficient (CC) and mean absolute error (MAE). CC indicates how strongly correlated predicted scores are to actual Jaccard scores for all foreground object segmentations evaluated. Values range between +1 and -1 inclusive, with values further from 0 indicating stronger predictive power. MAE is the average size of prediction errors, computed as the mean absolute difference between all predicted and actual Jaccard scores.</p><p>Ours: Cross-Set Generalization. To minimize concerns that prediction successes are due to over-fitting to the statistics of a particular dataset, we first evaluate how well our prediction models trained on two of the datasets perform on the third dataset. Overall, our approach performs well, as indicated by high CCs and low MAEs ( <ref type="table">Table 2</ref>, row 3). The system is successful, even when trained on completely disjoint datasets; e.g., what the system learned on everyday images (Weizmann, IIS) can successfully be leveraged on biomedical images (BU-BIL: CC = 0.61). This is possibly because algorithms tend to create binary masks that have consistent properties at various levels of success and failure severity, regardless of the dataset.</p><p>While the CPMC method was designed to generalize across different object types, it had less predictive strength than our approach on all studied datasets ( <ref type="table">Table 2, row   Table 2</ref>. Comparison of our model with CPMC <ref type="bibr" target="#b9">[10]</ref> and CNN features <ref type="bibr" target="#b24">[25]</ref> for predicting the Jaccard score indicating the quality of a foreground segmentation. We report performance scores for our method learned with cross-set training ("Ours:C") as well as single-set training ("Ours:S"). Higher correlation coefficient (CC) scores and lower mean absolute error (M) scores are better. We observe that the off-the-shelf CNN feature yields negligible predictive power ( <ref type="table">Table 2</ref>, row 2). We hypothesize the high MAE arises from an accumulation of errors due to using a high dimensional feature space. Our results further support our findings that the characteristics of segmentation errors are robustly and sufficiently learned from a small set of features describing the binary mask alone.</p><p>Ours: Single-Set Analysis. We next evaluate our prediction framework per dataset (i.e., Weizmann, IIS, BU-BIL) as well as across the three datasets (All). To evaluate, we train and test each of the four configurations using 10fold cross-validation. We consistently observe performance gains over CPMC and cross-set results ( <ref type="table">Table 2</ref>, row 4 versus rows 1-3). These findings highlight a possible benefit of learning how an algorithm behaves with a particular type of image set, when one can know the image type to be encountered at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initializing Segmentation Tools</head><p>We next examine the value of our PTP framework to predict when to pull the plug on human annotators and use computers instead, when segmenting a batch of images with a given human budget. Our focus is on initializing segmentation tools. The status quo is either that humans create coarse object segmentation input for every image or computers automatically position rectangles based on the image dimensions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Our system, instead, intelligently decides which among multiple automatic initialization methods is preferable for each image and then decides whether to involve humans instead (Section 2.1, Coarse Segmentation system).</p><p>We evaluate with all 522 images from Weizmann, IIS, and BU-BIL. We collect a coarse segmentation per image from crowd workers on Amazon Mechanical Turk. We compare the following methods for creating coarse segmentation inputs:</p><p>-Ours: For each image, the system deploys either a) the algorithm from eight options that has the largest predicted Jaccard score or b) a human. We leverage cross-dataset predictions (Section 3.1) to estimate the quality of algorithm-generated segmentations. We chose this predictor so our method cannot inadvertently learn and exploit any dataset-specific idiosyncrasies.</p><p>-Perfect Predictor: For each image, this system deploys the algorithm from eight options that has the largest actual Jaccard score. Images are then ordered by the actual quality scores. Human involvement is allocated to the images with lowest quality scores. This predictor reveals the best initializations possible with our system. -Chance Predictor: For each image, the system randomly deploys one algorithm from the eight options. Then, images for human involvement are randomly selected. This predictor illustrates the best a user can achieve today with the initialization options available in our system. -Rectangle <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>: This method illustrates the commonly-adopted automated method of positioning a bounding rectangle with respect to the image dimensions. Following <ref type="bibr" target="#b11">[12]</ref>, we set the foreground region based on the image boundary. We position the rectangle to occupy the image region after cropping 5% of pixels from the minimum image dimension on all sides. We randomly select images for human involvement.</p><p>To illustrate the versatility of our initialization system as a general-purpose approach for use with segmentation tools, we integrate our initialization method and the baselines with three tools important in the computer vision and medical imaging communities -Grab Cut <ref type="bibr" target="#b34">[35]</ref>, Chan Vese level sets <ref type="bibr" target="#b11">[12]</ref>, and Lankton level sets <ref type="bibr" target="#b25">[26]</ref>  <ref type="figure" target="#fig_0">(Figure 4)</ref>. Fully-Automated Initialization. For each segmentation tool, we compute the average segmentation quality resulting after the tool refines all computer-generated initializations for all 522 images. As seen on the left side of the three plots ( <ref type="figure">Figure 3</ref>, 0% human involvement), predicting a bestsuited automated input from eight options produces coarse segmentation estimates that the segmentation tools can refine more successfully than existing baselines (i.e., Chance Predictor, Rectangle). For example, for the Lankton level set algorithm, the resulting segmentation quality improves by 20 percentage points over the Rectangle baseline by using our approach. The one exception is with Grab Cut initialized with the Rectangle baseline. We hypothesize this exception is due to Grab Cut's shrinking bias, which means Grab Cut cannot recover when the initialization occupies a <ref type="figure">Figure 3</ref>. We compare four methods for distributing varying levels of human involvement to create initializations for three segmentation tools (a-c). Each plot shows the mean quality for 522 segmentations that resulted after the tools refined the initializations. Our predictor, which identifies the best input option produced by eight algorithms and a human, facilitates segmentation quality comparable to today's status quo (Rectangle, Chance Predictor) with significantly less human involvement. The brown circles identify where our system achieves comparable segmentation quality to relying exclusively on human input. On average, our approach eliminates the need for human annotation effort for 44% of images while achieving segmentation quality comparable to relying exclusively on human input. region smaller than the object itself.</p><p>Reducing Human Initialization Effort. We next examine the impact of actively allocating human involvement to create coarse segmentation input as a function of the budget of human effort available. For each segmentation tool, we compute the average segmentation quality resulting after the tool refines the collection of chosen computer and human initializations for all 522 images <ref type="figure">(Figure 3</ref>). Our approach typically outperforms random decisions (i.e., Chance Predictor, Rectangle) regarding how to distribute the initialization effort to humans and computers for all budget levels. Our approach also has the potential to outperform all three baselines for all segmentation tools by greater margins given improved prediction accuracy, as exemplified by the Perfect Predictor.</p><p>In the more challenging setting of eliminating human effort without compromising segmentation quality, our system yields exciting results. Specifically, our system achieves comparable quality to relying exclusively on human input (i.e., 100% human involvement) while using computer involvement for 67.5% of images for Grab Cuts, 35% of images for Chan Vese level sets, and 30% of images for Lankton level sets <ref type="figure">(Figure 3</ref>; see brown circles). Our results reveal that different segmentation tools can tolerate different amounts of unreliable computer input without compromising the overall segmentation quality attained when relying exclusively on human input.</p><p>Peak Segmentation Quality. Relying on a mix of human and computer efforts can outperform relying on either resource alone to create initial segmentations. For example, peak accuracy for Grab Cuts with our initialization approach is achieved with 70% human and 30% computer involvement <ref type="figure">(Figure 3a</ref>). There is a six percentage point improvement from relying on a mix of human and computer input over human input alone. For Chan Vese and Lankton level sets algorithms, performance gains are slight with the tools fluctuating around a peak plateau value from 65% to 100% human involvement <ref type="figure">(Figures 3b,c)</ref>. We attribute the latter performance fluctuations to slight differences when the two tools expand and shrink the human and algorithm initializations as needed to recover the desired boundaries. We attribute the larger performance gains for Grab Cut to the tool's shrinking bias, which means Grab Cut fails when humans produce boundaries that do not entirely subsume the true object region. More generally, our findings reveal that intelligently replacing human effort with computer effort is not only desirable to save money and time, but also to collect higher quality segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Segmentation Tool Output</head><p>Lastly, we examine the value of our PTP framework to predict when to pull the plug on computers and use human annotation instead. For this second task, given segmentations from algorithms, the system predicts which images humans should re-annotate in order to recover from failures (Section 2.1, Fine-Grained Segmentation system). Implementation. The system automatically feeds initializations from the best stand-alone method (i.e., Hough Transforms with radius 5) to the top-performing Lankton level set algorithm. Quality estimates of resulting segmentations are then predicted using our cross-dataset predictor (Section 3.1).</p><p>Baselines. To our knowledge, no prior work addressed predicting when to enlist human versus computer segmentation effort. Therefore, we use as a baseline the related stateof-art system of Jain &amp; Grauman <ref type="bibr" target="#b22">[23]</ref> (J &amp; G) which predicts how to best allocate a given budget of human time to annotate a batch of images. In particular, it predicts whether to have humans draw a segmentation from scratch (54 seconds) versus supply a bounding box (7 seconds) or coarse segmentation (20 seconds) as input to Grab Cut. The system was trained on everyday images for Grab Cut. We use publicly-available code. Note that the J &amp; G <ref type="bibr" target="#b22">[23]</ref> system requires human involvement for every image and so only becomes relevant at the budget level that supports humancreated bounding boxes for all images (i.e., 61 minutes). Moreover, that system is designed for Grab Cut, whereas our system is agnostic to the segmentation tool.</p><p>We also compare the quality of predictions from our approach to perfect and chance predictions for deciding when humans versus computers should segment images.</p><p>Experiments. We conduct studies on all 522 images from Weizmann, IIS, and BU-BIL. Following prior work <ref type="bibr" target="#b22">[23]</ref>, we budget 54 seconds for each segmentation a human creates from scratch. We examine the impact of actively allocating human effort using a budgeted approach, in terms of minutes, ranging from no human involvement (0 minutes) to getting all 522 images manually annotated (470 minutes). We compute the average segmentation quality resulting for all chosen human-drawn and computer-drawn segmentations at each allotted time budget.</p><p>For human input, we analyze both the settings where segmentations are created locally and remotely. For the local setting, we leverage the ground truth segmentations as perfect expert annotations (i.e., Jaccard score of 1). For the web-based setting, we collect segmentations from online crowd workers and measure quality as the Jaccard similarity of each crowdsourced segmentation to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>Our system consistently outperforms the baselines for a wide range of budgets, both for expert ( <ref type="figure" target="#fig_1">Figure 5a</ref>) and crowd <ref type="figure" target="#fig_1">(Figure 5b)</ref> involvement. For example, the benefit of our approach is greatest at about 50% human budget (i.e., 222 minutes), eliminating an average of 70 minutes of human annotation effort to achieve compa- <ref type="figure">Figure 6</ref>. Examples of images which computers segment more similarly to experts than crowd workers. As intended, our system often avoids involving crowd workers for these images. rable segmentation quality to the Chance baseline. In addition, our system achieves segmentation quality comparable to the state of art interactive approach <ref type="bibr" target="#b22">[23]</ref> but often requires 30-60 minutes less human annotation time. This time savings to achieve same segmentation quality is typically observed in the human budget range of 50 to 220 minutes ( <ref type="figure" target="#fig_1">Figure 5a</ref>). Our findings highlight the value of our generic prediction framework today as well as its rich potential for use with future improved segmentation tools.</p><p>Finally, our findings reveal that relying on a mix of human and computer effort can outperform methods that always assume human involvement. In particular, for the last 100 images assigned to receive human annotations (i.e., images with highest predicted algorithm scores), the system appropriately chooses computer-drawn segmentations over human-drawn segmentations for 10% of images. In other words, for those 10% of images, computers create segmentations more similar to the ground truth than crowd workers (i.e., higher Jaccard scores). Example images where algorithms segment better than the crowd are shown in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>We proposed two novel tasks for intelligently distributing segmentation effort between computers and humans. Both tasks relied on our proposed prediction module that successfully predicts the quality of candidate segmentations from three diverse datasets, with stronger predictive capabilities than the baselines. For the first task of creating initializations that segmentation tools refine, our proposed system eliminated the need for human annotation effort for an average of 44% of images while preserving the resulting segmentation quality achieved when relying exclusively on human input. For the second task of creating high quality segmentation results, our proposed system consistently preserved the resulting segmentation quality from a state of art interactive segmentation tool while regularly eliminating 30-60 minutes of human annotation time. We share our code to support application and future extensions of this work (http://vision.cs.utexas.edu/ HybridAlgorithmCrowdSystems/PullThePlug).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the quality of resulting segmentations created by three segmentation tools from the initial segmentation selected by our system from the eight initialization options.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Predicting when to replace segmentations created by a semi-automatic segmentation tool with segmentations created by (a) experts and (b) online crowd workers for 522 images. With both experts and crowd workers, our system typically achieves state-of-art performance (J &amp; G method<ref type="bibr" target="#b22">[23]</ref>) while saving up to 60 minutes of human effort (b; time difference between curves in the human budget range of 140 to 190 minutes).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge funding from the Office of Naval Research (ONR YIP N00014-12-1-0754) and National Science Foundation (IIS-1421943). We thank Mehrnoosh Sameki and Bo Xiong for their assistance with experiments as well as Qinxun Bai, Ajjen Joshi, and the anonymous reviewers for feedback to improve the article.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image segmentation by probabilistic bottom-up aggregation and cue integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalizing the Hough transform to detect arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">iCoseg: Interactive co-segmentation with intelligent scribble guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational b-spline level-set: A linear filtering approach for fast, deformable model evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Friboulet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thevenaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simultaneous active learning of classifiers &amp; attributes via relative feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The ignorant led by the blind: A hybrid human-machine vision system for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="29" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Click&apos;n&apos;Cut: Crowdsourced interactive segmentation with object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Charvillat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM Workshop on Crowdsourcing for Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geodesic active contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="79" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">In vivo cell-cycle profiling in xenograft tumors by quantitative intravital microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Chittajallu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Orth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weissleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Danuser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Mitchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="577" to="585" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transductive object cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Category independent object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="575" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single-cell magnetic imaging using a quantum diamond microscope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Glenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weissleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Walsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Connolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="736" to="738" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmentation from a box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3129" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How to use level set methods to accurately find boundaries of cells in biomedical images? Evaluation of six methods paired with automated and crowdsourced initial contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Theriault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sameki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI): Interactive Medical Image Computation (IMIC) Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How to collect segmentations for biomedical images? A benchmark evaluating the performance of experts, crowdsourced non-experts, and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Theriault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sameki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Purwada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Solski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Winter conference on Applications in Computer</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The WEKA data mining software: an update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predicting sufficient annotation strength for interactive foreground segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating segmentation error without ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alvino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bahlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Localizing region-based active contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lankton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tannenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image segmentation with a bounding box prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Minimization of region-scalable fitting energy for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1940" to="1949" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Repairing bad co-segmentation using its quality evaluation and segment propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimating image segmentation difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Data Mining in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detection and counting of red blood cells in blood cell images using hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mukherjee</surname></persName>
		</author>
		<idno>2012. 3</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page" from="3" to="309" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cost-sensitive active visual category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="44" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning localized perceptual similarity metrics for interactive categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications in Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="502" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Milcut: A sweeping line multiple instance learning paradigm for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
