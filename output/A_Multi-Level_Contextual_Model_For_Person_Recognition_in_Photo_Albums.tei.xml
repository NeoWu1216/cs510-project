<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-Level Contextual Model For Person Recognition in Photo Albums</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stevens Institute of Technology ‡ Adobe Research ♮ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stevens Institute of Technology ‡ Adobe Research ♮ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stevens Institute of Technology ‡ Adobe Research ♮ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stevens Institute of Technology ‡ Adobe Research ♮ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
							<email>xshen@adobe.com♮ganghua@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stevens Institute of Technology ‡ Adobe Research ♮ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stevens Institute of Technology ‡ Adobe Research ♮ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♮</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stevens Institute of Technology ‡ Adobe Research ♮ Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Multi-Level Contextual Model For Person Recognition in Photo Albums</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a new framework for person recognition in photo albums that exploits contextual cues at multiple levels, spanning individual persons, individual photos, and photo groups. Through experiments, we show that the information available at each of these distinct contextual levels provides complementary cues as to person identities. At the person level, we leverage clothing and body appearance in addition to facial appearance, and to compensate for instances where the faces are not visible. At the photo level we leverage a learned prior on the joint distribution of identities on the same photo to guide the identity assignments. Going beyond a single photo, we are able to infer natural groupings of photos with shared context in an unsupervised manner. By exploiting this shared contextual information, we are able to reduce the identity search space and exploit higher intra-personal appearance consistency within photo groups. Our new framework enables efficient use of these complementary multi-level contextual cues to improve overall recognition rates on the photo album person recognition task, as demonstrated through state-of-theart results on a challenging public dataset. Our results outperform competing methods by a significant margin, while being computationally efficient and practical in a real world application.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>After decades of research, the problem of face recognition as measured by standard benchmarks such as Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> is to the point of being nearly solved. For example, Schroff et al. <ref type="bibr" target="#b23">[24]</ref> achieved 99.63% verification accuracy using a deep convolutional neural network (CNN). That said, this impressive result is misleading because such benchmarks are typically skewed towards images with clearly visible, high quality faces (see <ref type="figure" target="#fig_0">Figure 1</ref>). However, when applying face recognition for tagging faces in the real-world photo albums, faces are often not so clearly visible and present many challenges due  to changes in body pose, illumination, heavy occlusion and so forth (see <ref type="figure" target="#fig_1">Figure 2</ref>). As an example, Zhang et al. <ref type="bibr" target="#b30">[31]</ref> observed a dramatic drop in recognition accuracy when a top performing LFW algorithm was applied to a photo album test set.</p><p>In addition to learning more discriminative and robust face features, we must look beyond the faces in order to approach human-level performance on thismore challenging task. In general, information beyond the faces can be viewed as context to guide recognition. Such "extra-face" context can naturally be divided into three levels: the person or body level, the photo level, and the photo group level, as depicted in <ref type="figure">Figure 3</ref>.</p><p>At the person level, perhaps the most obvious contextual cue is clothing, which has been shown to be an effective supplemental cue for face recognition in photo albums <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>. However, it is still not well understood how best to fuse face and body appearance features as the relative importance of these features depends on higher level context. In this paper, we explore several alternatives for fusing person-level appearance features. Contextual cues at the photo level include metadata (when present) such as geographical and temporal information <ref type="bibr" target="#b18">[19]</ref>, event labels, and social relationships <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref>. However, we cannot always rely on metadata to be present  <ref type="figure">Figure 3</ref>. The three levels of context that can be exploited for person recognition in photo albums: a) person-level context consists of face and body appearance features; b) photo-level context includes identity co-occurrences and mutual exclusion; c) grouplevel context presents higher intra-personal appearance consistency and reduced identity search space. and accurate. Therefore, we seek a robust method that can take advantage of such metadata when available, but can still exploit photo level context when the metadata is absent. Aside from the metadata, we have found that we can leverage a rough prior on the co-occurrences of particular individuals within a photo, as well as a soft mutual exclusion constraint, to substantially improve recognition.</p><p>Going beyond a single photo, we observe that album photos frequently occur in groups that are closely related, such as being taken on the same day or same event or same setting. When such groupings are present and can be automatically determined, then it becomes possible to exploit mutual information across the photos in a group to improve recognition. In particular, we can adapt the person appearance classifiers to a given group. Metadata, when available, can be used for effective groupings. However, in order to be robust in the absence of metadata, we propose an unsupervised method to determine the effective photo groups based on photo appearance.</p><p>In summary, in order to go "beyond faces" for person recognition in photo albums, it is necessary to exploit contextual information. This paper presents a framework that is both effective and efficient, based on three levels of contextual information, namely person, photo, and photo group. Our contributions are as follows:</p><p>• a practical and efficient multi-level contextual model achieving state-of-the-art results on the People In Photo Albums (PIPA) person recognition benchmark;</p><p>• an iterative joint inference process to leverage the photo-level context cues;</p><p>• an unsupervised, metadata-free method to discover relevant photos that provide group-level context for improved person recognition;</p><p>• an effective confidence-aware method for fusing the person-level appearance cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Researchers have been long interested in using context cues in recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. Clothing features <ref type="bibr" target="#b7">[8]</ref>, metadata <ref type="bibr" target="#b24">[25]</ref>, location and event labels <ref type="bibr" target="#b16">[17]</ref> have been used as contexts to improve face tagging. Geographic contexts, spatial contexts, temporal contexts or even more high-level cultural contexts have been used for image classification, object detection and classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>However, most of these context cues are from metadata or require manual labels beyond the identity domain and are inapplicable in the absence of these information. In this paper, we propose a contextual model that does not rely on metadata but can benefit from it when available.</p><p>The most relevant literature to ours are Zhang et al. <ref type="bibr" target="#b30">[31]</ref> and Oh et al. <ref type="bibr" target="#b20">[21]</ref>. Zhang et al. <ref type="bibr" target="#b30">[31]</ref> published the PIPA dataset to study person recognition. Both works observed that context cues beyond the faces help improve person recognition accuracy. Zhang et al. <ref type="bibr" target="#b30">[31]</ref> used discriminative information from poselets <ref type="bibr" target="#b1">[2]</ref>. Oh et al. <ref type="bibr" target="#b20">[21]</ref> carefully evaluated the effectiveness of different body regions, the scene context and some long term attributes (e.g., age and gender).</p><p>However, the context cues in their works are mostly at the person-level. They do not exploit the joint identity assignment of multiple instances at the photo-level or the mutual information across multiple photos at the group-level. In our work, we have found that recognition accuracy improves significantly when we incorporate context cues at multiple levels.</p><p>Our work leverages Conditional Random Fields (CRF) to jointly infer the identities of instances in the same photo to exploit the identity co-occurrences. In this sense, the methods from Stone et al. <ref type="bibr" target="#b25">[26]</ref> and Brenner et al. <ref type="bibr" target="#b2">[3]</ref> are relevant to ours. However, the former relies on the social context to estimate the relationships between people and the latter jointly processes the entire set as a sparse Markov Random Fields, which can be computationally expensive in processing a large-scale photo collection.</p><p>To our knowledge, the framework presented here is the first to effectively leverage context cues across multiple levels for person recognition in photo albums. In addition to person-level appearance features and photo-level identity  co-occurrences, we leverage groups of relevant photos specific to each testing photo as its group-level context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Multi-Level Contextual Model</head><p>This work addresses the identification setting for person recognition in a photo album. Specifically, we are given a set of photos containing a set of person instances that have been grouped into two disjoint sets: the gallery set where identity labels are assigned to each instance, and the probe set where the identity labels are unknown. Our task is to predict the identities of all unlabeled instances in the probe set. In this work, we assume that there is at least one labeled instance for each identity in the gallery set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the proposed framework processes the face and body regions separately with the group specific online learned SVMs and then fuses the outputs. The outputs are then iteratively updated with the photo level joint inference CRF.</p><p>Multiple regions present complementary yet discriminative information in the person-level context. The proposed method uses face and body regions and can be naturally expanded to incorporate more regions. We discuss the options to fuse the predictions with different regions in Section 3.2.</p><p>In related photos, people present more consistent appearance. The proposed method discovers groups of related photos to learn group specific SVMs online for prediction. We describe the details in Section 3.3.</p><p>As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, after leveraging the person-level and group-level context, the results are updated with an iterative image level joint inference CRF. In this step, we first estimate the identity co-occurrences based on the current predictions of the unlabeled instances and the given labeled instances. We then proceed to encode this prior knowledge to jointly infer identities of all unlabeled instances for each photo to update our predictions. After that, we update the prior knowledge based on the current predictions and repeat this process iteratively for several times. We describe this part in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Person-Level Context</head><p>Identity information exists in the appearance of clothes, hairstyles and other regions. Oh et al. <ref type="bibr" target="#b20">[21]</ref> carefully explored the effectiveness of different regions such as the head, body, upper body and scene. In this work, for simplicity, we only include the face and body regions shown in <ref type="figure">Figure 3</ref>. We use a face recognition system (detailed in the experiment section) to extract the face features from the face regions. We fine-tune a CNN pre-trained for image classification with the body regions to extract body features using the soft-max classification objective over identities. By fusing the information from different regions, personlevel context is incorporated.</p><p>The proposed method can be expanded to incorporate more regions. Without loss of generality, we assume C regions are used in the proposed framework. Assume there are M people in the photo set. With the c-th region, we can apply an identity classifier (will be detailed in Section 3.3) to obtain the prediction s c (x) as an M -dimensional vector. The y-th element in s c (x) indicates the probability that the instance x is of identity label y. Given the instance x, we proceed with C regions and obtain C prediction score vectors: s 1 (x), . . . , s C (x). We fuse the scores from different regions as our final prediction.</p><p>Specifically, we explore the following options to fuse the C prediction vectors to obtain boosted recognition result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Weighted Average Fusion</head><p>Zhang et al. <ref type="bibr" target="#b30">[31]</ref> combined the predictions from the poselet classifiers with a linear Support Vector Machine (SVM), which is equivalent of taking the element-wise weighted sum of all prediction score vectors, i.e.,</p><formula xml:id="formula_0">s(x) = C c=1 ω c s c (x),<label>(1)</label></formula><p>in which ω c is the weight for the c-th region. The weights can be learned with a binary linear SVM over the Cdimensional score vectors from the validation dataset <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Max Pooling Fusion</head><p>Without learning the weights, another straightforward option to fuse the predictions is taking the element-wise max operation, i.e.,</p><formula xml:id="formula_1">s(x) = max c∈[1,C] s c (x)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Confidence-aware Fusion</head><p>Having fixed combination weights {ω c } C c=1 may not be optimal. Another option for fusion is to assign instance specific weights based on the prediction confidence scores.</p><p>By sorting the elements in s c (x) in descending order:</p><formula xml:id="formula_2">[s 1 , s 2 , . . . , s M ]. , we define the weight ω c (x), ω c (x) = 1 Z (s 1 − s 2 ) (s 1 − s M ) , s(x) = C c=1 ω c (x)s c (x),<label>(3)</label></formula><p>where Z is the normalization term to ensure</p><formula xml:id="formula_3">C c=1 ω c (x) = 1.</formula><p>We evaluate all these options in the experiments (see Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Group-Level Context</head><p>In the identification setting for person recognition, the problem naturally fits into a multi-class classification paradigm. With a specific region feature to represent the instance, we can learn a classifier from all the gallery instances. We name it the global multi-class classifier. In this work, we use linear SVM as the classifier. Linear SVM is efficient and it generalizes well to unseen data with limited amount of training samples. For multi-class classification, we follow the 1-versus-all paradigm to train the multi-class SVM using the LIBLINEAR <ref type="bibr" target="#b4">[5]</ref> implementation.</p><p>When people change clothes, hairstyles or when the photos are taken from different viewpoints, the appearance of the same person can change dramatically. The large appearance variations lead to highly non-linear class boundary in the feature space and hence affect the robustness of the global classifier. However, we observe that when photos are properly grouped, the intra-personal variation is reduced, as shown in <ref type="figure" target="#fig_5">Figure 5</ref>, which allows us to leverage the mutual information inside the group to help person recognition. However, this extra information is not always available or is complete. Hence, we need a method to automatically discover the photo groups.</p><p>We first define the relevancy between two photos using the photo similarity and identity co-occurrence. The intuition is that when two photos are visually similar or contain the same persons, they are more likely to be related. Given an instance x in a query photo, we determine a set of "neighbor" photos most relevant to the query photo, and use them to exploit the context cues at group-level.</p><p>We use the caffe <ref type="bibr" target="#b12">[13]</ref> implementation of AlexNet <ref type="bibr" target="#b14">[15]</ref> to extract holistic image feature F I for photo I. Given two photos I i and I j , we denote the labeled instances in the two photos as {x 1 , x 2 , . . . , x Ni } and {x ′ 1 , x ′ 2 , . . . , x ′ Nj } respectively. The identity label for any instance x is denoted as Y (x). We define the affinity Λ i,j between photos I i and I j as</p><formula xml:id="formula_4">Λ i,j = cos(F Ii , F Ij ) * (1 + N ij N i + N j − N ij )/2,<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">N ij = 1≤u≤Ni, 1≤v≤Nj [Y (x u ) = Y (x ′ v )], cos(F Ii , F Ij ) = F Ii · F Ij ||F Ii ||· ||F Ij || ,</formula><p>where [P ] is the Iverson bracket, i.e., [P ] = 1 if P is true otherwise [P ] = 0. When N i = 0 or N j = 0, we set Λ i,j = cos(F Ii , F Ij ). N ij measures the identity co-occurrences between two photos, while cos(F Ii , F Ij ) indicates the photo similarity. The photo similarity part in Λ i,j is more important since the labeled instances are usually limited, which can lead to inaccurate estimation of the identity co-occurrences. We still choose to keep the identity co-occurrence part here because it slightly improves the recognition accuracy (around 0.3% on average) with an ignorable overhead.</p><p>We then use this affinity matrix for spectral embedding <ref type="bibr" target="#b19">[20]</ref>. In this way, we embed all photos into the space in which the neighbors are of higher relevancy. Technically, we calculate the normalized Laplacian matrix L from Λ,</p><formula xml:id="formula_6">L = D − 1 2 ΛD − 1 2 , D i,i = j Λ i,j .<label>(5)</label></formula><p>Then we find the K largest eigen-vectors of L, i.e., X 1 , . . . , X K . Assuming L is a N × N matrix, then each X k is a N dimensional vector. The K vectors are stacked as rows in a K × N matrix. We represent the n-th photo by the n-th column in this matrix, which is a K dimensional vector. After embedding, the euclidean distance between two photos indicates their relevancy. After the spectral embedding, given a probe instance in photo I, we choose the N contex nearest neighbors of I as its group-level context. We then train an online, group-specific SVM classifier with the labeled instances in its group-level context for prediction. Note that when there is no labeled instance in the photo group, we use the global SVM for prediction. If there is only one identity in the photo group, we assign the identity label to the probe instance as its group specific output.</p><p>The group specific classifier may suffer from the insufficient training samples problem. For regularization, we combine the prediction from the globally trained SVM with the prediction from the group specific SVM. We observe that taking average of the two predictions works well. Intuitively, the global SVM is adapted to the group in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Album Information</head><p>In many personal photo management softwares, the photos are usually organized as albums. If this information is available, given a photo, we simply use its photo album as the group-level context. An SVM is trained for each album as the group specific SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Photo-Level Context</head><p>As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, after fusing the recognition results from different regions, we further update the predictions with the photo-level context. At the photo-level, we jointly predict multiple instances in a single photo, in which we can encode the following prior knowledges.</p><p>First, intuitively, two people have a higher chance to appear in the same photo when they know each other. We can bias our predictions to assign co-occurred identities to instances in the same photo. Second, in general, two instances in the same photo are rarely the same person. We can largely reduce the possibilities of assigning duplicated labels in the same photo.</p><p>Given the current predictions, we incorporate these knowledges to update the predictions with the Conditional Random Fields (CRF). Technically, we define a label compatibility matrix ψ to encode the knowledge. Given two identity labels l a and l b , ψ(l a , l b ) indicates how likely the two identities are in the same photo. ψ(l a , l b ) =    ǫ l a = l b , α l a , l b co-occurred, <ref type="bibr" target="#b0">1</ref> otherwise,</p><p>where α &gt;= 1 is a parameter for the strength of the identity co-occurrence assumption; a small ǫ = 0.01 is used to enforce a soft mutual exclusion constraint; l a and l b are regarded as co-occurred, when two instances of them have been observed in the same photo. We have explored other options for α. For example, instead of having α as a constant value, we tried setting it proportional to the number of times l a , l b co-occurred in the photo set. However, due to the limited number of samples, this statistic is not reliable. We observe that having a constant α provides more stable improvement.</p><p>In the CRF, we look for an identity assignment Y = [y 1 , . . . , y N ], y n ∈ [1, M ] over the N instances {x n }, n ∈ [1, N ] in the image I. Y is regarded as a random variable conditioned on the image I. We jointly predict the identities of the N instances to maximize the potential E(Y ),</p><formula xml:id="formula_8">E(Y ) = N n=1 φ(y n ) + n,m ψ(y n , y m ).<label>(7)</label></formula><p>The unary potential φ(y n ) is from the current prediction. i.e., the M -dimensional score vector s(x n ), φ(y n ) = y n -th element in s(x n ).</p><p>For any labeled instance of identity label l, we set φ(y n ) = [y n = l], where [P ] is the Iverson bracket. The pairwise potentials are from the label compatibility matrix ψ. The Loopy Belief Propagation <ref type="bibr" target="#b17">[18]</ref> is applied for the CRF inference with the UGM implementation <ref type="bibr" target="#b22">[23]</ref>.</p><p>Lastly, since the label compatibility matrix is estimated from the current prediction, it may not be accurate enough. We propose to update the label compatibility matrix each time after we obtain the updated predictions from the joint inference. The updated label compatibility matrix is more accurate and helps the joint inference. This loop is iterated for T times. Typically after T = 5 iterations, it converges. We observe consistent improvement by having this iterative joint inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on the PIPA Dataset</head><p>The proposed method is evaluated on the PIPA dataset and compared with the state-of-the-art methods. We noticed some incorrect labels in the test set of PIPA. To accurately evaluate the proposed method, we manually curated the test set to remove label noise and merge redundant labels. The curation does not make the problem easier. We show some examples of this in the supplemental material. For fairness, we use the original labels when compared with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setting</head><p>The PIPA dataset is composed of the training set, validation set and test set. The training set is for model training and the test set is for evaluation. The test set has 7,868 photos in 357 albums. There are 12,886 labeled instances of 581 different individuals. The evaluation protocol on PIPA follows the person identification setting. The test set is split into the gallery set and probe set. Given the head bounding boxes of all instances in the test set and the identity labels for all instances in the gallery set, we train our person recognition system to predict the missing identity labels in the probe set. Then we switch the gallery set and probe set, repeat the experiment and report the average accuracy.</p><p>In the original setup, a probe instance may have near duplicate gallery instance. To study how well the person recognition system can address long-term appearance changes, Oh et al. <ref type="bibr" target="#b20">[21]</ref> proposed three more challenging and realistic splits. We incorporate their evaluation protocols and report our results on all the four splits: original, album, time and day.</p><p>Generally, the four splits are in the order of increasing difficulty. In the album splits, the gallery and probe instances are split across different albums. In the time splits, instances are split into newest versus oldest based on the photo-taken time. They manually setup the day splits to enforce appearance changes. We refer the readers to Oh et al. <ref type="bibr" target="#b20">[21]</ref> for details about the construction of these splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Face Region</head><p>We implemented a deep CNN based face recognition system following Sun et al. <ref type="bibr" target="#b26">[27]</ref> for feature extraction from face regions. The number of parameters in the CNNs in the system is about one-fourth of the AlexNet <ref type="bibr" target="#b14">[15]</ref>. On the face recognition benchmark LFW, it achieves an accuracy of 97.65% comparable to 97.35% from DeepFace <ref type="bibr" target="#b27">[28]</ref>.</p><p>On the PIPA dataset, with the given head bounding box, we apply a face detector we implemented following Li et al. <ref type="bibr" target="#b15">[16]</ref> to detect the largest face inside the bounding box. Then we extract the face feature from the detection box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Body Region</head><p>We follow Zhang et al. <ref type="bibr" target="#b30">[31]</ref> to fine tune the AlexNet <ref type="bibr" target="#b14">[15]</ref> using body regions on the training set. Given a head bounding box centered at (x, y) of size (w, h), we estimate the upper body region to be a box centered at (x − 0.5l, y) of size (2l, 3l), in which l = min(w, h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Score Fusion</head><p>To evaluate the different options for fusion, we train global multi-class classifiers with the face and body regions respectively for person recognition and fuse the results with all the options. An alternative to score-level fusion is to concatenate the features from all regions to represent the instance and proceed with the other components for prediction. We observe that this fusion method performs worse than any of the score fusion methods, as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The weighted average fusion method works best for the original splits. However, when the setup becomes more challenging, the learned fixed weights may not help all probe instances. In the most challenging day splits, it fails to improve the face recognition performance. The max pooling fusion gives consistent improvements after fusion. However, since the prediction scores from different regions are not calibrated, it is not the most effective. The performance of the confidence-aware fusion method is stable across all setups. We use this method in our following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Framework Components</head><p>We introduce several comparison methods, with which we demonstrate how the components in the proposed framework contribute to the improvement in <ref type="table">Table 2</ref>.</p><p>• baseline method: the confidence-aware fusion of the global face and body SVMs is our baseline method;</p><p>• baseline method with photo-level context: we directly update the results from the baseline method with the iterative image label joint inference CRF;  • baseline method with group-level context: we skip the iterative image label joint inference CRF in <ref type="figure" target="#fig_4">Figure 4</ref>;</p><p>• multi-level contextual model: the full proposed framework in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we demonstrate the effectiveness of exploiting person-level context. In <ref type="table">Table 2</ref>, we observe that both the group-level context and photo-level context help improve the recognition accuracy independently. Moreover, the proposed framework leverages contexts at multiple levels and achieves further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Metadata</head><p>In <ref type="table">Table 2</ref>, we also observe that the proposed framework can leverage extra information, when available, e.g., the album information, which is presented as grouping of photos. It helps because the same person usually has more consistent visual appearance in the same album and the total number of identities are limited in a photo album. As we can see, the album information consistently helps because it arbitrarily separate irrelevant photos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Parameters</head><p>We set the spectral embedding dimension to be 400. We choose top N context = 50 nearest neighbors after spectral embedding as the photo's group-level context. In <ref type="figure" target="#fig_8">Figure 7</ref>, we show examples of discovered group-level contexts. We observe the proposed method works with a range of values for the spectral embedding dimension. We evaluate the influence of N context to the recognition accuracy. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>, the performance is stable with N context in the range of 10 to 100. A very small N context = 5 degrades the accuracy due to limited training samples. With a very large N context , the group specific SVMs degenerate to the global SVM.</p><p>In estimating the label compatibility matrix for the photo-level joint inference, we set α = 2 for the strength of the identity co-occurrence prior and T = 5 for the number of iterations of the joint inference. The proposed method works with a range of reasonable values for these parameters.</p><p>Note that by setting α = 1, we only encode the identity mutual exclusion knowledge in the label compatibility matrix, which leads to an accuracy drop compared with α = 2. For example, in the original splits, the accuracy drops from 87.81% (α = 2) to 85.74% (α = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Results Comparison</head><p>We compare our results with the existing methods on PIPA in <ref type="table" target="#tab_1">Table 3</ref>. The proposed method significantly outperforms all the existing methods. The improvement is more significant for more challenging splits, which demonstrate the effectiveness of exploiting the contexts at multiple levels. When album information is available, our method benefits from the extra information and achieves even dramatic improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Computational Expense</head><p>Besides the improved recognition accuracy, we only use 3 deep CNNs in our method which is more efficient than previous state-of-the-art <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>. Note that Zhang et al. <ref type="bibr" target="#b30">[31]</ref> used more than 100 deep CNNs for feature extraction and Oh et al. <ref type="bibr" target="#b20">[21]</ref> used 17 deep CNNs. Applying a deep CNN for feature extraction takes a large portion of time budget. Hence our method is more practical. The photo-level context based update is very efficient that it takes only 90 seconds to process 7, 839 photos, in which half of the instances are unlabeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on the Gallagher Album</head><p>In this experiment, we test generalization of our method for another dataset. To this end, we pick the Gallagher Album <ref type="bibr" target="#b7">[8]</ref> dataset, which is a small collection of family photos consists of 589 images with 931 annotated faces from 32 people. We show that our method also improves on a single photo album.</p><p>Following the same identification setting, for each identity, we randomly select half of the instances into the gallery  set and the rest into the probe set. We repeat the process to obtain 3 random splits. To test the generalization of our system, we use the same parameters in the evaluation on PIPA. A small group of people dominate the identity distribution in this dataset. There are 5 people with totally 777 out of 931 instances. The rest of the people have a limited number of instances.</p><p>Because most identities are rare in the dataset, we need to select a very large group of photos as the group-level context to cover them, which is contrary to the motivation of exploiting the group-level context. Hence, it is better to regard the whole dataset as a single album and skip the group-level context component in the proposed framework. As shown in <ref type="table" target="#tab_2">Table 4</ref>, the group-level context does not help because of the long-tailed identity distribution. However, we observe consistent improvements with the person-level and photo-level contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a multi-level contextual model for person recognition. Our model exploits discriminative visual information from multiple regions. We propose to use a confidence-aware fusion method to integrate the discriminative information from all the regions. The proposed model uses photo group to reduce the identity search space and leverage the smaller intra-personal appearance changes within the same group. It naturally incorporates metadata such as the album information, when available, to help group photos, but does not rely on metadata. Additionally, the proposed method encodes an identity cooccurrence prior to jointly infer the identities of instances in the same photo. Multiple levels of context cues are leveraged with the proposed model. We demonstrate significant improvements over the state-of-the-art results on the challenging PIPA dataset while being more computationally efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Faces in the Labeled Faces in the Wild dataset: most faces are clear with good image quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Faces in the People In Photo Albums dataset: the large visual appearance variations are very challenging for face recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Photo Set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The proposed multi-level contextual model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The instances of the same person in four photo groups: the intra-personal visual appearance variations become smaller within each group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>The recognition accuracy with respect to the number of nearest neighbors in group-level context on the original splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Examples of group-level contexts (showing top 7 nearest neighbors): red boxes indicate the unlabeled instances and blue boxes indicate the labeled instances; the first photo in each row (green bounding box) is the testing photo.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of different strategies for multiple regions fusion.</figDesc><table>Splits 
Face 
Body 
Concatenated Features Weighted Average Max Pooling Confidence-aware 
original 67.89% 74.89% 
71.15% 
84.38% 
82.70% 
83.86% 
album 
64.87% 64.90% 
67.81% 
78.18% 
77.41% 
78.23% 
time 
58.86% 51.84% 
61.76% 
69.60% 
68.76% 
70.29% 
day 
54.23% 23.28% 
54.46% 
52.18% 
54.95% 
56.40% 

Table 2. Evaluation of different components in the proposed 
framework with curated labels: a) baseline method: fusion of 
global classifiers from face and body regions. b) baseline method 
with photo-level context; c) baseline method with group-level con-
text; d) multi-level contextual model; e) multi-level contextual 
model with album information. 

stage original 
album 
time 
day 
a) 
83.86% 78.23% 70.29% 56.40% 
b) 
84.84% 79.13% 71.94% 57.37% 
c) 
85.10% 79.63% 72.02% 57.33% 
d) 
88.20% 83.02% 77.04% 59.77% 

e) 
94.27% 83.79% 80.64% 61.77% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Recognition accuracy comparison with original labels: best results with and without extra information are highlighted. Oh et al. [21] 86.78% 78.72% 69.29% 46.61% Zhang et al.</figDesc><table>methods 
original 
album 
time 
day 
[31] 
83.05% 
-
-
-

our method 
88.75% 83.33% 77.00% 59.35% 

ours with 
album info. 
93.91% 83.44% 80.23% 61.62% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Recognition accuracy on the Gallagher Album dataset.</figDesc><table>components 
Exp 1 
Exp 2 
Exp 3 
face region 
global classifier 
83.40% 83.19% 81.93% 

body region 
global classifier 
80.88% 78.78% 78.15% 

baseline: 
confidence-aware fusion 
86.34% 87.61% 85.50% 

baseline with 
group-level context 
85.50% 86.13% 83.19% 

baseline with 
photo-level context 
89.08% 88.66% 87.61% 

full multi-level 
contextual model 
88.87% 86.55% 86.76% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is partially done when the first author was an intern at Adobe Research. Research reported in this publication was partly supported by the National Institute Of Nursing Research of the National Institutes of Health under Award Number R01NR015371. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. This work is also partly supported by US National Science Foundation Grant IIS 1350763 and GH's start-up funds from Stevens Institute of Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual identity recognition in personal photo albums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Gökturk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sumengen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint people recognition across photo collections using sparse markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mul-tiMedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="340" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding images of groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using group prior to identify people in consumer images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clothing cosegmentation for recognizing people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object categorization using co-occurrence, location and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="30" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: Updates and new reporting procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2014-003</idno>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Which faces to tag: Adding prior constraints into active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint people, event, and location recognition in personal photo collections using cross-domain context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Leveraging context to resolve identity in photo albums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paepcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Libraries, 2005. JCDL&apos;05. Proceedings of the 5th ACM/IEEE-CS Joint Conference on</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person recognition in personal photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context-aware person identification in personal photo collections. Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ugm: A matlab toolbox for probabilistic undirected graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Context-aided human recognitionclustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Autotagging facebook: Social network context improves photo annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving image classification with location context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03873</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">knock! knock! who is it?&quot; probabilistic person identification in tv-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bäuml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond frontal faces: Improving person recognition using multiple cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
