<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Learning of Edges</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Learning of Edges</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data-driven approaches for edge detection have proven effective and achieve top results on modern benchmarks. However, all current data-driven edge detectors require manual supervision for training in the form of hand-labeled region segments or object boundaries. Specifically, human annotators mark semantically meaningful edges which are subsequently used for training. Is this form of strong, highlevel supervision actually necessary to learn to accurately detect edges? In this work we present a simple yet effective approach for training edge detectors without human supervision. To this end we utilize motion, and more specifically, the only input to our method is noisy semi-dense matches between frames. We begin with only a rudimentary knowledge of edges (in the form of image gradients), and alternate between improving motion estimation and edge detection in turn. Using a large corpus of video data, we show that edge detectors trained using our unsupervised scheme approach the performance of the same methods trained with full supervision (within 3-5%). Finally, we show that when using a deep network for the edge detector, our approach provides a novel pre-training scheme for object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The human visual system can easily identify perceptually salient edges in an image. Endowing machine vision systems with similar capabilities is of interest as edges are useful for diverse tasks such as optical flow <ref type="bibr" target="#b30">[31]</ref>, object detection <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b12">13]</ref>, and object proposals <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b2">3]</ref>. However, edge detection has proven challenging. Early approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> relied on low-level cues such as brightness and color gradients. Reasoning about texture <ref type="bibr" target="#b24">[25]</ref> markedly improved results, nevertheless, accuracy still substantially lagged human performance.</p><p>The introduction of the BSDS dataset <ref type="bibr" target="#b1">[2]</ref>, composed of human annotated region boundaries, laid the foundations for a fundamental shift in edge detection. Rather than rely on complex hand-designed features, Dollár et al. <ref type="bibr" target="#b9">[10]</ref> proposed a data-driven, supervised approach for learning to (a) unsupervised semi-dense correspondence (b) motion discontinuities à image edges (c) image edges à motion discontinuities <ref type="figure">Figure 1</ref>. Our goal is to train an edge detector given only semidense matches between frames (a). While motion discontinuities imply the presence of image edges (b), the converse is not necessarily true as distinct image regions may undergo similar motion (c). In this work we exploit the sparsity of edges to overcome the latter difficulty. We show that the signal obtained from matches computed over a large corpus of video data is sufficient to train top-performing edge detectors. detect edges. Modern edge detectors have built on this idea and substantially pushed the state-of-the-art forward using more sophisticated learning paradigms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44]</ref>. However, existing data-driven methods require strong supervision for training. Specifically, in datasets such as BSDS <ref type="bibr" target="#b1">[2]</ref>, human annotators use their knowledge of scene structure and object presence to mark semantically meaningful edges. <ref type="bibr" target="#b0">1</ref> Moreover, recent edge detectors use Image-Net pre-training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref>. In this paper, we explore whether this is necessary: Is object-level supervision indispensable for edge detection? Moreover, can edge detectors be trained entirely without human supervision?  The only input to our approach is semi-dense matching results from <ref type="bibr" target="#b41">[42]</ref>. During training we alternate between: (1) computing flow based on the matches and edge maps (initialized to simple gradients), <ref type="bibr" target="#b1">(2)</ref> computing motion edges from the flow fields (green: positive edge samples; blue: discarded motion edges), <ref type="bibr" target="#b2">(3)</ref> training an edge detector using the motion edges as supervision, and (4) recomputing image edges using the new detector. The process is iterated on a large corpus of videos leading to increasingly accurate flow and edges.</p><p>We propose to train edge detectors using motion in place of human supervision. Motion edges are a subset of image edges, see <ref type="figure">Figure 1</ref>. Therefore motion edges can be used to harvest positive training samples. On the other hand, locations away from motion edges may also contain image edges. Fortunately, as edges are sparse, simply sampling such locations at random can provide good negative training data with few false negatives. Thus, assuming accurate motion estimates, we can potentially harvest unlimited training data for edge detection.</p><p>While it would be tempting to assume access to accurate motion estimates, this is arguably an unreasonably strong requirement. Indeed, optical flow and edge detection are tightly coupled. Recently, <ref type="bibr">Revaud et al. proposed</ref> EpicFlow <ref type="bibr" target="#b30">[31]</ref>: given an accurate edge map <ref type="bibr" target="#b10">[11]</ref> and semidense matches between frames <ref type="bibr" target="#b41">[42]</ref>, EpicFlow generates a dense edge-respecting interpolation of the matches. The result is a state-of-the-art optical flow estimate.</p><p>This motivates our approach. We begin with only semidense matches between frames <ref type="bibr" target="#b41">[42]</ref> and a rudimentary knowledge of edges (simple image gradients). We then repeatedly alternate between computing flow based on the matches and most recent edge maps and retraining an edge detector based on signal obtained from the flow fields. Specifically, at each iteration, we first estimate dense flow fields by interpolating the matching results using the edge maps obtained from the previous iteration. Given a large corpus of videos, we next harvest highly confident motion edges as positives and randomly sample negatives, and use this data to train an improved edge detector. The process is iterated leading to increasingly accurate flow and edges. An overview of our method is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>We perform experiments with the Structured Edge (SE) <ref type="bibr" target="#b10">[11]</ref> and Holistic Edge (HE) <ref type="bibr" target="#b43">[44]</ref> detectors. SE is based on structured forests, HE on deep networks; SE is faster, HE more accurate. Both detectors achieve state-ofthe-art results. The main result of our paper is that both methods, trained using our unsupervised scheme, approach the level of performance of fully supervised training. Finally, we demonstrate that our approach can serve as a novel unsupervised pre-training scheme for deep networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref>. Specifically, we show that when finetuning a network for object detection <ref type="bibr" target="#b11">[12]</ref>, starting with the weights learned for edge detection improves performance over starting with a network with randomly initialized weights. While the gains are modest, we believe this is a promising direction for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Edge Detection: Early edge detectors were manually designed to use image gradients <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> and later texture gradients <ref type="bibr" target="#b1">[2]</ref>. Of more relevance to this work are edge detectors trained in a data-driven manner. Since the work of <ref type="bibr" target="#b9">[10]</ref>, which formulated edge detection as a binary classification problem, progressively more powerful learning paradigms have been employed, including multi-class classification <ref type="bibr" target="#b22">[23]</ref>, feature learning <ref type="bibr" target="#b29">[30]</ref>, regression <ref type="bibr" target="#b34">[35]</ref>, structured prediction <ref type="bibr" target="#b10">[11]</ref>, and deep learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b43">44]</ref>. Recently, Weinzaepfel et al. <ref type="bibr" target="#b42">[43]</ref> extended <ref type="bibr" target="#b10">[11]</ref> to motion edge estimation. These methods all require strong supervision for training. In this work we explore whether unsupervised learning can be used instead (and as discussed select <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44]</ref> for our experiments).</p><p>Optical Flow: The estimation of optical flow is a classic problem in computer vision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>. A full overview is out-side of our scope, instead, our work is most closely related to methods that leverage sparse matches or image edges for flow estimation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b30">31]</ref>. In particular, as in <ref type="bibr" target="#b30">[31]</ref>, we use edge-respecting sparse-to-dense interpolation of matches to obtain dense motion estimates. Our focus, however, is not on optical flow estimation, instead, we exploit the tight coupling between edge and flow estimation to train edge detectors without human supervision.</p><p>Perceptual Grouping using Motion: Motion plays a key role for grouping and object recognition in the human visual system <ref type="bibr" target="#b20">[21]</ref>. In particular, Ostrovsky et al. <ref type="bibr" target="#b26">[27]</ref> studied the visual skills of individuals recovering from congenital blindness and showed that motion cues were essential to help facilitate the development of object grouping and representation. Our work is inspired by these findings: we aim to learn an edge detector using motion cues.</p><p>Learning from Video: There is an emerging interest for learning visual representations using video as a supervisory signal, for example by enforcing that neighboring frames have a similar representation <ref type="bibr" target="#b25">[26]</ref>, learning latent representations for successive frames <ref type="bibr" target="#b37">[38]</ref>, or learning to predict missing or future frames <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>. Instead of simply enforcing various constraints on successive video frames, Wang and Gupta <ref type="bibr" target="#b40">[41]</ref> utilized object tracking and enforce that tracked patches in a video should have a similar visual representation. The resulting network generalizes well to surface normal estimation and object detection. As we will demonstrate, our approach can also serve as a novel unsupervised pre-training scheme. However, while in previous approaches the training objective was used as a surrogate to encourage the network to learn a useful representation, our primary goal is to train an edge detector and the learned representation is simply a useful byproduct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Edges from Video</head><p>We start with a set of low level cues using standard tools in computer vision, including point correspondences and image gradients. We use DeepMatching <ref type="bibr" target="#b41">[42]</ref> to obtain semi-dense matches M between two consecutive frames (I, I ′ ). DeepMatching computes correlations at different locations and scales to generate the matches. Note that contrary to its name, the method involves no deep learning. For the rest of the paper, we fix the matching results M .</p><p>Our proposed iterative process is described in <ref type="figure" target="#fig_1">Figure 2</ref> and Algorithm 1. We denote the edge detector at iteration t by E t . For each image I j , we use E t j and G t j to denote its image edges and motion edges at iteration t. We initialize E 0 j to the raw image gradient magnitude of I j , defined as the maximum gradient magnitude over color channels. The gradient magnitude is a simple approximation of image edges, and thus serves as a reasonable starting point.</p><p>At each iteration t, we use EpicFlow <ref type="bibr" target="#b30">[31]</ref> to generate edge-preserving flow maps F t j given matches M j and pre-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Iterative Learning Procedure</head><p>Require: Pairs of frames (Ij, I ′ j ), matches Mj</p><formula xml:id="formula_0">1: E 0 = gradient magnitude operator, E 0 j = E 0 (Ij) ∀j 2: for t in 1...T do 3: Estimate flow F t j using previous edge maps E t−1 j F t j = EpicF low(Ij, I ′ j , Mj, E t−1 j ) ∀j 4: Detect motion edges G t j by applying E t−1 to F t j G t j = E t−1 (F lowT oRgb(F t j )) ∀j 5:</formula><p>Train new edge detector E t using motion edges G t</p><formula xml:id="formula_1">j E t = T rainEdgeDetector({Ij, G t j }) 6:</formula><p>Apply edge detector E t to all frames</p><formula xml:id="formula_2">E t j = E t (Ij) ∀j 7: end for 8: return E T and {E T j , F T j , G T j } vious edges E t−1 j</formula><p>. We next apply E t−1 on a colored version of F t j to get an estimate of motion edges G t j . G t j is further refined by aligning to superpixel edges. Next, for training our new edge detector E t , we harvest positives instances using a high threshold on G t j and sample random negatives away from any motion edges.</p><p>The above process is iterated until convergence (typically 3 to 4 iterations suffice). At each iteration the flow F t j and edge maps E t j and G t j improve. In the following sections we describe the process in additional detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method Details</head><p>EpicFlow: EpicFlow <ref type="bibr" target="#b30">[31]</ref> takes as input an image pair (I, I ′ ), semi-dense matches M between the images, and an edge map E for the first frame. It efficiently computes approximate geodesic distance defined by E between all pixels and matched points in M . For every pixel, the geodesic distance is used to find its K nearest matches, and the weighted combination of their motion vectors determines the source pixel's motion. A final optimization is performed by a variational energy minimization to produce an edge-preserving flow map with high accuracy. We refer readers to <ref type="bibr" target="#b30">[31]</ref> for additional details.</p><p>Motion Edge Detection: Detecting motion edges given optical flow estimates can be challenging, see <ref type="figure" target="#fig_2">Figure 3</ref>. Weinzaepfel et al. <ref type="bibr" target="#b42">[43]</ref> showed that simply computing gradients over a flow map produces unsatisfactory results and instead proposed a data-driven approach for motion edge detection (for a full review of earlier approaches see <ref type="bibr" target="#b42">[43]</ref>). In this work we employ a simpler yet surprisingly effective approach. We use an edge detector trained on image edges for motion edge estimation by applying the (image) edge detector to a color-coded flow map. The standard colorcoding scheme for optical flow maps 2D flow vectors into a 3D color space by encoding flow orientation via hue and magnitude via saturation <ref type="bibr" target="#b3">[4]</ref>. Motion edges become clearly visible in this encoding (3b) (we tried other color spaces but HSV worked best). Running an edge detector E on the colored flow map gives us a simple mechanism for motion edge detection (3c). Moreover, in our iterative scheme, as both our edge detector E t−1 and flow estimate F t improve with each iteration t, so do our resulting estimates of motion edges G t = E t−1 (F lowT oRgb(F t )).</p><p>Motion Edge Alignment: Motion edges computed from flow exhibit slight misalignment with their corresponding image edges. We found that this can adversely affect training, especially for HE which produces thick edges. To align the motion edges we apply a simple heuristic: after applying non-maximum suppression and thresholding, we align the motion edges to superpixels detected in the color image. Specifically, we utilize SLIC super-pixels <ref type="bibr" target="#b0">[1]</ref>, which cover over 90% of all image edges, and match motion and superpixel edge pixels using bipartite matching (also used in BSDS evaluation) with a tolerance of 3 pixels. Matched motion edge pixels are shifted to the superpixel edge locations and unmatched motion edges are discarded. This refinement, illustrated in <ref type="figure" target="#fig_2">Figure 3d</ref>, helps to filter out edges with weak image gradients and improves localization.</p><p>We emphasize that our goal is not to detect all motion edges. A subset with high precision is sufficient for training. Given a large video corpus, high-precision motion edges should provide a dense coverage of image edges. However, due to our alignment procedure our sampling is slightly biased. In particular, motion edges with weak corresponding image edges are often missing. This limitation and its impact on performance is discussed in Section 4.</p><p>Training E: The aligned motion edge maps G t serve as a supervisory signal for training an edge detector E t . Positives are sampled at locations with high scoring motion edges in G t . Negatives are uniformly sampled from location with motion edges below a small threshold. Note that locations with ambiguous motion edge presence (G t with intermediate scores) are not considered in training. As we will demonstrate, samples harvested in this manner provide a strong supervisory signal for training E. Video Dataset: For training, we combine videos from two different datasets: the Video Segmentation Benchmark (VSB) <ref type="bibr" target="#b15">[16]</ref> and the YouTube Object dataset <ref type="bibr" target="#b27">[28]</ref>. We use all HD videos (100 + 155) in both datasets. We drop all the annotations for YouTube object dataset. This collection of videos (∼250) contains more than 500K frames and has sufficient diversity for training an edge detector.</p><p>Frame Filtering: Given the vast amount of available data, we apply a simple heuristic to select the most promising frames for motion estimation. We first fit a homography matrix between consecutive frames using ORB descriptor matches <ref type="bibr" target="#b31">[32]</ref> (which are fast to compute). We then remove frames with insufficient matches, very slow motion (max displacement &lt;2 pixels), very large motion (average displacement &gt;15 pixels), or a global translational motion. These heuristics remove frames where optical flow may be either unreliable or contain few motion edges. For all experiments we used this pruned set of ∼50K frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Edge Detector Details</head><p>We experiment with the Structured Edge (SE) <ref type="bibr" target="#b10">[11]</ref> and Holistic Edge (HE) <ref type="bibr" target="#b43">[44]</ref> detectors, based on structured forests and deep networks, respectively. SE has been used extensively due to its accuracy and speed, e.g. for flow estimation <ref type="bibr" target="#b30">[31]</ref> and object proposals <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b2">3]</ref>. HE is more recent but achieves the best reported results to date. When trained using our unsupervised scheme, both methods approach similar performance as when trained with full supervision.</p><p>Structured Edges (SE): SE extracts low-level image features, such as color and gradient channels, to predict edges. The method learns decision trees by using structured labels (patch edge maps) to determine the split function at each node. During testing, each decision tree maps an input patch to a local edge map. The final image edge map is the average of multiple overlapped masks predicted by each tree at each location, leading to a robust and smooth result. We use the same parameters as in <ref type="bibr" target="#b10">[11]</ref> for training. The forest has 8 trees with maximum depth of 64. Each tree is trained using a random subset (25%) of 10 6 patches, with equal number of positives and negatives. During training, we convert a local edge map to a segmentation mask as required by SE by computing connected components in the edge patch. We discard patches that contain edge fragments that do not span the whole patch (which result in a single connected component). During each iteration of training, the forest is learned from scratch. During testing, we run SE over multiple scales with sharpening for best results.</p><p>Holistic Edges (HE): HE uses a modified VGG-16 network <ref type="bibr" target="#b33">[34]</ref> with skip-layer connections and deep supervision <ref type="bibr" target="#b21">[22]</ref>. Our implementation generally follows <ref type="bibr" target="#b43">[44]</ref>. We remove all fully connected layers and the last pooling layer, resulting in an architecture with 13 conv and 4 max pooling layers. Skip-layers are implemented by attaching linear classifiers (1 × 1 convolutions) to the last conv layer of each stage, their outputs are averaged to generate the final edge map. In our implementation, we remove the deep supervision (multiple loss functions attached to different layers) as we found that a single loss function has little performance penalty (.785 vs .790 in ODS score) but is easier to train.</p><p>We experimented with both fine-tuning a network pretrained on ImageNet <ref type="bibr" target="#b32">[33]</ref> and training a network from scratch (random initialization). For fine-tuning, we use the same hyper-parameter as in <ref type="bibr" target="#b43">[44]</ref> with learning rate 1e−6, weight decay .0002, momentum .9, and batch size 10. When training from scratch, we add batch normalization <ref type="bibr" target="#b18">[19]</ref> layers to the end of every conv block. This accelerates training and also improves convergence. We also increase learning rate (1e−5) and weight decay (.0005) when training from scratch. We train the network for 40 epochs in each iteration, then reduce learning rate by half. Unlike for SE, we can reuse the network from previous iterations as the starting point for each subsequent iteration.</p><p>The somewhat noisy labels, in particular missing positive labels, prove to be challenging for training HE. The issue is partially alleviated by discarding ambiguous samples during back propagation. Furthermore, unlike in <ref type="bibr" target="#b43">[44]</ref>, we randomly select negative samples (40× as many negatives as positives) and discard negatives with highest loss (following the same motivation as in <ref type="bibr" target="#b36">[37]</ref>). Without these steps for dealing with noisy labels convergence is poor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>Our method produces motion edges G t , image edges E t , and optical flow F t at each iteration t. We provide an extensive benchmark for each task tested with two different edge detectors (SE and HE). Our main result is that the image edge detectors, trained using videos only, achieve comparable results as when trained with full supervision. As a byproduct of our approach, we also generate competitive optical flow and motion edge results. Finally, we show that pre-training networks using video improves their performance on object detection over training from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motion Edge Detection</head><p>While our focus is not on motion edge detection, identifying motion edges reliably is important as motion edges serve as our only source of supervision. Thus our first experiment is to benchmark motion edges. We use the Video Segmentation Benchmark (VSB) <ref type="bibr" target="#b15">[16]</ref> which has annotated ground truth motion edges every 20 frames. We report results on the 282 annotated frames in the test set (we remove frames without motion edges and the final frame of each video as <ref type="bibr" target="#b42">[43]</ref> requires 3 frames). We evaluate using three standard metrics <ref type="bibr" target="#b1">[2]</ref>: fixed contour threshold (ODS), per-image best threshold (OIS), and average precision (AP). As we are concerned about the high precision regime, we introduce an additional measure: precision at 20% recall (P20). Non-maximum suppression is applied to all motion edges prior to evaluation.</p><p>In <ref type="table">Table 1</ref> we report results of four baselines and the motion edges G T obtained from the final iteration of our approach (SE/HE-VIDEO). The baselines include: image edges (SE/HE-IMAGE), gradient magnitude of optical flow (EPICFLOW), a method which combines superpixel segmentation with motion cues (GALASSO <ref type="bibr" target="#b15">[16]</ref>), and a recent datadriven supervised approach (WEINZAEPFEL <ref type="bibr" target="#b42">[43]</ref>).</p><p>Our method, albeit simple, has a precision .66∼.67 at 20% recall, only slightly worse than <ref type="bibr" target="#b42">[43]</ref>, even though it was not trained for motion edge detection. It substantially outperforms all other baselines in the high precision regime. While our goal is not motion edge detection per-se, this result is important as it enables us to obtain high quality positive samples for training an image edge detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Edge Detection</head><p>We next investigate edge detection performance. Results are reported on the Berkeley Segmentation Dataset and Benchmark (BSDS) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2]</ref>, composed of 200 train, 100 validation, and 200 test images. Each image is annotated with ground truth edges. We again evaluate accuracy using the same three standard metrics: ODS, OIS and AP.</p><p>Can an image edge detector be trained using motion edges? Our first experiment tests this question. We use all ground truth motion edges available in VSB (591 images) to train both SE and HE. The results are reported in <ref type="table">Table 2</ref> (SE-VSB, HE-VSB). For both methods, results are within 2-4 points ODS compared to training with image edge supervision (SE-BSDS, HE-BSDS). Our results suggest that using motion edges to learn an image edge detector is feasible.</p><p>We next present results using videos as the supervi-  As these results show, using video supervision achieves competitive results (within 3-5%). Interestingly, learning from video slightly outperforms training using the ground truth motion edges. We attribute this to the small size of VSB. For HE, we experiment with starting with an ImageNet pre-trained model (HE) and training from scratch (HE † ). Pre-training on ImageNet benefits HE across all training scenarios. This is encouraging as it implies that object level knowledge is useful for edge detection. On the other hand, our video supervision scheme also benefits from ImageNet pre-training, thus implying that in our current setup we are not training the model to its full potential.</p><p>To probe how performance evolves, we plot the ODS scores at each iteration for both methods in <ref type="figure" target="#fig_5">Figure 4</ref>. Raw image gradient at iteration 0 has ODS of .543 (not shown). Our iterative process provides a significant improvement from the image gradient, with most of the gains coming in the first iteration. Performance saturates after about 4 iterations (for the last iteration, we use 4 million samples for SE and 80 epochs for HE, which slightly increases accuracy).</p><p>We provide visualizations of edge results (before NMS) in <ref type="figure">Figure 5</ref>. SE misses some weak edges but edges are well aligned to the image content. HE generally produces thicker edges due to use of downsampled conv feature maps which makes it difficult to produce sharp image edges. HE-VIDEO/HE † -VIDEO results have thinner edges than HE-BSDS/HE † -BSDS, potentially due to the sampling strategy used for training with motion edges. When training using video, we also observe that the edge detection output is less well localized and more likely to miss weak edges, which likely accounts for much of the performance differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optical Flow</head><p>We benchmark optical flow results on the Middlebury <ref type="bibr" target="#b3">[4]</ref> and MPI Sintel <ref type="bibr" target="#b6">[7]</ref> datasets. Middlebury is widely used and   consists of complex motions with small displacements. Sintel is obtained from animated sequences and features large displacements and challenging lighting conditions. We use the 'final' version of Sintel and test on the train set with public ground truth. As our goal is to test the quality of generated edge maps, we focus only on variants of EpicFlow <ref type="bibr" target="#b30">[31]</ref>, the highest performing method on Sintel as of CVPR 2015. <ref type="table">Table 3</ref> shows the average endpoint error (AEE) of EpicFlow using different edge maps for Sintel and Middleburry. Most edge maps give rise to relatively similar results (AEE around 3.6∼3.8) on Sintel. In particular, originally EpicFlow used SE-BSDS edges; the results with SE-VIDEO edges are nearly identical. Top results are obtained with HE-BSDS, while HE-VIDEO and HE † -VIDEO are slightly worse. On Middleburry the method rankings are similar.</p><p>As an upper bound, we also include EpicFlow given ground truth (GT) motion edges (derived from the ground truth flow). Accuracy is only slightly better than with the best learned edge maps. This suggests that the performance of EpicFlow is saturated given the current matches.</p><p>Finally, in <ref type="figure" target="#fig_5">Figure 4</ref> we plot AEE on Sintel for each iteration. All methods improve over the initial flow (AEE 4.016, not shown) and results again saturate after a few iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Object Detection</head><p>Finally, we test whether our unsupervised training scheme for edge detection can be used to pre-train a network for object detection. The question of whether strong supervision is necessary for learning a good visual representation for object detection is of much recent interest <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref>. While not the focus of our work, we demonstrate that our scheme can likewise be used for network initialization.</p><p>For these experiments, we use the HE † edge detector (without ImageNet pre-training). We perform experiments using PASCAL VOC 2007 <ref type="bibr" target="#b11">[12]</ref> and the Fast R-CNN <ref type="bibr" target="#b16">[17]</ref>  object detector with proposals from <ref type="bibr" target="#b38">[39]</ref>. Results are evaluated by mean Average Precision (mAP). We compare results using two networks, VGG <ref type="bibr" target="#b33">[34]</ref> and ZF <ref type="bibr" target="#b44">[45]</ref>, and four pretraining schemes: ImageNet pre-training; no pre-training; pre-trained on BSDS (HE † -BSDS), and pre-trained using video (HE † -VIDEO). All networks are fine-tuned using the train-val set for 40K iterations (120K iterations when training from scratch). Results are summarized in <ref type="table">Table 4</ref>. VGG Results: We attempted to train VGG <ref type="bibr" target="#b33">[34]</ref> from scratch on VOC (with various learning rates plus batch normalization and dropout) but failed to obtain meaningful results. Even after 120K iterations detection performance remains poor (∼15 mAP). When the network is pre-trained on BSDS for edge detection, we were able to achieve 42.1 mAP on PASCAL. Interestingly, when training using video, we observe a further 2 point improvement in mAP (even though the same network is inferior for edge detection).</p><p>ZF Results: We also experimented with training a smaller ZF network <ref type="bibr" target="#b44">[45]</ref> which has only 5 convolutional layers. We modify the network slightly for edge detection to facilitate the alignment between outputs from different layers 2 . With ImageNet pre-training, Fast R-CNN with our modified ZF network achieves 58.6 mAP on PASCAL, which is close to the ZF result originally reported in <ref type="bibr" target="#b16">[17]</ref>. With no pre-training, mAP drops to 38.2. Pre-trained for edge detection, either with or without supervision, improves results by ∼3 mAP over training from scratch.</p><p>Overall we conclude that pre-training for edge detection (with or without supervision) improves the performance of training an object detector from scratch. However, Ima-geNet pre-training still achieves substantially better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Limitations</head><p>Why doesn't unsupervised training outperform supervised training for edge detection? In theory, a sufficiently large corpus of video should provide an unlimited training set and an edge detector trained on this massive corpus should outperform the much smaller supervised training set. However, there are a number of issues that currently limit performance. (1) Existing flow methods are less accurate at weak image edges, in addition, our alignment scheme also removes weak edges. Thus weak image edges are missing from our training set. (2) Further improving image edges does not improve optical flow, see <ref type="table">Table 1</ref>. We conjecture that the matches between frames are the limiting factor for EpicFlow and until these are improved neither optical flow nor edges will improve in the current scheme. (3) Training is harmed by noisy labels, and in particular, the missing positive labels. The false negatives, if not handled properly, tend to dominate the gradients in the late stages of training.</p><p>Is the unsupervised learning scheme capturing objectlevel information? The extent of an object is defined by its edges and conversely many edges can only be identified with knowledge of objects. Our results on both edge and object detection support this connection: on one hand, ImageNet pre-training is useful for edge detection, possibly because it injects object-level information into the network. On the other hand, pre-training a network for edge detection also improves object detection. In principle, an edge detection network has to learn high-level shape information, which might explain the effectiveness of pre-training. However, we observe that pre-training on ImageNet still benefits edge detection under all scenarios; moreover, Ima-geNet pre-training is still substantially better than video pre-training for object detection. Hence, perhaps unsurprisingly, the current unsupervised scheme is not as effective as ImageNet pre-training at capturing object level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this work, we proposed to harvest motion edges to learn an edge detector from video without explicit supervision. We developed an iterative process that alternated between updating optical flow using edge results, and learning an edge detector based on the flow fields, leading to increasingly accurate edges and flows.</p><p>The main result of our paper is that edge detectors, trained using our unsupervised scheme, approach the same level of performance as fully supervised training.</p><p>We additionally demonstrated our approach can serve a novel unsupervised pre-training scheme for deep networks. While the gains from pre-training were modest, we believe this is a promising direction for future exploration.</p><p>In the long run we believe that unsupervised learning of edge detectors has the potential to outperform supervised training as the unsupervised approach has access to unlimited data. Our work is the first serious step in this direction. . Illustration of edge detection results on five sample images (same as used in <ref type="bibr" target="#b10">[11]</ref>). The first two rows show the original image and ground truth. The second and third rows are results from SE, trained using BSDS or VIDEO. The remaining rows show the results of variants of HE on BSDS or VIDEO. HE † indicates that the network is trained from scratch. Use viewer zoom functionality to see fine details.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The only input to our approach is semi-dense matching results from [42]. During training we alternate between: (1) computing flow based on the matches and edge maps (initialized to simple gradients), (2) computing motion edges from the flow fields (green: positive edge samples; blue: discarded motion edges), (3) training an edge detector using the motion edges as supervision, and (4) recomputing image edges using the new detector. The process is iterated on a large corpus of videos leading to increasingly accurate flow and edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of motion edge detection. (a) Input images. (b) Colorized EpicFlow results F on the input images. (c) Motion edges G computed by applying an edge detector E to the colorized flow. (d) Motion edges G after alignment, non-maximum suppression, and aggressive thresholding. The aligned motion edge map G serves as a supervisory signal for training an edge detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Convergence of ODS and AEE over iterations. See text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 5. Illustration of edge detection results on five sample images (same as used in [11]). The first two rows show the original image and ground truth. The second and third rows are results from SE, trained using BSDS or VIDEO. The remaining rows show the results of variants of HE on BSDS or VIDEO. HE † indicates that the network is trained from scratch. Use viewer zoom functionality to see fine details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. Motion edge results on the VSB benchmark. See text.</figDesc><table>Method 
ODS OIS AP P20 

HUMAN 

.63 
.63 
-
-

SE-IMAGE 

.45 
.48 
.33 .39 

HE-IMAGE 

.47 
.52 
.35 .49 

EPICFLOW 

.39 
.47 
.33 .55 
GALASSO [16] 
.34 
.43 
.23 .34 
WEINZAEPFEL [43] 
.53 
.55 
.37 .71 

SE-VIDEO 

.44 
.48 
.34 .67 

HE-VIDEO 

.45 
.47 
.32 .66 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Table 4. Object detection results (mean AP) on PASCAL VOC 2007 test using VGG (left) and ZF (right). See text for details.</figDesc><table>pre-training mAP 
IMAGENET 
66.9 

NONE 

15.6 

HE 

 † -BSDS 
42.1 

HE 

 † -VIDEO 
44.2 

pre-training mAP 
IMAGENET 
58.6 

NONE 

38.2 

HE 

 † -BSDS 
41.4 

HE 

 † -VIDEO 
41.1 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Human annotation of edge structure in local patches (without context) is quite noisy and is matched by machine vision approaches. Humans excel when given context and the ability to reason about object presence<ref type="bibr" target="#b46">[47]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We change kernel size of all pooling layers to 2 and modify padding to 3 and 2 in conv1 and conv2, respectively. We also attach classifiers (1 × 1 convs) on all conv layers and up-sample and merge the results into a single edge map as in<ref type="bibr" target="#b43">[44]</ref>. Finally, when training from scratch, we add batch normalization after every conv layer. The ZF network has an ODS of .715 when trained using BSDS and .681 when trained using videos.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Saining Xie for help with the HE detector and Ahmad Humayun, Yan Zhu, and Yuandong Tian and many others for valuable discussions and feedback. The work was conducted when Yin Li was an intern at FAIR. Yin Li gratefully acknowledges the support of the Intel ISTC-PC while completing the writing of the paper at Georgia Tech.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A database and evaluation methodology for optical flow. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-for-low and low-for-high: Efficient boundary detection from deep object feat. and its app. to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised learning of edges and object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Groups of adjacent contour segments for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fevrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the quantitative evaluation of edge detection schemes and their comparison with human performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Fram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Deutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TOC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The design and use of steerable filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical symposium east. International Society for Optics and Photonics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual boundary prediction: A deep neural prediction network and quality dissection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Principles of Gestalt psychology. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koffka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual parsing after recovery from blindness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminatively trained sparse code gradients for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liefeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ORB: an efficient alternative to SIFT or SURF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiscale centerline detection by learning a scale-space distance transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">high-quality object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal feat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selective search for object recog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Recognition by linear combinations of models. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to Detect Motion Boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The role of image understanding in contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
