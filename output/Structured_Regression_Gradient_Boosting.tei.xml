<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured Regression Gradient Boosting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Diego</surname></persName>
							<email>ferran.diego@iwr.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Heidelberg Collaboratory for Image Processing (HCI) Interdisciplinary Center for Scientific Computing (IWR)</orgName>
								<orgName type="institution">University of Heidelberg</orgName>
								<address>
									<postCode>69115</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
							<email>fred.hamprecht@iwr.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Heidelberg Collaboratory for Image Processing (HCI) Interdisciplinary Center for Scientific Computing (IWR)</orgName>
								<orgName type="institution">University of Heidelberg</orgName>
								<address>
									<postCode>69115</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structured Regression Gradient Boosting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new way to train a structured output prediction model. More specifically, we train nonlinear data terms in a Gaussian Conditional Random Field (GCRF) by a generalized version of gradient boosting. The approach is evaluated on three challenging regression benchmarks: vessel detection, single image depth estimation and image inpainting. These experiments suggest that the proposed boosting framework matches or exceeds the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many problems in machine learning involve the prediction of outputs that are not a single value, but a more complicated object like a sequence, a graph or an image. Such problems are referred to as structured output prediction <ref type="bibr" target="#b25">[26]</ref>. Markov random fields have become popular for structured prediction thanks to their ability to exhibit desirable global behavior based on the specification of local or sparse interactions only. For generative models, the parameters can be found by maximum likelihood <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref>; and for both generative and conditional models by discriminative training <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>. The latter is generally found to work better except when training data is extremely scarce. This success is at least partially attributed to the fact that such models learn to minimize the loss function of interest for a specific task, rather than solve a more general problem, namely learning to generate images. The most frequent approach to discriminative learning of structured models is via max margin (structSVM, <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b44">45]</ref>). One limitation of structSVMs is that the model must be linear in its parameters in a joint feature space. Recent alternatives include regression tree fields <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, where nonparametric potentials are learned by minimizing a loss function using a projected gradient method.</p><p>In this paper we propose, for the first time, a generalization of gradient boosting that allows directly training a Given input x i and structured ground truth y i , we iterate the following: given the current prediction ϕ m−1 (x i ), compute gradient r i,m of the structured loss. Next, train shallow regression trees that produce good fits b k,m to the structured loss gradient image and filtered versions thereof. Use these fits to parametrize the data terms of a Gaussian conditional random field. Its MAP solution h m (x i ) is added to the current strong structured learner with a weight αm found through line search. Repeat M times.</p><p>structured regression model and exploit all its benefits for structured regression tasks in a principled way (Sect. 2). The procedure, summarized in <ref type="figure" target="#fig_0">Fig. 1</ref> and Algorithm 1, is described in detail in section 2. In section 3, we study performance on a number of benchmarks for which many papers have previously pushed the accuracy to levels that are remarkable given the difficulty of those problems. We close with a brief summary in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Discrete and continuous conditional random fields (CRFs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12]</ref> are structured prediction models that model explicitly the relation among latent variables. Learning the optimal parameters of these models has been studied extensively in the literature. StructSVMs <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b44">45]</ref> have been applied to learn the parameters of a linear joint feature function over the input-output pairs while maximizing a margin <ref type="bibr" target="#b41">[42]</ref>. In contrast, non-linear mappings on joint feature functions have been learnt using gradient boosting while minimizing the negative conditional loglikelihood <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref>. Following the idea of discriminative learning, few methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> have been proposed for structured regression that share only partial ideas with the Gradient Boosting framework. Ratliff et al. <ref type="bibr" target="#b29">[30]</ref> add a new feature as a nonlinear function of the original base features using a weak classifier trained from a previous prediction. Parker et al. <ref type="bibr" target="#b28">[29]</ref> instead reformulates the structured perceptron to reweight the training set at each iteration as in AdaBoost. StructBoost <ref type="bibr" target="#b36">[37]</ref> incorporates only the notion of weak structured predictors into structSVM that are generated by finding the most violated constraint. In contrast, the proposed method is the unique method that extends gradient boosting framework to structured output prediction as an ensemble of weak structured learners while keeping the original formulation as a specific case.</p><p>Closest in spirit to our work is StructBoost <ref type="bibr" target="#b36">[37]</ref> which supports a nonlinear structured learning by combining a set of weak non-linear structured learners. This approach differs from the proposed approach in the following points. First, StructBoost generalizes AdaBoost or LPBoost to structured learning; our approach is based on gradient boosting which is in itself a generalization of Adaboost. Second, the weak structured learner of StructBoost is a function that maps an input-output pair to a scalar value that measures the compatibility of the input and output. Instead, our proposed weak structured learner maps the observed variable to a structured output, not just a compatibility measure. Third, StructBoost is formulated to maximize a margin; in contrast, our method minimizes an empirical risk without maximizing the margin as in GCRFs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43]</ref> that are the state-of-the-art on some tasks such as image denoising. Finally, our proposed weak learner extends GCRF <ref type="bibr" target="#b32">[33]</ref> to be more expressive by generalizing the data term as a set of convolutional kernels and by learning a nonparametric regression tree used in <ref type="bibr" target="#b11">[12]</ref>.</p><p>The term "Structured Gradient Boosting" has been used before in <ref type="bibr" target="#b28">[29]</ref>, though with a completely different goal: learning the parameters of structured perceptron algorithm <ref type="bibr" target="#b4">[5]</ref> that is reformulated to reweight the training set at each iteration as in AdaBoost; there, the structure prediction is relegated to the structured perceptron without learning any weak learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We build on the gradient boosting framework, but unlike the original formulation use structured output weak learners. Gradient boosting aims at approximating a function φ * : R p → R by a linear combination of weak learners. Instead, we propose Structured Regression Gradient Boosting (SRGB) to approximate a mapping ϕ * : R p → R q by a function ϕ M of the form</p><formula xml:id="formula_0">ϕ M (x) = M t=1 α t h t (x).<label>(1)</label></formula><p>Here, α t ∈ R are real-valued weights, h t : R p → R q are weak structured regression predictors and x ∈ R p is the input vector, e.g. an observed image or all features derived from an image.</p><formula xml:id="formula_1">Given structured training exemplars { x i , y i } N i=1</formula><p>, where x i ∈ R p and y i ∈ R q , we aim to minimize an empirical risk function</p><formula xml:id="formula_2">L ϕ M = N i=1 L y i , ϕ M (x i ) .<label>(2)</label></formula><p>Here, L (y, ϕ(x)) is a loss function that is differentiable w.r.t. each dimension of the predicted structured output. The risk function L in <ref type="formula" target="#formula_2">(2)</ref> is minimized in a greedy manner by iteratively adding up weak structured regression predictors as in the Gradient Boosting framework <ref type="bibr" target="#b8">[9]</ref>. Other approaches such as structSVM endow the r.h.s. of Eq. (2) with an additional regularization term. In our case, regularization is accomplished by restrictions on the set of admissible weak learners, and by choosing a suitable number of iterations M . Boosting updates can be interpreted as first-order approximations of the gradient descent direction in function space <ref type="bibr" target="#b8">[9]</ref>. At each iteration m, we seek for the weak structured learner h m that best predicts the average (negative) gradient direction to minimize the empirical risk function L(ϕ m ).</p><p>Denote by</p><formula xml:id="formula_3">r i,m = − ∂L(y i , ϕ m−1 (x i )) ∂ϕ m−1 (x i )<label>(3)</label></formula><p>the column vector summarizing the negative gradient direction of the loss given the current prediction w.r.t. all dimensions of the current structured output prediction. We then want to solve, in each boosting iteration,</p><formula xml:id="formula_4">h m = argmin h(·)∈H N i=1 1 2 h(x i ) − r i,m 2 F<label>(4)</label></formula><p>where || · || F is the Frobenius norm. The weak learners h ∈ H in our case correspond to Gaussian Conditional Random Fields with potentials encoded by nonparametric regression trees (see Section 2.2). A nice feature of this choice is that the final predictor (the strong learner) is just a sum of GCRFs and is thus itself a single GCRF, affording very fast inference at test time.</p><p>An important contribution is the way in which the parameters of this GCRF are learned discriminatively, namely by simplifying the minimization of Eq. (4) in terms of an alternative quadratic formulation that has the same global minimum as the original formulation but which can be found in closed form, without resorting to projected gradient descent.</p><p>Once a weak learner h m has been found, its weight α m in the final predictor is found through line search:</p><formula xml:id="formula_5">α m = argmin α N i=1 L y i , ϕ m−1 (·) + αh m (x i ) .<label>(5)</label></formula><p>For the loss function, we adopt standard choices from Gradient Boosting, namely the exponential loss <ref type="bibr" target="#b1">2</ref> for regression. In these definitions, ϕ j (x i ) is the j th element of the structured prediction function ϕ(x i ), and y i j is the corresponding element of the structured training label y i .</p><formula xml:id="formula_6">L y i , ϕ(x i ) = q j=1 e −y i j ϕj (x i ) and the log loss L y i , ϕ(x i ) = q j=1 log(1 + e −2y i j ϕj (x i ) for binary clas- sification, or the l 2 -norm L y i , ϕ(x i ) = q j=1 (y i j − ϕ j (x i ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weak structured regressor: GCRF</head><p>Denote with x ∈ R p an observed image, and with z ∈ R p its corresponding labelling.</p><p>We model the conditional probability with a Gaussian random field with parameters W,</p><formula xml:id="formula_7">p(z|x; W) ∝ exp − 1 2 (z − µ(x)) T P (z − µ(x)) . (6)</formula><p>Note that in our experiments, we choose to let the precision matrix P be a constant, that is, independent of x. Even so, the model is a Gaussian Conditional (as opposed to a Gaussian Markov) Random Field because the observations x enter only via a nonparametric function µ(·). Inspired by <ref type="bibr" target="#b42">[43]</ref>, we express the connectivity of our conditional Markov random field in terms of convolution kernels {f k } K k=1 . Unlike <ref type="bibr" target="#b42">[43]</ref>, we generalize the data term by learning regression trees that approximate linear functions of the latent variables, given the observed image x. These linear operations include finite differences and shifts.</p><p>More specifically, we posit</p><formula xml:id="formula_8">P = K k=1 F T k F k and µ(x) = K k=1 F T k b k (x; W)<label>(7)</label></formula><p>so that the MAP solution of (6) can be found by solving</p><formula xml:id="formula_9">argmin z K k=1 F k · z − b k (x; W) 2 F<label>(8)</label></formula><p>All matrices F k are Toeplitz matrices representing convolution kernels in the spatial domain. As in <ref type="bibr" target="#b42">[43]</ref>, the first matrix F 1 is the identity matrix, while the others represent derivatives, see <ref type="figure">Fig. 2</ref>. In <ref type="bibr" target="#b42">[43]</ref>, b 1 (x; W) = x and b k&gt;1 (x; W) = 0. In contrast, we learn the functions {b k } K k=1 (see section 2.2) to match differentiated and shifted label images, see <ref type="figure">Fig. 3</ref>.</p><p>In matrix notation, the objective function from <ref type="formula" target="#formula_9">(8)</ref> becomes</p><formula xml:id="formula_10">      F 1 . . . F K    z −    b 1 . . . b K       T       F 1 . . . F K    z −    b 1 . . . b K       = (Fz − b) T (Fz − b)</formula><p>where F = [F 1 ; . . . ; F K ] stacks all convolution Toeplitz matrices column-wise and b = b 1 ; . . . ; b K does so for the filter responses. Eq. <ref type="formula" target="#formula_9">(8)</ref> can be solved in closed form: <ref type="figure">Figure 2</ref>. Example of the filter responses on r i,m = y i to be learnt by {b k (x)} 3 k=1 as described in Eq. (12) for identity and horizontal-and vertical-derivative filters, respectively. <ref type="figure">Figure 3</ref>. The filters used in the proposed model and grouped according to the neighborhood connectivity. Using only the identity filter amounts to the original gradient boosting framework (K = 1). It is here complemented with the first order vertical and horizontal finite differences (K = 3) plus diagonal finite differences (K = 5). In addition, adjacent labels can be queried by means of the shift operators (K = 13).</p><formula xml:id="formula_11">z(x; b) = F T F −1 F T b (9) F1r i,m F 2 r i,m F 3 r i,m f1 = I f2 = [−1, 1, 0] f3 = [−1, 1, 0] T</formula><formula xml:id="formula_12">. . . K=13 +1 +1 +1 +1 +1 +1 +1 +1 -1 -1 -1 -1 +1 K=1 K=5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning</head><p>We now show how to obtain one of the weak learners h m (x; b) in the form of a GCRF, which in turn depends on the response functions b = {b k,m (x)} K k=1 at the m th iteration:</p><formula xml:id="formula_13">h m , {b k,m } K k=1 = argmin b(·) ∈ B N i=1 1 2 h(x i ; b) − r i,m 2 F (10) s.t. h(x i ; b) = argmin z z T F T Fz − 2z T F T b(x) + const.,</formula><p>Note that Eq. (10) is a bilevel optimization problem; finding the best weak structured regressor requires also learning an estimated response function {b k,m } K k=1 that best predicts an underlying signal z close to the current gradient direction r i,m at each sample x i . Thanks to the closed-form solution from Eq. (9), Eq. <ref type="formula" target="#formula_0">(10)</ref> can be formulated as a direct optimization problem,</p><formula xml:id="formula_14">{b k,m } K k=1 = argmin b(·)∈B N i=1 1 2 F T F −1 F T b − r i,m 2 F .<label>(11)</label></formula><p>This optimization problem can be rewritten in terms of a different quadratic objective function with the same minimizer:</p><formula xml:id="formula_15">{b k,m } K k=1 = argmin b(·)∈B N i=1 1 2 b − Fr i,m 2 (12) = argmin b(·)∈B N i=1 1 2    b 1 . . . b K    −    F 1 . . . F K    r i,m 2 F .</formula><p>Gradient descent based techniques have been used to learn parametric or non-parametric functions potentials for a GCRF <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>. These techniques aim to find the global optimum of Eq. (11) with little or no restrictions on B. Instead, following the boosting paradigm, we optimize only over a narrow class B, in our case the class of shallow regression trees, to obtain weak learners. Limitations on tree depth are also used in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref> and are an alternative to early stopping of the gradient descent <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>In practice, we approximately minimize the objective in Eq. (12) by training K shallow regression trees that, given an input image or its features x, try to approximate the filtered negative gradient loss images associated with the training images in the least-squares sense. This is a simple problem which, in addition, parallelizes naively over the K subproblems.</p><p>See Algorithm 1 for a summary of all steps 1 .</p><p>1 A helpful video that shows the temporal evolution of the prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Inference</head><p>Given the learned real-valued weights {α t } M t=1 and the weak learners for each filter response {{b k,t } M t=1 } K k=1 , the global structured regression function ϕ becomes</p><formula xml:id="formula_16">ϕ M (x) = M t=1 α t F T F −1 F T b t (x)<label>(13)</label></formula><formula xml:id="formula_17">= F T F −1 F T M t=1 α m b t (x) .<label>(14)</label></formula><p>Computing Eq. <ref type="formula" target="#formula_0">(14)</ref> using direct methods is prohibitive due to the large number of variables (number of pixels). As a consequence, we solve the equivalent problem</p><formula xml:id="formula_18">F T F ϕ M (x) = F T M t=1 α t b t (x) by conjugate gra- dient descent instead.</formula><p>Note that at training time, the learning of the nonparametric filter responses entails solving M linear systems of equations; at test time, however, ϕ M (x) requires solving the linear system only once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training phase of Structured Regression Gradient Boosting (SRGB)</head><p>Input: Training samples and labels { x i , y i } N i=1 , number of filters K, number of iterations M , shrinkage factor 0 &lt; γ ≤ 1 Initialization: ϕ 0 (·) = 0 1: for m = 1, . . . , M do 2:</p><formula xml:id="formula_19">r i,m = − ∂L(y i ,ϕ m−1 (x i )) ∂ϕ m−1 (x i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for k = 1, . . . , K do 4:</p><p>Train shallow regression trees b k,m to match the kth filter response 5:</p><formula xml:id="formula_20">argmin b(·) N i=1 1 2 b(x i ) − F k r i,m 2 F 6: end for 7: h m (x; b) = F T F −1 F T b m (x) 8: α m = argmin α N i=1 L y i , ϕ m−1 (·) + αh m (x i ) 9:</formula><p>ϕ m (·) = ϕ m−1 (·) + γα m h m (·) 10: end for Output: ϕ M (·)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>The performance and the versatility of our method is evaluated on three different regression problems: blood vessel delineation in fundus images, depth estimation from single images, and Chinese character inpainting. Across the datasets, we provide comparison with baseline methods and with the state-of-the-art on those datasets. and the negative gradient direction at each iteration can be viewed at hci.iwr.uni-heidelderg. <ref type="bibr">de</ref>  <ref type="table">Table 2</ref>. Vessel segmentation. Performance for various choices of K (and thus implicitly for the connectedness of the Gaussian Conditional Random Field) and for different number of autocontext stages. In the first stage, we differentiate between learning independently a nonparametric regression tree for each of the filter responses and learning jointly the influence among filter responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vessel Segmentation</head><p>The first problem is the segmentation of blood vessels in retinal scans. We test our approach on the DRIVE dataset <ref type="bibr" target="#b40">[41]</ref>, which contains 20 training images and 20 test images of size 565 × 584 as well as the corresponding manual vessel segmentation and ROI masks.</p><p>We compare our approach with the latest and stateof-the-art methods: CS <ref type="bibr" target="#b30">[31]</ref>, Fully-Connected CRF <ref type="bibr" target="#b27">[28]</ref>, SE <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, KernelBoost <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, N 4 -fields <ref type="bibr" target="#b9">[10]</ref>, Learning Boost <ref type="bibr" target="#b10">[11]</ref> and NN-projections <ref type="bibr" target="#b37">[38]</ref>. The latter two approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b10">11]</ref> refine and enhance the segmentation given a base segmentation algorithm, in this case <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b2">[3]</ref>, respectively. Earlier, the features were learned discriminatively using the Gradient Boosting framework <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> or a neural network <ref type="bibr" target="#b9">[10]</ref>. Following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, we use hand-crafted features as implemented in <ref type="bibr" target="#b39">[40]</ref> that comprise smoothing filters, edge detectors and eigenvalues of the structure tensor at different scales (49 features in total). Finally, following KernelBoost <ref type="bibr" target="#b1">[2]</ref>, we implement the autocontext <ref type="bibr" target="#b47">[48]</ref> idea by training a cascade of 4 sequential Structured Regression Gradient Boosting predictors. Each predictor receives all the original features plus the output of the previous stage as input. The number of boosting iterations was set to M = 500, the tree depth is limited to 3 levels for each weak filter learner, the regularizing shrinkage factor (see Algorithm 1) is set to γ = 0.1 and the log-loss is used. All of the above are standard choices for gradient boosting.</p><p>Precision/recall curves for the baseline Gradient Boosting framework and the proposed method at different levels of the cascade are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>; while <ref type="table">Table 2</ref> shows the F-measure obtained for different relation among latent variables and for different number of cascade iterations. There is a clear advantage over the baseline at the first stage when neighboring information is considered. The increase in performance is reduced at the last stage, although our proposed approach requires fewer cascade stages to achieve a given performance level.   <ref type="figure">Figure 5</ref>. Results for the DRIVE dataset <ref type="bibr" target="#b40">[41]</ref> in the form of the recall/precision curves. Our approach is slightly above the performance of the current state-of-the-art methods by Ganin et al. <ref type="bibr" target="#b9">[10]</ref>, Becker et al. <ref type="bibr" target="#b2">[3]</ref> and Sironi et al. <ref type="bibr" target="#b37">[38]</ref>, and much better than <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b30">[31]</ref>. <ref type="table">Table 1</ref> and <ref type="figure">Fig. 5</ref> show the F-measure and the precision/recall curves obtained with the different methods, respectively. Our approach is slightly better than the state- <ref type="figure">Figure 6</ref>. Example of vessel segmentation on a region with complex topology. From left to right: raw image, ground truth; Kernel Boost <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, N 4 Fields <ref type="bibr" target="#b9">[10]</ref>, NN-Projections <ref type="bibr" target="#b37">[38]</ref> and Structured Regression Gradient Boosting. Arguably some of these predictions are more accurate than the ground truth. of-the-art N 4 fields <ref type="bibr" target="#b9">[10]</ref>, NN-Projection <ref type="bibr" target="#b37">[38]</ref> and Kernel-Boost <ref type="bibr" target="#b2">[3]</ref>, and outperforms the other competitors with only a single stage of the cascade because the first-round weak structured learners aim to learn coarse and rough details and the later ones aim to accurately learn finer details. Some qualitative results are shown in <ref type="figure" target="#fig_3">Fig. 7</ref>. Similarly to <ref type="bibr" target="#b37">[38]</ref>, <ref type="figure">Fig. 6</ref> shows the results on the same particularly complex region of a test image, with several thin junctions and low contrast, for N 4 fields <ref type="bibr" target="#b9">[10]</ref>, NN-Projection <ref type="bibr" target="#b37">[38]</ref> and Ker-nelBoost <ref type="bibr" target="#b2">[3]</ref>. Our approach is able to reconstruct correctly most of the topology of the blood network except for the junction of a tiny blurred vessel. Moreover, as can be seen from <ref type="figure">Fig. 6</ref>, our method predicts even better vessel locations than the ground truth in some part of the image and with a very high confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Depth Estimation</head><p>The second problem is the depth estimation from single image. We test our approach on the Make3D dataset <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, which contains 400 training images and 134 test images as well as the corresponding depth (we use the stan-dard training/test split provided with the dataset). Following <ref type="bibr" target="#b15">[16]</ref>, we resize the images to 345 × 460 pixels before training (maintaining the aspect ratio of the input images).</p><p>We rely on the main idea of depth transfer <ref type="bibr" target="#b15">[16]</ref> based on non-parametric learning <ref type="bibr" target="#b20">[21]</ref> to compute the features. The features consists on finding the top-N best candidate images in a similar database of the input image, and on using SIFT Flow <ref type="bibr" target="#b21">[22]</ref> to warp these images as well as the depths to the query image. Moreover, we extend our features by adding another "coarse" depth map obtained from DCNF <ref type="bibr" target="#b22">[23]</ref> without any inpainting enhancement. Finally, we train just a single Structured Regression Gradient Boosting, and use a square loss as an empirical risk. The number of boosting iterations M was set to 50 and the shrinkage is set to 0.1 but the tree depth is limited to 5 levels for each weak filter learner.</p><p>In <ref type="table" target="#tab_2">Table 3</ref> we compare our approach with different set of convolutional kernels and with state-of-the-art methods. From the table, we conclude that exploiting smooth prior as 4− and 8−neighborhood increases the baseline performance and it outperforms with a large margin if more non-Method Error rel log10 rms Depth MRF <ref type="bibr" target="#b33">[34]</ref> 0.530 0.198 -Make3D <ref type="bibr" target="#b35">[36]</ref> 0.370 0.187 -Semantic Labelling <ref type="bibr" target="#b19">[20]</ref> 0.379 0.148 -Laplace CRF <ref type="bibr" target="#b0">[1]</ref> 0  local information is used to predict the value of a current pixel. Moreover this framework allows to infer the depth by a non-linear combination of multiple observations as features and outperforms the state-of-the-art. Some examples of qualitative evaluations are shown in <ref type="figure">Fig. 8</ref>. It is shown that only Gradient Boosting model gives rather coarse predictions, but our model yields much better visualizations by adding smoothness term and more nonlocal prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Chinese character inpainting</head><p>The third problem is learning calligraphy properties for the reconstruction of the occluded parts of handwritten Chinese characters from the KAIST Hanja2 database <ref type="figure" target="#fig_4">(Fig. 9)</ref>. We used the original 300 training images and 100 test images in <ref type="bibr" target="#b26">[27]</ref>. Each character is occluded by a centered grey box of varying size. Following <ref type="bibr" target="#b26">[27]</ref>, the accuracy is measured on a dataset with small occlusions, and the predictions are visualized on images with a larger occlusion area. We differ from the DTF <ref type="bibr" target="#b26">[27]</ref> model slightly in terms of features and neighborhood. Instead of looking at most 80 pixels away, we restricted our search area to only 31 pixels away, but also including the difference between these two pixels. Moreover, we restricted the relation among latent variables to 4-and 8-neighbor connections, in this case K = 3 and K = 5 respectively. Finally, we train just a single Structured Regression Gradient Boosting, and use a log loss as a empirical risk. The number of boosting iterations M was set to 100 and the shrinkage is set to 0.1 but the tree depth is again limited to 3 levels for each weak filter learner.</p><p>The results are shown in <ref type="table">Table 4</ref>. We include the baseline decision tree result (DT) of <ref type="bibr" target="#b26">[27]</ref>, a tree ensemble result of 10 trees (RF) from <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref>, the MRF and DTF results taken from <ref type="bibr" target="#b26">[27]</ref>, the Regression Tree Field (RTF) approaches from <ref type="bibr" target="#b13">[14]</ref>, the convex quadratic relaxation approach (QP-M3N) of <ref type="bibr" target="#b12">[13]</ref>, and the structured local predictors (SLP) <ref type="bibr" target="#b31">[32]</ref> and structured labels in Random Forests (SLRF). Additionally, we compare the classifica- <ref type="figure">Figure 8</ref>. Examples of depth predictions on the Make3D dataset (Best viewed on screen). From left to right: original image, ground truth, gradient boosting baseline (K = 1), our gradient boosting with pairwise connectivity (K = 3) and our approach with pairwise connectivity and with the data term computed from a fully connected 5×5 neighborhood (K = 27). The gradient boosting gives rather coarse prediction mainly due to the coarse depth estimations used as features. In contrast, our full model yields much better predictions. tion results for Gradient Boosting and our proposed structured prediction. Our baseline method outperforms most of sophisticated state-of-the-art methods; thanks to learning how to perform the gradient descent for reconstructing the occluded parts. Hence, our proposed method that learns jointly the interaction among neighboring pixels is very competitive, and achieves the best result on this task. Following the works in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b16">17]</ref>, <ref type="figure" target="#fig_4">Fig. 9</ref> shows the some qualitative results obtained on the large occlusion dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Practicalities: choice of parameters</head><p>For vessel segmentation, we used the parameters published in <ref type="bibr" target="#b1">[2]</ref>.</p><p>For the other experiments, the number of iterations M was increased when the loss on the training set was found to be high (indicative of underfitting). The maximum depth of the regression trees was chosen depending on the dynamic range of the desired output. For targets in [−1, 1] a depth of 3 was used, and for a larger range a depth of 5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Outlook</head><p>We propose a structured regression gradient boosting method. Analogous to gradient boosting, a non-linear structured output regression is obtained as a combination of a set of weak structured learners, while the original formulation is kept in the framework as a specific case. Inspired by <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b11">12]</ref>, we use a GCRF as a weak structured learner, whose parameters are learnt discriminatively by means of nonparametric regression trees. Our proposed approach is comparable to and sometimes exceeds the state-of-the-art even with less feature tuning for three challenging benchmarks. We also observe that the proposed approach has improved performance over gradient boosting, demonstrating the usefulness of this nonlinear structured regression method.</p><p>Future work includes generalization to multiclass predictions, automated selection of the most useful filters F , learning of features and letting precision matrix P depend on the input.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Structured regression gradient boosting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Results for the DRIVE dataset<ref type="bibr" target="#b40">[41]</ref> in the form of the recall/precision curves. Our approach outperforms the baseline in the last stages of the cascade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Examples of vessel segmentation on the DRIVE dataset (best viewed on screen). From left to right: original image, Kernel Boost<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, N 4 Fields<ref type="bibr" target="#b9">[10]</ref>, NN-Projections<ref type="bibr" target="#b37">[38]</ref> and the method presented here. True positives are shown in black, false positives in green and false negatives in magenta. The human-generated ground truth is somewhat subjective, but is the only one available and is the standard by which all methods are measured. We closely emulate the human expert while reducing false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Chinese characters with large occlusions: inpainting result on test set. All the characters are also shown in<ref type="bibr" target="#b13">[14,</ref> Fig. 7].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>/Staff/fdiego/SRGB/.</figDesc><table>Method 

Prec. Recall 
F 
AUC 
CS [31] 
78.81 
74.74 
76.72 
84.12 
FC-CRF [28] 
79.10 
78.08 
78.55 
− 
Kernel Boost [2, 3] 
81.10 
79.75 
80.42 
88.84 
SE [8, 7] 
68.22 
65.33 
66.74 
70.70 
N 4 [10] 
80.41 80.76 80.58 
88.93 
Learning Boost [11] 79.57 
79.47 
79.51 
− 
NN Projections [38] 81.28 
79.95 
80.61 
84.97 
SRGB 
81.67 80.16 80.91 89.17 

Table 1. Vessel segmentation. Comparison to the state of the art 
on the DRIVE dataset. 

F-measure 
iter 0 
iter 1 
iter 2 
iter 3 
sep. 
joint 
K = 1 
78.82 
80.33 
80.62 
80.81 
K = 3 
79.53 78.93 80.59 
80.80 
80.86 
K = 5 
79.70 78.17 
80.57 
80.74 
80.75 
K = 13 
79.54 
77.93 80.68 80.89 80.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Depth Estimation. State-of-the-art and baseline compar-
isons on the Make3D dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>DT RF MRF DTF RTF 1D RTF 2D SLP QP M 3N SLRFTable 4. Chinese characters: accuracy for inpainting of small occlusions.</figDesc><table>SRGB 
SRGB 
SRGB 
(avg) [17] 
[17] 
[27] 
[27] 
[14] 
[14] 
[32] 
[13] 
[17] 
(K = 1) (K = 3) (K = 5) 
68.52 
74.95 75.18 76.01 
76.39 
77.55 
78.07 
79.36 
78.09 
77.66 
79.37 
79.87 

Input 
Truth 
RF 
MRF 
GMRF 
DTF 
RTF 
K = 1 
K = 3 
K = 5 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning the right model: Efficient max-margin learning in laplacian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kernel-Boost: Supervised Learning of Image Features For Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised Feature Learning for Curvilinear Structure Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient second-order gradient boosting for conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gradient Tree Boosting for Training Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashenfelter</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">N4-fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to boost filamentary structure segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Loss-specific training of non-parametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning convex qp relaxations for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regression tree fields: An efficient, non-parametric approach to image labeling problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient 3d scene labeling using fields of trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured labels in random forests for semantic labelling and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training conditional random fields using virtual evidence boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Nonparametric scene parsing via label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Advanced Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decision tree fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning fully-connected CRFs for blood vessel segmentation in retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Structured Gradient Boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosting Structured Prediction for Imitation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A D</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chestnutt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate and Efficient Linear Structure Segmentation by Leveraging Ad Hoc Features with Learned Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured local predictors for image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Held</surname></persName>
		</author>
		<title level="m">Gaussian Markov Random Fields: Theory and Applications. Monographs on Statistics and Applied Probability</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning 3-d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop on 3dRR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">StructBoost: Boosting methods for predicting structured output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Projection onto the Manifold of Elongated Structures for Accurate Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multiscale Centerline Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tretken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ilastik: Interactive learning and segmentation toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strähle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="230" to="233" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ridge based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning crfs using graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning gaussian conditional random fields for low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The logistic random field a convenient graphical model for learning parameters for mrf-based labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Lyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Contextual models for object detection using boosted random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Auto-context and its application to highlevel vision tasks and 3d brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to recognize shadows in monochromatic natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
