<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Dusk till Dawn: Modeling in the Dark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenović</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">CMP</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of North Carolina at Chapel Hill</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Eidgenössisch Technische Hochschule Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuang</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">CMP</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">CMP</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Dusk till Dawn: Modeling in the Dark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Internet photo collections naturally contain a large variety of illumination conditions, with the largest difference between day and night images. Current modeling techniques do not embrace the broad illumination range often leading to reconstruction failure or severe artifacts. We present an algorithm that leverages the appearance variety to obtain more complete and accurate scene geometry along with consistent multi-illumination appearance information. The proposed method relies on automatic scene appearance grouping, which is used to obtain separate dense 3D models. Subsequent model fusion combines the separate models into a complete and accurate reconstruction of the scene. In addition, we propose a method to derive the appearance information for the model under the different illumination conditions, even for scene parts that are not observed under one illumination condition. To achieve this, we develop a cross-illumination color transfer technique. We evaluate our method on a large variety of landmarks from across Europe reconstructed from a database of 7.4M images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image retrieval and 3D reconstruction have made big strides in the past decade. Recently, image retrieval and Structure-from-Motion (SfM) methods have been combined to achieve modeling from 100 million images <ref type="bibr" target="#b13">[10]</ref>. Combining them can not only tackle scale but also allows to reconstruct spatially complete models with high levels of detail <ref type="bibr" target="#b24">[21]</ref>. A key observation is that an increasing number of images in the collections ease the registration of images taken under very different illumination conditions into a sin-  gle 3D model. A feat that is not achieved by direct matching techniques, but rather by discovering sequences of matching images with a gradual change of the illumination, see <ref type="figure" target="#fig_2">Figure 2</ref> for such a transition sequence.</p><p>A sparse 3D reconstruction of feature points, obtained from a mixed set of day and night images, is reliable and naturally occurs in large-scale photo collections. This is due to the presence of "transition" images and due to the fact, that some of the detected features after photometric normalization provide sufficiently stable matches across illumination transitions. Examples of such feature points, the corresponding image patches, and their normalized descriptor patches are shown in <ref type="figure">Figure 4</ref>.</p><p>However, while beneficial for SfM, the registration of mixed illumination images creates challenges for dense 3D reconstruction, which delivers poor results or even fails in the presence of day and night images <ref type="bibr" target="#b18">[15]</ref>. In particular, mixed illuminations cause erroneous dense correspondences due to accidental photo-consistency in multi-view stereo that distort the texture composition of the models.</p><p>As a first contribution of the paper, we propose a method for automatically separating day and night images based on the sparse scene graph produced by SfM and a learned day/night color model. The separated sets of day and night images then allow to compute reliable dense reconstructions for each of the two modalities separately. While two separate models on first sight may be seen as a drawback, we demonstrate that they often contain regions in which only one of the models provides reliable surface reconstruction. As expected, we observe that usually daytime images are significantly more frequent and, due to better illumination conditions, lead to overall superior models over nighttime models. Interestingly, we observed several situations where night images provide better reconstruction than their daytime counterparts: (i) when lights at night illuminate or texture areas that are shadowed or ambiguous during the day, and (ii) when areas with repeated and confusing textures are not lit during the night, allowing unambiguous dense matching in those areas. Our second contribution is to fuse the initially separated dense models into a superior model combining the strengths of both modalities.</p><p>Finally, as a third contribution, we introduce a method of color transfer to consistently re-color the composite 3D areas for each illumination condition, even for areas that were not reconstructed under the illumination, i.e., we will compute a nighttime color even for geometry that is only reconstructed in the day model.</p><p>In summary, our contributions achieve a more complete and accurate dense 3D reconstruction for mixed day-and nighttime images that are typically present in Internet photo collections. Previously, the joint modeling of day and nighttime images caused disturbing artifacts or even lead to reconstruction failures. Additionally, we are able to reconstruct a complete color representation for the dense model surfaces leveraging the corresponding appearance characteristics of the daytime and nighttime images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The seminal paper of Snavely et al. <ref type="bibr" target="#b26">[23,</ref><ref type="bibr" target="#b27">24]</ref> first proposed reconstruction from unordered Internet photo collections. To determine overlapping views, Snavely et al. performed exhaustive pairwise geometric verification. While this ensures the highest possible discovery rate, it impairs the scalability of their system due to the quadratic complexity growth in the number of images. During the following years, several methods for tackling scalability of unordered photo collection reconstruction were proposed: appearance-based clustering methods for grouping the images <ref type="bibr" target="#b15">[12,</ref><ref type="bibr" target="#b9">6]</ref>, vocabulary tree based approaches <ref type="bibr" target="#b4">[1,</ref><ref type="bibr" target="#b17">14]</ref>, and most recently streaming based methods leveraging augmented appearance indexing <ref type="bibr" target="#b13">[10]</ref>. Although the systems successfully scaled the reconstruction to tens of millions of images, they lost the ability to reconstruct details of the scene in the process. Recently, Schönberger et al. <ref type="bibr" target="#b24">[21]</ref> proposed a method to overcome this limitation of not being able to reconstruct details. Their method leverages a tightly-coupled SfM and image retrieval system <ref type="bibr" target="#b20">[17]</ref> to overcome the loss of fine details in the models while keeping the scalability of the state-of-the-art reconstruction systems. Our reconstruction system is inspired by this method. Snavely et al. <ref type="bibr" target="#b26">[23,</ref><ref type="bibr" target="#b27">24]</ref> empirically observed the difficulty in registering night images due to their noisiness and darkness. In our system, we overcome this limitation by registering night images mainly through transition images under intermediate illumination conditions during dusk and dawn (see <ref type="figure" target="#fig_2">Figure 2</ref>). Snavely's system <ref type="bibr" target="#b25">[22]</ref> provided an option to manually select day or night images to explore similar viewpoints and illuminations. In contrast, our system automatically classifies and clusters day and night images. In addition, we use the clustering to improve reconstruction results.</p><p>Schindler and Dellaert <ref type="bibr" target="#b22">[19]</ref> proposed a method for analyzing the point in time at which a photo was taken. In contrast to our approach, their method was relying on observable changes of the scene geometry, e.g., construction or demolition of buildings, which typically happens over longer periods of time. Our method focuses on modeling the illumination changes over the course of a day. Recently, Matzen et al. <ref type="bibr" target="#b19">[16]</ref> proposed an approach to model and extract temporal scene appearance changes in 3D reconstructions. They perform temporal segmentation of the 3D model to obtain objects whose appearance changed over time. The recovered object appearance changes (wall art, signs, billboards, storefronts, etc.) relate to scene texture changes but not to illumination changes due to their search of change over longer periods of time. In contrast, our algorithm aims at determining periodic short term (over the course of a day) temporal scene appearance and illumination changes. Hence, our proposed approach deals with much smaller appearance differences in segmented parts of the reconstruction. These changes are caused by different illuminations during daytime and nighttime and are not correlated with scene texture changes.</p><p>Martin-Brualla et al. <ref type="bibr" target="#b18">[15]</ref> proposed to compute timelapse mosaics from unordered Internet photo collections of landmarks. They observed the difficulties posed by the presence of night and day images in the same reconstruction. Specifically, they noted that mixing day and night images within the same model introduces "unrealistic twilight effects". In this paper, we propose an approach that overcomes these failure cases and obtains a correct representation of the 3D model for both modes of illumination.  Ji et al. <ref type="bibr" target="#b14">[11]</ref> proposed a system to automatically create illumination mosaics for a given outdoor scene from Internet photos. Their work strives to depict temporal variability of the observed scene by presenting a 2D image of the scene with varying illumination along the rows of the image. They perform a search for a chain of images that exercise smooth illumination variation and that are all related through a homography mapping. In contrast, our method considers all available images and not only the images related through homographies. Instead of illumination modeling in 2D, our approach achieves illumination separation and modeling in 3D for the entire scene. Moreover, the ordering of Ji et al. <ref type="bibr" target="#b14">[11]</ref> heavily relies on the color of the sky shown in the images. Whereas our system can perform day-night separation even with no sky present in any of the images.</p><p>Veride et al. <ref type="bibr" target="#b28">[25]</ref> learned a feature detector which is stable under significant illumination changes, facilitating the matching between day-and nighttime images. They observed that standard feature detectors exhibit significant temporal sensitivity, i.e., reduced repeatability under different illumination conditions. We exploit this temporal sensitivity "flaw" of the standard detectors to efficiently split a given 3D model into groups of cameras and points that have the highest illumination change across groups, i.e., a group for the day and another for the night.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Before delving into the details of our method for day and night model reconstruction, we provide an overview as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. It starts with a database of unordered images. During the initial phase of the reconstruction, we build a sparse 3D model using SfM (see Section 4). In support of sparse modeling, we index all images in the database using a min-Hash and find reconstruction seeds by leveraging geometrically verified hash-collisions. Next, our SfM algorithm uses these seeds to build sparse 3D models for the scenes contained in the photo collection. Specifically, it uses a feedback loop to gradually extend the reconstruction by dedicated queries against the database. The resulting sparse model contains day and night images registered into the same model and represented as one scene graph.</p><p>In the next step, a dense scene model is obtained. Given the previously observed difficulties and artifacts caused by mixed day and night images, we deviate from the standard approach of directly proceeding to dense reconstruction. We first split the scene graph into day and night clusters to separate the images of the different illumination conditions (see <ref type="bibr">Section 5)</ref>. This in essence separates the scene graph into two scene graphs -one for daytime images and one for nighttime images. Then, we perform separate dense geometry estimation for the images in each of the scene graphs yielding two separate dense 3D models (see Section 6). Subsequently, the two dense models are aligned into one common model representing the overall dense scene geometry. As part of computing the dense scene geometry, we obtain the color information of the point cloud under the two illumination conditions, i.e., a daytime color and a nighttime color for each point. Given that not all parts of the common model are necessarily visible both at daytime and at nighttime, we then determine the missing color information through cross-illumination transfer. Specifically, we use one illumination condition to find similar patches with a corresponding color in the other illumination. The color information of the patches under one illumination is then used to compose the missing color information for the point under the other illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Reconstruction</head><p>In this section, we detail our approach for efficiently reconstructing all 3D models contained in a given image database. We use a generic database from <ref type="bibr" target="#b24">[21]</ref> with over 7.4 million images downloaded from Flickr through keywords of famous landmarks, cities, countries, and architectural sites. The approach starts with an initial clustering procedure to find putative spatially related images. These spatially related images are subsequently used to seed an iterative reconstruction process that repeatedly extends the 3D model through a tight integration of the image retrieval and SfM module similar to the approach by Schönberger et al. <ref type="bibr" target="#b24">[21,</ref><ref type="bibr" target="#b23">20]</ref>. In contrast to their system, our approach exhaustively builds models for the entire image database. Due to the massive number of images in the database, exhaustive reconstruction imposes several challenges in terms of efficiency, which we address through an initial clustering procedure and a parallelized implementation of their system.</p><p>Clustering To seed our iterative reconstruction process efficiently, we find independent sets of spatially overlapping images using the clustering approach by Chum et al. <ref type="bibr" target="#b7">[4]</ref>.</p><p>This approach first indexes all database images in a min-Hash table and then uses spatially verified hash collisions as cluster seeds. Next, an incremental query expansion <ref type="bibr" target="#b8">[5,</ref><ref type="bibr" target="#b21">18]</ref> with spatial verification extends the initial clusters with additional images of the same landmark. The nearestneighbor images in this query expansion step then define the graph of overlapping images, the so-called scene graph. Given that query expansion is a depth first search strategy, the resulting scene graph is only sparsely connected. However, in order to achieve a successful reconstruction, SfM requires a denser scene graph than provided by the clustering method. Therefore, we first densify the scene graph as described in the following paragraph before using it in SfM. Compared to the approach in <ref type="bibr" target="#b24">[21]</ref>, which takes a single query image as input for the reconstruction, this clustering step reduces the number of query images dramatically. Rather than seeding the reconstruction with 7.4M query images, the clustering procedure identifies 19,546 individual landmarks used to initialize the subsequent reconstruction procedure and thereby reduces the number of seeds by 3 orders of magnitude.</p><p>Densification Next, we densify the initially sparse scene graph for improved reconstruction robustness and completeness. In the spirit of Schönberger et al. <ref type="bibr" target="#b24">[21]</ref>, we leverage the spatially verified image pairs and their visual word matches along with an affine model to serve as hypotheses for subsequent exhaustive feature matching and epipolar verification. From this exhaustive verification, we not only obtain a higher number of feature correspondences but we also determine additional image pairs to densify the scene graph. More importantly, beyond the benefit of additional image pairs, the significantly increased number of feature correspondences is essential for establishing feature tracks from day to night images through dusk and dawn. Only through these transitive connections, we are able to reliably register day and night images into a single 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure-from-Motion</head><p>The densified scene graph is the input to the subsequent incremental SfM algorithm, which treats each edge in the graph as a putative image pair for reconstruction and attempts to reconstruct every connected component within a cluster. Connected components with less than 20 registered images are discarded for the purposes of day/night modeling as they typically lack a sufficient number of transition images during dusk and dawn.</p><p>Extension To boost registration completeness, a final extension step issues further queries for all registered images in each reconstructed connected component. If new images are found and spatially verified, we again perform scene graph densification and use SfM to register the new views into the previously reconstructed models. While significantly increasing the size of the reconstructed models, the extension process also improves the performance of the day/night modeling step. Typically, the initial set of images obtained in clustering often only contains images from one modality, i.e., either day or night, even though our largescale image database contains images of both modalities for almost all landmarks. The iterative extension overcomes this problem by incrementally growing the model from day to night or vice versa through transition images during dusk and dawn (see <ref type="figure" target="#fig_2">Figure 2</ref> for an example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Day/Night Clustering</head><p>After the exhaustive 3D reconstruction stage of all landmarks in the database, we proceed with clustering the images inside each of the 3D models into two groups: dayand nighttime. For crowd-sourced data, the clustering cannot simply rely on embedded EXIF time stamps. In our experiments, the majority of images either have no time stamp information at all or the information is clearly corrupt. We speculate that most images are taken on vacation and people do not adjust the time zone in their cameras. For most landmarks with many registered images, day-and nighttime images are registered into the same model as a result of the extension step (see Section 4). It is well known that standard feature (keypoint) detectors <ref type="bibr" target="#b28">[25]</ref> suffer under illumination sensitivity, i.e., the reliability of keypoint detectors degrades significantly when the images originate from outdoor scenes during different times of the day or generally different illumination conditions. In this case, the detectors commonly produce keypoints at different locations for day and night lighting conditions <ref type="bibr" target="#b28">[25]</ref>. This even holds true when the images are taken from the same viewpoint. Our key insight is to exploit this behavior in order to split the images inside a SfM model into two groups. Our clustering is based on the number of commonly observed 3D points for each pair of images with similar viewpoints. This enables us to identify day and night images registered within a model. For efficient grouping, we leverage a bipartite visibility graph <ref type="bibr" target="#b16">[13]</ref>, as explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Min-cut on Bipartite Visibility Graph</head><p>A 3D model produced by SfM can be interpreted as a bipartite visibility graph G = (I ∪ P, E) <ref type="bibr" target="#b16">[13]</ref>, where the images i ∈ I and the points p ∈ P are the vertices of the graph. The edges of the graph are then defined by the visibility relations between cameras and points, i.e., if a point p is visible in an image i, then there exists an edge (i, p) ∈ E. We define the set of points observed by an image i as:</p><formula xml:id="formula_0">P(i) = {p ∈ P | (i, p) ∈ E}.<label>(1)</label></formula><p>Our day/night clustering separates the vertices of the graph (the cameras and points) into two groups: one corresponding to day cameras and points and the second for the night cameras and points. More formally, we define two <ref type="figure">Figure 4</ref>. Colosseum, Rome. Two feature tracks containing both day and night images/features. Each row depicts two images labeled as day and night, respectively, followed by a subset of feature patches depicted in two rows, one for day and one for night features, respectively. Intensity normalized patches, grayscale versions used for SIFT description, are shown to the right of the respective color patches. Notice the variation in lighting conditions for day and night, expressed as a significant color difference of patches. Best viewed in color.</p><p>label vectors representing the group assignment. Vector α i for the images and vector α p for the points:</p><formula xml:id="formula_1">α i = {α i ∈ {0, 1} | i ∈ I}, α p = {α p ∈ {0, 1} | p ∈ P},<label>(2)</label></formula><p>where label variables α i and α p correspond to image i and point p, and label α i , α p = 0 denotes day and label α i , α p = 1 night. We formulate the problem of separating day from night images as an energy optimization. We propose the following energy function E over the graph G that measures the quality of the labeling α i , α p :</p><formula xml:id="formula_2">E(α i , α p , G) = i∈I U i (α i ) + (i,p)∈E P i,p (α i , α p ). (3)</formula><p>The term P i,p (α i , α p ) describes the pairwise potentials associated with the edges enforcing a smooth labeling of the cameras and points with respect to their mutually observed scene information. A standard Potts model is used for the pairwise potentials, that is P i,p (α i , α p ) = 0 for α i = α p and P i,p (α i , α p ) = 1 otherwise. The 3D points incur no unary cost for being assigned either label. The unary cost U i (α i ) for images is based on the day/night illumination model discussed below. The clustering of all images and points in a model is achieved by minimizing the objective</p><formula xml:id="formula_3">α i , α p = argmin αi,αp E(α i , α p , G)<label>(4)</label></formula><p>using the min-cut/max-flow algorithm of Boykov et al. <ref type="bibr" target="#b6">[3]</ref>. <ref type="figure">Figure 4</ref> shows examples of 3D point tracks that contain both day and night labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Day/Night Illumination Model</head><p>We use a day/night illumination model to estimate the likelihood of an image being taken during day or night respectively. As a feature for the prediction, a spatial color histogram in the opponent color space <ref type="bibr" target="#b12">[9]</ref> I = (R + G + B)/3, O 1 = (R + G − 2B)/4 + 0.5, is used. To reduce the influence of occlusions and background clutter, a three-band spatial histogram is computed over a region of the image directly related to the reconstructed object, as depicted in <ref type="figure" target="#fig_4">Figure 5</ref>. The bottom two stripes of the histogram equally split the bounding box of feature points that have been reconstructed as 3D points in the model. The top band covers the sky area above the landmark, up to the top edge of the image. The color is uniformly quantized and each spatial band of the histogram is separately normalized by the number of pixels per region. The final illumination descriptor is obtained by concatenating the color histograms for the three spatial bands. In our experiments, we use n = 4 bins per color channel resulting in an image descriptor of dimensionality D = 3n 3 = 192.</p><formula xml:id="formula_4">O 2 = (R − 2G + B)/4 + 0.5,<label>(5)</label></formula><p>To classify the illumination descriptors into daytime and nighttime, a linear SVM <ref type="bibr" target="#b5">[2]</ref> is trained on ground-truth labeled images of our largest model (Colosseum, Rome). The same trained SVM is used to compute the unary terms for each image i in all reconstructed models:</p><formula xml:id="formula_5">U i (α i ) = 0 if α i = SVMp(i), c·SVMs(i)· |P(i)| otherwise,<label>(6)</label></formula><p>where SVMp(i) and SVMs(i) denote the SVM's label prediction and the absolute value of the prediction score of im-age i, respectively. The confidence constant c of the trained SVM has higher confidence for higher values c &gt; 0 and in our experiments we set c = 1. The cardinality of the set of observed points P(i) is equal to the number of edges that connect image i to 3D points in the visibility graph. The label of image i is decided based on the labels of its observed points (pairwise term) and by the confidence of the linear SVM prediction (unary term). In order for this process to be fair for all images, we multiply the SVM score by the number of observed points |P(i)| for the final unary term. This number defines the percentage of observed points that should have different labels to change the SVM prediction for the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Day/Night Modeling</head><p>After obtaining the image clustering, we first aim to reconstruct the separate models and then combine them into a joint model to produce consistent geometry and texture within each modality, as detailed in this section. Typically, there is an uneven distribution of day and night images, causing one of the modalities to have lower scene coverage. In addition, the different illumination conditions during day and night allow for reconstruction of details that are clearly visible during the day but not at night and vice versa. For example, many landmarks are lit during the night and a reconstruction of fine details is oftentimes possible for night images while during the day those structures are hidden in shadows. Hence, in the second step, we fuse the geometry of the two models in order to obtain better completeness in terms of scene coverage and reconstruction of fine details. To obtain consistent color for the fused model, we recolor the structure of the respective other modality through repainting of visible structure and inpainting of structures not covered by images. The following sections describe our proposed approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Dense Reconstruction</head><p>For dense reconstruction, we first separate the sparse model into its day and night modalities based on the labels α i and α p . For most models, there are enough images during day and night to allow for dense reconstruction in both modalities. We split the graph G into two disjoint sub-graphs: G d for the day modality, and G n for the night modality. We separate the tracks of points that are visible in both day and night images. The two graphs serve as the input to the dense reconstruction system by Furukawa and Ponce <ref type="bibr" target="#b10">[7,</ref><ref type="bibr" target="#b11">8]</ref>. Separate reconstruction of day and night images removes many of the disturbing artifacts present when using all images in a model (see <ref type="figure" target="#fig_6">Figure 6</ref>). To mitigate reconstruction artifacts caused by sky regions, we create segmentation masks using an improved version of the approach proposed by Ji et al. <ref type="bibr" target="#b14">[11]</ref>. In distinction to their approach, we leverage the sparse point cloud as an additional clue for  deciding whether parts of the image belong to the sky or not. The outputs of this step are separate models for day and night. In the next section, we describe an approach that fuses the two models and leverages the benefits of the respective other modality for increased model completeness and detail reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Fusion</head><p>Typically, the scene coverage of day and night models are very different due to a multitude of reasons. First, parts of the scene may not be covered by any images in one of the modalities, e.g., caused by occlusion or lack of images. In addition, we found that for some scenes, parts of the reconstruction are not covered by images at all during the night due to restricted access in those areas, e.g., the inside of the Colosseum. A second reason for different scene coverage is the different illumination conditions causing dynamic range issues for the cameras that often prevent reliable reconstruction of scene parts, even though they are theoretically visible. Especially for night images, parts are often under-illuminated or lack any illumination at all. There is a similar issue for day images as well, e.g., shadows caused by intense sunlight often prevent reconstruction of structure. One such case is depicted in <ref type="figure" target="#fig_6">Figure 6</ref>. Using the default parameters, the dense reconstruction method by Furukawa and Ponce <ref type="bibr" target="#b11">[8]</ref> is very conservative in terms of creating dense points, i.e., 3D structure only appears in high confidence areas. Therefore, geometric fusion of the two models enables the use of structure that is more accurately reconstructed from day or night images. As a first step, we perform alignment of the two models into the same reference frame using the correspondences from points that appear both in night and day images. However, such fused models contain both day and night points and thus suffer from inconsistent coloring. In the following paragraphs, we describe a joint repainting and inpainting procedure to color the fused day points in the night model and vice versa (see <ref type="figure" target="#fig_7">Figure 7</ref>). For simplicity, we explain the procedure for the case of coloring the fused point cloud using the night images, but the approach is analogous in the opposite direction.</p><p>Repainting As explained in the previous paragraph, many dense points are reconstructed in day but not in night models, even though they are covered by night images. We project these points into all night images and determine their color as the median of all projections. For occlusion handling, we enforce depth consistency with the sparse point cloud. The depth of the dense points must be within the 10th and 90th percentile of the depth range of the observed sparse points of an image. While this cannot account for fine-grained occlusions, in our experiments, the extracted colors are not affected by occluded observations due to the robust averaging of colors.</p><p>Inpainting For those points that are not visible in any night image, we propose a novel inpainting method. The method learns the appearance mapping between known corresponding day and night patches to predict the color of unseen points. To establish dense correspondence between day and night patches, we first project all points into day and night images. Any point that projects both into day and night images defines a correspondence that we use to infer the appearance of a day point during the night. Each of the correspondences usually projects into multiple day and night images. An average color histogram is extracted from a 5 × 5 patch around the projected image location, for each correspondence between day and night images. While we tried to incorporate shape information as descriptors, we found color histograms to be sufficiently distinctive features and best performing for the task of inpainting. Using these histograms as input, we train a nearest-neighbor regressor to map from day patches to night patches. To inpaint the color of points that only project to day images, we extract the average day color histogram for that point and use our trained regressor to predict its most likely appearance during the night. This inpainting method enables us to obtain a model during the night that is as complete as during the day. In all our experiments, we use N = 20 nearest neighbors for the regression and D = 96 dimensional histograms for the appearance descriptor.</p><p>Blending Even though we are using a robust average in the repainting step, low-coverage points sometimes suffer from abrupt changes in appearance in 3D space whenever the field of view of one image ends. To counteract this artifact, we propose to blend these points by predicting their appearance using the same mapping as in the inpainting step. We improve the color of any point with a track length t &lt; t min .</p><p>The originally repainted color is then blended with the inpainted color based on the track length of the point. The blended color of a point is calculated as</p><formula xml:id="formula_6">c bl = t min − t t min · c inp + t t min · c rep ,<label>(7)</label></formula><p>where c inp and c rep denote the inpainted and repainted colors, respectively. In all experiments we set t min = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>After describing our novel approach for day/night modeling, we now evaluate our method on the entire 7.4M image database and present results for a variety of scenes. Our experiments demonstrate that the proposed algorithm robustly generalizes to different illumination conditions.</p><p>Reconstruction The iterative reconstruction process for the database of 7.4 million images converges in 3 iterations for all clusters in the database and takes around one week on a single desktop machine. We produce day and nighttime models for any reconstructed cluster that has a sufficient number of registered images, i.e., at least 30 day and 30 night images. We find 1,474 such models out of the initial set of 19,546 clusters used to seed the reconstruction pipeline. These models have 239,717 unique, registered images contained in 845 disjoint landmarks. The average ratio of day to nighttime images in the reconstructions is 9:1.</p><p>Clustering To evaluate our clustering approach, we handlabeled 13,931 images of 6 different landmarks present in the dataset using the two classes of labels "day" and "night" (see <ref type="table">Table 1</ref>). For the sake of comparison, we also introduce a baseline method for image clustering into day-and nighttime images using k-means clustering with two clusters on the HSV color histograms of the images. Our clustering approach achieves almost perfect classification for the day and night images. Even in the challenging case with only few night images. We outperform k-means on all landmarks and, most importantly, we can classify night images very accurately, which is crucial for avoiding artifacts in day/night modeling. This is even more notable considering that night images are significantly outnumbered in most of the models. <ref type="figure" target="#fig_8">Figure 8</ref> impressively demonstrates the improved completeness and accuracy of night models by the geometric fusion. In addition, <ref type="figure" target="#fig_6">Figure 6</ref> also depicts an example of the opposite direction, where the structure of day model is improved through the night model. We encourage the readers to view the supplementary material for additional impressions and videos. <ref type="figure" target="#fig_7">Figure 7</ref> demonstrates the proposed repainting, inpainting, and blending method applied to a building facade in a low-coverage part of the Pantheon reconstruction. The structure is not reconstructed in the original night model ( <ref type="figure" target="#fig_8">Figure 8)</ref>. Hence, the entire structure consists of repainted points from the day reconstruction. In addition, our method effectively inpaints structure that is not visible in any night images and removes artifacts through blending.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours Baseline</head><p>Landmark # D # N TP FP TP FP <ref type="table">Table 1</ref>. Quantitative evaluation of clustering accuracy for night images. Ground-truth labels obtained through manual labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We introduced a novel algorithm that handles and benefits from the variety of scene illuminations naturally present in large-scale Internet photo collections. This is in stark contrast to previous methods that treated multiple illuminations as a nuisance or failure condition. We exploit the additional information to obtain a more complete and accurate 3D model and to create multi-illumination appearance information for the 3D model. The proposed method demonstrates that we can leverage the additional information provided by the different illuminations to boost modeling quality for both geometry and appearance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was done while J. L. Schönberger was at the University of North Carolina at Chapel Hill. We thank Marc Eder for helping with experiments. F. Radenović, O. Chum and J. Matas were supported by the MSMT LL1303 ERC-CZ and GACR P103/12/G084 grants. J. L. Schönberger, D. Ji and J.-M. Frahm were supported in part by the National Science Foundation No. IIS-1349074, No. CNS-1405847, and the MITRE Corp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Night model of St. Peter's Cathedral in Rome reconstructed by our method. Left: Model obtained from night images only. Right: Fused, recolored model from day and night images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Tyn Church, Prague. Registration of day and night images into the same model through smoothly varying illumination in intermediate images during dusk and dawn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The proposed day/night modeling pipeline starting with sparse modeling to day-night clustering and the final dense modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Colosseum, Rome. Examples of image color histogram description area. Coordinates of features reconstructed as 3D points define the bounding boxes used to compute three histograms. Using the 3D model information, we successfully segment out confusing background and are able to focus the description on the three important parts: sky, upper and lower part of the reconstructed landmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Moulin Rouge, Paris. Standard dense modeling using day and night images creates disturbing artifacts, while a separate modeling for day and night images produces consistent geometry and coloring. Fusion and recoloring improves completeness, appearance, and accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Pantheon, Rome. Example of repainting, inpainting, and blending for building facade that is not present in the original night reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Example of reconstructions produced by our method for St. Peter's Basilica in Vatican, Colosseum in Rome, Astronomical Clock in Prague, Altare della Patria in Rome, and Pantheon in Rome.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Castel</forename><surname>St</surname></persName>
		</author>
		<idno>1400 129 99.22 6.20 93.02 6.98</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Astronomical Clock</title>
		<idno>2243 1375 97.89 5.60 80.15 2.98</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altare</forename><forename type="middle">D</forename><surname>Patria</surname></persName>
		</author>
		<idno>2.52 92.72 4.20</idno>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">357</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter&amp;apos;s Basilica</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building Rome in a Day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale discovery of spatially related images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Total recall: Automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building Rome on a Cloudless Day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fite-Georgel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raguram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards internet-scale multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den Boomgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Geerts</surname></persName>
		</author>
		<title level="m">Color invariance. IEEE PAMI</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heinly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synthesizing Illumination Mosaics from Internet Photo-Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling and recognition of landmark image collections using iconic scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Location recognition using prioritized feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<idno>ECCV. 2010. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MatchMiner: Efficient Spanning Structure Mining in Large Image Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time-lapse mining from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scene chronology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient image detail mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic temporal inference on reconstructed 3D scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From Single Image Query to Detailed 3D Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scene reconstruction and visualization from internet photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photo tourism: exploring photo collections in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Modeling the world from internet photo collections. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TILDE: A Temporally Invariant Learned DEtector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
