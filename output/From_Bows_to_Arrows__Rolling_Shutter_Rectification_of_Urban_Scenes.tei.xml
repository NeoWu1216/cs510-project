<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Bows to Arrows: Rolling Shutter Rectification of Urban Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Rengarajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aravind</surname></persName>
							<email>3aravind@ee.iitm.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From Bows to Arrows: Rolling Shutter Rectification of Urban Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rule of perspectivity that 'straight-lines-mustremain-straight' is easily inflected in CMOS cameras by distortions introduced by motion. Lines can be rendered as curves due to the row-wise exposure mechanism known as rolling shutter (RS). We solve the problem of correcting distortions arising from handheld cameras due to RS effect from a single image free from motion blur with special relevance to urban scenes. We develop a procedure to extract prominent curves from the RS image since this is essential for deciphering the varying row-wise motion. We pose an optimization problem with line desirability costs based on straightness, angle, and length, to resolve the geometric ambiguities while estimating the camera motion based on a rotation-only model assuming known camera intrinsic matrix. Finally, we rectify the RS image based on the estimated camera trajectory using inverse mapping. We show rectification results for RS images captured using mobile phone cameras. We also compare our single image method against existing video and nonblind RS rectification methods that typically require multiple images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, inferring scene geometry has become possible from as little as a single image <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b33">33]</ref>. The images of interest are often man-made structures (e.g. in the Manhattan world) which have predominant straight lines. From the knowledge of these lines, properties such as vanishing points, horizon, and zenith can be estimated <ref type="bibr" target="#b33">[33]</ref> to aid in applications such as scene classification <ref type="bibr" target="#b13">[14]</ref> and depth estimation <ref type="bibr" target="#b29">[29]</ref>. For the single image scenario, camera motion is a hindrance to scene understanding. It introduces image distortions which are increasingly becoming commonplace. The CMOS mobile phone cameras employ a shutter mechanism in which the pixels on the sensor plane are exposed in a row-wise manner from top to bottom with a constant inter-row delay. Hence, even a small camera motion due to handshake can cause visible distortions in the captured image, a phenomenon referred to as the rolling in which curves are corrected to lines respecting the orthogonality of vertical and horizontal edges using the proposed method, and (c) rectified image using the lens correction method <ref type="bibr" target="#b35">[35]</ref> shutter (RS) effect <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b31">31]</ref>. In a high exposure setting, motion blur further complicates the distortions, but in typical daylight scenarios, the blur is negligible and only the RS distortion is most perceivable. The RS effect is more prominent in imaging urban scenes, since the row-varying camera motion alters the straightness of lines in the scene. This results in the manifestation of curves in the captured image in place of lines which can be problematic for scene inference methods that rely on line properties. The image shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(a) is captured using a Motorola Moto G2 camera with handshake during exposure. The vertical lines in the 3D scene are affected by the row-varying camera motion and appear as curves.</p><p>Video rectification (i.e. correction of RS distortions) has been studied elaborately in RS imaging <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">32]</ref>; however, these methods are not applicable for the case of single image RS removal, since the RS motion is estimated by using the correspondences between successive video frames. While there are works for absolute RS camera pose estimation that use only a single image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b1">2]</ref>, they assume known correspondences between image and world points. It is also possible to use inertial sensor information to infer RS motion especially for videos <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref> and for long exposure motion blur images <ref type="bibr" target="#b30">[30]</ref>, but the low acquisition rate of inertial sensors prohibits usage on a single RS image. A combined RS and motion blur framework without the inertial information is proposed for change detection in <ref type="bibr" target="#b25">[25]</ref>. Though the camera motion experienced by the RS image is actually estimated, it is carried out in a non-blind fashion by assuming knowledge of a clean image with no RS artifacts. In <ref type="bibr" target="#b31">[31]</ref>, a single image deblurring algorithm is proposed for images affected by both RS and motion blur. The camera motion is inferred through the spatial variation of blur across rows by backprojecting the local point spread functions (PSFs) to a higher dimensional camera trajectory. However, it cannot handle the RS-only case (i.e. without motion blur), since the PSFs will all be identical impulses leading to no RS correction.</p><p>To the best of our knowledge, no work exists that corrects RS geometric distortions using the information from only a single observation. It is this challenging scenario that we tackle in this paper. We address the case where the exposure time of each row is short enough to ignore the presence of motion blur, while at the same time, the RS effect is unavoidable due to inter-row exposure delay. Without the availability of any other information (from additional images or from inertial sensors), the problem is very illposed. We do not have the liberty to use image correspondences as in a multi-image scenario; nor can we impose image priors such as gradient sparsity as in deblurring problems, since the RS image is already free of blur. Hence, we propose to exploit the presence of lines in man-made structures to impose constraints that enable camera motion estimation. Specifically, we use the 'straight-lines-mustbe-straight rule' for perspective cameras <ref type="bibr" target="#b7">[8]</ref> on RS curves. Even for small motion, vertical lines are affected by the RS mechanism. The resultant curves can potentially reveal the underlying camera motion.</p><p>Incidentally, studies on curve extraction exist in the realm of lens distortion. In one of the earliest works <ref type="bibr" target="#b7">[8]</ref>, curves are formed by linking spatially closer edge pixels. In a recent work <ref type="bibr" target="#b4">[5]</ref>, the distorted lines are modelled as circular arcs. In <ref type="bibr" target="#b2">[3]</ref>, small arcs are detected by using a modified Hough transform embedding the radial distortion parameter. An important distinction between methods extracting curves for lens and for RS distortions is that the former benefits from apriori knowledge of the lens distortion model which helps in tailoring the algorithm to treat curves that can be expected to occur in the captured image. In contrast, in the case of RS distortions, the extent and the properties of curves completely depend on the camera trajectory during the exposure and cannot be predicted beforehand. In addition, while it is well known that wide-angle lens defects can be handled with camera pre-calibration, this is not applicable here, since the RS effect depends on the amount of camera motion which essentially is unique for each capture. In <ref type="bibr" target="#b35">[35]</ref>, automatic calibration of intrinsic parameters of a camera including lens properties is performed exploiting low-rank textures from images. Correcting the curvature of the RS image in <ref type="figure" target="#fig_0">Fig. 1</ref>(a) using <ref type="bibr" target="#b35">[35]</ref> results in partial curvature corrections as shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, since the nature of the two distortions are completely different.</p><p>We develop a curve detection method that automatically links local line segments into curves based on spatial and angular proximities. Since we are interested only within a single image exposure, we model the row-wise variation of rotation-only RS motion as a polynomial similar to the one used in <ref type="bibr" target="#b31">[31]</ref> which suits the camera motion adequately for short exposures. We propose an optimization problem with line, angle, and length constraints on the detected curves to estimate the camera motion thus enabling us to rectify the RS effect. We devise an inverse mapping procedure that assigns an intensity value to every pixel in the rectified image from its corresponding RS pixel based on the estimated camera motion. The rectified RS image using the proposed method is shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. The curves are corrected to lines, and in addition, the orthogonality of the ceiling, wall, and ground planes is preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Main Contributions</head><p>• This is the first work of its kind to address the problem of correcting geometric distortions due to the RS mechanism from a single image devoid of motion blur and lens distortions. The key idea is to exploit the information embedded in the curves of the RS image to reveal the underlying RS camera motion.</p><p>• This is also the first work to study geometric ambiguities while estimating row-wise rotation-only camera motion and suggest remedies to resolve them using line properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RS Rectification</head><p>After describing our RS motion model in Section 2.1, we proceed to describe curve detection procedure in Section 2.2, camera motion estimation in Section 2.3, and image rectification algorithm in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Camera Model</head><p>For a static CMOS camera located at the world origin, the scene point X is related to the image point through the camera intrinsic matrix K <ref type="bibr" target="#b11">[12]</ref>. We refer this image as the global shutter (GS) image. The image point in the homogeneous representation is given by</p><formula xml:id="formula_0">x GS = KX.<label>(1)</label></formula><p>When the CMOS camera moves during exposure, each row of the image sensor plane experiences its own camera pose due to the row-wise sequential acquisition. This results in distortions in the captured image which is referred to as the RS image. For images captured with hand-held cameras, only the rotations play a major role, as noted in works dealing with camera motion in conventional global exposure cameras <ref type="bibr" target="#b34">[34]</ref> as well as in RS cameras <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b31">31]</ref>.</p><p>Hence, we employ a rotation-only model for the camera motion. Let the number of rows in the RS image be M , and let the row number be indexed by y. The camera pose for the y th row is given by the 3D rotation angles r x (y), r y (y), and r z (y), and the equivalent orthogonal rotational matrix is denoted by R(y). Thus, the series of rotation matrices {R(y)} M y=1 represents the camera motion during exposure. During RS acquisition, the scene point X is mapped on the image plane due to the camera motion R(y) as follows <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_1">x RS = KR(y)X,<label>(2)</label></formula><p>where x RS is the homogeneous representation of the RS image point and</p><formula xml:id="formula_2">x RS (2)/ x RS (3) = y.</formula><p>For a same scene point X, the relationship between the image points in the GS and RS images is written using <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> as</p><formula xml:id="formula_3">x GS = KR −1 (y)K −1 x RS .<label>(3)</label></formula><p>Given an RS image distorted by camera motion, our aim is to recover the undistorted GS image. This is not possible without estimating the underlying camera trajectory.</p><p>Recovering the motion of each row independently is very ill-posed given only a single image. The number of unknowns to be estimated for 3D camera motion is 3M which is very high. To alleviate this problem, we adopt a polynomial model for camera motion during exposure. Using a simpler model such as a spherical linear interpolation model <ref type="bibr" target="#b26">[26]</ref> restricts the trajectory to be piecewise-linear, while higher order camera models such as B-splines <ref type="bibr" target="#b24">[24]</ref> are more suitable for modelling video frames, but complex and unnecessary for a single RS image.</p><p>We model the rotation trajectory along each axis r i (y) where i ∈ {x, y, z} as a polynomial of degree n. Hence, we have</p><formula xml:id="formula_4">r i (y) = α i0 , y = 1 n j=0 α ij y−1 M j , 2 ≤ y ≤ M<label>(4)</label></formula><p>where α ij is the j th polynomial coefficient for the i th axis motion. These coefficients are denoted by α, and equivalently, we obtain R(y) for all y. In this model, the number of unknowns to be estimated reduces to 3(n + 1) (with known camera matrix K). We use n = 3, and hence, the number of unknowns is 12. The validity of this model is discussed further in the experiments section. While a similar model was used earlier for the RS deblurring problem in <ref type="bibr" target="#b31">[31]</ref>, unlike that work, our model does not require the knowledge of the inter-row delay time of the RS camera. For a high exposure time setting, a polynomial model may not suffice due to the complex camera motion. However, this setting would also lead to motion blur which is beyond the scope of this paper. We also do not handle depthdependent RS effect <ref type="bibr" target="#b28">[28]</ref> caused by translations of fast moving cameras (e.g. mounted on vehicles) in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Curve Detection</head><p>In this section, we develop a procedure to detect both curves and lines which serve as features for camera motion estimation.</p><p>Line segment extraction We first extract edges from the RS image using the Canny detector <ref type="bibr" target="#b5">[6]</ref>. Let θ denote the angle of a line with respect to the horizontal axis. To detect line segments, we discretize the angle space (−90 • , 90 • ] in steps of 1 • and denote it by S. For each angle θ ∈ S, we map edge pixels to the Hough domain <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">27]</ref> and detect lines from the peaks of the Hough accumulator matrix <ref type="bibr" target="#b8">[9]</ref>. We then extract local line segments based on the edge pixels situated near the Hough lines. We fix a minimum length for line segments to avoid trivial edges. Each edge pixel is assigned to one of the line segments based on its Euclidean distance. Pixels that are at a distance greater than a threshold to all line segments are ignored.</p><p>Curve grouping Our next step attempts to link these line segments into curves (where possible) based on their spatial proximity. In addition, we classify the RS curves into three groups corresponding to horizontal, vertical, and slanted lines in the unknown GS image. These groups are denoted by G h , G v and G s , respectively. Instead of manually binning the angle space to group curves, we follow a greedy approach in which we start with a seed angle and progressively increase the size of the bin to link more and more line segments that have proximal angles. We continue this process until a stopping point which is determined by the longest curve length along the vertical axis.</p><p>To create the vertical group G v , we consider a bin B θs ⊂ S of angles around the seed angle θ s = 90 • . Let the bin size be B. We consider only the line segments which belong to B θs , and join them into curves based on their endpoint and angle proximities. As the bin size B is increased to include more line segments, longer curves are formed (see <ref type="figure" target="#fig_1">Fig. 2</ref>(a1) to (a4)). This continues until the point after which there are no more line segments to link to get longer curves. This stopping point is determined by the saturation in the maximum curve length with increasing bin size, which is shown as a red point in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. To detect curves for the horizontal group G h , we repeat this process with seed angle</p><formula xml:id="formula_5">θ s = 0 • .</formula><p>For scenes which lack long edges, the stopping point is difficult to determine. Hence, we fix the maximum bin size as 30 • for G v and a smaller value of 10 • for G h , since lines closer to horizontal are least affected (as they are exposed to just one homography), and lines closer to vertical are most affected (as they are acted upon by many homographies). Once a subset of line segments are linked and classified to form the vertical and horizontal groups, G v and G h , the remaining ones are considered for the slanted group G s . These are linked to form curves corresponding to GS lines at angles other than 90 • and 0 • . Outlier removal There is a possibility that some of the edge pixels have been wrongly grouped as curves, while in reality they are not. As this will affect our camera motion estimation, we follow a curve rejection procedure. We fit a third degree polynomial using the edge pixels corresponding to each curve independently, and the ones which result in a large error are removed from further consideration. Detected curves post outlier removal are shown in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>. Note that many of the small edge groups on the rightmost red wall have been discarded.</p><p>The final set of curves is denoted by {c i } Nc i=1 , where N c is the total number of curves. Each c i is a set of edge pixels, and it belongs to one of the groups G v , G h , or G s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Camera Motion Estimation</head><p>The edge pixel locations belonging to the RS curve c i are denoted by</p><formula xml:id="formula_6">x RS = {(x ij , y ij )} Ni j=1 ,</formula><p>where N i is the number of pixels in c i , x ij is the column index and y ij is the row index. Let α (equivalently, R(y) or [ r x (y), r y (y), r z (y)]) be a camera motion estimate. Considering x RS = [x ij , y ij , 1] T , applying (3) would result in the estimated RS-corrected point,</p><formula xml:id="formula_7">x GS . Let (x ′ ij , y ′ ij ) be the corresponding 2D point where x ′ ij = x GS (1)/ x GS (3) and y ′ ij = x GS (2)/ x GS (3)</formula><p>. If α is the ground-truth motion, then the points (x ′ ij , y ′ ij ) will lie on a line at a particular angle. Hence, to evaluate the goodness of an estimate α, we develop a line desirability cost that penalizes higher curvatures.</p><p>Line Desirability Cost We fit a line through {(x ′ ij , y ′ ij )} Ni j=1 using least squares which results in a line with parameters (ρ i , θ i ), where ρ i is the orthogonal distance of the line from the origin, and θ i is the angle of the line with respect to the horizontal axis. We formulate the error due to this line fitting for curve c i as</p><formula xml:id="formula_8">e line i = 1 N i Ni j=1 (x ′ ij sin θ i + y ′ ij cos θ i − ρ i ) 2 .<label>(5)</label></formula><p>The total line cost for this camera motion estimate is the sum of the above cost value for all the curves. We make an important note that, since the camera exposure sequence is row-wise, the motion information is better embedded in curves that span longer along the vertical axis. Hence, we weight the individual curve costs by a factor based on the length in pixels that the original curve spans along the vertical axis. Thus, the total cost based on the line constraint for all curves is given by</p><formula xml:id="formula_9">E line = 1 N c Nc i=1 w i e line i ,<label>(6)</label></formula><p>where w i is the normalized weight of c i calculated based on its length along the vertical axis. The length of the row span of c i is given by u i = max j (y ij ) − min j (y ij ) + 1. We then calculate the normalized weight as w i = u i / Nc k=1 u k . Geometric Ambiguities Minimizing the line desirability cost (6) would correct curves into lines, but there can be multiple solutions as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. It is straightforward to observe that multiple solutions can result from a global rotation r z since it preserves the straightness of lines (first column in <ref type="figure" target="#fig_2">Fig. 3</ref>). Due to the row-wise variation, ambiguities arise in motion estimation that are unique to RS images. Since a global r y corresponds to horizontal translation, a row-wise increase or decrease in r y causes horizontal shearing (third column in <ref type="figure" target="#fig_2">Fig. 3</ref>). Correspondingly, a global r x translates image up or down, and hence a row-wise change in r x causes vertical stretching or shrinking (rows in <ref type="figure" target="#fig_2">Fig. 3)</ref>. Therefore, all these transformations do not disrupt straightness of lines and potentially pose problems in minimizing the line cost. To arrive at a preferred solution, we additionally impose constraints involving the angles and the lengths.</p><p>Angle Desirability Cost To control arbitrary inplane rotation and horizontal shearing, we introduce a desirability cost to control the angles of lines. Ideally, we want the points on the curve c i to be corrected to a GS line with an angle that it actually exhibits in the scene. Though it is not possible to know the exact angles of all GS lines, we note that the confidence of assuring a vertically (or horizontally) oriented RS curve as a vertical (or horizontal) line in the scene is presumably higher as compared to lines at any other angles. Hence, we add an angle cost for curves only in the groups G v and G h . The angle desirability cost for a curve is defined as the squared error between the angle of the least squares fit line (ρ i , θ i ) and the angle corresponding to the curve. This cost for the curve c i is given by</p><formula xml:id="formula_10">e ang i = b i (θ i − θ i ) 2 ,<label>(7)</label></formula><p>where b i = 1 for c i ∈ {G v , G h }, and 0 otherwise, and θ i = 90 • for c i ∈ G v , and θ i = 0 • for c i ∈ G h . The total angle desirability cost is thus written as</p><formula xml:id="formula_11">E ang = 1 Nc k=1 b k Nc i=1 e ang i .<label>(8)</label></formula><p>Minimizing the line cost (6) subject to a low value for the angle desirability cost (8) will ensure that the solution lies in the angle constrained subspace shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Length Desirability Cost To avoid the vertical scaling ambiguity, we add a cost to control the length of the RS corrected curves due to only r x . Even though r z also affects the length of the row-span of lines, it does not lead to complete vertical shrinkage of lines, since r z is controlled by angle cost. Let (x r ij , y r ij ) be the RS corrected point of (x ij , y ij ) using only r x (y) considering r y (y) and r z (y) to be 0. Then, the length of the row span of the r x -corrected c i is given by u r i = max j (y r ij ) − min j (y r ij ) + 1. The length desirability cost is thus written as</p><formula xml:id="formula_12">E len = 1 N c Nc i=1 (u i − u r i ) 2 ,<label>(9)</label></formula><p>where u i is the length of the RS curve c i as defined earlier.</p><p>A low value of this cost limits arbitrary vertical scaling, thus restricting the solution to lie within the length constrained subspace indicated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>Optimization We estimate the polynomial coefficients of the camera motion in an iterative manner by minimizing the line desirability cost <ref type="bibr" target="#b5">(6)</ref> subject to the constraints that the angle cost (8) and the length cost (9) be smaller than preset thresholds. In <ref type="figure" target="#fig_2">Fig. 3</ref>, this is equivalent to locating the intersection of the angle and length constrained subspaces. Thus, we have α * = arg min α E line subject to E ang &lt; ǫ 1 , E len &lt; ǫ 2 , <ref type="formula" target="#formula_0">(10)</ref> where we set ǫ 1 = 10 −4 and ǫ 2 = 1. Initialized with zero camera motion, the constrained nonlinear least squares problem (10) is solved iteratively which converges to a final solution α * . We use the fmincon function in MATLAB to solve <ref type="bibr" target="#b9">(10)</ref>. Corresponding to the polynomial coefficient vector α * , the resultant rotation matrix R * (y) can be obtained for any y using (4).</p><p>To avoid global perspective correction in certain images, zeroth degree motion parameters (global r x and r y rotation) can be left out during optimization, and only the coefficients from first degree could be estimated for these two rotations. Employing global r z in optimization will derotate slanted lines to be vertical (which is visually pleasing) even if the user captures with z-rotation. Not estimating r x at all would leave small curvature in slanted lines, though the result might be visually acceptable in most scenarios. We estimate only nonlinear r x trajectory; linear r x motion (which does not cause curvature) cannot be estimated without very strong priors (e.g. building windows are square).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Image Rectification</head><p>Once the camera motion is estimated, our task is to estimate an intensity for every pixel of the GS image (i.e. the rectified image). The size of the GS image is assumed to be the same as that of the RS image. A forward mapping procedure could be applied using (3) to map intensities from RS pixel locations to GS pixel locations. The drawback of this approach is that due to the row-wise camera motion, not all pixels in the GS image might get an intensity value. Hence, we follow an inverse mapping procedure in which we map every GS pixel to an RS coordinate through the polynomial camera motion.</p><p>For every pixel x GS = (x ′ , y ′ ), we find a real number y * such that applying the motion corresponding to y * on x GS (using the estimated motion R * (y * ) obtained from α * ) would result in vertical RS coordinate y * after warping. This is a nonlinear least squares problem of a continuous variable, and we use Levenberg-Marquardt algorithm <ref type="bibr" target="#b20">[20]</ref> to solve for y * . This algorithm is iterative, and is initialized with y ′ for faster convergence. Let this estimated RS coordinate location be (x * , y * ). We then assign the intensity I RS (x * , y * ) from the RS image as the intensity I GS (x ′ , y ′ ) in the estimated GS image. We bilinearly interpolate the intensity values of four nearest integer locations while arriving at I RS (x * , y * ). The rectification steps are summarized in Algorithm 1. Algorithm 1 RS rectification using inverse mapping.</p><formula xml:id="formula_13">Inputs: Estimated motion R * , RS image I RS for all x GS = (x ′ , y ′ ) in GS image do x GS ← [x ′ , y ′ , 1] T y * = arg min y (y − y) 2 , where x RS ← KR * (y)K −1 x GS , x RS = (x, y) ← [ x RS (1), x RS (2)]/ x RS (3) x RS ← KR * (y * )K −1 x GS , and x RS = (x * , y * ) ← [ x RS (1), x RS (2)]/ x RS (3) I GS (x ′ , y ′ ) ← I RS (x * , y * ) end for Output: GS image I GS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we demonstrate our motion estimation and image rectification results. We also evaluate the performance of the polynomial model against other existing models for an RS image affected by camera motion in the publicly available handshake dataset <ref type="bibr" target="#b16">[17]</ref>. Finally, we compare the performance of our single image method against video as well as nonblind RS rectification methods. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> was taken with a MotoG2 mobile phone. <ref type="figure" target="#fig_3">Fig. 4(a)</ref> shows the detected curves in vertical (green), horizontal (blue), and slanted (yellow) directions. The vertically oriented edges are curved the most, followed by slanted curves, while the horizontal curves are the least affected. The blue lines are not exactly horizontal in this case. We solve (10) using the edge pixels along the curves to estimate camera motion that straightens the curves while respecting angle and length constraints. The RS image is rectified using the final motion estimate as described in Algorithm 1. The rectified image, that was shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, is reproduced in <ref type="figure" target="#fig_3">Fig. 4</ref>(f) but with overlaid edges. Observe that the RS curves are successfully corrected with green lines  aligned to vertical, blue lines aligned to horizontal, and yellow lines retaining their straightness, without disrupting global vertical scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Estimation and Rectification The RS image in</head><p>In <ref type="figure" target="#fig_3">Fig. 4</ref>, we also show the motion trajectory estimates and the corresponding rectified images over iterations while solving <ref type="bibr" target="#b9">(10)</ref>. The algorithm initially corrects the curvatures by varying the rotational motion trajectories. It then proceeds to vary the inplane rotation r z to impose orthogonality between vertical and horizontal lines. The blue lines in <ref type="figure" target="#fig_3">Fig. 4</ref>(f) are closer to horizontal than in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>. The estimated r x motion is minimal without affecting vertical scale, and at the same time, suffices to correct the curvature of slanted lines. It is therefore clear that our method is able to rectify the distortions due to RS effect. The variation of the objective value in (10) and the estimated α * over iterations is shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. The importance of RS rectification for geometric analysis based on vanishing points is provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Motion Models</head><p>We have modelled the camera trajectory during sequential exposure of rows by a polynomial. This is unlike other works that use weighted row-wise motion based on the motion of certain fixed rows (known as key rows). To study the suitability of the polynomial model, we compare its performance against two other RS models, namely, spherical linear interpolation model <ref type="bibr" target="#b26">[26]</ref> (with five equally spaced key rows) and Gaussian interpolation model <ref type="bibr" target="#b9">[10]</ref> (with ten equally spaced key rows). We generate the RS image using a camera trajectory <ref type="bibr">(</ref>  <ref type="figure">Figure 6</ref>. Motion model analysis. Top row. (a) RS image simulated using handshake motion from the dataset of <ref type="bibr" target="#b16">[17]</ref>, rectified images using (b) polynomial, (c) linear interpolation <ref type="bibr" target="#b26">[26]</ref>, and (d) Gaussian interpolation <ref type="bibr" target="#b9">[10]</ref> models. Bottom row shows zoomed-in patches and the plots of ground truth and estimated trajectories.</p><p>picked from the handshake camera motion dataset of <ref type="bibr" target="#b16">[17]</ref>. We employ the same cost function for all the three models to estimate camera motion. The RS image, and the rectified images using polynomial, linear, and Gaussian interpolation models are shown, respectively, in Figs. 6(a), (b), (c), and (d). The estimated motion trajectories are shown in Figs. 6(b2), (c2), and (d2), in which the continuous green path denotes the ground truth trajectory and the dotted paths denote estimated trajectories. Note that the polynomial model closely follows the handshake dataset motion, while the linear interpolation model uses piecewise approximation, and the Gaussian interpolation model exhibits a residual throughout the trajectory. The average angular error for r y ∈ (−0.1, 0.2) • between the ground truth and estimated trajectories for the three models are 0.0012 • , 0.0050 • , 0.0069 • , respectively. The polynomial model exhibits the least estimation error. Though the errors for the other two models are small in value, the residual effect is strikingly visible in the rectified image. The zoomed-in patch in <ref type="figure">Fig. 6</ref>(a1) shows a high curvature region of the RS image. While the rectified patch (b1) corresponding to the polynomial model is visually better, the other two patches (c1) and (d1) show wavy artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Video and Nonblind RS Rectification</head><p>To the best of our knowledge, there are no existing works that deal with RS rectification given a single image. While <ref type="bibr" target="#b31">[31]</ref> uses a single image, it works only in the presence of motion blur. Nevertheless, we compare our method against two contemporary video RS rectification and stabilization methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b9">10]</ref> and a non-blind GS-RS registration method <ref type="bibr" target="#b25">[25]</ref>. For <ref type="bibr" target="#b26">[26]</ref> and <ref type="bibr" target="#b9">[10]</ref>, we use the RS video dataset (captured using mobile phones) as well as the outputs provided by them. For <ref type="bibr" target="#b25">[25]</ref>, the authors sent us their estimated camera motion between the GS and RS images that we captured using a Google Nexus 4 phone and made available to them. Subsequently, we rectified the RS image using our Algorithm 1 (instead of the polynomial model, we directly used their estimated row-wise motion). We must mention here that the aim of these three comparisons is to only gauge whether our blind RS image rectification measures up to the performance of these video and non-blind methods, and not whether our proposed method outperforms them. While <ref type="bibr" target="#b26">[26]</ref> and <ref type="bibr" target="#b9">[10]</ref> use an RS-affected video sequence, <ref type="bibr" target="#b25">[25]</ref> needs the original GS image. In contrast, our method is blind and works on a single RS image. <ref type="figure">Fig. 7</ref>(a) shows a frame of an RS video from the dataset of <ref type="bibr" target="#b26">[26]</ref>; this video is chosen as it has heavy RS distortions. The video suffers mainly from inplane rotations. The vertical posts in the scene appear bent during the capture. The method of <ref type="bibr" target="#b26">[26]</ref> rectifies and stabilizes the video, and the output frame corresponding to (a) is shown in <ref type="figure">Fig. 7(b)</ref>, in which the bent poles are correctly straightened. The global shift is due to video stabilization. In our proposed method, we use only the image (a) to detect curves and to estimate motion. Our single image RS rectification output is shown in <ref type="figure">Fig. 7(c)</ref>. It can be seen that the performance of our method is comparable to <ref type="bibr" target="#b26">[26]</ref> in correcting the RS curves.</p><p>A skew-distorted frame from an RS video of the dataset from <ref type="bibr" target="#b9">[10]</ref> is shown in <ref type="figure">Fig. 7(d)</ref>. The corresponding frame from the rectified video <ref type="bibr" target="#b9">[10]</ref> is shown in <ref type="figure">Fig. 7(e)</ref>, and the result of our single image rectification is shown in <ref type="figure">Fig. 7(f)</ref>. Note that our method corrects the distortions very well; slanted pillars are correctly rendered as vertical.</p><p>Finally, we compare our method with the nonblind rectification of <ref type="bibr" target="#b25">[25]</ref>. We captured two images of the same scene, one without motion (GS image in <ref type="figure">Fig. 7(g)</ref>) and one with motion (RS image in <ref type="figure">Fig. 7(h)</ref>). We then estimated the camera motion using <ref type="bibr" target="#b25">[25]</ref>, and corrected the RS effect using Algorithm 1. The nonblind rectified image is shown in <ref type="figure">Fig. 7</ref>(i). We then use our proposed method to esti-  <ref type="figure">Figure 7</ref>. Comparison with video rectification. (a) RS video frame, (b) rectified video frame <ref type="bibr" target="#b26">[26]</ref>, (c) our single image rectification output, (d) RS video frame, (e) rectified video frame <ref type="bibr" target="#b9">[10]</ref>, and (f) our single image rectification output. Comparison with non-blind rectification.</p><p>(g) GS reference image, (h) RS image, (i) rectified output through the camera motion estimated between (g) and (h) using <ref type="bibr" target="#b25">[25]</ref>, and (j) our single image RS rectification output of (h).</p><p>mate motion from only the RS image, and our blind rectification output is shown in <ref type="figure">Fig. 7</ref>(j). The performance of our method is clearly on par with the two-image nonblind method. To quantify the rectification, we calculated the error during global homography estimation between the GS image and the other three images using 4-point RANSAC estimation based on SIFT correspondences <ref type="bibr" target="#b21">[21]</ref>. The errors for the RS, nonblind, and blind rectified images are 1.4226, 0.3031, and 0.3854, respectively. Our blind rectified output matches the GS image through a global homography (since the row-wise variations are rectified) as good as the nonblind rectified image.</p><p>Presence of Curved Objects Although our method assumes the scene to contain man-made structures with straight lines, it can handle the presence of few natural objects too. <ref type="figure" target="#fig_6">Fig. 8(a)</ref> shows an image of a scene with a tree that is naturally curved and jagged. The RS effect is visually less in this image, but there is a vertical misalignment as can be seen from the marked red line. The presence of the tree affects our cost minimization only marginally, since there are a number of other lines that exert a greater influence on the cost. The rectification rotates the image to closely align with the vertical as shown in <ref type="figure" target="#fig_6">Fig. 8(b)</ref>. <ref type="figure" target="#fig_6">Fig. 8(c)</ref> shows an RS image with many natural curves (white boards) and our method leaves some RS residuals in the rectified image as shown in <ref type="figure" target="#fig_6">Fig. 8(d)</ref>. We have further discussed the effect of the presence of curves and curve breaks in the supplementary material.</p><p>Run-time We implemented our method in MATLAB with warping operations accelerated by C-mex. Our method takes approximately 25 seconds for curve detection, 10 seconds to estimate camera motion, and 5 seconds for RS rectification, for an 816x612 image on a 3.4GHz machine with 8GB RAM. More RS rectification examples are provided in the supplementary material. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Image curved due to RS effect, (b) rectified image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Curve detection (best viewed in PDF). (a1-a4) Curve linking by increasing the bin size for vertical group (Gv) detection. Line segments grouped into curves are shown in same color. (b) Maximum curve length vs. bin size. (c) Final curve detection for all groups Gv, G h , and Gs after outlier removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of our optimization problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Results of our optimization over iterations (best viewed in PDF). (a) first iteration, (b-e) intermediate iterations, and (f) final iteration. Top row. Rectified images using the current motion estimate overlaid with vertical (green), horizontal (blue), and slanted (yellow) edges. Bottom row. Estimated camera trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The line cost reduces in each iteration. Initially, the mean squared difference between α of successive iterations is large when the optimization takes large steps, and it decreases when approaching the final solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>The presence of curved objects (a) Input image containing a tree that is misaligned with the vertical axis, and (b) rectified image. (c) An RS image with many natural curves (white boards), and (d) rectified image with incomplete rectification.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank the reviewers for their valuable comments. The first author thanks Subeesh Vasu for his help in running some comparison methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this paper, we handled the challenging task of correcting RS distortions from a single image of urban scenes. Using the proposed curve detection procedure, we automatically picked good lines and curves as features. We then formulated an optimization problem based on line, angle, and length desirability costs on these features to solve for the underlying (rotation-only) camera motion using which we finally rectify the RS image through inverse mapping. With video and nonblind RS rectification methods being the state-of-the-art, our work opens up new vistas for tackling the RS effect in single images. Experiments reveal that our method, despite being single-image based, performs commendably against existing multi-image methods.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous object pose and velocity computation using a single view from a rolling shutter camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ait-Aider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andreff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lavest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3952</biblScope>
			<biblScope unit="page" from="56" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">R6P-rolling shutter absolute camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Albl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kukelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Line detection in images showing significant lens distortion and application to distortion correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alemán-Flores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Santana-Cedrés</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Removing rolling shutter wobble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic radial distortion estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bukhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Dailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical imaging and vision</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="45" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic single-image 3d reconstructions of indoor manhattan world scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Research</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Straight lines have to be straight. Machine Vision and Applns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Devernay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Use of the Hough transformation to detect lines and curves in pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Calibration-free rolling shutter removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Photography</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stabilizing cell phone video using inertial measurement sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hanning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forslow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Forssén</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ringaby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tornqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Callmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Method and means for recognizing complex patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">654</biblScope>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic 3-d motion estimation for rolling shutter video rectification from visual and inertial measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysis and compensation of rolling shutter effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1323" to="1330" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">levmar: Levenberg-marquardt nonlinear least squares algorithms in C/C++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lourakis</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>web page</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Global optimization of object pose and motion from a single rolling shutter image with automatic 2d-3d matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Magerand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ait-Aider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pizarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="456" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gyro-based multi-image deconvolution for removing handshake blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3366" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A splinebased trajectory representation for sensor fusion and rolling shutter cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patron-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sibley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Change detection in the presence of motion blur and rolling shutter effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pichaikuppan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="123" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient video rectification and stabilisation for cell-phones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ringaby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Forssén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Picture processing by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="147" to="176" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rolling shutter stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Saurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Bouguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3-d depth reconstruction from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A smartphone application for removing handshake blur and compensating rolling shutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sindelar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2160" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rolling shutter motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rolling shutter distortion removal based on curve interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1045" to="1050" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Geometric image parsing in man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tretyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="305" to="321" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="186" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Camera calibration with lens distortion from low-rank textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2321" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
