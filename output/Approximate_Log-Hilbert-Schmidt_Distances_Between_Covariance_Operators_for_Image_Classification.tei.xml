<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Approximate Log-Hilbert-Schmidt Distances between Covariance Operators for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">HÃ </forename><surname>Quang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Analysis and Computer Vision (PAVIS)</orgName>
								<orgName type="institution">Istituto Italiano di Tecnologia (IIT)</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>San Biagio</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Analysis and Computer Vision (PAVIS)</orgName>
								<orgName type="institution">Istituto Italiano di Tecnologia (IIT)</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
							<email>loris.bazzani@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Dartmouth College</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
							<email>vittorio.murino@iit.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Analysis and Computer Vision (PAVIS)</orgName>
								<orgName type="institution">Istituto Italiano di Tecnologia (IIT)</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Approximate Log-Hilbert-Schmidt Distances between Covariance Operators for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel framework for visual object recognition using infinite-dimensional covariance operators of input features, in the paradigm of kernel methods on infinite-dimensional Riemannian manifolds. Our formulation provides a rich representation of image features by exploiting their non-linear correlations, using the power of kernel methods and Riemannian geometry. Theoretically, we provide an approximate formulation for the Log-Hilbert-Schmidt distance between covariance operators that is efficient to compute and scalable to large datasets. Empirically, we apply our framework to the task of image classification on eight different, challenging datasets. In almost all cases, the results obtained outperform other state of the art methods, demonstrating the competitiveness and potential of our framework. Input image K1 Kernel Approximate Covariance Operators K2 Kernel Infinite Feature extraction R G B Ix</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Covariance descriptors are a powerful image representation approach in computer vision. In this approach, an image is compactly represented by a covariance matrix encoding correlations between different features extracted from that image. This representation has been demonstrated to work very well in numerous vision tasks, including tracking <ref type="bibr" target="#b23">[24]</ref>, object detection and classification <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref>, and image retrieval <ref type="bibr" target="#b6">[7]</ref>. Covariance descriptors, properly regularized if necessary, are symmetric positive definite (SPD) matrices, which do not form a vector subspace of Euclidean space under the standard matrix addition and scalar multiplication operations, but form a Riemannian manifold. The optimal measure of similarity between covariance descriptors is thus not the Euclidean distance, but a metric that captures this manifold structure. One of the most commonly used Riemannian metrics in the literature is the Log-Euclidean metric developed by <ref type="bibr" target="#b0">[1]</ref>. This is a so-called bi-invariant Riemannian metric under which the manifold is flat, that is having zero curvature. Therefore, it is efficient to compute and can be used to define many positive definite kernels, allowing kernel methods to be applied directly on the manifold. This latter property has been exploited successfully in various recent work in vision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. However, a major limitation of covariance matrices is that they only capture linear correlations between input features.</p><p>In this work, we propose to use infinite-dimensional covariance operators as image representations. These are covariance matrices of infinite-dimensional features, which are induced implicitly when a positive definite kernel (K 1 in <ref type="figure" target="#fig_0">Fig. 1</ref>), such as the Gaussian kernel, is applied to the original image features. These covariance operators capture in particular non-linear correlations between the original input features. Each image is then represented by one such covariance operator.</p><p>For tasks such as image classification, we require the notion of distance between image representations, which in this case means the distance between the corresponding covariance operators. It is known that covariance operators, properly regularized, lie on the infinite-dimensional Riemannian manifold of positive definite operators <ref type="bibr" target="#b16">[17]</ref>. On this manifold, the generalization of the Log-Euclidean metric is the Log-Hilbert-Schmidt (Log-HS) metric <ref type="bibr" target="#b20">[21]</ref>. Having computed the Log-HS distances between the covariance operators, another positive definite kernel (K 2 in <ref type="figure" target="#fig_0">Fig. 1</ref>) can be computed using these distances and used as input to a kernel classifier, e.g. SVM.</p><p>In essence, the above two steps, namely image representation by covariance operators and kernel classification, together make up a two-layer kernel machine. The kernel K 1 in the first layer, with the low-level image features as input, induces covariance operators which capture non-linear correlations between these features. The kernel K 2 in the second layer, with the Log-HS distances between covariance operators as input, allows the application of kernel methods, e.g. SVM, to these operators. The result of this double kernelization process is a more powerful representation that can better capture the expressiveness of the image features by exploiting both the power of kernel methods and the Riemannian manifold setting of covariance operators.</p><p>However, as with many kernel methods, one drawback of the original Log-HS metric formulation in <ref type="bibr" target="#b20">[21]</ref> is that it tends not scale well to large datasets.</p><p>To carry out the above kernelization efficiently, we develop the following novel mathematical and computational framework. In this paper, we propose an approximate Log-HS distance formulation. This is done by approximating the implicit infinite-dimensional covariance operators above by explicit finite-dimensional covariance matrices, which are computed using explicit approximate feature maps of the original kernel (K 1 in <ref type="figure" target="#fig_0">Fig. 1</ref>). We demonstrate that the approximate Log-HS distance is substantially faster to compute than the true Log-HS distance, with relatively little loss in the performance of the resulting algorithm. The main theoretical contribution of the present work is the mathematical derivation and justification of the approximate Log-HS distance, which goes beyond that for kernel approximation.</p><p>In summary, the novel contributions of our work are the following. Mathematically and computationally,w e present an approximate formulation for the Log-HS distance between covariance operators that is substantially faster to compute than the original formulation in <ref type="bibr" target="#b20">[21]</ref> and that is scalable to large datasets, while substantially maintaining an effective discriminating capability. Empirically, we apply our framework to the task of visual object recognition, using eight challenging, publicly available datatsets, ranging from Fish, Virus, and Texture to Scene recognition. On these datasets, our proposed method compares very favorably with previous state of the art methods in terms of classification accuracy and especially in computational efficiency, demonstrating the competitiveness and potential of our framework.</p><p>Related work. Infinite-dimensional covariance operators of low-level features have been applied to the task of image classification recently by <ref type="bibr" target="#b20">[21]</ref>, which formulated the Log-HS metric, and by <ref type="bibr" target="#b13">[14]</ref>, which used the formulation for Bregman divergences proposed in <ref type="bibr" target="#b34">[35]</ref>. While they both work very well, these methods tend to be computationally intensive and not scalable to large datasets. There exists a large literature on large-scale kernel approximation, which focuses on approximating either the feature maps or the kernel matrices <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27]</ref>, but not on the covariance operators. Approximate affine-invariant distances between covariance operators for image classification have recently been considered in <ref type="bibr" target="#b11">[12]</ref>. However, the affine-invariant distance cannot be used to define positive definite kernels <ref type="bibr" target="#b14">[15]</ref> and, as we show in Sec. 3, this approximation approach is not scalable to large datasets.</p><p>Organization. We give an overview of the Riemannian distances between finite-dimensional covariance matrices and their generalizations to infinite-dimensional covariance operators in Sec. 2. The core of the paper is Sec. 3, in which we present our approximate Log-HS distance formulation, using two methods for computing approximate feature maps and covariance operators. Empirical results on the task of visual object recognition, using eight different datasets, are reported in Sec. 4. Proofs for all mathematical results are given in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Distances between covariance matrices and covariance operators</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Riemannian manifold of SPD matrices</head><p>Let n â N be fixed. Covariance matrices of size n Ã n, properly regularized if necessary, are instances of the set Sym ++ (n) of SPD matrices, which forms a finite-dimensional Riemannian manifold, see e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>. The most commonly encountered Riemannian metric on Sym ++ (n) is the affine-invariant metric, in which the geodesic distance between two SPD matrices A and B is</p><formula xml:id="formula_0">daiE(A, B)=|| log(A â1/2 BA â1/2 )||F ,<label>(1)</label></formula><p>where F denotes the Frobenius norm, which for A = (a ij ) n i,j=1 is given by ||A|| 2 F = n i,j=1 a 2 ij , and log denotes the principal matrix logarithm. From a practical perspective, the distance (1) tends to be computationally intensive for large scale datasets. This motivated the development of the Log-Euclidean metric framework of <ref type="bibr" target="#b0">[1]</ref>, in which the geodesic distance between A and B is given by</p><formula xml:id="formula_1">d logE (A, B)=|| log(A) â log(B)||F ,<label>(2)</label></formula><p>This distance is faster to compute than the distance (1), particularly when computing all pairwise distances on a large set of SPD matrices. Furthermore, the Log-Euclidean metric can be used to define many positive definite kernels, such as the Gaussian kernel, which is not possible using the affine-invariant metric <ref type="bibr" target="#b14">[15]</ref>. We wish to emphasize that the Log-Euclidean metric is itself a Riemannian metric on Sym ++ (n), a so-called bi-invariant metric, which is a fact not always discussed in recent theoretical work in computer vision, e.g. <ref type="bibr" target="#b12">[13]</ref>. This metric arises from the commutative Lie group structure of Sym ++ (n), namely A â B = exp(log(A)+log(B)), introduced by <ref type="bibr" target="#b0">[1]</ref>, where the corresponding geodesic curves and the geodesic distance (2) are derived. In fact, for two commuting matri-ces A and B, the Log-Euclidean and affine-invariant distances are identical, i.e. d logE (A, B)=daiE(A, B). One can see that in Eq. (2), the SPD structure of A and B is encoded via the principal matrix logarithm, which is given by log(A)=U diag(log Î» 1 ,...,log Î» n )U T , where A = U diag(Î» 1 ,...,Î» n )U T is the spectral decomposition for A (note that if A has a negative eigenvalue, the principal logarithm is not defined). In contrast, the standard Euclidean distance ||A â B|| F is defined solely in terms of the entries of A â B, without reflecting any structure in A and B. Consequently, even though Sym ++ (n) has zero curvature under the Log-Euclidean metric, the geodesic distance (2) nevertheless captures better the geometry of Sym ++ (n) than the Euclidean distance ||A â B||F . This has also been consistently demonstrated empirically, see e.g <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>The SPD matrices considered in the current work are covariance matrices of features extracted from input data. Specifically, let XâR n . Let x =[ x 1 ,...,x m ] be a data matrix sampled from X , where m is the number of observations. In the setting of the current work, there is one such matrix for each image, namely the matrix of low-level features sampled at (a subset of) the pixels in the image. Each image is then represented by the n Ã n covariance matrix</p><formula xml:id="formula_2">Cx = 1 m xJmx T : R n â R n ,<label>(3)</label></formula><p>where Jm is the centering matrix, defined by Jm = Im â 1 m 1m1 T m with 1m =(1,...,1) T â R m . In practice, Cx is generally only positive semi-definite and thus to apply the Riemannian structure of Sym ++ (n), it is often necessary to consider the regularized version (Cx + Î³In) for some Î³&gt;0. For two covariance matrices Cx and Cy, we therefore consider the distance between the regularized versions (Cx + Î³I) and (Cy + Î¼I), given by</p><formula xml:id="formula_3">d logE = || log(Cx + Î³In) â log(Cy + Î¼In)||F ,<label>(4)</label></formula><p>for some regularization parameters Î³&gt;0, Î¼&gt;0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Infinite-dimensional covariance operators</head><p>The covariance matrix Cx only measures the linear correlations between the features in the input data. A powerful method to capture non-linear input correlations is by (i) first mapping the original input features into a high dimensional feature space H, using an implicit nonlinear feature map induced by a positive definite kernel; (ii) then computing the covariance operators in the feature space H.</p><p>Specifically, let X be an arbitrary non-empty set. Let x =[ x1,...,xm] be a data matrix sampled from X , where m â N is the number of observations. Let K be a positive definite kernel on XÃX and HK its induced reproducing kernel Hilbert space (RKHS). Let H be any feature space for K, which we assume to be a separable Hilbert space, with the corresponding feature map Î¦: XâH , so that K(x, y)= Î¦(x), Î¦(y) H for all pairs (x, y) âXÃX. For concreteness, we can identify H with the RKHS HK , or with the space of square summable se-</p><formula xml:id="formula_4">quences â 2 = {(a k ) kâN : â k=1 |a k | 2 &lt; â}. The feature map Î¦ gives the (potentially infinite) mapped data matrix Î¦(x)=[ Î¦ ( x1),...,Î¦(xm)] of size dim(H) Ã m in the fea- ture space H. The corresponding covariance operator for Î¦(x) is defined to be C Î¦(x) = 1 m Î¦(x)JmÎ¦(x) T : HâH,<label>(5)</label></formula><p>which can be considered as a (potentially infinite) matrix of size dim(H) Ã dim(H).I fX = R n and K(x, y)= x, y , then C Î¦(x) = Cx as in <ref type="formula" target="#formula_2">(3)</ref>.</p><p>Infinite-dimensional affine-invariant metric. As in the finite-dimensional case, we need to consider the regularized covariance operator (C Î¦(x) + Î³IH), Î³&gt;0, which lies on the infinite-dimensional manifold Î£(H) of positive definite operators on H. Let A : HâHbe a bounded linear operator and A * be its adjoint operator. We recall that A : HâHis said to be a Hilbert-Schmidt operator, denoted by A â HS(H),if</p><formula xml:id="formula_5">||A|| 2 HS =tr(A * A)= â k=1 Î» k (A * A) &lt; â,<label>(6)</label></formula><p>where || ||HS denotes the Hilbert-Schmidt norm, the infinite-dimensional generalization of the Frobenius norm, and {Î» k (A * A)} â k=1 denotes the set of eigenvalues of A * A. By the formulation of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref></p><formula xml:id="formula_6">, the infinite-dimensional affine-invariant distance daiHS between (C Î¦(x) + Î³I) and (C Î¦(y) + Î¼I) is given by daiHS[(C Î¦(x) + Î³I), (C Î¦(y) + Î¼I)] (7) = || log[(C Î¦(x) + Î³I) â1/2 (C Î¦(y) + Î¼I)(C Î¦(x) + Î³I) â1/2 ]||eHS,</formula><p>with the extended Hilbert-Schmidt norm || || eHS given by</p><formula xml:id="formula_7">||A + Î³I|| 2 eHS = ||A|| 2 HS + Î³ 2 .</formula><p>Log-Hilbert-Schmidt metric. The generalization of the Log-Euclidean metric to the infinite-dimensional manifold Î£(H) has recently been formulated by <ref type="bibr" target="#b20">[21]</ref>. In this metric, termed Log-Hilbert-Schmidt metric,o rLog-HS for short,</p><formula xml:id="formula_8">the distance d logHS [(C Î¦(x) + Î³IH), (C Î¦(y) + Î¼IH)] between (C Î¦(x) + Î³IH) and (C Î¦(y) + Î¼IH) is given by d logHS [(C Î¦(x) + Î³IH), (C Î¦(y) + Î¼IH)] (8) = || log(C Î¦(x) + Î³IH) â log(C Î¦(y) + Î¼IH)||eHS,</formula><p>which has a closed form in terms of the corresponding Gram matrices (we refer to <ref type="bibr" target="#b20">[21]</ref> for the explicit expression).</p><p>Regularization. The form of regularization (A + Î³I), Î³&gt;0, which is often empirically necessary to ensure positive definiteness in the case dim(H) &lt; â, is always necessary, both theoretically and empirically, when dim(H)= â, since in this case log(A), with A being a covariance operator, is always unbounded (see <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>).</p><p>As in the finite-dimensional case, two key advantages of the Log-HS distance d logHS over the affine-invariant distance d aiHS are: (i) the d logHS distance is faster to compute than the d aiHS distance; (ii) it is straightforward to define many commonly used positive definite kernels, such as the Gaussian kernel, using the d logHS distance, which is not the case with the d aiHS distance. These advantages are fully exploited in the current work.</p><p>In the next section, we describe how to approximate Formula <ref type="bibr" target="#b7">(8)</ref> to compute the pairwise distances on a set of data matrices {x i } N i=1 when N and m are large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approximate Log-HS Distance</head><p>While kernel methods are powerful in learning nonlinear structures in data, they tend not to scale well, in terms of computational complexity, to large datasets, which are common in vision problems such as object recognition and fine-grained categorization. In the typical kernel learning setting, the feature map Î¦ is high-dimensional (and often infinite-dimensional, as in the case of the Gaussian kernel) and thus is only used implicitly. Instead, exact kernel methods carry out computations using Gram matrices and thus their computational complexities depend on the sizes of the Gram matrices, which become very large for large datasets.</p><p>A commonly used approach that has emerged recently to reduce the computational cost of kernel methods is to compute an explicit approximate feature mapÎ¦D : Xâ R D , where D is finite and D&lt; &lt;dim(H), so that</p><formula xml:id="formula_9">Î¦ D (x),Î¦D(y) R D =KD(x, y) â K(x, y), with (9) lim DââK D (x, y)=K(x, y) â(x, y) âXÃX.<label>(10)</label></formula><p>The approximate feature mapÎ¦D is then used directly in the learning algorithms instead of the Gram matrices.</p><p>In our setting, we use the approximate feature maps to compute the corresponding finite-dimensional approximate covariance operators. The Log-Euclidean distance between the approximate operators is then used as the approximate version of Log-HS distance between the infinitedimensional covariance operators. Thus we are not interested in the approximate kernel valuesK D (x, y) per se, but the approximate covariance operators and the corresponding approximate Log-HS distance. The mathematical justification for the latter goes beyond that for kernel approximation and is the main theoretical contribution of this work.</p><p>In fact, as we show in Theorems 1 and 2 below, Eq. (10), which guarantees the convergence of the approximate kernel value to the true kernel value, is not sufficient to guarantee the convergence of the approximate Log-HS distance to the true Log-HS distance as D ââ . This convergence is non-trivial and requires further assumptions, which are practically realizable.</p><p>Approximate covariance operator and approximate Log-HS distance. With the approximate feature map Î¦D, we have the corresponding data matrixÎ¦</p><formula xml:id="formula_10">D (x)= [Î¦ D (x 1 ),...,Î¦ D (x m )]</formula><p>of size D Ã m, and the approximate covariance operator has the form</p><formula xml:id="formula_11">CÎ¦ D (x) = 1 mÎ¦ D (x)JmÎ¦D(x) T : R D â R D ,<label>(11)</label></formula><p>which is a matrix of size D Ã D, instead of the potentially infinite matrix C Î¦(x) of size dim(H) Ã dim(H).</p><p>We then consider the following as an approximate version of the Log-HS distance given in Formula <ref type="formula">(8)</ref>:</p><formula xml:id="formula_12">log CÎ¦ D (x) + Î³ID â log CÎ¦ D (y) + Î¼ID F .<label>(12)</label></formula><p>Key theoretical question. We need to determine whether Formula <ref type="formula" target="#formula_0">(12)</ref> is truly a finite-dimensional approximation of Formula <ref type="bibr" target="#b7">(8)</ref>, in the sense that</p><formula xml:id="formula_13">lim Dââ log(CÎ¦ D (x) + Î³ID) â log(CÎ¦ D (y) + Î¼ID) F = || log(C Î¦(x) + Î³IH) â log(C Î¦(y) + Î¼IH)||eHS.<label>(13)</label></formula><p>The answer to this question is the main mathematical contribution of the current paper. It turns out that in general, this is not possible. This is because the infinite-dimensional Log-HS distance is generally not obtainable as a limit of the finite-dimensional Log-Euclidean distance as the dimension approaches infinity <ref type="bibr" target="#b20">[21]</ref>. More precisely, we have Theorem 1 Assume that Î³ = Âµ, Î³&gt;0, Âµ&gt;0. Then</p><formula xml:id="formula_14">lim Dââ log(CÎ¦ D (x) + Î³ID) â log(CÎ¦ D (y) + Î¼ID) F = â.</formula><p>The infinite limit in Theorem 1 stands in sharp contrast to that of Eq. (10) on the approximability of the kernel value K(x, y) itself, which is satisfied by both approximation schemes based on Fourier features presented below.</p><p>In practice, however, it is reasonable to assume that we can use the same regularization parameter for both CÎ¦</p><formula xml:id="formula_15">D (x)</formula><p>and CÎ¦ D (y) , that is to set Î³ = Î¼. In this setting, we obtain the necessary convergence, as follows. Theorem 2 Assume that Î³ = Âµ&gt;0. Then In light of Theorems 1 and 2, subsequently we employ the same regularization parameter Î³&gt;0 to compute approximate Log-HS distances between all regularized operators (CÎ¦ D (x) + Î³ID). In this work, we employ two methods for computing the approximate feature mapÎ¦ D , namely Random Fourier features <ref type="bibr" target="#b24">[25]</ref> and Quasi-random Fourier features <ref type="bibr" target="#b33">[34]</ref>, presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fourier feature maps</head><p>Random Fourier feature maps. This is the approach in <ref type="bibr" target="#b24">[25]</ref> for computing approximate feature maps of shiftinvariant kernels. Let K : R n Ã R n â R be a kernel of the form K(x, y)=k(xây) for some positive definite function k on R n . By Bochner's Theorem <ref type="bibr" target="#b25">[26]</ref>, there is a finite positive measure Ï on R n such that</p><formula xml:id="formula_16">k(x â y)= R n e âi Ï,xây dÏ(Ï) (15) = R n Ï(Ï)ÏÏ(x)ÏÏ(y)dÏ, where ÏÏ(x)=e âi Ï,x .</formula><p>Without loss of generality, we can assume that Ï is a probability measure on R n , so that K(x, y)=EÏâ¼Ï[ÏÏ(x)ÏÏ(y)]. By symmetry, K(x, y)= 1 2 [K(x, y)+K(y, x)], so that by the relation <ref type="bibr" target="#b0">1</ref> 2 [e âi Ï,xây + e i Ï,xây ]=cos( Ï, x â y ) we have</p><formula xml:id="formula_17">K(x, y)= R n cos( Ï, x â y )Ï(Ï)dÏ.<label>(16)</label></formula><p>To approximate K(x, y), we can sample D points {Ïj} D j=1 from the distribution Ï and compute the empirical version</p><formula xml:id="formula_18">KD(x, y)= 1 D D j=1 cos( Ïj,xâ y ) Dââ â ââââ K(x, y)<label>(17)</label></formula><p>almost surely by the law of large numbers. Let W = (Ï1,...,ÏD) be a matrix of size n Ã D, with each column Ïj â R n randomly sampled according to Ï. Motivated by the cosine addition formula, cos( Ïj,x â y )= cos Ïj,x cos Ïj,y +sin Ïj,x sin Ïj,y , we define</p><formula xml:id="formula_19">cos(W T x)=(cos Ïj,x ) D j=1 ,<label>(18)</label></formula><formula xml:id="formula_20">sin(W T x)=(sin Ïj,x ) D j=1 .<label>(19)</label></formula><p>The desired approximate feature map is the concatenation</p><formula xml:id="formula_21">Î¦D(x)= 1 â D (cos(W T x); sin(W T x)) â R 2D ,<label>(20)</label></formula><p>with Î¦ D (x),Î¦D(y) =KD(x, y). In the case of the Gaus-</p><formula xml:id="formula_22">sian kernel K(x, y)=e â ||xây|| 2 Ï 2 (used in the experiments in Sec. 4), we have Ï(Ï)= (Ï â Ï) n (2Ï) n e â Ï 2 ||Ï|| 2 4 â¼N 0, 2 Ï 2 .<label>(21)</label></formula><p>Quasi-random Fourier feature maps. The Random Fourier feature maps above arise from the Monte-Carlo approximation of the kernel K expressed as the integral in Eq. (15), using a random set of points Ï j 's sampled according to the distribution Ï. An alternative approach, proposed by <ref type="bibr" target="#b33">[34]</ref>, employs Quasi-Monte Carlo integration <ref type="bibr" target="#b8">[9]</ref>, in which the Ï j 's are deterministic points arising from a low-discrepancy sequence in [0, 1] n . We describe this approach in detail in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">New positive definite kernels using approximate Log-HS distances</head><p>In our framework, starting with a shift-invariant kernel K 1 in <ref type="figure" target="#fig_0">Fig. 1</ref>, we compute the approximate feature map Î¦ D (x) using the methods in Sec. 3.1 and the corresponding approximate covariance operators using Eq. (11). The approximate Log-HS distances between these approximate covariance operators are then computed using Eq. <ref type="bibr" target="#b11">(12)</ref>.</p><p>With the approximate Log-HS distances, we can define a new positive definite kernel (K 2 in <ref type="figure" target="#fig_0">Fig. 1</ref>), for example</p><formula xml:id="formula_23">exp(â|| log(CÎ¦ D (x) + Î³ID) â log(CÎ¦ D (y) + Î³ID)|| p F /Ï 2 ),<label>(22)</label></formula><p>for 0 &lt;pâ¤ 2, with p =2giving the Gaussian kernel and p =1giving the Laplacian kernel. This new kernel can then be used in a classifier, e.g. SVM. The complete pipeline for our framework is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Computational complexity</head><p>We present here the computational analysis of the proposed approximation in Eq. (12) as well as the comparison with the approximate affine-invariant distance proposed by <ref type="bibr" target="#b11">[12]</ref>, according to the formula</p><formula xml:id="formula_24">log[(CÎ¦ D (x) + Î³I) â1/2 (CÎ¦ D (y) + Î¼I)(CÎ¦ D (x) + Î³I) â1/2 ] F .<label>(23)</label></formula><p>The main computational cost in Eq. <ref type="formula" target="#formula_0">(12)</ref> is the SVD for (CÎ¦ D (x) + Î³I) and (CÎ¦ D (y) + Î¼I), which takes time O(D 3 ). At first glance, the computational complexity for the approximate affine-invariant distance in Eq. (23), which consists of a matrix square root and inversion, two matrix multiplications and an SVD, is also O(D 3 ). However, computationally, the key difference between Eq. (12) and Eq. <ref type="formula" target="#formula_1">(23)</ref>is that in Eq. We also note that for N pairs of data matrices, the computational complexity of the exact Log-HS formulation <ref type="bibr" target="#b20">[21]</ref> and the RKHS Bregman divergences <ref type="bibr" target="#b13">[14]</ref> is of order O(N 2 m 3 ). Thus for D&lt;mand N large, the approximate Log-HS formulation will be much more efficient to compute than both the exact Log-HS and the RKHS Bregman divergences (see the actual running time comparison between the approximate and exact Log-HS formulations in the experiments below). Algorithm 1: Summary of the proposed method.</p><p>Further comparison with <ref type="bibr" target="#b11">[12]</ref>.In <ref type="bibr" target="#b11">[12]</ref>, the authors proposed two methods: (i) Nearest Neighbor using effectively the approximate affine-invariant distance given in Eq. <ref type="bibr" target="#b22">(23)</ref> with Î³ = Î¼ =0and (ii) the CDL algorithm using the representation log(CÎ¦ D (x) ). Both of these methods require the assumption that CÎ¦ D (x) is positive definite, which is never guaranteed. In fact, when D&gt;m , CÎ¦ D (x) is always rankdeficient and neither its inverse nor log can be computed. Thus neither the CDL nor the approximate affine-invariant distance can be used. Theoretically, since it does not employ any regularization, the approximate affine-invariant distance in <ref type="bibr" target="#b11">[12]</ref> will not approach the exact affine-invariant distance ( <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>) for large D, which always requires regularization (see Sec. 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In this section, we show the performance of the proposed method compared with other state-of-the-art approaches on eight image classification datasets. In all the datasets, we used the same features and experimental protocols of the compared state-of-the-art approaches (see details below). The following methods were evaluated: LogE, using the Log-Euclidean metric, Stein, using the Stein (also called Jensen-Bregman LogDet) divergence <ref type="bibr" target="#b6">[7]</ref>, Log-HS, using the Log-HS metric <ref type="bibr" target="#b20">[21]</ref> induced by the Gaussian kernel (K 1 in <ref type="figure" target="#fig_0">Fig. 1</ref>, but only on the Fish dataset, see below), and Approx LogHS and QApprox LogHS induced by the Gaussian kernel, using the proposed random Fourier and Quasi-random Fourier approximation methods in Sec. 3, respectively. For a good trade-off between speed and accuracy, we set the approximate feature dimension D = 200 (Eq. 20) for Approx LogHS and QApprox LogHS (more details below). Except with Stein, all experiments used LIBSVM <ref type="bibr" target="#b5">[6]</ref> for classification, with the Gaussian kernel defined on top of the corresponding metric (K 2 in <ref type="figure" target="#fig_0">Fig. 1</ref>). For Stein, since the corresponding Gaussian kernel is generally not guaranteed to be positive definite <ref type="bibr" target="#b27">[28]</ref>, we used the Nearest Neighbor approach as in <ref type="bibr" target="#b6">[7]</ref>. We also compared the above methods with CDL <ref type="bibr" target="#b31">[32]</ref>, one of the state-of-the-art approaches in covariance-based learning. On the Fish dataset, we also carried out experiments with the Euclidean (E) and Hilbert-Schmidt (HS) metrics to demonstrate the advantage of the Riemannian geometric framework 1 . The performance is evaluated in terms of classification accuracy (more details below). All parameters were chosen by cross-validation.</p><p>For each image, at the pixel location (x, y), the following image features were extracted (the actual features vary between the different datasets, since for fairness we followed the same protocols of the methods we compared with)</p><p>F (x, y)=[x, y, I(x, y), |Ix|, |Iy|, |Ixx|, |Iyy|, R(x, y),G(x, y),B(x, y), |G (o,s)</p><p>x,y |]</p><p>where I,I x ,I y ,I xx ,I yy denote the intensity and its firstand second-order derivatives, respectively, R,G, and B de- , KTH-TIPS2b Material <ref type="bibr" target="#b4">[5]</ref>, Texture <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, ETHZ <ref type="bibr" target="#b9">[10]</ref>, UIUC <ref type="bibr" target="#b19">[20]</ref>, Tiny-Graz03 <ref type="bibr" target="#b32">[33]</ref>, and ETH80 <ref type="bibr" target="#b17">[18]</ref>.</p><p>note the color values, and G o,s x,y denotes the Gabor filter at orientation o and scale s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The Fish recognition dataset <ref type="bibr" target="#b2">[3]</ref> contains 27, 370 fish images acquired from live video. There are altogether 23 classes of fish and the number of images per class ranges from 21 to 12, 112, with a mean resolution of 150 Ã 120 pixels. The significant variations in color, pose and illumination inside each class make this dataset very challenging. We conducted two different experiments on this dataset, using the R,G,B features from Eq. (24), as in <ref type="bibr" target="#b20">[21]</ref>. The first experiment (named Exp1) is a small scale experiment which used the same protocol and the 10 splits provided by <ref type="bibr" target="#b20">[21]</ref>, consisting altogether of 345 images, divided in 115 images for training (5 per class) and 230 images for testing (10 per class) . We analyzed the performances of Approx LogHS and QApprox LogHS with respect to the original Log-HS metric formulation of <ref type="bibr" target="#b20">[21]</ref>. Furthermore, we compared the computational cost and running time of the two approximate versions with respect to <ref type="bibr" target="#b20">[21]</ref>. In the second experiment (named Exp2), the entire Fish dataset was used to show the scalability of the proposed method. We note that this experiment was not carried out using the exact Log-HS metric due to its high computational cost in terms of memory and speed. In this case, since the number of testing samples are different between the classes, the classification performance is evaluated using the Average Precision (AP) measure, a standard metric used by the PASCAL challenge <ref type="bibr" target="#b10">[11]</ref>.</p><p>The Virus Classification dataset <ref type="bibr" target="#b15">[16]</ref> contains 15 different virus classes. Each class has 100 images of size 41 Ã 41. For this dataset, following <ref type="bibr" target="#b13">[14]</ref>, we employed the 25-dimensional feature vector consisting of the intensity, 4 gradients, and 20 Gabor filters in Eq. (24)at4 orientations and 5 scales. We used the 10 splits provided by the authors in a leave-one-out manner, i.e. 9 splits for training and 1 split as query, repeating the procedure 10 times.</p><p>The KTH-TIPS2b Material dataset <ref type="bibr" target="#b4">[5]</ref> contains images of 11 materials captured under 4 illuminations, in 3 poses and at 9 scales, giving 108 images for each sample in a category, with 4 samples per material. For each category, we trained on 3 samples and tested on the remaining sample. For this dataset, following <ref type="bibr" target="#b13">[14]</ref>, we extracted the 23-dimensional feature consisting of the R,G,B values and 20 Gabor filters in Eq. (24)at4 orientations and 5 scales.</p><p>The Texture dataset for our experiments was created by combining 111 texture images of the Brodatz dataset <ref type="bibr" target="#b3">[4]</ref> and 61 of the CURET dataset <ref type="bibr" target="#b7">[8]</ref>, as done in <ref type="bibr" target="#b6">[7]</ref>. Unfortunately, we were not able to reproduce the experiments in <ref type="bibr" target="#b6">[7]</ref> since the exact protocols, i.e. the number of patches extracted from each image and the number of training/testing images, were not specified. We therefore carried out a similar experiment by extracting 150 patches of size 20 Ã 20 from each image, taking 140 as training and 10 as testing, repeating the entire procedure 10 times. For this dataset, we extracted the 5-dimensional feature vector [x, y, I, |I x |, |I y |] as in <ref type="bibr" target="#b6">[7]</ref>.</p><p>For person re-identification, we used two sequences of the ETHZ dataset <ref type="bibr" target="#b9">[10]</ref>. SEQ. #1 contains 83 pedestrians in 4, 857 images. SEQ. #2 contains 35 pedestrians in <ref type="bibr" target="#b0">1,</ref><ref type="bibr">936</ref> images. As in <ref type="bibr" target="#b13">[14]</ref>, we used 10 images from each subject for training and the rest for testing. Following <ref type="bibr" target="#b13">[14]</ref>, we extracted the 17-dimensional feature vector consisting of [x, y, R, G, B] and the first-and second-order color derivatives, i.e. [| âr âx |, | âr ây |, | â 2 r âx 2 |, | â 2 r ây 2 |], for r = R, G, B. The performance was evaluated using the Average Precision metric.</p><p>The UIUC dataset <ref type="bibr" target="#b19">[20]</ref> contains 18 different material categories collected in the wild, with a total of 216 images. Following <ref type="bibr" target="#b11">[12]</ref>, we extracted the 19-dimensional vector consisting of 3 colors, 4 gradients, and 12 Gabor filters in Eq.(24)at4 orientations and 3 scales. As in <ref type="bibr" target="#b11">[12]</ref>, we split the database into training and test sets by randomly assigning half of the images of each class to the training set and testing on the rest. This process was repeated 10 times.</p><p>The TinyGraz03 dataset <ref type="bibr" target="#b32">[33]</ref> contains 1148 indoor and outdoor scenes with an image resolution of 32 Ã 32 pixels. The images are divided in 20 classes with at least 40 samples per class. We used the recommended train/test split provided by the authors. For this dataset, following <ref type="bibr" target="#b11">[12]</ref>, at each pixel we extracted the 7-dimensional feature vector [|I x |, |I y |, |I xx |, |I yy |,R,G,B] from Eq. <ref type="bibr" target="#b23">(24)</ref>. This dataset is highly challenging and the correct recognition rate achieved by humans is only 30% <ref type="bibr" target="#b32">[33]</ref>.</p><p>The ETH80 dataset <ref type="bibr" target="#b17">[18]</ref> contains images of eight object categories: apples, cows, cups, dogs, horses, pears, tomatoes, and cars. Each category includes ten object subcategories (e.g. various dogs) in 41 orientations, resulting in 410 images per category. We randomly chose 21 images for training and the rest for testing, repeating the procedure 10 times. For this dataset, following <ref type="bibr" target="#b14">[15]</ref>, at each pixel we extracted the 5-dimensional feature vector [x, y, I(x, y), |I x |, |I y |] from Eq. <ref type="bibr" target="#b23">(24)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis and discussion of results</head><p>Classification performance compared with exact Log-HS metric. First of all, we ran a comparative experiment on a subset of the Fish dataset <ref type="bibr" target="#b2">[3]</ref>, as in <ref type="bibr" target="#b20">[21]</ref>, to analyze the performances of the Approx LogHS and QApprox LogHS formulations with respect to the original Log-HS formulation of <ref type="bibr" target="#b20">[21]</ref>. For this dataset, we show results obtained using R,G,B features as in <ref type="bibr" target="#b20">[21]</ref> with all methods. Column two of Tab.1 shows the mean and standard deviation values for the classification accuracies computed over all 10 random splits. The first important observation we note is that Approx LogHS and QApprox LogHS gave results (first and second rows) which are comparable with those using the Log-HS metric and substantially better than other methods, namely LogE, Stein, and CDL. Furthermore, the Riemannian distance LogE substantially outperforms the Euclidean distance E, and similarly LogHS, Approx LogHS, and QApprox LogHS all outperform the Hilbert-Schmidt distance HS. This clearly demonstrates the advantage of the Riemannian geometric framework.</p><p>Running time compared with exact Log-HS metric. Most importantly, Approx LogHS and QApprox LogHS incur much smaller computation costs compared to Log-HS. In fact, using MATLAB on an Intel Xeon E5-2650, 2.60 GHz PC, we obtained a speed up of 30Ã with QApprox LogHS (Train: 6.7sec. Test: 18sec.) and more than 50Ã with Approx LogHS (Train: <ref type="bibr" target="#b2">3</ref>  <ref type="table">Table 2</ref>. Best results obtained on seven different datasets in terms of classification accuracy. The first two rows represent the proposed method using the two-layer kernel machine using Approx LogHS and QApprox LogHS with Gaussian SVM. The last row represents the state-of-the-art results on each dataset.</p><p>With this computational speed-up, we next ran a second experiment (third column) using the whole Fish dataset. The classification accuracy shows an improvement of 10% and 11.5% for Approx LogHS and QApprox LogHS, respectively, with respect to LogE and an improvement of 15.4% and 16.9% with respect to the Stein divergence <ref type="bibr" target="#b6">[7]</ref>.</p><p>Comparison against other state of the art (SoA) methods. <ref type="table">Table 2</ref> shows the results of the proposed method (first two rows) on seven different datasets in comparison with the respective state-of-the-art results. The best result <ref type="bibr" target="#b11">[12]</ref> reported for the Virus dataset is an accuracy of 82.5% (last row). Our classification accuracy is slightly lower than this result but it outperforms all the other competitors (LogE, Stein and CDL,by9.6%, 31.8%, and 12%, respectively). Our results on the KTH-TIPS2b Material dataset improves the state of the art <ref type="bibr" target="#b13">[14]</ref>by3.5% (third column). The accuracy of the proposed method on the Texture dataset (forth column) is 23.1% higher of the best of the other competitors. The fifth and sixth columns of <ref type="table">Table  2</ref> report the results on two sequences of the ETHZRe-ID dataset. In this case, we obtained an improvement of 1.8% over the state-of-the-art <ref type="bibr" target="#b13">[14]</ref>. Regarding the UIUC and the TinyGraz03 datasets, the improvement over the previous results of <ref type="bibr" target="#b11">[12]</ref>, is 2.7% and 3%, respectively. Our method outperforms the recent state of the art <ref type="bibr" target="#b28">[29]</ref> also on the ETH80 dataset with an improvement of 11.4%.</p><p>These improvements in classification accuracies all demonstrate the effectiveness of our proposed method. More importantly, we emphasize that our approximate Log-HS formulations are much more computationally efficient than both the RKHS Bregman divergences in <ref type="bibr" target="#b13">[14]</ref> and the approximate affine-invariant distance used in <ref type="bibr" target="#b11">[12]</ref>, as discussed in Sec. 3.3.</p><p>Increasing the approximate feature dimension.W e also carried out a set of experiments to show that the classification accuracy improves when increasing the approximate feature dimension D in Eq. <ref type="bibr" target="#b19">(20)</ref>, as expected. We tested this on the TinyGraz03 dataset, with D = 200, 400, 1000, with the results reported in Tab. 3. However, as also expected,  <ref type="table">Table 3</ref>. Results on the TinyGraz03 dataset <ref type="bibr" target="#b32">[33]</ref> with increasing values of the approximate feature dimension D. We also reported the training time in seconds below each accuracy.</p><p>the improvement in classification accuracy comes at a larger computational cost. In fact, the training time for D = 1000 is 63 and 48 times slower than D = 200 for Approx LogHS and QApprox LogHS, respectively. In other words, there is a very large computational speed up factor from D = 1000 to D = 200 with a very low drop in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion, discussion, and future work</head><p>In this paper, we have presented a novel mathematical and computational framework for visual object recognition using infinite-dimensional covariance operators of image features, in the paradigm of kernel methods and Riemannian geometry. Our approximate Log-HS distance formulation is substantially faster to compute than the original Log-HS, with relatively little loss in classification accuracy, and is scalable to large datasets. Empirically, our framework compares very favorably with previous state of the art methods in terms of classification accuracy and especially in computational complexity.</p><p>As ongoing work, we are investigating the direction of replacing low-level hand-crafted features used in our current experiments with feature learning techniques, such as convolutional networks. Preliminary experiments on the Fish dataset show that we obtain an improvement of approximately 15% by using convolutional features in combination with the proposed approximate Log-HS distance formulation. We will report the full results in a future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Model of the proposed framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>D (x) + Î³ID) â log(CÎ¦ D (y) + Î³ID) F = || log(C Î¦(x) + Î³IH) â log(C Î¦(y) + Î³IH)||eHS.<ref type="bibr" target="#b13">(14)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b11">(12)</ref>,(CÎ¦ D (x) +Î³I) and (CÎ¦ D (y) +Î¼I) are uncoupled, whereas in Eq. (23), they are coupled. Thus if we have N data matrices {x1,...,xN }, to compute their pairwise approximate Log-HS distances using Eq. (12), we need to compute an SVD for each (CÎ¦ D (x i ) + Î³I), with time complexity O(ND 3 ). On the other hand, to compute their pairwise approximate affine-invariant distances using Eq. (23), we need to compute an SVD for each pair (CÎ¦ D (x i ) + Î³I), (CÎ¦ D (x j ) + Î³I), with time complexity O(N 2 D 3 ). Thus the approximation of the Log-HS distance is O(N ) times faster than the approximation of the affine-invariance distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Sample images for datasets used in this work. From top to bottom: Fish Recognition<ref type="bibr" target="#b2">[3]</ref>, Virus Classification<ref type="bibr" target="#b15">[16]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Input: Set of images. Output: Kernel matrix (used as input to a classifier, e.g. SVM). Parameters: Kernels K1, K2, regularization parameters Î³ = Î¼&gt;0, approximate feature dimension D. Procedure:1. For each image, extract a data matrix x =[x1,...,xm] of low-level features from m pixels.</figDesc><table>2. For each image, compute the approximate feature mapÅ 
Î¦D(xi), 1 â¤ i â¤ m, associated to kernel K1, according 
to Eq. (20), and the corresponding approximate 
covariance operator CÎ¦ 
D (x) , according to Eq. (11). 
3. For each pair of images, compute the approximate 
Log-HS distance between the corresponding covariance 
operators, according to Eq. (12). 

4. Using kernel K2, compute a kernel matrix using the 
above approximate Log-HS distance, e.g. according to 
Eq. (22). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>.6sec. Test: 9.9sec.) with respect to the baseline Log-HS (Train: 175.7sec. Test: 565.1sec.). Because of this substantial speed up in running time, subsequently we focused solely on the performance of Approx LogHS and QApprox LogHS for all the other datasets.</figDesc><table>Method 

Virus 
KTH-TIPS2b 
Texture 
ETHZRe-ID 
UIUC 
TinyGraz03 
ETH80 
Acc % 
Acc % 
Acc % 
Acc-Seq1 % 
Acc-Seq2 % 
Acc % 
Acc % 
Acc % 

Approx 
LogHS 

81.5% 
(Â±2.1) 

83.6% 
(Â±5.4) 

76.9% 
(Â±0.5) 

92.0% 
(Â±0.3) 

93.2% 
(Â±0.5) 

50.1% 
(Â±3.7) 
60% 
95.0% 
(Â±0.5) 

QApprox 
LogHS 

76.5% 
(Â±3.2) 

83.46% 
(Â±5.6) 

76.4% 
(Â±0.6) 

91.9% 
(Â±0.5) 

93.0% 
(Â±0.5) 

44.7% 
(Â±3.6) 
57% 
94.9% 
(Â±0.6) 

LogE 
71.9% 
(Â±4.0) 

74.1% 
(Â±7.4) 

52.9% 
(Â±0.8) 

89.9% 
(Â±0.2) 

91.9% 
(Â±0.4) 

37.8% 
(Â±2.6) 
40% 
71.1% 
(Â±1.0) 

Stein [7] 
49.7% 
(Â±4.8) 

73.1% 
(Â±8.0) 

38.4% 
(Â±0.7) 

89.6% 
(Â±0.8) 

90.9% 
(Â±0.2) 

27.9% 
(Â±1.7) 
24% 
67.5% 
(Â±0.4) 

CDL [32] 
69.5% 
(Â±3.1) 

76.3% 
(Â±5.11) 

53.8% 
(Â±0.5) 

86.8% 
(Â±0.6) 

88.8% 
(Â±1.2) 

36.3% 
(Â±2.0) 
41% 
56.0% 
(Â±0.6) 

SoA 
82.5% 
(Â±2.9) 
[12] 
80.1% 
(Â±4.6) 
[14] 
N/A 
90.2% 
(Â±1.0) 
[14] 
91.4% 
(Â±0.8) 
[14] 
47.4% 
(Â±3.1) 
[12] 
57% 
[12] 

83.6% 
(Â±6.1) 
[29] 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Due to lack of space, the experimental results for the E and HS metrics on the other datasets will be reported in the longer version of the paper.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geometric means in a novel vector space structure on symmetric positive-definite matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Positive Definite Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A research tool for long-term and continuous analysis of fish assemblage in coral-reefs using underwater camera footage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Boom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Textures: a photographic album for artists and designers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brodatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>Dover Publications</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-specific material categorisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mallikarjuna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TIST</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="27" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Jensen-Bregman LogDet divergence with application to efficient similarity search for covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2161" to="2174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Koenderink. Reflectance and texture of real-world surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-dimensional integration: the quasi-Monte Carlo way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sloan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="133" to="288" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth and appearance for mobile scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximate infinitedimensional region covariance descriptors for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geodesic exponential kernels: When curvature and linearity conflict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lauze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bregman divergences for infinite dimensional covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kernel methods on the Riemannian manifold of symmetric positive definite matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segmentation of virus particle candidates in transmission electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kylberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uppstroem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-O</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borgefors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-M</forename><surname>Sintorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Microscopy</title>
		<imprint>
			<biblScope unit="volume">245</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="147" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonpositive curvature: A geometrical approach to Hilbert-Schmidt operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larotonda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Differential Geometry and its Applications</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analyzing appearance and contour based methods for object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Log-Euclidean kernels for sparse representation and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-parametric filtering for geometric detail extraction and material representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Log-Hilbert-Schmidt metric between positive definite operators on Hilbert spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">San</forename><surname>Biagio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Affine-invariant Riemannian distance between infinite-dimensional covariance operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Q</forename><surname>Minh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geometric Science of Information</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Riemannian framework for tensor computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="66" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Covariance tracking using model update based on Lie algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="728" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Methods of Modern Mathematical Physics: Fourier analysis, self-adjointness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Memory efficient kernel approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new metric on the manifold of kernel matrices with application to matrix geometric means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grassmann manifold for nearest points image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="190" to="196" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tosato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<title level="m">Characterizing humans on Riemannian manifolds. PAMI</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1972" to="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pedestrian detection via classification on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1713" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Covariance discriminative learning: A natural and efficient approach to image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2496" to="2503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scene categorization from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Annual Workshop of the Austrian Association for Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quasi-Monte Carlo feature maps for shift-invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">From sample similarity to ensemble similarity: Probabilistic distance measures in reproducing kernel Hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="917" to="929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
