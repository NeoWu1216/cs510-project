<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>fyang@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland College Park</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
							<email>wongun@nec-labs.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
							<email>ylin@nec-labs.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we investigate two new strategies to detect objects accurately and efficiently using deep convolutional neural network: 1) scale-dependent pooling and 2) layerwise cascaded rejection classifiers. The scale-dependent pooling (SDP) improves detection accuracy by exploiting appropriate convolutional features depending on the scale of candidate object proposals. The cascaded rejection classifiers (CRC) effectively utilize convolutional features and eliminate negative object proposals in a cascaded manner, which greatly speeds up the detection while maintaining high accuracy. In combination of the two, our method achieves significantly better accuracy compared to other state-of-the-arts in three challenging datasets, PASCAL object detection challenge, KITTI object detection benchmark and newly collected Inner-city dataset, while being more efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, deep convolutional neural network (CNN) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> contributed much to various computer vision problems including image classification, object detection, semantic segmentation, etc, thanks to its capability to learn discriminative features (or representations) at different levels of granularities. A number of recent studies <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref> suggest that high level visual semantics (such as motif, parts, or objects) are appearing in the middle of deep architecture which in turn provide strong cues to recognize complex visual concepts. Leveraging on the representational power of CNN, a number of methods are proposed to detect objects in natural images using CNN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b41">42]</ref>. R-CNN <ref type="bibr" target="#b13">[14]</ref> has been proposed for object detection and achieves promising results, where a pre-trained network is fine-tuned to classify object proposals. However, both training and testing suffer from low efficiency since the network * This work was done at NEC Laboratories America. <ref type="figure">Figure 1</ref>. We present a fast and accurate object detection method using the convolutional neural network. Our method exploits the convolutional features in all layers to reject easy negatives via cascaded rejection classifiers and evaluate surviving proposals using our scale-dependent pooling method. performs a forward pass on every single object proposal independently. Convolutional filters are repeatedly applied to thousands of object proposals that are redundant and expensive. In order to reduce the computational cost, recent CNN based object detectors, such as Fast RCNN <ref type="bibr" target="#b12">[13]</ref> and spatial pyramid pooling networks (SPPnet) <ref type="bibr" target="#b15">[16]</ref>, share the features generated by the convolutional layers and apply a multi-class classifier for each proposal. In Fast RCNN, convolutional operations are done only once on the whole image, while the features for object proposals are pooled from the last convolutional layer and fed into fully-connected (fc) layers to evaluate the likelihood of object categories. Compared to R-CNN <ref type="bibr" target="#b13">[14]</ref>, these methods improve the efficiency in the order of magnitude via shared convolutional layers. For instance, Fast RCNN achieves 3× and 10 ∼ 100× speedup at training and test stage, respectively. In order to deal with scale variation, multi-scale image inputs are often used where one set of convolutional features are obtained per image scale. Despite its success, these approaches have certain drawbacks that make them less flexible. First, Fast RCNN does not handle small objects well. Since the bounding boxes are pooled directly from the last convolutional layer rather than being warped into a canonical size, they may not contain enough information for decision if the boxes are too small. Multi-scale input scheme fundamentally limits the applicability of very deep architecture like <ref type="bibr" target="#b31">[32]</ref> due to memory constraints and additional computational burden. In addition, pooling a huge number of candidate bounding boxes and feeding them into highdimensional fc layers can be extremely time-consuming.</p><p>In this work, we attempt to address aforementioned drawbacks and propose a new CNN architecture for an accurate and efficient object detection in images. The first contribution is that, unlike previous works, our method produces only one set of convolutional features for an image while handling the scale variation via multiple scaledependent classifier models. Our intuition is that visual semantic concepts of an object can emerge in different convolutional layers depending on the size of the target objects, if proper supervision is provided in the training process. For instance, if a target object is small, we may observe a strong activation of convolutional neurons in earlier layers (e.g. conv3) that encode specific parts of an object. On the other hand, if a target object is large, the same part concept will emerge in much later layers (e.g. conv5). Based on this intuition, we represent a candidate bounding box using the convolutional features pooled from a layer corresponding to its scale (scale-dependent pooling (SDP)). The features are fed into multiple scale-dependent object classifiers to evaluate the likelihood of object categories. As for the second contribution, we present a novel cascaded rejection classifier (CRC) where the cascading direction is defined over the convolutional layers in the CNN. We treat the convolutional features in early layers as a weak classifier in the spirit of boosting classifiers <ref type="bibr" target="#b9">[10]</ref>. Although the features from earlier convolutional layers might be too weak to make a strong evaluation of an object category, they are still useful to quickly reject easy negatives. Combining the two strategies, we can explicitly utilize the convolutional features at all layers instead of using only the last one as previous works do. Our method is illustrated in <ref type="figure">Figure 1</ref>. We evaluate our model using three object detection datasets, PASCAL object detection challenge <ref type="bibr" target="#b7">[8]</ref>, KITTI object detection benchmark <ref type="bibr" target="#b10">[11]</ref> and newly collected Innercity dataset. In all experiments, we observe that our method can achieve significantly higher detection accuracy compared to the other methods with much higher computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>CNN for object detection. Before the emergence of CNN, deformable part model (DPM) <ref type="bibr" target="#b8">[9]</ref> has been the state-of-theart object detector for years. With the exceptional power on image classification, CNN has been applied to object detection and achieves promising results <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>. In <ref type="bibr" target="#b33">[34]</ref>, detection is treated as a regression problem to object bounding box masks. A deep neural network is learned to generate and localize object boxes. Erhan et al. <ref type="bibr" target="#b6">[7]</ref> design a deep network to propose class-agnostic bounding boxes for generic object detection. Sermanet et al. <ref type="bibr" target="#b30">[31]</ref> use a regression network pre-trained for classification tasks to predict object bounding boxes in an exhaustive and computationally expensive way. Each bounding box is associated with a confidence score indicating the presence of an object class. Recently, Girshick et al. <ref type="bibr" target="#b13">[14]</ref> propose the R-CNN framework that uses object proposals generated by selective search to fine-tune a pre-trained network for detection tasks. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> extend R-CNN by gradually generating bounding boxes within a search region and imposing a structured loss to penalize localization inaccuracy in network fine-tuning. To reduce the cost of forward pass for each proposal in R-CNN, Fast RCNN <ref type="bibr" target="#b12">[13]</ref> has been proposed by sharing convolutional features and pooling object proposals only from the last convolutional layer. More recently, Faster RCNN <ref type="bibr" target="#b28">[29]</ref> replaces the object proposals generated by selective search by a region proposal network (RPN) and achieves further speed-up. Neural network cascades. The Viola-Jones cascaded face detector <ref type="bibr" target="#b35">[36]</ref> and its extensions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref> have been widely used. The idea of eliminating candidates by combining a series of simple features is also applied to CNNs. Sun et al. <ref type="bibr" target="#b32">[33]</ref> present an ensemble of networks by combining networks focusing on different facial parts for facial point detection. Li et al. <ref type="bibr" target="#b20">[21]</ref> use a shallow detection network with small scale input images to first reject easy non-face samples, and then apply two deeper networks to eliminate more negatives while maintaining a high recall. A calibration network is appended after each detection network for bounding box calibration. More recently, Angelova et al. <ref type="bibr" target="#b0">[1]</ref> combine a tiny deep network and a modified AlexNet to achieve realtime pedestrian detection. The tiny deep network removes a large number of candidates and leaves a manageable size of candidates for the large network to evaluate. Our approach is significantly different from prior methods in that we consider cascaded classifiers by utilizing features from different convolutional layers within a single network, that does not introduce any additional computation. Using convolutional features. A few works exploit features from different convolutional layers, either by concatenating them or by other popular encoding techniques. One of the most representative works is <ref type="bibr" target="#b14">[15]</ref>, where neuron activations at a pixel of different feature maps are concatenated as a vector as a pixel descriptor for localization and segmentation. Similarly, in the fully convolutional network <ref type="bibr" target="#b22">[23]</ref>, feature maps from intermediate level and high level convolutional layers are combined to provide both finer details and higher-level semantics for better image segmentation. Xu et al. <ref type="bibr" target="#b39">[40]</ref> extract convolutional features in the same way and encode these feature vectors by VLAD and Fisher vector for efficient video event detection. DeepProposal <ref type="bibr" target="#b11">[12]</ref> is used to generate object proposals in a coarse-to-fine manner. Proposals are first generated in higher level convolutional layers that preserve more semantic information, and are gradually refined in lower layers that provide better localization. Similarly, Karianakis et al. <ref type="bibr" target="#b16">[17]</ref> use lower-level convolutional features to generate object proposals by sliding window and remove background proposals, while refining them using higher-level convolutional features in a hierarchical way. For edge detection, Bertasius et al. <ref type="bibr" target="#b2">[3]</ref> extract a sub-volume from every convolutional layers, perform three types of pooling and again concatenate these values into a single vector, which is further fed into fc layers. Recently, Xie and Tu <ref type="bibr" target="#b38">[39]</ref> propose a holistically-nested edge detection scheme inspired by <ref type="bibr" target="#b19">[20]</ref>. In the network, sideoutputs are added after several early convolutional layers to provide deep supervision for predicting edges at multiple scales. A weighted-fusion layer combines all the sideoutputs where the combination weights are learned during network training. In contrast to these works, our approach does not explicitly combine convolutional features, but learns classifiers separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scale-Dependent Pooling</head><p>Scale variation is a fundamental challenge in visual recognition. Previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref> often adopted a sliding window technique with image pyramids to handle the scale variation of target objects. Similar techniques are applied to recent CNN based object recognition methods: they treat the last convolutional layer's outputs (conv5 of AlexNet) as the features to describe an object and apply a classifier (fc layers) on top of the extracted features. R-CNN <ref type="bibr" target="#b13">[14]</ref> warps an image patch within a proposal that produces fixed dimensional feature output for classification. The independent warping process prohibits us to share any convolutional operations across proposals in the same image, which fundamentally limits the efficiency. In contrast, SPPnet <ref type="bibr" target="#b15">[16]</ref> and Fast RCNN <ref type="bibr" target="#b12">[13]</ref> share the convolutional features in an image and pool the features at the last convolutional layer to describe an object. In these methods, the scale variation is tackled either via image pyramid inputs or brute-force learning that directly learns the scale variation via the convolutional filters. However, the image pyramid introduces additional computational burden and requires large amount of GPU memories, and brute-force learning via convolutional filters is difficult.</p><p>As for a new contribution, we introduce a scaledependent pooling (SDP) technique (illustrated in the Figure 2) to effectively handle the scale variation in object detection problem. Our method is built based on the recent Fast RCNN <ref type="bibr" target="#b12">[13]</ref> method that pools the features for each bounding box proposal from the last convolutional layer of CNN. The region inside of each proposal is divided into a spatial grid (7 × 7 or 6 × 6) and features are pooled using max-pooling over each grid. Our SDP method examines the scale (height) of each object proposal and pools the features from a corresponding convolutional layer depending on the height. For instance, if an object proposal has a height between 0 to 64 pixels, the features are pooled from the 3 rd convolutional layer of CNN (SDP 3). On the other hand, if an object proposal has a height larger than 128 pixels, we pool the features from the last convolutional layer (SDP 5) (see <ref type="figure" target="#fig_0">Figure 2</ref>). The fc layers attached to SDPs have their own set of parameters so as to learn scale-specific classification models from different sets of feature inputs.</p><p>The main benefit of SDP is that we can effectively tackle the scale variation of target objects while computing the convolutional features only once per image. Instead of artificially resizing the input images in order to obtain a proper feature description, the SDP selects a proper feature layer to describe an object proposal. It reduces computational cost and memory overhead caused by redundant convolutional operations. Another benefit is that the SDP results in a compact and consistent representation of object proposals. Since the brute-force approach of Fast RCNN <ref type="bibr" target="#b12">[13]</ref> pools the features for object proposals from the last convolutional layer, often the same features are repeated over the spatial grid if an object proposal is very small. The max-pooling or multiple pixel stride in convolutional layers progressively reduces the spatial resolution of the convolutional features over layers. Thus, at the conv5 layer, there is only one feature for large number of pixels (16 pixels for both AlexNet <ref type="bibr" target="#b17">[18]</ref> and VGG16 <ref type="bibr" target="#b31">[32]</ref>). In the extreme case, if the object proposal is as small as 16 × 16 pixels, all the grid features may be filled with a repeating single feature. Learning from such an irregular description of object examples may prohibit us from learning a strong classification model. Since the SDPs distribute the proposals depending on the scale, we can provide more consistent signal through the learning process, leading to a better detection model.</p><p>The idea of using intermediate convolutional layers to complement high level convolutional features has also been exploited for image classification and segmentation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref>, video event detection <ref type="bibr" target="#b39">[40]</ref>, image retrieval <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref> and edge detection <ref type="bibr" target="#b38">[39]</ref>. We note that our approach differs from previous works that directly concatenate or combine outputs of multiple convolutional layers. We explicitly associate classification models (fc layers) with different convolutional layers, which provides strong supervision to enforce more discriminative convolutional filters. Unlike <ref type="bibr" target="#b38">[39]</ref>, we do not fuse multiple side-outputs into a single output, so that each side-output is not affected by other outputs, and can be adjusted independently to handle a specific scale of objects, making the training more flexible and robust.</p><p>We present our SDP model based on VGG16 <ref type="bibr" target="#b31">[32]</ref> in <ref type="figure" target="#fig_0">Figure 2</ref>. This SDP model has 3 branches after conv3, conv4 and conv5. Each branch consists of a RoI pooling layer connected to 2 successive fc layers with ReLU activations and dropout layers for calculating class scores and bounding box regressors, similarly to <ref type="bibr" target="#b12">[13]</ref>. We initialize the model parameters of convolutional layers and the fc layers in the SDP 5 with the Image-Net pre-trained model of VGG16 <ref type="bibr" target="#b31">[32]</ref>. The fc layers in the SDP 3 and SDP 4 are randomly initialized. During fine-tuning, input object proposals are first distributed into 3 groups based on their height and then fed into corresponding RoI pooling layers to pool the features from corresponding convolutional outputs. Gradients are back-propagated from 3 branches to update corresponding fc layers and convolutional filters. By providing supervision about the scale of input object proposals, we explicitly enforce neurons to learn for different scales of objects, so that the convolutional layers are able to discover small objects at early stage. The experiments demonstrate that this simple modification effectively improves detection accuracy on small objects by a large margin (see Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Cascaded Rejection Classifiers</head><p>One major computational bottleneck in our SDP method and Fast RCNN <ref type="bibr" target="#b12">[13]</ref> is on the evaluation of individual object proposals using high dimensional fc layers. When there are tens of thousands of proposals, time spent for the perproposal evaluation dominates in the entire detection process (see <ref type="table">Table 4</ref>). As for the second contribution, we introduce a novel cascaded rejection classifier (CRC) scheme that requires minimal amount of additional computation. Cascaded detection framework has been widely adopted in visual detection problems that includes <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>. The core idea is to use as little as possible computations to reduce the object proposals quickly and use complex and timeconsuming features for only few highly likely candidate locations. Recently, a few methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1]</ref> are proposed to use cascaded detection framework with CNN, but most of them employ another simpler network to "preprocess" object proposals and use a deeper architecture to evaluate surviving candidates. Unlike the others, we exploit the convolutional features progressively in a single network to build the cascaded rejection classifiers. Since cascaded classifiers gradually reject negatives using stronger features, they resemble the behaviour of deep networks where more semantic and constrained features gradually emerge to help discriminate objects. Therefore, our choice of boosting classifiers is reasonable and consistent with the network characteristics. Our model does not require any additional convolutional feature computation.</p><p>We adopt the popular discrete AdaBoost <ref type="bibr" target="#b9">[10]</ref> algorithm to learn CRCs after each convolutional layer. Following the intuition of our SDP models, we learn separate rejection classifiers per scale-group (R l s where s and l represent a scale-group and the convolutional layer) in order to keep the classifiers compact while effective (see <ref type="figure" target="#fig_1">Figure 3</ref>). In following paragraphs, we assume that we have a CNN model trained with SDPs without loss of generality. Learning CRCs. Let us first define necessary notations to learn a CRC R l s . Suppose we have N proposals that belong to a scale group s, B = [B 1 , B 2 , ..., B N ] and corresponding foreground label y i , i = 1, ..., N . y i = 1 if it contains a foreground object and y i = 0, otherwise. We pool the corresponding features x i for B i ∈ B from the convolutional layer l using the CNN model trained with our SDPs. In our experiments, we use the RoIP ooling scheme of <ref type="bibr" target="#b12">[13]</ref>, which gives m × m × c dimensional features, where m = 7 and c is the number of channels in the convolutional layer. Through this process, we obtain a training dataset of</p><formula xml:id="formula_0">X = [x 1 , x 2 , ..., x N ] ∈ R m 2 c×N , and Y = {0, 1} ∈ R N .</formula><p>Given the training dataset, we learn a linear boosting classifier R l s with <ref type="bibr" target="#b9">[10]</ref> that aggregates a set of weaklearners' response outputs, R l</p><formula xml:id="formula_1">s (x) = T t=1 w t h t (x),</formula><p>where h t is a weak learner, w t is the corresponding weight and the output is the classification score. In this work, a weak learner h t is a decision stump that outputs 1 if the value x v at the v th feature dimension is greater than a decision threshold δ v and -1 otherwise, that can be written as h t (x) = sign(x v − δ v ). We learn 50 weak-learners per R l s . After learning the boosting classifier, we train the rejection threshold that keeps 99% of positive training examples. All surviving training examples are passed to train the next rejection classifier R l+1 s . In order to learn progressively stronger rejection classifiers without additional computational cost, the weak-learners used in the previous R l s are used to initialize the boosting classifier training in the next R l+1 s . CRCs in Testing. Since we know which features must be pooled after training the CRCs, we pool only the necessary features in the testing time (the feature pooling layer in <ref type="figure" target="#fig_1">Figure 3)</ref>. Given the 50 dimensional pool of weak-learners, we approximate the boosting classifier with 2 fc layers and a hyperbolic tangent tanh layer, so as to utilize the computational modules in the CNN framework. The first fc layer applies the translation of the features with δ v , which is followed by the tanh layer that approximates the sign function. Finally, all the weak-learners are aggregated via the last fc layer to produce the final boosting classification score using w. If available (l &gt; 1), the previous rejection classifier R l−1 s score is added before rejecting an object proposal. The detailed structure of the CRC is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. We observe that the cascaded rejection classifiers achieves about 3.2× speedup for the proposal evaluation (4.6× when combined with truncated SVD <ref type="bibr" target="#b12">[13]</ref>, see <ref type="table">Table 4</ref>) with a marginal loss of accuracy. Instead of simply connecting boosting classifiers to the network, we show that a boosting classifier can be decomposed into a composition of network layers, which provides new insights to converting traditional classifiers into deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets. We evaluate our model with SDP and CRC on 3 datasets: KITTI detection benchmark <ref type="bibr" target="#b10">[11]</ref> , PASCAL VOC2007 <ref type="bibr" target="#b7">[8]</ref> and newly collected Inner-city dataset. The KITTI dataset is composed of 7, 481 images for training, and 7, 518 images for testing. The training dataset contains 28, 742, 4, 487, and 1, 627 number of car, pedestrian and cyclist annotations. Since the ground-truth annotation of testing set is not publicly available, we use the training/validation split of <ref type="bibr" target="#b37">[38]</ref> for the analysis. For more thorough analysis, we collected a new dataset (Inner-city). The dataset is composed of 24, 509 images which are collected using a camera mounted on a car. The dataset is composed of 16, 028 training and 8, 481 testing images which contains 60, 658, 36, 547, 16, 842, and 14, 414 numbers of car, person, bike and truck instances, respectively. The images are sub-sampled 15 frames apart from 47 number of video sequences to avoid having highly correlated images. Networks. Our CNN model is initialized with a deep network architecture (VGG16 <ref type="bibr" target="#b31">[32]</ref>) trained on the Image-Net classification dataset <ref type="bibr" target="#b29">[30]</ref>. Rather than having SDP branches for all convolutional layers, we add 3 SDP branches after 3 convolutional layers before max pooling, which are conv3 3 (SDP 3), conv4 3 (SDP 4) and conv5 3 (SDP 5) of VGG16, to ensure the features are discriminative enough. We use scale groups of height between [0, 64) for SDP 3, [64, 128) for SDP 4, and [128, ∞) for SDP 5. The fc layers in the SDP 5 are initialized with the pretrained model parameters, while the fc layers in the SDP 3 and SDP 4 are randomly initialized. All the fc layers have 4096 dimensional outputs. After fine-tuning, we train rejection classifiers for each scale group using the convolutional features from conv1 2, conv2 2, conv3 3, conv4 3 and conv5 3, resulting in 12 rejection classifiers. Due to limited amount of training samples of VOC2007, we only use two SDP branches after conv3 3 ([0, 64) for SDP 3) and conv5 3 ([64, ∞) for SDP 5). We use 2048-dim fc layers for SDP 3 which give better detection accuracy than that by 1024-dim and 4096-dim fc layers. Training Parameters. Following the procedure introduced in [13], we randomly sample two images, from which we randomly sample 128 positive and negative object proposals per scale group as a minibatch. The negative object proposals are sampled from all the proposals that have less than 0.5 overlap with any positive ground-truth annotation. For all the experiments, we use initial learning rate of 0.0005 and decrease it by 0.1 after every 30K iterations. We use the momentum 0.9 and the weight decay 0.0005. The final model is obtained after 90K iterations. We found that using smaller dropout ratio helps to improve the detection accuracy on KITTI and Inner-city in our experiments, so we use a dropout ratio 0.25 after fc layers, while 0.5 for VOC2007. Box Proposals. We obtain the bounding box proposals using Edgebox <ref type="bibr" target="#b44">[45]</ref> and augment them with ACF <ref type="bibr" target="#b4">[5]</ref> detection outputs trained for Car and Person categories. We observe that using only generic box proposal methods often misses small target objects, which leads to poor detection accuracy. We use the same proposals provided by <ref type="bibr" target="#b12">[13]</ref> for VOC2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Detection Accuracy Analysis</head><p>We first discuss the detection accuracy on the KITTI train/validation dataset, Inner-city dataset, and VOC2007 dataset. We mainly compare our model against two baseline methods using Fast RCNN models <ref type="bibr" target="#b12">[13]</ref> with AlexNet <ref type="bibr" target="#b17">[18]</ref> and VGG16 <ref type="bibr" target="#b31">[32]</ref> architectures. For the KITTI train/validation experiment, all the training and testing images are rescaled to 500 pixel height which gives the best accuracy given GPU (K40/K80) memory constraints. We use multi-scale image inputs of 400, 800, 1200, 1600 pixel heights for the AlexNet baseline and 400, 800 pixel heights for the VGG16 baseline to handle the scale variation as well as possible. We also compare the VGG16 baseline and our SDP using other single scale inputs (400 and 800). In Inner-city experiments, we keep the original size of the image (420 pixel height) and use 420, 840, 1260, 1680 for the AlexNet baseline. We use both single scale input (600) and multi-scale inputs (300, 400, 500, 600 for training and 500, 600 for testing) for SDP models in VOC2007 experiments. In order to highlight the challenges posed by the scale variation, we present the accuracy comparison over different size groups in <ref type="table" target="#tab_1">Table 1</ref>. Following KITTI <ref type="bibr" target="#b10">[11]</ref>, we use 0.7 overlap ratio for the Car category and 0.5 for the others in the evaluation. In the VOC2007 and Inner-city evaluation, we use 0.5 overlap ratio across all categories. Results by SDP. <ref type="table" target="#tab_1">Table 1</ref> shows that the multi-scale image input baselines achieves similar detection accuracy across different scale groups, since features are pooled at appropriate scales. On the other hand, deeper architecture with a single image input baseline achieves higher accuracy on larger objects exploiting the rich semantic features in the deep architecture, but performs relatively poorly on small objects. Even using only 400 pixel heights, SDP already achieves better overall accuracy than VGG16 using 2 scales. Our SDP model with the same VGG16 architecture achieves highest accuracy on almost all scale groups over all the categories. Given that the multi-scale setting takes more time and occupies more GPU memory, our SDP is more efficient and practical in detecting various scales of objects. More importantly, we greatly improve the detection accuracy on the smallest scale group by 5 ∼ 20% thanks to the SDP branches for the intermediate convolultional layers, which confirms our hypothesis that small objects can be better recognized at lower layers if proper supervision is provided. Another important observation is that we achieve larger improvement on the Car category which has the largest number of training examples. Since our model has additional parameters to be trained (fc layers in SDP 3 and SDP 4), we expect that our model will improve even more when more training examples are provided. This is demonstrated in the Inner-city experiments presented in <ref type="table" target="#tab_1">Table 1</ref> that contains larger number of training examples. A few qualitative results are presented in <ref type="figure" target="#fig_2">Figure 4</ref>. On VOC2007, we improve the AP on several categories and obtain the overall mAP 68.2% and 69.4% using single scale and multi-scale setting, respectively, which are 1.3% higher than <ref type="bibr" target="#b12">[13]</ref> using the same configuration. Especially, we observe significant improvements on small objects like bottle and plants.</p><p>It should be noted that the number of training samples, especially those containing small objects, are much less than that on KITTI dataset. Since we train the fc layers connected to the intermediate layers from scratch, insufficient samples may negatively affect the performance. Results by CRC. Next, we evaluate the performance of our CRCs. As described in Sec. 4, we reject object proposals through our CRCs throughout the convolutional layers. With the CRC modules (denoted as SDP+CRC in <ref type="table" target="#tab_1">Table 1</ref>), the performance decreases very marginally, indicating that CRCs successfully eliminate negative proposals while maintaining a high recall rate for positives (see <ref type="table" target="#tab_2">Table 3</ref> for details), even though we only use 50 feature dimensions at each convolutional layer. The results demonstrate that the intermediate convolutional layers can be exploited in a hierarchical way. Fine-tuning with CRC. We train the network with the CRC modules in the training process. The CRC modules can serve as a hard-negative mining process to learn better classification model in the network, since many easy negatives are rejected before reaching the SDP modules. Instead of randomly sampling 128 proposals in the training process, we sample 128 proposals among the survived proposals after using all the CRCs. We run the fine-tuning for additional 50K iterations with initial learning rate 0.0001 with step size 20K iterations. We freeze the learning rate of convolutional layers to avoid CRC parameters being invalid after the fine-tuning. We observe that the additional fine-tuning (SDP+CRC ft) helps to improve the accuracy over the SDP+CRC marginally. In KITTI testing result (Table 2), we observe larger improvement with the additional </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Discussions</head><p>Rejection ratio. By using CRCs, we aim to improve the efficiency for the proposal evaluation by progressively re-  . Qualitative results on KITTI validation set and Inner-city dataset using FRCN <ref type="bibr" target="#b12">[13]</ref>+VGG16 baseline and our SDP model. We obtain the detection threshold for visualization at the precision 0.8. The color of bounding boxes means the types of objects. Notice that our method with SDP detects small objects much better than the baseline method. The figure is best shown in color. troduced by CRCs. <ref type="table">Table 4</ref> analyzes detailed computational break-down of various methods. We measure the time spent in each component of the network, such as convolutional operations, fc layer computations, pre-and post-processing, etc. We compare our CRCs with the truncated SVD approach <ref type="bibr" target="#b12">[13]</ref> that aims to reduce the dimensionality of the fc layers. We follow the strategy in <ref type="bibr" target="#b12">[13]</ref> to keep the top 1024 singular values from the 1 st fc layer and the top 256 singular values from the 2 nd fc with respect to each SDP branch. In addition, we combine CRC and SVD, i.e., using CRC to eliminate proposals and SVD to compress fc layers in SDPs, to achieve further speed-up. We include the baseline methods without SVD as a reference. The truncated SVD approach alone achieves about 2.3× gain in proposal evaluations. The CRC modules alone obtain 3.2× speed up for the same operation. We gain 4 ∼ 5× for each SDP by rejecting 70 ∼ 80% of proposals, but the additional computation introduced by CRC reduces the overall gain slightly. When combined with the SVD and CRC, we obtain 4.6× efficiency gain in proposal evaluations and 2.4× in total (including conv operations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we investigate two new strategies to detect objects efficiently using deep convolutional neural network, 1) scale-dependent pooling and 2) layer-wise cas- <ref type="table">Table 4</ref>. Run-time comparison (ms per image) among the baseline methods, our method with truncated SVD <ref type="bibr" target="#b12">[13]</ref>, our method with CRC and SVD+CRC on KITTI dataset. fcS, fcM , and fcL refer to the SDP classifiers for the scale group [0, 64), [64, 128), [128, ∞), respectively. "box eval." represents the time spent for individual box evaluation including fc layers and CRC rejections. The times were measured using an Nvidia K40 GPU under the same experimental environment. caded rejection classifiers. The scale-dependent pooling (SDP) improves detection accuracy especially on small objects by fine-tuning a network with scale-specific branches attached after several convolutional layers. The cascaded rejection classifiers (CRC) effectively utilize convolutional features and eliminate negative object proposals in a cascaded manner, which greatly speeds up the detection while maintaining high accuracy. Our experimental evaluation clearly demonstrates the benefits of SDP and CRC in CNN based object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Details of our Scale-dependent Pooling (SDP) model on 16-layer VGG net. For better illustration, we show the groups of convolutional filters between max pooling layers as a cube, where filters are arranged side-by-side, separated by lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Structure of the rejection classifier approximation by network layers. Blue cuboid corresponds to a proposal on the feature maps. Color squares are feature points that need to be pooled out to form the feature vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Qualitative results on KITTI validation set and Inner-city dataset using FRCN [13]+VGG16 baseline and our SDP model. We obtain the detection threshold for visualization at the precision 0.8. The color of bounding boxes means the types of objects. Notice that our method with SDP detects small objects much better than the baseline method. The figure is best shown in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Detection AP (%) of the other state-of-the-art approaches and our method on KITTI test set. Following KITTI protocol, results are grouped into three levels of difficulties: Easy (E), Moderate (M) and Hard (H).</figDesc><table>Car 
Pedestrian 
Cyclist 
Method 
E 
M 
H 
E 
M 
H 
E 
M 
H 
Regionlet [37] 
84.75 76.45 59.70 73.14 61.15 55.21 70.41 58.72 51.83 
DPM-VOC+VP [28] 74.95 64.71 48.76 59.48 44.86 40.37 42.43 31.08 28.23 
3DVP [38] 
87.46 75.77 65.38 
-
-
-
-
-
-
SubCat [26] 
84.14 75.46 59.71 
-
-
-
-
-
-
CompACT-Deep [4] 
-
-
-
70.69 58.74 52.71 
-
-
-
DeepParts [35] 
-
-
-
70.49 58.67 52.78 
-
-
-
FRCN [13]+VGG16 85.98 72.32 60.16 75.50 62.53 58.14 68.82 54.21 47.98 
SDP 
88.34 81.69 69.72 76.89 64.44 59.72 70.13 60.08 52.93 
SDP+CRC 
88.33 81.17 70.00 76.28 63.12 58.30 71.06 60.24 53.17 
SDP+CRC ft 
90.33 83.53 71.13 77.74 64.19 59.27 74.08 61.31 53.97 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>fine-tuning. We believe that it will achieve more improvement if all the model parameters including CRC modules are trained. We plan to explore this as a future direction. Test set evaluation. To compare with existing approaches on KITTI test set, we train our SDP and CRC model on the full training set, and evaluate it on the test set. The results are shown inTable 2. We use the same configuration and learning parameters as in the previous experiments. Without using any stereo information, our approach outperforms all compared methods on all levels of difficulties and achieves the best results. In particular, Our method using SDP again outperforms the Fast RCNN baseline by 9% on average, verifying the effectiveness of SDP module. Notably, our method improves AP by 16.7% over Fast RCNN baseline on Hard case of Car category, where most samples are of small size or occluded. This is a clear evidence showing the discriminative power of our SDP module.</figDesc><table>Detection AP (%) of baselines and our models on KITTI validation set and Inner-city dataset, divided by size groups, and VOC2007 
dataset. S1, S2, S3, S4 and S indicate the size group of [0, 64), [64, 128), [128, 256), [256, ∞) and [0, ∞). We use 4 scale image pyramid 
for FRCN [13]+AlexNet, and 1 and 2 scale image input for the others. 07 \ diff: training without difficult examples. ms: multi-scale 
training and testing. Best numbers are bold-faced. 

KITTI Train/Validation 
Car 
Pedestrian 
Cyclist 
mAP 
Methods 
Inputs 
S 1 
S 2 
S 3 
S 4 
S 
S 1 
S 2 
S 3 
S 4 
S 
S 1 
S 2 
S 3 S 4 
S 
S 
FRCN [13]+AlexNet 
4 
52.8 60.7 75.8 55.5 61.6 19.7 47.5 88.4 24.1 61.4 42.0 51.6 44.9 0.0 46.5 56.5 

FRCN [13]+VGG16 

1 (400) 33.9 68.3 82.8 68.8 57.3 7.9 50.4 95.3 55.8 64.6 19.0 63.8 66.6 0.0 42.3 54.7 
1 (500) 42.2 70.0 85.1 65.9 62.3 12.6 55.9 94.6 44.9 66.8 29.1 63.8 68.7 0.0 48.8 59.3 
1 (800) 47.6 70.0 84.8 60.5 64.5 14.7 54.5 94.5 47.2 66.4 34.9 61.2 67.4 0.0 50.4 60.4 
2 
47.4 70.2 83.1 54.5 64.1 14.9 55.2 94.5 63.1 66.5 35.8 61.2 65.9 0.0 50.4 60.3 

SDP 

1 (400) 59.1 73.8 84.7 73.6 70.7 12.6 54.8 94.9 70.7 65.7 29.3 65.6 71.7 0.0 49.4 61.9 
1 (500) 64.2 74.4 86.0 68.4 73.7 17.3 58.4 94.9 44.8 66.9 37.5 67.3 68.6 0.0 54.6 65.1 
1 (800) 65.2 73.5 86.0 61.0 73.8 16.9 57.1 94.3 44.1 65.5 36.5 61.5 61.9 0.0 49.9 63.1 
SDP+CRC 
1 (500) 63.9 74.3 85.8 68.2 73.5 17.5 52.0 93.7 45.9 65.5 35.1 65.7 69.2 0.0 52.9 64.0 
SDP+CRC ft 
1 (500) 63.9 74.2 85.5 62.9 73.7 17.6 50.0 93.4 61.0 65.9 35.8 66.5 67.6 0.0 53.1 64.2 

Inner-city Dataset 
Car 
Pedestrian 
Bike 
Truck 
mAP 
Methods 
Inputs S 1 S 2 S 3 S 4 
S 
S 1 S 2 S 3 S 4 
S 
S 1 S 2 S 3 S 4 
S 
S 1 S 2 S 3 S 4 
S 
S 
FRCN [13]+AlexNet 
4 
74.6 78.9 82.9 94.9 82.4 43.9 69.1 77.8 75.4 63.7 26.2 42.3 45.9 2.2 36.3 28.7 51.5 60.0 67.0 48.7 55.0 
FRCN [13]+VGG16 
1 
63.9 80.0 86.4 93.7 80.5 35.2 71.3 83.3 77.3 64.3 28.2 57.5 68.7 0.5 50.6 26.0 62.1 70.0 54.0 53.6 62.2 
SDP 
1 
76.2 84.2 86.9 95.2 85.5 51.1 78.0 83.0 81.5 73.9 40.3 65.4 65.2 43.2 57.9 44.1 67.0 71.5 75.1 65.6 70.7 
SDP+CRC 
1 
75.7 83.8 86.5 95.0 85.0 50.9 75.9 80.2 78.3 71.7 38.4 61.5 63.7 41.5 55.1 43.9 66.8 71.0 75.6 65.5 69.3 
SDP+CRC ft 
1 
75.0 84.1 87.2 95.6 84.9 51.1 76.7 80.2 77.8 72.2 41.6 64.6 64.7 46.9 58.2 45.8 69.1 69.9 74.2 66.4 70.4 

VOC2007 Dataset 
Methods 
train set aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP 
SPPnet BB [16] 07 \ diff 73.9 72.3 62.5 51.5 44.4 74.4 73.0 74.4 42.3 73.6 57.7 70.3 74.6 74.3 54.2 34.0 56.4 56.4 67.9 73.5 63.1 
R-CNN BB [14] 
07 
73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0 
FRCN [13] 
07 
74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8 66.9 
FRCN [13] ms 
07 
74.5 78.5 66.8 57.8 38.0 80.1 78.7 83.5 43.3 74.5 67.4 81.5 82.8 72.6 66.8 32.4 67.1 70.6 75.9 68.9 68.1 
SDP 
07 
73.2 78.1 68.4 50.3 47.4 78.1 75.0 80.9 45.5 68.5 68.3 79.2 80.8 76.5 74.2 38.6 65.2 66.1 76.3 73.0 68.2 
SDP ms 
07 
74.9 77.4 68.1 55.0 49.8 80.1 76.4 82.2 47.3 70.4 67.1 80.2 82.7 75.8 75.3 40.6 66.8 69.7 75.2 73.8 69.4 
SDP+CRC 
07 
74.4 75.5 66.8 49.5 43.9 77.0 75.5 79.6 43.8 68.6 65.3 76.7 79.9 76.5 75.6 36.5 63.1 62.8 77.1 72.1 67.1 
SDP+CRC ms 
07 
74.8 77.6 66.8 51.7 47.1 76.0 77.7 80.1 45.5 69.8 63.2 76.7 79.4 75.0 76.4 39.3 63.3 65.7 76.2 71.9 67.7 
SDP+CRC ft 
07 
75.4 77.3 68.6 51.3 44.0 77.3 76.7 80.3 45.6 71.7 65.8 77.4 81.2 77.0 76.8 36.8 65.1 63.2 77.1 72.8 68.1 
SDP+CRC ms ft 
07 
76.1 79.4 68.2 52.6 46.0 78.4 78.4 81.0 46.7 73.5 65.3 78.6 81.0 76.7 77.3 39.0 65.1 67.2 77.5 70.3 68.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Percentage (%) of surviving proposals after applying CRC, and the corresponding recall rate (%) on KITTI validation set. R [n 1 ,n 2 ) refers to the rejection classifier for the scale group [n1, n2). Overall Layer ratio recall ratio recall ratio recall ratio recall conv1 2 66.2 97.6 83.9 98.1 94.8 100 81.6 98.6 conv2 2 44.2 95.5 59.2 96.2 92.9 99.7 65.4 97.1 conv3 3 16.7 92.1 25.1 93.4 72.3 96.5 38.0 94.ducing the number of proposals. In Table 3, we analyze the percentage of surviving proposals with respect to the initial number of input proposals after applying CRCs, as well as the corresponding recall rate of positives after each CRC. The table shows that our CRCs successfully reject a large number of input proposals while keeping a high recall for the true objects. For each scale group, CRCs can remove over 70 ∼ 80% input proposals, so that only around 20 ∼ 30% proposals go through fc layers. Run-time efficiency. We investigate the efficiency gain in-KITTI Examples: Car, Pedestrian, Cyclist Inner-city Examples: Car, Person, Bike, Truck</figDesc><table>R [0,64) 
R [64,128) 
R [128,∞) 
0 
conv4 3 
-
-
12.6 90.3 48.6 92.0 30.6 91.2 
conv5 3 
-
-
-
-
28.8 89.9 28.8 89.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Componentconv fc fc S fc M fc L rej. box eval. misc. total</figDesc><table>[13]+AlexNet 
799 512 
0 
0 
0 
0 
512 
164 1476 
[13]+VGG16 
282 719 
0 
0 
0 
0 
719 
21 
1022 

SDP 
286 
0 
204 254 283 
0 
741 
90 
1117 
SVD 
285 
0 
97 
116 114 
0 
327 
95 
707 
speedup 
1.0 
-
2.10 2.19 2.48 -
2.27 
0.95 1.58 
CRC 
282 
0 
44 
46 
63 
79 
232 
27 
541 
speedup 
1.0 
-
4.64 5.52 4.49 -
3.19 
3.33 2.06 
SVD+CRC 
283 
0 
24 
25 
31 
81 
161 
27 
471 
speedup 
1.0 
-
8.50 10.16 9.13 -
4.60 
3.33 2.37 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ferguson. Real-time pedestrian detection with deep network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">From generic to specific deep representations for visual recognition. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5774</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepedge: A multi-scale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4380" to="4389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning complexityaware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crosstalk cascades for framerate pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2155" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascade object detection with deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal-Japanese Society For Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeepProposal: Hunting objects by cascading deep convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Fast R-CNN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Boosting convolutional features for robust object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karianakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno>abs/1503.06350</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deeplysupervised nets. CoRR, abs/1409</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5185</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The treasure beneath convolutional layers: Cross-convolutional-layer pooling for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4749" to="4757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Handling occlusions with franken-classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploiting local features from deep networks for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno>abs/1504.05133</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to detect vehicles by clustering appearance patterns. T-ITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1412.5661</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-view and 3D deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2232" to="2245" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data-driven 3D voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno>abs/1504.06375</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A discriminative CNN video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="249" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">segDeepM: Exploiting segmentation and context in deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4703" to="4711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
