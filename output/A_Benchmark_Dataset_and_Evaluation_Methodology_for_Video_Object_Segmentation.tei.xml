<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the years, datasets and benchmarks have proven their fundamental importance in computer vision research, enabling targeted progress and objective comparisons in many fields. At the same time, legacy datasets may impend the evolution of a field due to saturated algorithm performance and the lack of contemporary, high quality data. In this work we present a new benchmark dataset and evaluation methodology for the area of video object segmentation. The dataset, named DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, Full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motionblur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. In addition, we provide a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics that measure the spatial extent of the segmentation, the accuracy of the silhouette contours and the temporal coherence. The results uncover strengths and weaknesses of current approaches, opening up promising directions for future works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation is a binary labeling problem aiming to separate foreground object(s) from the background region of a video. A pixel-accurate, spatio-temporal bipartition of the video is instrumental to several applications including, among others, action recognition, object tracking, video summarization, and rotoscoping for video editing. Despite remarkable progress in recent years, video object segmentation still remains a challenging problem and most existing approaches still exhibit too severe limitations in terms of quality and efficiency to be applicable in practical applications, e.g. for processing large datasets, or video post-production and editing in the visual effects industry.</p><p>What is most striking is the performance gap among state-of-the-art video object segmentation algorithms and closely related methods focusing on image segmentation <ref type="figure">Figure 1</ref>: Sample sequences from our dataset, with ground truth segmentation masks overlayed. Please refer to the supplemental material for the complete dataset. and object recognition, which have experienced remarkable progress in the recent years. A key factor bootstrapping this progress has been the availability of large scale datasets and benchmarks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref>. This is in stark contrast to video object segmentation. While several datasets exists for various different video segmentation tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, none of them targets the specific task of video object segmentation.</p><p>To date, the most widely adopted dataset is that of <ref type="bibr" target="#b46">[47]</ref>, which, however, was originally proposed for joint segmentation and tracking and only contains six low-resolution video sequences, which are not representative anymore for the image quality and resolution encountered in today's video processing applications. As a consequence, evaluations performed on such datasets are likely to be overfitted, without reliable indicators regarding the differences between individual video segmentation approaches, and the real performance on unseen, more contemporary data becomes difficult to determine <ref type="bibr" target="#b5">[6]</ref>. Despite the effort of some authors to augment their evaluation with additional datasets, a standardized and widely adopted evaluation methodology for video object segmentation does not yet exists.</p><p>To this end, we introduce a new dataset specifically designed for the task of video object segmentation. The dataset, which will be made publicly available, contains fifty densely and professionally annotated high-resolution Full HD video sequences, with pixel-accurate ground-truth data provided for every video frame. The sequences have been carefully captured to cover multiple instances of major challenges typically faced in video object segmentation. The dataset is accompanied with a comprehensive evaluation of several state-of-the-art approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>. To evaluate the performance we employ three complementary metrics measuring the spatial accuracy of the segmentation, the quality of the silhouette and its temporal coherence. Furthermore, we annotated each video with specific attributes such as occlusions, fast-motion, non-linear deformation and motion-blur. Correlated with the performance of the tested approaches, these attributes enable a deeper understanding of the results and point towards promising avenues for future research. The components described above represent a complete benchmark suite, providing researchers with the necessary tools to facilitate the evaluation of their methods and advance the field of video object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section we provide an overview of datasets designed for different video segmentation tasks, followed by a survey of techniques targeting video object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Datasets</head><p>There exist several datasets for video segmentation, but none of them has been specifically designed for video object segmentation, the task of pixel-accurate separation of foreground object(s) from the background regions.</p><p>The Freiburg-Berkeley Motion Segmentation dataset <ref type="bibr" target="#b4">[5]</ref> MoSeg is a popular dataset for motion segmentation, i.e. clustering regions with similar motion. Despite being recently adopted by works focusing on video object segmentation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref>, the dataset does not fulfill several important requirements. Most of the videos have low spatial resolution, segmentation is only provided on a sparse subset of the frames, and the content is not sufficiently diverse to provide a balanced distribution of challenging situations such as fast motion and occlusions.</p><p>The Berkeley Video Segmentation Dataset (BVSD) <ref type="bibr" target="#b43">[44]</ref> comprises a total 100, higher resolution sequences. It was originally meant to evaluate occlusions boundary detection and later extended to over-and motion-segmentation tasks (VSB100 <ref type="bibr" target="#b18">[19]</ref>). However, several sequences do not contain a clear object. Furthermore, the ground-truth, available only for a subset of the frames, is fragmented, with most of the objects being covered by multiple manually annotated, disjoint segments, and therefore this dataset is not well suited for evaluating video object segmentation.</p><p>SegTrack <ref type="bibr" target="#b46">[47]</ref> is a small dataset composed of 6 densely annotated videos of humans and animals. It is designed to be challenging with respect to background-foreground color similarity, fast motion and complex shape deformation. Although it has been extensively used by several approaches, its content does not sufficiently span the variety of challenges encountered in realistic video object segmentation applications. Furthermore, the image quality is not anymore representative of modern consumer devices, and due to the limited number of available video sequences, progress on this dataset plateaued. In <ref type="bibr" target="#b24">[25]</ref> this dataset was extended with 8 additional sequences. While this is certainly an improvement over the predecessor, it still suffers of the same limitations. We refer the reader to the supplemental material for a comprehensive summary of the properties of the aforementioned datasets, including ours.</p><p>Other datasets exist, but they are mostly provided to support specific findings and thus are either limited in terms of total number of frames, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47]</ref>, or do not exhibit a sufficient variety in terms of content <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref>. Others cover a broader range of content but do not provide enough ground-truth data for an accurate evaluation of the segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref>. Video datasets designed to benchmark tracking algorithms typically focus on surveillance scenarios with static cameras <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>, and usually contain multiple instances of similar objects <ref type="bibr" target="#b49">[50]</ref> (e.g. a crowd of people), and annotation is typically provided only in the form of axis-aligned bounding boxes, instead of pixelaccurate segmentation masks necessary to accurately evaluate video object segmentation. Importantly, none of the aforementioned methods includes contemporary high resolution videos, which is an absolute necessity to realistically evaluate the actual practical utility of such algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Algorithms</head><p>We categorize the body of literature related to video object segmentation based on the level of supervision required.</p><p>Unsupervised approaches have historically targeted over-segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b50">51]</ref> or motion segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> and only recently automatic methods for foregroundbackground separation have been proposed <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52]</ref>. These methods extend the concept of salient object detection <ref type="bibr" target="#b33">[34]</ref> to videos. They do not require any manual annotation and do not assume any prior information on the object to be segmented. Typically they are based on the assumption that object motion is dissimilar from the surroundings. Some of these methods generate several ranked segmentation hypotheses <ref type="bibr" target="#b23">[24]</ref>. While they are well suited for parsing large scale databases, they are bound to their underlying assumption and fail in cases it does not hold.</p><p>Semi-supervised video object segmentation methods propagate a sparse manual labeling, generally given in the form of one or more annotated frames, to the entire video  sequence. While being different from each other, they often solve an optimization problem with an energy defined over a graph structure <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48]</ref>. To model long-range spatiotemporal connections some approaches use fully connected graphs <ref type="bibr" target="#b34">[35]</ref>, higher-order potentials <ref type="bibr" target="#b21">[22]</ref>. The recent work of Märki et al. <ref type="bibr" target="#b30">[31]</ref> efficiently approximates non-local connections minimizing the graph energy in bilateral space. Supervised approaches assume manual annotation to be repeatedly added during the segmentation process, with a human correcting the algorithm results in an iterative fashion <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>. These methods generally operate online, forward processing frames to avoid overriding of previous manual corrections. They guarantee high segmentation quality at the price of time-consuming human supervision, hence they are suited only for specific scenarios such as video post-production.</p><p>We evaluate a large set of the state-of-the-art approaches on our proposed dataset, providing new insights and several pointers to areas for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Description</head><p>In this section we describe our new dataset DAVIS (Densely Annotated VIdeo Segmentation) specifically designed for the task of video object segmentation. Exam-ple frames of some of the sequences are shown in <ref type="figure">Figure 1</ref>. Based on experiences with existing datasets we first identify four key aspects we adhere to, in order create a balanced and comprehensive dataset.</p><p>Data Amount and Quality. A sufficiently large amount of data is necessary to ensure content diversity and to provide a uniformly distributed set of challenges. Furthermore, having enough data is crucial to avoid over-fitting and to delay performance saturation, hence guaranteeing a longer lifespan of the dataset <ref type="bibr" target="#b5">[6]</ref>. The quality of the data also plays a crucial role, as it should be representative of the current state of technology. To this end, DAVIS comprises a total of 50 sequences, 3455 annotated frames, all captured at 24fps and Full HD 1080p spatial resolution. Due to the computational complexity being a major bottleneck in video processing, the sequences have a short temporal extent (about 2-4 seconds), but include all major challenges typically found in longer video sequences, see <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Experimental Validation. For each video frame, we provide pixel-accurate, manually created segmentation in the form of a binary mask. While we subdivide DAVIS into training-and a test-set to provide guidelines for future works, in our evaluation, we do not make use of the partition, and instead consider the dataset as a whole, since most of the evaluated approaches are not trained and a grid-search estimation of the optimal parameters would be infeasible due to the involved computational complexity.</p><p>Object Presence. Intuitively each sequence should contain at least one target foreground-object to be separated from the background regions. The clips in DAVIS contain either one single object or two spatially connected objects. We choose not to have multiple distinct objects with significant motion in order to be able to fairly compare segmentation approaches operating on individual objects against those that jointly segment multiple objects. Moreover, having a single object per sequence disambiguates the detection performed by methods which are fully automatic. A similar design choice made in <ref type="bibr" target="#b26">[27]</ref> has been successfully steering research in salient object detection from its beginnings to the current state-of-the-art. To ensure sufficient content diversity, which is necessary to comprehensively assess the performance of different algorithms, the dataset spans four evenly distributed classes (humans, animals, vehicles, objects) and several actions.</p><p>Unconstrained Video Challenges. To enable a deeper analysis and understanding of the performance of an algorithm, it is fundamentally important to identify the key factors and circumstances which might have influenced it. Thus, inspired by <ref type="bibr" target="#b49">[50]</ref> we define an extensive set of video attributes representing specific situations, such as fast-motion, occlusion and cluttered background, that typically pose challenges to video segmentation algorithms. Attributes are summarized in <ref type="table" target="#tab_1">Table 1</ref>. They are not exclu- sive, therefore a sequence can be annotated with multiple attributes. Their distribution over the dataset, i.e. number of occurrences, and their pairwise dependencies are shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The annotations enable us to decouple the analysis of the performance into different groups with dominant characteristics (e.g. occlusion), yielding a better understanding of each methods' strengths and weaknesses.</p><formula xml:id="formula_0">A C B C C S D B D E F E A F M H O IO L R M B O C C O V S C S V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Validation</head><p>In order to judge the quality of a segmentation, the choice of a suitable metric is largely dependent on the end goal of the final application <ref type="bibr" target="#b9">[10]</ref>. Intuitively, when video segmentation is used primarily a classifier within a larger processing pipeline, e.g. for parsing large scale datasets, it makes sense to seek the lowest amount of mislabeled pixels. On the other hand, in video editing applications the accuracy of the contours and their temporal stability is of highest importance, as these properties usually require the most painstaking and time-consuming manual input. In order to exhaustively cover the aforementioned aspects we evaluate the video segmentation results using three complementary error metrics. We describe the metrics in Section 4.1 and we empirically validate their complementary properties on the proposed dataset in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Metrics Selection</head><p>In a supervised evaluation framework, given a groundtruth mask G on a particular frame and an output segmentation M , any evaluation measure ultimately has to answer the question how well M fits G. As justified in <ref type="bibr" target="#b36">[37]</ref>, for images one can use two complementary points of view, regionbased and contour-based measures. As videos extends the dimensionality of still images to time, the temporal stability of the results must also be considered. Our evaluation is therefore based on the following measures.</p><p>Region Similarity J . To measure the region-based segmentation similarity, i.e. the number of mislabeled pixels, we employ the Jaccard index J defined as the intersection-over-union of the estimated segmentation and the groundtruth mask. The Jaccard index has been widely adopted since its first appearance in PASCAL VOC2008 <ref type="bibr" target="#b11">[12]</ref>, as it provides intuitive, scale-invariant information on the number of mislabeled pixels. Given an output segmentation M and the corresponding ground-truth mask G it is defined as J = |M ∩G| |M ∪G| . Contour Accuracy F. From a contour-based perspective, one can interpret M as a set of closed contours c(M ) delimiting the spatial extent of the mask. Therefore, one can compute the contour-based precision and recall P c and R c between the contour points of c(M ) and c(G), via a bipartite graph matching in order to be robust to small inaccuracies, as proposed in <ref type="bibr" target="#b27">[28]</ref>. We consider the so called F-measure F as a good trade-off between the two, defined as F = 2PcRc</p><p>Pc+Rc . For efficiency, in our experiments, we approximate the bipartite matching via morphology operators.</p><p>Temporal stability T . Intuitively, J measures how well the pixels of the two masks match, while F measures the accuracy of the contours. However, temporal stability of the results is a relevant aspect in video object segmentationsince the evolution of object shapes is an important cue for recognition and jittery, unstable boundaries are unacceptable in video editing applications. Therefore, we additionally introduce a temporal stability measure which penalizes such undesired effects.</p><p>The key challenge is to distinguish the acceptable motion of the objects from the undesired instability and jitter. To do so, we estimate the deformation needed to transform the mask at one frame to the next one. Intuitively, if the transformation is smooth and precise, the result can be considered stable.</p><p>Formally, we transform mask M t of frame t into polygons representing its contours P (M t ). We then describe each point p i t ∈ P (M t ) using the Shape Context Descriptor (SCD) <ref type="bibr" target="#b2">[3]</ref>. Next, we pose the matching as a Dynamic Time Warping (DTW) <ref type="bibr" target="#b38">[39]</ref> problem, were we look for the matching between p i t and p j t+1 that minimizes the SCD distances between the matched points while preserving the order in which the points are present in the shapes.</p><p>The resulting mean cost per matched point is used as the measure of temporal stability T . Intuitively, the matching will compensate motion and small deformations, but it will not compensate the oscillations and inaccuracies of the contours, which is what we want to measure. Occlusions and very strong deformations would be misinterpreted as contour instability, so we compute the measure on a subset of sequences without such effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics Validation</head><p>To verify that the use of these measures produces meaningful results on our dataset, we compute the pairwise correlation between the region similarity J and the contour  <ref type="figure">Figure 3</ref>: Correlation between the proposed metrics. Markers correspond to video frames. Colors encode membership to a specific video sequence. The contour accuracy measure F exhibits a slight linear dependency with respect to the region similarity J (left), while it appears uncorrelated to the temporal stability T (right).</p><p>accuracy F and between F and the temporal stability measure T . The degree of correlation is visualized in <ref type="figure">Figure 3</ref>. As can be expected, there is a tendency towards linear correlation between J and F <ref type="figure">(Figure 3, left)</ref>, which can be explained by the observation that higher quality segmentations usually also result in more accurate contours. We note, however, that the level of independence is enough to justify the use of both measures. To get a qualitative idea of the differences between the two measures, <ref type="figure">Figure 4</ref> shows two results of discrepant judgments between J and F. The temporal stability measure T and the contour accuracy F instead are nearly uncorrelated <ref type="figure">(Figure 3, right)</ref>, which is also expected since temporal instability does not necessarily impact the per-frame performance. <ref type="figure">Figure 4</ref>: Discrepancy between metrics. Ground truth in red and an example segmentation result in green. On the left, the result is penalized by J because in terms of number of pixels there is a significant amount of false negatives (head and foot), while with respect to the boundary measure F the missed percentage is lower. On the right the response of both measures is switched. The discrepancy in terms of pixels is low because the erroneous area is small, but the boundaries are highly inaccurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluated Algorithms</head><p>We evaluate a total of twelve video segmentation algorithms, which we selected based on their demonstrated state-of-the-art performance and source code availability, and two techniques commonly used for preprocessing. The source code was either publicly available or it was shared by the authors upon request.</p><p>Within the unsupervised category we evaluate the performance of NLC <ref type="bibr" target="#b12">[13]</ref>, FST <ref type="bibr" target="#b32">[33]</ref>, SAL <ref type="bibr" target="#b42">[43]</ref>, TRC <ref type="bibr" target="#b17">[18]</ref>, MSG <ref type="bibr" target="#b4">[5]</ref> and CVOS <ref type="bibr" target="#b44">[45]</ref>. The three latter approaches generates multiple segments per-frame, and therefore, as suggested in <ref type="bibr" target="#b4">[5]</ref>, we solve the bipartite graph matching that maximizes region similarity in terms of J to select the most similar to the target object. Among the semi-supervised approaches, SEA <ref type="bibr" target="#b39">[40]</ref>, JMP <ref type="bibr" target="#b13">[14]</ref>, TSP <ref type="bibr" target="#b6">[7]</ref> and HVS <ref type="bibr" target="#b20">[21]</ref> are initialized using the first-frame. HVS is meant for hierarchical over-segmentation, hence we search the hierarchy level and the corresponding segments that maximizes J of the first frame, keeping the annotation fixed throughout the entire video. FCP <ref type="bibr" target="#b34">[35]</ref> uses a pair of annotated object proposals to initialize the classifiers. In our evaluation KEY <ref type="bibr" target="#b23">[24]</ref> is deemed to be semi-supervised since we override their abjectness score and instead use the ground-truth to select the optimal hypotheses which is then refined solving a series of spatio-temporal graph-cuts.</p><p>The selected algorithms span the categories devised in Section 2 based on the level of supervision. However, interactive approaches with manual feedback could theoretically yield optimal results, and are not directly comparable with un-and semi-supervised approaches, since the number of user edits, e.g. strokes, should be also taken into account. Therefore we cast JMP <ref type="bibr" target="#b13">[14]</ref> into a semi-supervised method that propagates masks to consecutive frames similar to SEA <ref type="bibr" target="#b39">[40]</ref>. We reduce the number of categories in <ref type="table">Table 2</ref> and <ref type="table">Table 3</ref> accordingly.</p><p>Additionally we evaluate the performance of a salient object detector and the performance of an object proposal generator, as their output is a useful indicator with respect to the various video segmentation algorithms that are built upon them. We extract per-frame saliency from CIE-Lab images (SF-LAB, <ref type="bibr" target="#b33">[34]</ref>) and from inter-frame motion (SF-MOT, <ref type="bibr" target="#b33">[34]</ref>), while we use ground-truth to select the hypotheses of the object proposal generator (MCG, <ref type="bibr" target="#b35">[36]</ref>) maximizing the per-frame Jaccard region similarity J .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Quantitative Evaluation</head><p>In this section we report the results of the fifteen evaluated approaches. We first provide different statistics evaluated for each of the three error measures (regions, contours, temporal), and then discuss evaluation results at the attribute level (e.g., performance with respect to appearance changes).</p><p>For each of the methods we kept the default parameters fixed throughout the entire dataset. Despite a considerable effort to speed-up the computation (parallelizing preprocessing steps such as motion estimation or extraction  <ref type="table">Table 2</ref>: Overall results of region similarity (J ), contour accuracy (F) and temporal (in-)stability (T ) for each of the tested algorithms. For rows with an upward pointing arrow higher numbers are better (e.g., mean), and vice versa for rows with downward pointing arrows (e.g., decay, instability).</p><p>of boundary preserving regions) and to reduce the memory footprint (caching intermediate steps), several methods based on global optimization routines cannot be easily accelerated. Therefore, in order to be able to evaluate all methods with respect to each other, we were forced to down-sample the videos to 480p resolution. Due to the enormous processing power required, we performed experiments on different machines and partly on a cluster with thousands of nodes and heterogeneous CPU cores. Indicative runtimes are reported in the supplementary material. The evaluation scripts, the input data, and the output results are made publicly available 1 .</p><p>We exclude from the evaluation the first frame, which is used as ground-truth by semi-supervised methods, and the last frame which is not processed by some of the approaches. The overall results and considerations are reported in Section 6.1 and summarized in <ref type="table">Table 2</ref>, while the attributes-based evaluation is discussed in Section 6.2 and summarized in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Error Measure Statistics</head><p>For a given error measure C we consider three different statistics. Let R = {S i } be the dataset of video sequences S i and letC(S i ) be the error measure average on S i . The mean is the average dataset error defined as M C (R) = 1 |R| S∈RC (S i ). The decay quantifies the performance loss (or gain) over time. Let Q i = {Q 1 i , .., Q 4 i } be a partition of S i in quartiles, we define the decay as</p><formula xml:id="formula_1">D C (R) = 1 |R| Qi∈RC (Q 1 i ) −C(Q 4 i ).</formula><p>The object recall measures the fraction of sequences scoring higher than a threshold, defined as O C (R) = 1 |R| S∈R ✶C (Si)&gt;τ , with τ = 0.5 in our experiments.</p><p>The region-based evaluation for all methods is summarized in <ref type="table">Table 2</ref>. The best performing approach in terms of mean intersection-over-union is NLC <ref type="bibr" target="#b12">[13]</ref> (M J = 0.641), closely followed by FCP <ref type="bibr" target="#b34">[35]</ref> (M J = 0.631). However, the latter has better object recall O J and less decay D J . We report that, at the time of submission, our concurrent work 1 https://github.com/fperazzi/davis BVS <ref type="bibr" target="#b30">[31]</ref> scored M J = 0.665, therefore being the best performer in terms of region similarity, with the advantage of having the parameters tuned on this specific dataset.</p><p>With the exception of FCP <ref type="bibr" target="#b34">[35]</ref>, which solves a global optimization problem over a fully connected graph, the semi-supervised approaches TSP <ref type="bibr" target="#b6">[7]</ref>, SEA <ref type="bibr" target="#b39">[40]</ref>, HVS <ref type="bibr" target="#b20">[21]</ref> and JMP <ref type="bibr" target="#b13">[14]</ref> propagate the initial manual segmentation iteratively to consecutive frames and thus exhibit temporal performance decay as reflected in the results. To alleviate this problem, propagating using bigger steps and interpolating the results in-between can reduce the drift and improve the overall results <ref type="bibr" target="#b13">[14]</ref>. TRC <ref type="bibr" target="#b17">[18]</ref> and MSG <ref type="bibr" target="#b4">[5]</ref> belong to a class of methods that uses motion segmentation as a prior, but the resulting over-segmentation of the object reflects negatively on the average performance. CVOS <ref type="bibr" target="#b44">[45]</ref> uses occlusion boundaries, but still encounters similar issues. Differently from TRC and MSG, CVOS performs online segmentation. It scales better to longer sequences in terms of efficiency but experiences higher decay.</p><p>Aiming at detecting per-frame indicators of potential foreground object locations, KEY <ref type="bibr" target="#b23">[24]</ref>, SAL <ref type="bibr" target="#b42">[43]</ref>, and FST <ref type="bibr" target="#b32">[33]</ref> try to determine prior information sparsely distributed over the video sequence. The prior is consolidated enforcing spatio-temporal coherence and stability by minimizing an energy function over a locally connected graph. While the local connectivity enables propagation of the segmentation similar to those of the semi-supervised approaches listed above, these methods suffer less decay as annotations are available at multiple different time frames.</p><p>Within the preprocessing category, the oracle MCG <ref type="bibr" target="#b35">[36]</ref> is an informative upper-bound for methods seeking the best possible proposal per-frame. It has the highest region-based performance J and superior object recall M J . The performance of MCG, also supported by the good performance of FCP and KEY that use concurrent object proposal generators, indicates that this could be a promising direction for more future research. As expected, in video sequences motion is a stronger low-level cue for object presence than color. Consequently salient motion detection SF-MOT <ref type="bibr" target="#b33">[34]</ref> shows a significantly better performance than SF-LAB.  <ref type="table">Table 3</ref>: Attribute-based aggregate performance. For each method, the respective left column corresponds to the average region similarity J over all sequences with that specific attribute (e.g., AC), while the right column indicates the performance gain (or loss) for that method for the remaining sequences without that respective attribute. Only a subset of the most informative attributes from <ref type="table" target="#tab_1">Table 1</ref> are shown here. Please refer to the supplemental material for the complete evaluation.</p><p>The evaluation clearly shows that both the aggregate and individual performance of the approaches leave abundant room for future research. For instance, in <ref type="bibr" target="#b22">[23]</ref> it is observed that a Jaccard index of J ≥ 0.7 seems to be sufficiently accurate while J ≤ 0.6 already represents a significant departures from the original object shape. The top techniques evaluated on DAVIS are still closer to the latter.</p><p>In terms of contour accuracy the best performing approaches are NLC and JMP. The former uses a large number of superpixels per-frame (∼2000) and a discriminative ensemble of features to represent them. In contrast, JMP exploits geodesic active contours to refine the object boundaries. The motion clusters of TRC and MSG, as well as the occlusion boundaries of CVOS generate sub-optimal results along the boundaries. The top ranked methods in terms of temporal stability are those that propagate segmentation on consecutive frames (JMP, SEA). As expected those that are used on a per-frame basis and cannot enforce continuity over time, such as MCG and SF-(*) generate considerably higher temporal instability. As a sanity check, we evaluate the temporal stability of the ground truth and we get T = 0.093, which is lower than any of the sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Attributes-based Evaluation</head><p>As discussed in Section 3 and <ref type="table" target="#tab_1">Table 1</ref> we annotated the video sequences with attributes each representing a different challenging factor. These attributes allow us to identify groups of videos with a dominant feature e.g., presence of occlusions, which is key to explaining the algorithms' performance. However, since multiple attributes are assigned to each sequence, there might exists hidden dependencies among them which could potentially affect an objective analysis of the results. Therefore, we first conduct a statistical analysis to establish these relationship, and then detail the corresponding evaluation results.</p><p>Attributes Dependencies. We consider the presence or absence of each attribute in a video sequence to be represented as a binary random variable, the dependencies between which can be modelled by a pairwise Markov random field (MRF) defined on a graph G with vertex set V ∈ {1, . . . , 16} and (unknown) edge set E. The absence of an edge between two attributes denotes that they are independent conditioned on the remaining attributes. Given a collection of n = 50 binary vectors denoting the presence of attributes in each video sequence, we estimate E via ℓ 1 penalized logistic regression. To ensure robustness in the estimated graph we employ stability selection <ref type="bibr" target="#b29">[30]</ref>. Briefly, this amounts to performing the above procedure on n/2-sized subsamples of the data multiple times and computing the proportion of times each edge is selected. Setting an appropriate threshold on this selection probability allows us to control the number of wrongly estimated edges according to Theorem 1 in <ref type="bibr" target="#b29">[30]</ref>. For example, for a threshold value of 0.6 and choosing a value of λ which on average selects neighbourhoods of size 4, the number of wrongly selected edges is at most 4 (out of 16 2 = 256 possible edges). The estimated dependencies are visualized in <ref type="figure" target="#fig_0">Figure 2 (right)</ref>. As expected there is a mutual dependency between attributes such as fast-mostion (FM) and motionblur (MB), or interacting-object (IO) and shape-complexity (SC). We refer the reader to the supplementary material for further details.</p><p>Results. In <ref type="table">Table 3</ref> we report the performance on subsets of the datasets characterized by a particular attribute. Due to space limitations we reduce the analysis in the paper to the most informative and recurrent attributes. Furthers details can be found in the supplementary material.</p><p>Appearance changes (AC) poses a challenge to several approaches, in particular for those methods strongly relying on color appearance similarity such as HVS and TCP. For example, TSP performance drops almost 50% as a consequence of the Gaussian process it uses to update the appearance model and therefore not being robust enough to strong appearance variations. Despite the dense connectivity of its conditional random field, FCP also experiences a considerable loss of performance. The reason resides in a sub-optimal automatic choice of the annotated proposals. Likely the proposals did have enough variety to span the entire object appearances causing the classifiers to overfit.</p><p>Dynamic background (DB) scenes, e.g. flowing water, represent a major difficulty to the class of unsupervised methods, such as NLC and SAL, which adopt distinctive motion saliency as the underlying assumption to predict the object location. Interestingly the assumption of a completely closed motion boundary curve coinciding with the object contours can robustly accommodate background deformations (FST). Finally, MSG and TRC experience a considerable performance degradation as the motion clusters they rely on <ref type="bibr" target="#b4">[5]</ref> are constructed from dissimilarities of point-trajectories, under the assumption that translational models are a good approximation for nearby points, which is not true on deforming image regions.</p><p>Fast motion (FM) is a problem for any of the algorithms exploiting motion information as the condition is a major challenge to reliable optical-flow computation. Note that there is a strong dependency between fast motion and motion-blur (MB) <ref type="figure" target="#fig_0">(Figure 2</ref>, right), yielding fuzzy object boundaries almost impossible to separate from the background region. Methods such as TRC and MSG use pointtracks for increased robustness towards fast motion, but are still susceptible with respect to motion-blur due to the sensitivity of the underlying variational approach used for densification of the results. NLC is the only method which has none or negligible loss of performance in both circumstances, possibly because the saliency computation is still reliable on a subset of the frames, and their random-walk matrix being non-locally connected is robust to fast motion.</p><p>Occlusions (OCC) being one of the well known challenges in video segmentation, only a small subset of the algorithms, which propagate sequentially manually annotated frames such as SEA and JMP, struggle with this type of situation. As expected, methods that exploit large range connectivity such as NLC, FCP and KEY are quite robust to these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>To the best of our knowledge, this work represents the currently largest scale performance evaluation of video object segmentation algorithms. One of course has to consider that the evaluated approaches have been developed using different amounts and types of input data and ground-truth, or were partially even designed for different problems and only later adapted to the task of video object segmentation. However, the primary aim of our evaluation is not to determine a winner, but to provide researchers with high-quality, contemporary data, a solid standardized evaluation procedure, and valuable comparisons with the current state-ofthe-art. We hope that the public availability of this dataset and the identified areas for potential future works will motivate even more interest in such an active and fundamentally important field for video processing.</p><p>As any dataset, also DAVIS will have a limited life-span. Therefore we welcome external contributions to extend it, generalizing it to other segmentation tasks such as oversegmentation, or to other applications such as video alpha matting, semantic video segmentation, video retrieval, and action recognition.</p><p>Currently, running time efficiency and memory requirements are a major bottleneck for the usability of several video segmentation algorithms. In our experiments we observed that a substantial amount of time is spent preprocessing images to extract boundary preserving regions, object proposals and motion estimates. We encourage future research to carefully select those components bearing in mind they could compromise the practical utility of their work. Efficient algorithms will be able to take advantage of the Full HD videos and accurate segmentation masks made available with this dataset. Leveraging high resolution might not produce better results in terms of region-similarity, but it is essential to improve the segmentation of complex object contours and tiny object region.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Left: Attributes distribution over the dataset. Each bin indicates the number of occurrences. Right: Mutual dependencies among attributes. The presence of a link indicates high probability of an attribute to appear in a sequence, if the one on the other end is also present.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>List of video attributes and corresponding description. We extend the annotations of<ref type="bibr" target="#b49">[50]</ref> (top) with a complementary set of attributes relevant to video object segmentation (bottom). We refer the reader to the supplementary material for the list of attributes for each in video in the dataset, and corresponding visual examples.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank the human and animal "actors" who contributed to the creation of DAVIS. In particular, we thank Lucia Colombo for her logistic support throughout the entire duration of the project. This work was partially funded by an SNF award (200021 143598).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label propagation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video snapcut: robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<idno>2002. 4</idno>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W F</forename><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Propagating multi-class pixel labels throughout video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WNYIPW</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An open source tracking testbed and evaluation web site</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PETS 2005</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What is a good evaluation measure for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jumpcut: Non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The pets04 surveillance ground-truth data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detection free tracking: Exploiting motion and topology for segmenting and tracking under entanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stability selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="473" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Nicolas</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A largescale benchmark dataset for event recognition in surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G A</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Desai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Supervised evaluation of image segmentation and object proposal techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fundamentals of speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Egocentric recognition of handled objects: Benchmark and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Saliency-Aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wenguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Occlusion boundary detection and figure/ground assignment from optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3-d motion segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Motion coherent tracking with multi-label MRF optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Touchcut: Fast image and video segmentation using single-touch interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Evaluation of super-voxel methods for early video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Discontinuityaware video object cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<idno>2012. 3</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
