<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scale-Aware Alignment of Hierarchical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
							<email>yuhua.chen@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
							<email>dai@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
							<email>jponttuset@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ESAT/PSI</orgName>
								<orgName type="institution">VISICS</orgName>
								<address>
									<settlement>Leuven</settlement>
									<region>KU</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scale-Aware Alignment of Hierarchical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image segmentation is a key component in many computer vision systems, and it is recovering a prominent spot in the literature as methods improve and overcome their limitations. The outputs of most recent algorithms are in the form of a hierarchical segmentation, which provides segmentation at different scales in a single tree-like structure. Commonly, these hierarchical methods start from some lowlevel features, and are not aware of the scale information of the different regions in them. As such, one might need to work on many different levels of the hierarchy to find the objects in the scene. This work tries to modify the existing hierarchical algorithm by improving their alignment, that is, by trying to modify the depth of the regions in the tree to better couple depth and scale. To do so, we first train a regressor to predict the scale of regions using mid-level features. We then define the anchor slice as the set of regions that better balance between over-segmentation and undersegmentation. The output of our method is an improved hierarchy, re-aligned by the anchor slice. To demonstrate the power of our method, we perform comprehensive experiments, which show that our method, as a post-processing step, can significantly improve the quality of the hierarchical segmentation representations, and ease the usage of hierarchical image segmentation to high-level vision tasks such as object segmentation. We also prove that the improvement generalizes well across different algorithms and datasets, with a low computational cost. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generic image segmentation has been part of computer vision and image processing communities since the advent of these fields many decades ago. The definition of the problem, although vague, is easy to give and understand: "to divide the pixels of an image into different pieces, where each piece represents a distinguished thing in the image." Martin et al. <ref type="bibr" target="#b18">[19]</ref> provided these instructions to annotators to create the Berkeley Segmentation Database (BSDS), which proved that the problem of image segmentation was, indeed, well defined, as humans provided consistent partitions of the images up to refinement. In other words, image segmentation is inherently a multi-scale problem.</p><p>We refer to flat image segmentation techniques as those whose output is a single partition of the image pixels into sets <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>. In these cases, in order to capture the aforementioned multi-scale nature of objects, one needs to sweep different parameterizations to obtain multiple partitions that contain the different scales when working with flat segmentation techniques.</p><p>On the other hand, hierarchical segmentation produces a single multi-scale structure that aims at capturing the objects at all scales <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22]</ref>. These types of structures have been successfully used in image filtering <ref type="bibr" target="#b27">[28]</ref>, semantic segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>, salient object detection <ref type="bibr" target="#b33">[34]</ref>, object proposals generation <ref type="bibr" target="#b21">[22]</ref>, or video segmentation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>The representation power of these hierarchies comes at a cost, however, which is the difficulty to handle them from a practical (coding) point of view. While a flat partition can be represented by a matrix of labels of each pixel, hierarchical structures need a much more complex representation. In this context, the Ultrametric Contour Map (UCM) <ref type="bibr" target="#b0">[1]</ref> representation is the one that gained more traction and it is widely used in the literature. In it, flattening the hierarchy can be achieved simply by thresholding the UCM. The process of flattening or pruning a hierarchy is therefore of paramount importance for segmentation, because it is the main proxy used towards the final application. This work presents a novel technique to improve the flattening of any given hierarchy, that is, to get better flat partitions from the same hierarchical segmentation. <ref type="figure" target="#fig_0">Figure 1</ref> motivates this work. In the first row we can see different flat partitions extracted from the same hierarchy. To get the regions representing the four lions we need to search in three different flat partitions, extracted at three different levels of the hierarchy. The second row shows our results, where the same hierarchy is aligned to have all objects represented in the same flat partition.</p><p>In other words, the threshold level of the hierarchy better relates with the scale of the objects, not only in the same image, but also across images. To further grasp the intuition of our work, <ref type="figure" target="#fig_1">Figure 2</ref> shows a UCM and its interpretation as a region tree (a). In it, the needed regions to form the car are spread into different scale levels (thresholds of the UCM), as marked by the red band. Our proposed realigned hierarchy (b) aims at containing them all in the same scale.</p><p>Since the hierarchies are constructed based on low-level features (edges, color, etc.), the scale of the objects is not imposed to be coherent. We propose to learn the concept of object scale from mid-level features within the hierarchy. Our objective is to take advantage of these mid-level features as much as possible without getting to high-level features that would allow us to go beyond scale. This way, the global approach would be to construct the hierarchies using low-level features, and then exploit mid-level features to realign them, thus taking the maximum advantage of the most simple features possible.</p><p>Our alignment also aims at providing a global alignment among different images, that is, providing levels of scale that keep meaning even when changing images, allowing higher-level methods to generalize in a more straightforward manner. Specifically, we train a regressor to predict whether each region of the hierarchy is oversegmented, un-dersegmented, or correctly segmented; and we rescale the hierarchy according to the prediction of this classifier. Back to the example in <ref type="figure" target="#fig_0">Figure 1</ref>, the majority of regions in the first column (bottom) are undersegmented, in the middle column they are correctly segmented, and oversegmented in the last column.</p><p>We perform comprehensive experiments using four different hierarchical segmenters, and we obtain consistent improvements on all hierarchies which proves the usefulness of our approach and its generalization power. The remainder of the paper is organized as follows. First, Section 2 gives a brief overview of the related work. Then Section 3 presents our algorithm for re-scaling and aligning hierarchies. We demonstrate the effectiveness of our method in the experiments in Section 4 and draw the conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Hierarchical Segmentation: There is a rich literature of hierarchical segmentation. As stated in the introduction, our focus in this paper is not to develop a better hierarchical segmentation algorithm, but to provide a better alignment of a given hierarchy. Hierarchical segmentation typically starts from various local information embedded in an affinity matrix, such as Pointwise Mutual Information <ref type="bibr" target="#b12">[13]</ref>, or multiscale local brightness, color, and texture cues <ref type="bibr" target="#b0">[1]</ref>. It then greedily constructs a hierarchy of regions by iteratively merging the most similar sets of regions according to a certain metric. The result of hierarchical segmentation is commonly represented as an Ultrametric Contour Map (UCM), where different levels of segmentation can be produced by applying different thresholds to the UCM. This work proposes to realign the hierarchies in order to make the thresholds of the UCM more closely related to the scale of objects. Hierarchical segmentation has become the major trend in image segmentation and most of top-performance segmenters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14]</ref> fall into this category.</p><p>Multiple Segmentations: Working with multiple segmentations at the same time has been used in the computer vision community for a long time, with the idea that, while none of the segmentations is likely to partition the image perfectly, some parts in some segmentations might be useful. Hoiem et al. <ref type="bibr" target="#b10">[11]</ref> use this idea to estimate the scene structure. A similar idea was exploited by Russell et al. <ref type="bibr" target="#b26">[27]</ref> to discover objects, and by Malisiewicz et al. <ref type="bibr" target="#b17">[18]</ref> to improve the spatial support of regions for recognition. By realigning the hierarchies we aim to minimize the number of partitions from a hierarchy needed to obtain reasonable results, since we concentrate same-scale regions in the same partition. Our work also shares some similarities with <ref type="bibr" target="#b31">[32]</ref>, where they flatten supervoxel hierarchies in videos by finding a slice with uniform entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicting Segmentation Quality by Classification:</head><p>Classification has been exploited to predict segmentation quality in many works. Ren et al. <ref type="bibr" target="#b24">[25]</ref> use a linear classifier base on Gestalt features <ref type="bibr" target="#b19">[20]</ref> to distinguish good and bad segmentations. Their negative training data are generated by randomly placing a ground-truth mask over an image. A similar idea is used to select parameters by Peng et al. <ref type="bibr" target="#b20">[21]</ref> to select λ in graph-cut based interactive segmentation. They compute the segmentation with different λ, then select the one with highest predicted quality. More recently, Carreira et al. <ref type="bibr" target="#b1">[2]</ref>, Pont-Tuset et al. <ref type="bibr" target="#b21">[22]</ref>, and Endres et al. <ref type="bibr" target="#b6">[7]</ref> use a regression forest to predict the good overlap between segments (object proposals) and ground truth objects. We use similar features to <ref type="bibr" target="#b1">[2]</ref>, which are based on graph partition, region, and Gestalt properties.</p><p>Scale-aware Vision Algorithms: Our work also bear a resemblance to the scale-aware algorithms for other vision tasks. For instance, exploiting the scale information has proven helpful for semantic image segmentation <ref type="bibr" target="#b2">[3]</ref> and pedestrain detection <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b5">[6]</ref> show that vision algorithms employing super-resolved images (higher-resolution) perform better than using low-resolution images directly. Other scale-aware applications include object tracking <ref type="bibr" target="#b14">[15]</ref> and image thumbnailing <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Flattening and Re-scaling Hierarchies</head><p>As discussed in the introduction, while segmentation hierarchies contain a rich multiscale decomposition of the image, it is not trivial to distill such knowledge because the hierarchies generated by current methods are not fully scale-aware. Simply taking a layer yields a segmentation of which some parts are under-segmented while others are over-segmented. In this section, we present our method which aligns the scales of segmentation hierarchies, making image hierarchies easier to use in practice. We start <ref type="figure">Figure 3</ref>. Examples of the slices and paths of the segmentation tree, where one path of the tree is shown in green (a) and one slice is shown in grey (b). In (b), all nodes in blue are in L − , and all nodes in red are in L + . Our approach re-aligns the hierarchy using the anchor slice. The aligned tree is shown in (c).</p><formula xml:id="formula_0">(a) (b) (c)</formula><p>with scale labeling, and then present the alignment strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Flattening Hierarchies via Scale Labeling</head><p>Let's denote the segmentation tree of image I by T , with node v i indicating its i-th node. The nodes correspond to regions (segments) of I. Given T , our task is to find a tree slice L to divide all nodes v i 's (segments) into three groups: L − , L, and L + indicating under-, properly-and over-segmented, respectively. See <ref type="figure">Figure 3</ref> The problem is formulated as a three-class labeling problem. For each node v i , we use x(v i ) ∈ {−1, 0, 1} as its class label, with −1, 0, and 1 indicating the membership of v i to L − , L, and L + respectively. Assume now that a func- <ref type="bibr" target="#b0">1]</ref> is provided to measure the granularity of image segments, where negative values stand for under-segmented, 0 for properly-segmented, and positive for over-segmented regions. The magnitude of f (v i ) signals the deviation from being properly-segmented. Section 3.1.2 presents the proposed learning algorithm for f (v i ).</p><formula xml:id="formula_1">tion f (v i ) : v i → [−1,</formula><p>The labeling of all v i 's could be done by greedily taking the best-scoring class for each node. However, not any labeling represents a valid slice of the tree. Following the definition in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>, a tree slice is a set of nodes such that every path P n , n ∈ {1, 2, ..., N } from the leaf nodev n to the root node v 0 contains one and only one node v in the slice. <ref type="figure">Figure 3</ref>(a) shows one of these paths in green.</p><p>From the nature of segmentation hierarchies, the labels of parent nodes v p i should be equal or smaller than their child nodes v i . Intuitively, if a region is correctly segmented, the parent cannot be oversegmented. On the other hand, the parent of an undersegmented region will also be undersegmented. Putting the two constraints together, the labeling problem can be formulated as:</p><formula xml:id="formula_2">X = arg min X E(X) E(X) = vi∈L #(v i ) · f (v i ) 2 + λ vi / ∈L #(v i ) · l(v i ) s.t ∀n : v∈Pn ✶ L (v) = 1 ∀v : x(v) &gt;= x(v p )<label>(1)</label></formula><p>where #(v) is the size (number of pixels) of segment (node) v, λ is a weighting value for the two energy terms, and l(v i ) is the loss function defined for v i ∈ {L − , L + }, it encourages the sign of x(v i ) to be consistent with f (v i ).</p><formula xml:id="formula_3">l(v i ) = max(0, −f (v i ) · x(v i )).<label>(2)</label></formula><p>The loss function penalizes two contradictory cases: (i) segments in the group of under-segmented with positive scores; and (ii) segments in the group of over-segmented with negative scores. The problem will be solved via dynamic programming, as explained in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Inference by Dynamic Programming</head><p>The optimization problem in Equation 1 is highly structured and can be solved recursively by Dynamic Programming. For the subtree rooted at node v, its optimal slice L(v) is either the node v itself or the union of the optimal slices of all its child nodes v c 's, depending on whose energy is lower. Thus, the problem has optimal substructure <ref type="bibr" target="#b4">[5]</ref> and so it naturally fits to the framework of dynamic programming to find the global optimal solution. The problem proceeds from bottom to the top of the tree. For each subtree rooted at the current node v, the energy of v ∈ L(v) is computed and the energy of the optimal slices of all its child nodes is requested for comparison. The algorithm traverses back, and all comparison will be completed when the algorithm reaches the root node, and the global optimal of Equation 1 is obtained. The method is highly efficient with complexity O(N ), where N is the total number of nodes. The global optimal of the energy can be found by applying Algorithm 1 to the root node, and the optimal slice is the corresponding set of nodes labeled to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Predicting the Scales of Segments</head><p>In order to predict the scales (under-, properly-, or oversegmented) of the segments, we follow the route of modern computer vision systems to learn a predictor from humanannotated training data. To this end, we define a measure to compare the scale of an image segment r to that of the corresponding human-annotated segment g. The correspondence is built up by computing the overlap between</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Dynamic Programming in a Tree</head><p>Input</p><formula xml:id="formula_4">: tree node v i if v i is a leaf node then C vi ← #(v i ) · max(0, −f (v i )) E * vi ← #(v i ) · f (v i ) 2 else C vi ← vj ∈{v c } C vj + #(v i ) · max(0, −f (v i )) E * vi ← min( vj ∈{v c } E * vj +λ·#(v i )·max(0, f (v i )), #(v i ) · f (v i ) 2 + λ · vj ∈{v c } C vj ) end if return C vi , E * vi</formula><p>computer-generated segments and human-annotated onesthe most-overlapping human-annotated segment is taken as the ground-truth of the computer-generated ones. The overlap is computed with the Intersection over Union (IoU).</p><p>After having the ground-truth segment g, the scale of the segment r is then defined as:</p><formula xml:id="formula_5">S(r) = #(g) − #(r) max(#(r), #(g)))</formula><p>.</p><p>The , the scales of the segments by segmentation methods can be computed and used as the training data to train our scale predictor.</p><p>As to the learning method, we employ a regression forest as the predictor f (v). As to the features, we use a set of low-, and middle-level features, mainly following the work done for object proposals <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. The features are designed to capture a variety of region properties, and the detailed list of the features is provided in Section 4.1.</p><p>The main difference between our prediction and the previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref> is that they predict the quality of segments, while we predict the scale of the segments. We argue that its is easier to quantify the granularity of the segments than its quality, apart from providing more specific information such as under-segmented or over-segmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchy Re-scaling with Labeled Scales</head><p>After setting the optimal slice, we use it as an anchor to stretch the segmentation tree accordingly. In our experiments, we use the threshold value of each optimal node as a control point, and linearly interpolate the original hierarchy.</p><p>We represent the segmentation trees as UCMs <ref type="bibr" target="#b0">[1]</ref>, which are a matrix with size (2h+1) * (2w+1), where h is the height of the original image, and w is its width. For each pair of neighboring pixels in the image, the value in the UCM matrix represents their boundary strength (between 0 and 1). A partition at a certain scale can be extracted by thresholding the UCM at a certain strength value. Our algorithm is summarized in Algorithm 2, where the function Boundary finds the corresponding elements of boundary of a region r in the UCM, and InnerArea its inner area. We perform a local linear transform on the UCM map, and align the optimal slice to threshold 0.5, for the convenience of later use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our approach on the segmentation hierarchies generated by multiple segmentation methods, and further examine its usefulness on the task of object segmentation. The goal is to demonstrate that the proposed method is able to improve general segmentation hierarchies and the improvement is reflected to high-level vision tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings</head><p>Dataset: We benchmark the performance of our approach on the BSDS500 dataset <ref type="bibr" target="#b0">[1]</ref>, which includes 500 images (200 for training, 100 for validation, and 200 for testing). Each image is annotated by 5 different people on average. As segmentation evaluation measures, we use Segmentation Covering (SC), Probabilistic Rand Index (PRI), and Variation of Information (VI); all at Optimal Dataset Scale (ODS) and Optimal Image Scale (OIS) -see <ref type="bibr" target="#b23">[24]</ref> for a review of these measures and scales. We select these three particular measures given their wide acceptance in previous work <ref type="bibr" target="#b0">[1]</ref>.</p><p>Segmentation Techniques: As to the hierarchical segmentation techniques, we chose the following due to popularity, good performance, and the availability of public code:</p><p>• UCM <ref type="bibr" target="#b0">[1]</ref>: A widely-used hierarchical segmentation method. Discriminative features are learned for local boundary detection and spectral clustering is applied on top of it for boundary globalization.</p><p>• MCG <ref type="bibr" target="#b21">[22]</ref>: A unified framework for segmentation and object proposals. It combines information from multiple resolutions of the image to produce image segmentations and object proposals.</p><p>• SCG <ref type="bibr" target="#b21">[22]</ref>: The single-resolution version of MCG. It gets competitive results and is faster than MCG.</p><p>• PMI <ref type="bibr" target="#b12">[13]</ref>: A recent work for unsupervised boundary detection. It can be applied for image segmentation as well in order to generate a hierarchical segmentation.</p><p>Training: The training set and the validation set of BSDS500 are pooled together as the training set for our regression forest. The four segmentation methods are used to generate hierarchies, over which the training samples (segments) are extracted. We train method-specific regression forests as the scale predictor. Since a large portion of regions in the hierarchies are very small and features extracted from them are not reliable, we exclude regions smaller than 50 pixels for the training of the predictor. Specifically, for each region r, we find its corresponding ground-truth region g by taking the human-annotated one with the highest IoU score. The relative scale of r is then computed with Equation 3 for the regression target of r. As to the features for r, we draw on the success of object proposals <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. There, a large pool of middle-level features have been defined for segment description. The features used are summarized as follows:</p><p>• Graph partition properties: cut, ratio cut, normalized cut, unbalanced normalized cut.</p><p>• Region properties: area, perimeter, bounding box size, major and minor axis lengths of the equivalent ellipse, eccentricity, orientation, convex area, Euler number.</p><p>• Gestalt properties: inter-and intra-region texton similarity, inter-and intra-region brightness similarity, inter-and intra-region contour energy, curvilinear continuity, convexity.</p><p>Readers are referred to <ref type="bibr" target="#b1">[2]</ref> for the details of these features. We extract the features from a subset of layers uniformly sampled from the hierarchies, over the range of UCM values. As to the parameters of our method, we set 100 trees for the random forest; and λ in Equation 1 is set to 0.1 to balance information from the three groups, because there are more segments over and under the optimal slice L. <ref type="table">Table 1</ref> shows the results of our method evaluated on top of the four segmentation techniques. The improvements achieved by our alignment are considerable and, more importantly, they are consistent across different methods. The method improves more on ODS than OIS, because OIS accesses the ground-truth segmentations to search for the bestperforming threshold, which somehow diminish the influence of the learned knowledge. We argue that ODS is more practical than OIS in a real vision systems, because for real applications there is no human-annotated segmentations. <ref type="figure" target="#fig_5">Figure 4</ref> shows segmentation examples of MCG and aligned MCG by our method. As the figure shows, the aligned hierarchies generate characteristics closer to what human expect when flat segmentations are sampled out of the hierarchies. More particularly, after alignment, sampled segmentations of the hierarchies generate consistent responses across all parts of the image: all parts undersegmented, to all parts properly-segmented, and finally to all over-segmented while sampling from the top to the bottom of the hierarchies. This alignment greatly simplifies the use of hierarchical image segmentation for other high-level vision tasks. <ref type="figure">Figure 5</ref> shows qualitative results with different hierarchies. Our approach shows a consistent improvement over the original results. Again, since our approach is scaleaware, regions at the same level of the hierarchy are of similar scales across all areas of the images after the alignment. Also, our method demonstrates better ability of preserving region scale across images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We also tested the method in the scenario where the random forests are trained with segments from all of the four methods, and applied to all of them at test time. This gives slightly poorer results but in turn shows that our method can be applied in a method-agnostic approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to Other Methods</head><p>As the previous section shows, the MCG aligned by our method generally performs the best. Here, we compare MCG-aligned to other competing methods. The results are summarized in <ref type="table">Table 2</ref>  of PFE+MCG <ref type="bibr" target="#b32">[33]</ref>. It is noteworthy that our method and theirs are complementary, and the combination of the two may yield even better results. Their method is to improve feature embedding for a better local distance measure, while we aim to improve the hierarchy of existing segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation towards Object Segmentation</head><p>Segmentation per se is rarely the final objective of real applications, it is rather a middle tool towards, for instance, object segmentation <ref type="bibr" target="#b21">[22]</ref> or semantic segmentation <ref type="bibr" target="#b15">[16]</ref>. This section is devoted to show that better aligned hierarchies also help in this scenario.</p><p>We first perform the evaluation using the object annotations provided on the BSDS300 set by <ref type="bibr" target="#b6">[7]</ref> (we retrain on only BSDS300 train instead of BSDS500). The intuitive idea is to measure how well we can segment these objects by selecting regions from the different flattened hierarchies. <ref type="figure">Figure 6</ref> (left) shows the achievable quality that an oracle could reach if selecting the regions from the original hierarchies or the ones with our newly-proposed alignment. The X axis corresponds to the number of needed regions, i.e., the lower the better.</p><p>We can observe that the aligned hierarchies consistently need less regions to get the same quality in all the tested hierarchies. In PMI, for instance, we need to select 5 regions to achieve the same quality that we can get with 4 on the aligned hierarchy. The combinatorial space of all possible 4-region combinations is significantly smaller and thus the search is more probable to succeed. On the other direction, if we limit the number of regions we get improvements up to 3 points (9%) in the achievable quality.</p><p>To further illustrate the scalability of the hierarchy alignment on a larger dataset, we evaluated our alignment algorithm on the Pascal VOC 2012 Segmentation set <ref type="bibr" target="#b7">[8]</ref>. We retrain our scale predictor using the training set of Pascal 2012. In it, only the segmentation of foreground objects are given, in contrast to BSDS which is fully annotated. Thus during training we only consider all the segments that have overlap with foreground object annotations. The scale predictor is trained as described in Sec 3.1.2, the only difference is that g can only be foreground object. This strategy introduces extra bias towards foreground objects, because no information about the scale of background is given in the training phase. However, we are still able to improve alignment of segmentation hierarchies. As shown in <ref type="figure">Figure 6 (right)</ref>, we see that for the range of 2-3 regions (the one in which the MCG object proposal work), the aligned hierarchy provides a 2.5-point improvement (∼6%), which shows that our method generalizes to larger datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Running Time</head><p>Our approach takes approximately 3 seconds in total for each image, of which 2.39 seconds are spent on feature extraction from the segments. The prediction of regression forest takes about 0.45 seconds, and the dynamic programming takes 0.05 seconds for the inference. Finally, 0.11 seconds are spent for re-scaling the UCM. All times are measured on a standard desktop machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we presented a novel technique to align segmentation hierarchies, which learns and predicts the scale of their segments. We formulated the scale prediction for the segments in a hierarchy as a graph label problem, which is solved by dynamic programming. With the labeled scales as constraints, we then re-align the segmentation hierarchies by stretching the UCM maps.</p><p>The method is evaluated on four different segmentation hierarchies on BSDS500, and it consistently improves their quality. We also showed that the improvement of segmentation hierarchies by our alignment is reflected well to a higher-level task of getting object segmentations on the BSDS300 as well as the larger, more challenging PASCAL Segmentation dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example of improved hierarchy alignment: The original hierarchy (top row) needs three different flat partitions to represent the four objects (highlighted in red). Our aligned hierarchy (bottom row) correctly puts all objects in the same level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Our proposed hierarchy realignment: Given a hierarchy (a) in which the objects at the same scale are not well aligned (represented in the same scale level), we produce a realigned hierarchy (b) that has the similar-scale regions in the same level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b) for an example of nodes in the three groups. The visual representation of a slice can be seen in Figure 2 as red bands covering different regions and in Figure 3 as gray bands. An example of the flat partitions resulting from the three types of slices can be found in Figure 1 (bottom row), where the left partition is mainly oversegmented (L − ), the middle one correctly segmented (L), and the right one undersegmented (L + ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>value of S(r) is in [−1, 1], with negative values for under-, 0 for properly-and positive values for oversegmented regions, the magnitude of the values representing the extent of being under-or over-segmented, which casts to what we expected from f (v) (see Section 3.1). With Equation 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2</head><label>2</label><figDesc>Rescaling HierarchyInput: Optimal Slice S, UCM map M ucm for r ∈ S do b ← Boundary(r) a ← InnerArea(r) m ← min(M ucm (b)) M ucm (a) ← Mucm(a) 2m end for b all ← Boundary(S) m min ← min(M ucm (b all )) M ucm (b all ) ← 1+Mucm(ball)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of MCG (first row) and MCG improved by our approach (second row). Original images are shown in the left most. Segmentations of Optimal Dataset Scale (ODS) are given in the middle. From left to right we find different scales, fine to coarse. The red bounding box indicates the scale with best results achieved by MCG, and the blue box for ours. It can be seen that our approach provides better alignment, both across images and within one image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Qualitative comparison of segmentation results, hierarchies are flattened by Optimal Dataset Scale (ODS) Flattened hierarchies for object detection: Achievable object quality by an oracle selecting regions from the flattened partitions, with respect to the number of regions needed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>and demonstrate that segmentation quality can be improved by our alignment. In particular, the aligned MCG achieves the best result in SC and VI. After alignment, the results are on par with the newest method</figDesc><table>SC (↑) 
PRI (↑) 
VI (↓) 

ODS OIS 
ODS OIS 
ODS OIS 

Ncut [29] 
0.45 0.53 
0.78 0.80 
2.23 1.89 
Felz-Hutt [10] 
0.52 0.57 
0.80 0.82 
2.21 1.87 
Mean Shift [4] 
0.54 0.58 
0.79 0.81 
1.85 1.64 
Hoiem [12] 
0.56 0.60 
0.80 0.77 
1.78 1.66 
UCM [1] 
0.59 0.65 
0.83 0.86 
1.69 1.48 
ISCRA [26] 
0.59 0.66 
0.82 0.85 
1.60 1.42 
PFE+mPb [33] 
0.62 0.67 
0.84 0.86 
1.61 1.43 
PFE+MCG [33] 
0.62 0.68 
0.84 0.87 
1.56 1.36 

MCG [22] 
0.61 0.67 
0.83 0.86 
1.57 1.39 
MCG+Ours 
0.63 0.68 
0.83 0.86 
1.53 1.38 

Table 2. Segmentation results on BSDS500 test set, with a com-
parison to the state-of-the-art competitors. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Codes are publicly available at: https://github.com/ yuhuayc/alignhier</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The authors gratefully acknowledge support by armasuisse.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03339</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mean shift: a robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introduction to algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Is image superresolution helpful for other vision tasks? In WACV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Category-independent object proposals with diverse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recovering occlusion boundaries from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="346" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crisp boundary detection using pointwise mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning full pairwise affinities for spectral segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1690" to="1703" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scale-aware object tracking with convex shape constraints on RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A pylon model for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scaleaware fast R-CNN for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08160</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving spatial support for objects via multiple segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Vision science: Photons to phenomenology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parameter selection for graph cut based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised assessment of segmentation hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised evaluation of image segmentation and object proposal techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image segmentation by cascaded region agglomeration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using multiple segmentations to discover objects and their extent in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Binary partition tree as an efficient representation for image processing, segmentation, and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salembier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garrido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="561" to="576" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scale and object aware image thumbnailing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiresolution hierarchy co-clustering for semantic segmentation in sequences with small variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Varas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alfaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flattening supervoxel hierarchies by the uniform entropy slice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Piecewise flat embedding for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Harf: Hierarchy-associated rich features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
