<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Multimodal Learning in Audiovisual Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>xuelongli@opt.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>710119</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">â€ </forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
							<email>luxiaoqiang@opt.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>710119</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Multimodal Learning in Audiovisual Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In view of the advantages of deep networks in producing useful representation, the generated features of different modality data (such as image, audio) can be jointly learned using Multimodal Restricted Boltzmann Machines (MRB-M). Recently, audiovisual speech recognition based the M-RBM has attracted much attention, and the MRBM shows its effectiveness in learning the joint representation across audiovisual modalities. However, the built networks have weakness in modeling the multimodal sequence which is the natural property of speech signal. In this paper, we will introduce a novel temporal multimodal deep learning architecture, named as Recurrent Temporal Multimodal RB-M (RTMRBM), that models multimodal sequences by transforming the sequence of connected MRBMs into a probabilistic series model. Compared with existing multimodal networks, it's simple and efficient in learning temporal joint representation. We evaluate our model on audiovisual speech datasets, two public (AVLetters and AVLetters2) and one self-build. The experimental results demonstrate that our approach can obviously improve the accuracy of recognition compared with standard MRBM and the temporal model based on conditional RBM. In addition, RTMRBM still outperforms non-temporal multimodal deep networks in the presence of the weakness of long-term dependencies. U 1 J 2 J 3</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Robust Automatic Speech Recognition (ASR) has been the key to the natural human-computer interfaces in most cases, but it's challenged by the noisy environments. One example of such an environment is street, where the traffic noise makes it very hard for recognizing the speech. Considering that vision is free of audio noise and can provide complemental information to audio in the noisy condi-  tion <ref type="bibr" target="#b9">[10]</ref>, even clean environment, researchers have paid attention to the Audiovisual Speech Recognition (AVSR) that makes use of the information from both audio and visual modalities. And the proposed several types of AVSR models have shown that they indeed have certain improvement over the ASR based on only audio <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>In the past decades of years, several approaches have been proposed to fuse the speech information from the audio and visual modalities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. However, on account of the different statistical properties between the modalities <ref type="bibr" target="#b19">[20]</ref>, it's difficult to capture patterns across them. Recent works on deep learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> have verified the efficiency of deep networks in producing useful representations for various kinds of data, such as image, audio and text. It can be expected to explore the highly correlated representation across modalities after learning each channel data with single deep network. Based on this, multimodal deep networks have been proposed to jointly learn the generated features of different modalities and obtained state-of-the-art performance <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. But many tasks are inherently sequential. For example, each utterance is an ordered sequence of phonemes or visemes (motions of mouth lips) in the AVSR, where the latter is influenced by the former. And the built multimodal networks almost fail to model the temporal multimodal data, which ignore the correlation among the components of the utterance.</p><p>In this paper, we propose a novel temporal multimodal network to model audiovisual sequence in an unsupervised fashion, which we refer to as the Recurrent Temporal Multimodal Restricted Boltzmann Machine (RTMRBM). <ref type="figure" target="#fig_1">Figure 1</ref> shows a simple illustration of the proposed model. In each frame (time slice), the mouth lip and sound spectrogram are jointly learned using multimodal networks. The learned joint representations across modalities at different frames are directly connected from start to end, which makes the current frame learned based on previous one. In general, the proposed model has three advantages in the AVSR. First, it has the ability to extract semantic information from each modality data and learn the joint representation across audiovisual modalities. Second, it is a directed graphical model that can model temporal audiovisual sequences well as the joint representations among all frames are dependent. Third, the simple connection among the sequence of frames makes it easy to train as well as the standard MRBM. We evaluate our model on three audiovisual speech datasets, two public (AVLetters and AVLetters2) and one self-build (AVDigits). Our experiment results verify that the proposed model can learn better joint representation than non-temporal multimodal networks and temporal network based on Conditional RBM (CRBM). In addition, compared with typical multimodal network, RTMRBM can still performs well when faced with the weakness of longterm dependencies.</p><p>In the following sections, we first survey the related works about AVSR in Section 2. In Section 3, we review the representative multimodal model, then we develop the proposed RTMRBM, and introduce the inference as well as learning algorithm in it. Section 4 conducts different sets of experiments for evaluating the model on the three datasets, and corresponding results are reported and discussed. Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Classic AVSR Systems. AVSR has been studied in a few years, amounts of work about it can be roughly grouped into two categories: feature fusion and decision fusion <ref type="bibr" target="#b13">[14]</ref>. The former aims to classify the concatenation of audio and visual features with a single classifier, but it has the weakness of separating out the noisy features. And the latter fuses the class-conditional probabilities of two classifiers with appropriate weights that depend on the contribution of each modality, such as multi-stream Hidden Markov Mod-els (HMMs). However, The classic AVSR method based on multi-stream HMMs does not generalize very well because the weights that vary with time are hard to estimate. More importantly, both feature fusion and decision fusion have weakness in building a connection between audio and visual modalities at the level of semantics, where they are considered highly correlated <ref type="bibr" target="#b18">[19]</ref>. AVSR based on Deep Learning. In recent years, deep learning methods have performed its effectiveness in generating useful feature representation. Most of the generated features from different kinds of data are considered as semantic correlated <ref type="bibr" target="#b19">[20]</ref>. For the AVSR task, Ngiam et al. <ref type="bibr" target="#b12">[13]</ref> proposed a kind of multimodal deep networks, Multimodal Deep Autoencoder (MDAE), which learns the layers of modality-specific network that consists stacks of RBMs firstly. Then, the joint representations across the generated features of audiovisual modalities are learned using Multimodal RBM (MRBM). Besides, the pre-trained M-DAE is fine-tuned to minimize reconstruction errors of both modalities. Huang and Kingsbury <ref type="bibr" target="#b9">[10]</ref> combined two Deep Belief Networks (DBNs) with the MRBM, and each DBN is used to model one type of modality. The organized Multimodal DBN (MDBN) has shown to outperform the accuracy of recognition by multi-stream HMMs. Similar frameworks have also been served to other tasks, such as multimodal retrieval <ref type="bibr" target="#b19">[20]</ref>. MRBM has shown its ability in fusing the audio and visual modalities into a joint representation in the aforementioned networks. But the temporal information is not considered, which apparently deviates from the natural property of audiovisual speech signal. Recently, Amer et al. <ref type="bibr" target="#b0">[1]</ref> attempted to model the audiovisual sequences for the first time. They made use of CRBM <ref type="bibr" target="#b24">[25]</ref> to model each modality sequence in the task of AVSR, which made the modality-specific network sequence connected. Then the joint representation across modalities was generated. But, the multimodal network based on CRBM makes the MRB-M complex, and it's difficult to learn the joint representation across multiple modalities because there're full connectivity among all the pairs of single modality layer and shared hidden layer <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Model</head><p>In this work, our proposed model aims at fusing the temporal audio and visual representations into a joint representation sequence. In the following subsections, we first briefly review the MRBM model which is used to learn the joint representation across modalities. Then we introduce the RTMRBM model and explain the inference and learning procedure in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multimodal Restricted Boltzmann Machine</head><p>The RBM is an undirected graphical model that defines a probability distribution of visible units using hidden unit- s <ref type="bibr" target="#b17">[18]</ref>. Under the case of the multimodal input (we will take audiovisual inputs as an example), MRBM ( <ref type="figure" target="#fig_2">Figure 2</ref>) defines the joint distribution over audio modality a, visual modality v, and shared hidden units h <ref type="bibr" target="#b18">[19]</ref>,</p><formula xml:id="formula_0">P (a, v, h) = 1 Z exp (âˆ’E (a, v, h)) ,<label>(1)</label></formula><p>where Z is the partition function and E is an energy function given by</p><formula xml:id="formula_1">E (a, v, h) = âˆ’a T W a h âˆ’ v T W v h âˆ’ a T b a âˆ’ v T b v âˆ’ h T b h ,<label>(2)</label></formula><p>where a and v are the binary visible units of audio and visual input, and h is the binary shared hidden units. W a is a matrix of pairwise weights between elements of a and h, and similar for W v . b a , b v , b h are bias vectors for a, v, and h, respectively. To obtain the joint likelihood P (a, v), h is marginalized out from the distribution,</p><formula xml:id="formula_2">P (a, v) = âˆ‘ h exp(âˆ’E(a, v, h))/Z.<label>(3)</label></formula><p>For the MRBM model, similar to the standard RB-M, Contrastive Divergence (CD) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> or Persistent CD (PCD) <ref type="bibr" target="#b25">[26]</ref> is used to approximate the gradient to maximize the joint likelihood, i.e., P (a, v). This is the typical maximum likelihood learning for MRBM. Finally, the learned shared hidden units h is treated as the joint representation across modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Multimodal Learning</head><p>Although MRBM is good at learning the joint representation across modalities, it fails to capture the temporal information about the multimodal sequence, especially in AVSR. Specifically, the audio inputs of MDAE or MDBN are the concatenation of several frames of audio spectrogram, similarly for visual modality. These frames are only a part of each utterance representing phonemes or visemes, which ignores the continuity of all the frames that belong to the utterance. In addition, the joint representations obtained from multimodal networks are directly concatenated without considering the interaction and influence among them. To overcome the aforementioned problems, audiovisual representations should be viewed as sequence and modeled by temporal multimodal networks.</p><p>To model the audio and visual representation sequences simultaneously, it's intuitive to organize a sequence of M-RBMs. The joint representations are considered to be connected among MRBMs, where the latter representations are dependent on the former. It's more credible than the generated representation of single modality <ref type="bibr" target="#b12">[13]</ref>, which can provide complement information for each modality. Researchers have also verified that merged information is more useful than the summation of single channel <ref type="bibr" target="#b20">[21]</ref> in the field of cognitive science. In addition, the simple connections among MRBMs can identify the dependency correlation between joint and audiovisual layers, which means it can learn useful representation across modalities.</p><p>In fact, the organized network is a modification of Temporal RBM (TRBM) <ref type="bibr" target="#b23">[24]</ref> which consists of a sequence of RBMs, where the hidden layer of current RBM depends on the previous RBMs. Although the TRBM has shown its effectiveness in modeling unimodal data, such as a sequence of bouncing ball or motion captures, it can not deal with the multimodal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Recurrent Temporal Multimodal RBM</head><p>The proposed RTMRBM models sequences of audio rep-</p><formula xml:id="formula_3">resentation {a t } T t=1 , video representation {v t } T t=1 , where a t âˆˆ {0, 1} Na , v t âˆˆ {0, 1}</formula><p>Nv , t is the time step and T is the sequence length. <ref type="figure" target="#fig_3">Figure 3</ref> shows an illustration of the RTMRBM network. Specifically, the audio and visual representation {a t , v t } form the aforementioned MRBM with the shared hidden units h t âˆˆ {0, 1} N h at time step t. Actually, it's hard to infer the shared hidden units h t depended on former h tâˆ’1 exactly when the hidden layers of MRBMs are connected, because the required exact ratio of two M-RBM partition functions is hard to evaluate <ref type="bibr" target="#b23">[24]</ref>. Inspired by the recurrent TRBM <ref type="bibr" target="#b21">[22]</ref>, through making the connection between visual layer and the hidden layer directed and using mean-field update instead, exact inference becomes easy. Therefore, as described in <ref type="figure" target="#fig_3">Figure 3</ref>, we add the joint layers {J t } T t=1 on the top of MRBMs, which connects the sequence of MRBMs, where J t âˆˆ R N h .</p><p>The joint distribution over a t , v t , and h t given the previous joint units J tâˆ’1 at time t is defined by the equation  E is the energy function of the MRBM given by</p><formula xml:id="formula_4">P (a t , v t , h t |J tâˆ’1 ) = 1 Z Jtâˆ’1 exp (âˆ’E (a t , v t , h t |J tâˆ’1 )) ,<label>(4)</label></formula><formula xml:id="formula_5">W a W a W v W v U J 1 h 2 h 3 h init b init b 1 a 2 a 3 a 3 v 2 v 1 v Multimodal RBM Sequence</formula><formula xml:id="formula_6">E (a t , v t , h t |J tâˆ’1 ) = âˆ’a T t W a h t âˆ’ v T t W v h t âˆ’a T t b a âˆ’ v T t b v âˆ’ h T t b h âˆ’h T t UJ tâˆ’1 = E (a t , v t , h t ) âˆ’ h T t UJ tâˆ’1 ,<label>(5)</label></formula><p>where model parameters</p><formula xml:id="formula_7">{ W a , W v , b a , b v , b h }</formula><p>are the matrixes of connection weights and biases for layers as in Eq.2. As for the matrix U âˆˆ R NJ Ã—NJ , it's about the pairwise weights of J tâˆ’1 and J t , which is the only difference compared with MRBM. When t = 1, b init is treated as the input instead of term (b h + UJ tâˆ’1 ). Obviously, the energy function in Eq.5 consists of the energy of standard MRBM (Eq.2) and the term based on former joint units J tâˆ’1 , which makes the RTMRBM model the multimodal sequence.</p><p>The mean-field value J t is utilized to make the exact inference easier <ref type="bibr" target="#b11">[12]</ref>, which is essentially the expected value of h t given the audio and visual representation {a t , v t }. Therefore, the sequence {J t } T t=1 is treated as the learned joint representation, and that is obtained as follows,</p><formula xml:id="formula_8">J t = Ïƒ ( a T t W a + v T t W v + b h + UJ tâˆ’1 ) ,<label>(6)</label></formula><p>where Ïƒ (Â·) is the element-wise logistic sigmoid function. When t = 1, b init is used as before. Note that, Eq.6 makes the connected MRBM sequence into a kind of Recurrent Neural Network (RNN), where joint units J t is timedependent. Given the former joint units J tâˆ’1 , it's easy to obtain the joint probability over audio and visual representation {a t , v t } by marginalizing out the hidden units h t ,</p><formula xml:id="formula_9">P (a t , v t |J tâˆ’1 ) = âˆ‘ ht exp (âˆ’E (a t , v t , h t |J tâˆ’1 )) / Z Jtâˆ’1 .<label>(7)</label></formula><p>Based on the joint probability at time t, the joint probability over two sequences of audio and visual representation satisfies (see <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref> for more about the second equation),</p><formula xml:id="formula_10">F = log P ( {a t } T t=1 , {v t } T t=1 ) = T âˆ‘ t=1 log P (a t , v t |J tâˆ’1 ).<label>(8)</label></formula><p>To model the audiovisual sequences, we'd like to maximize the joint likelihood F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference and Learning in RTMRBM</head><p>Inference in RTMRBM is achieved by giving previous joint units J tâˆ’1 and calculating the activation state of shared hidden layer h t when a t and v t are fixed. Specifically, using the advantage of conditional independence in RBM, each unit in the shared hidden layer h t is activated with the probability,</p><formula xml:id="formula_11">P (h t k = 1|a t , v t , J tâˆ’1 ) = Ïƒ ( a T t W a Â·k + v T t W v Â·k + b h k + U kÂ· J tâˆ’1 ) .<label>(9)</label></formula><p>Given joint representation sequence {J t } T t=1 , the RTMRB-M is decoupled into a sequence of MRBMs, and CD or PCD can be used for learning. Therefore, inferring the audiovisual representation {a t , v t } is also required.</p><p>For the representation layers {a t , v t } which both connect to the shared hidden layer h t , when h t is observed, they cannot affect each other. Hence, the conditional distribution of units in a t and v t take the form as follows,</p><formula xml:id="formula_12">P (a ti = 1|v t , h t , J tâˆ’1 ) = Ïƒ (W a iÂ· h t + b a i ) ,<label>(10)</label></formula><formula xml:id="formula_13">P ( v tj = 1|v t , h t , J tâˆ’1 ) = Ïƒ ( W v jÂ· h t + b v j ) .<label>(11)</label></formula><p>Compared with standard MRBM, the inference in RTM-RBM is performed similarly except the joint units. The joint units come from previous time t âˆ’ 1 have impact on the activation of shared hidden units and audiovisual representation (indirectly) at time t. In other words, given J tâˆ’1 , the learning of current MRBM depends on shared audiovisual representation and previous sequence, which makes the MRBM time-dependent.</p><p>Learning in the proposed model RTMRBM is performed by learning model parameters</p><formula xml:id="formula_14">{W a , W v , b a , b v } and { U, b h , b init } .</formula><p>The former relates to standard MRB-M, which we will focus on. The latter is related to joint units J t and can be learned using the same learning rules as RTRBM <ref type="bibr" target="#b21">[22]</ref>, which is based on Backpropagation Through Time (BPTT) algorithm <ref type="bibr" target="#b14">[15]</ref>.</p><p>Similar with MRBM, parameters {W a , W v , b a , b v } can be learned based on CD approximation but time-dependent. Specifically, b a and b v are learned as follows,</p><formula xml:id="formula_15">b a := b a + Î± T âˆ‘ t=1 (E P data [a t ] âˆ’ E Precon [a t ]) ,<label>(12)</label></formula><p>b</p><formula xml:id="formula_16">v := b v + Î± T âˆ‘ t=1 (E P data [v t ] âˆ’ E Precon [v t ]) ,<label>(13)</label></formula><p>where Î± is a learning rate, E P data is the data-dependent expectation, and E Precon is the data-reconstruction's expectation but depends on the joint representation sequence</p><formula xml:id="formula_17">{J t } T âˆ’1 t=1</formula><p>. W a and W v of F are updated using gradient ascent which consists of two terms, one is about inferring the joint representation J t given audiovisual representation {a t , v t }, the other one relates to MRBM at time t given previous joint representation J tâˆ’1 ,</p><formula xml:id="formula_18">âˆ† W a F = T âˆ’1 âˆ‘ t=1 a t ( âˆ† J t+1 F âŠ™ J t âŠ™ (1 âˆ’ J t ) ) T + T âˆ‘ t=1 âˆ† W a log P (a t , v t |J tâˆ’1 ),<label>(14)</label></formula><formula xml:id="formula_19">âˆ† W v F = T âˆ’1 âˆ‘ t=1 v t ( âˆ† J t+1 F âŠ™ J t âŠ™ (1 âˆ’ J t ) ) T + T âˆ‘ t=1 âˆ† W v log P (a t , v t |J tâˆ’1 ),<label>(15)</label></formula><p>where âŠ™ denotes element-wise product. Note that the second term in Eq.14 is the summation over the negative gradient of MRBM at time t with regard to W a , which is computed as the standard MRBM using CD approximation</p><formula xml:id="formula_20">( E P data [ a t h T t ] âˆ’ E Precon [ a t h T t ])</formula><p>, and similarly for the weight matrix W v of visual modality (Eq.15). The term âˆ† J t+1 F in both Eq.14 and Eq.15 takes the form</p><formula xml:id="formula_21">âˆ† J t F = U T ( J t+1 âŠ™ (1 âˆ’ J t+1 ) âŠ™ âˆ† J t+1 F ) +U T âˆ† b h log P (a t , v t |J tâˆ’1 ) . (16) âˆ† J t F is computed recursively, and J T +1 = 0. The term âˆ† b h log P (a t , v t |J tâˆ’1 )</formula><p>is also computed with CD approximation. We summarize the learning procedure in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learning in RTMRBM</head><formula xml:id="formula_22">Input: Audio representation {a t } T t=1 , Visual representation {v t } T t=1 , CD steps K, number of iteration N . Output: Model parameters { W a , W v , b a , b v , b h , b init } .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we show the results of RTMRBM compared with other models on three datasets, including M-DAE, MDBN, and CRBM. Different Signal Noisy Ratio (S-NR) are also added to audio signal for evaluating the performance. In addition, we analyze the impact of the joint-joint weight matrix U on sub-sequence when modeling the whole multimodal sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Experiments are conducted on three datasets, two public datasets: AVLetters <ref type="bibr" target="#b10">[11]</ref>, AVLetters2 <ref type="bibr" target="#b5">[6]</ref>, and one self-build dataset: AVDigits. The AVDigits dataset is built to examine the performance of RTMRBM, which contains different semantic information compared with the other two. AVLetters contains 10 speakers speaking the letters A to Z at three times each. This dataset provides pre-extracted lip regions of 60 Ã— 80 pixels and audio features (raw audio is not provided) Mel-Frequency Cepstrum Coefficient (MFC-C). Similar with <ref type="bibr" target="#b0">[1]</ref>, the training set of this dataset contains the first two times of each letter spoken by each speaker, and the rest is for the test set. Hence, the training set and testing set both contain the same set of speakers, which is speaker dependent.</p><p>AVLetters2 is a high-definition of AVLetters. It's about reading letters from A to Z, spoke by five people, seven times for each letter. Similar with <ref type="bibr" target="#b5">[6]</ref>, letters spoken by four people are for training and the rest one is for testing. This is different from previous train/test split, which is speaker independent.</p><p>AVDigits is a self-build dataset, which is about speaking digits. We ask 6 people to face the camera and speak digits 0 to 9 at nine times each. All videos are recorded in fullfrontal pose, and all subjects are required to keep their heads fixed as far as possible. Speakers are also asked to close their mouth at the begin and end when speaking prepared digits. The recording video devices is SONY cx290, the visual modality of each utterance is digitized in 1920 Ã— 1080 at 25fps, and audio is recorded at 48kHz, 16-bit resolution. Letters spoken by four people are exploited to train and the rest two are exploited to test, which is also speaker independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Preprocessing</head><p>The audio and visual data are separated and preprocessed, respectively. For audio signal, spectrogram (MFCC instead in AVLetters) is extracted with 20ms hamming window and 10ms overlap. The frequency points of Discrete Fourier Transforms are 500, which results in 251 dimension vector of the signal window. The obtained spectral coefficient vector is reduced to 50 dimensions using PCA whitening.</p><p>For visual signal, cascade object detector <ref type="bibr" target="#b26">[27]</ref> is used to extract the Region-of-Interest that encompasses the mouth. The extracted region is rescaled to 60 Ã— 80 pixels and reduced to 100 principal components using PCA whitening as well. We use 4 contiguous audio frames and 1 video frames as the inputs for each time step simultaneously, which are almost the same duration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>As Deep Auto-encoder (DAE) can be used to obtain the efficient binary codes for both audio and visual information <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>, the audiovisual representations are pre-extracted using modality-specific DAEs. In addition, lots of experiments have verified that pretraining method can indeed affect the performance of RNN <ref type="bibr" target="#b3">[4]</ref>. We find that the initialized W a , W v , b a , b v , and b h from MDAE can capture more shared information across audiovisual modalities. The pairwise weight U and initial value b init can be initialized to small random value.</p><p>For the task of AVSR, the joint representation generated from the RTMRBM is treated as the audiovisual fusion, which can be learned in an unsupervised manner. Since each speaking example has varying duration, we divided the fusion result into 1 and 3 equal slices, similar to <ref type="bibr" target="#b12">[13]</ref>. Each slice consists of several audio and visual frames, and mean-pooling is performed over them. Then, the obtained features of each slice are concatenated and classified using a linear SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>To evaluate the joint representation learned by our proposed RTMRBM model, we conduct sets of experiments on both unimodal and multimodal data, i.e. audio modality, visual modality, and both of them. For the unimodal fashion, we present only one modality and set the other one to be zero during the learning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Speaker Dependent</head><p>In the speaker dependent experiments, we compare RTM-RBM with the performance of several methods on AVLetters, which includes prior methods based on hand-craft features and multimodal deep networks. <ref type="table">Table.</ref>1 shows the comparison results of mean accuracy over all the letters. For the three kinds of modalities, RTMRBM outperforms all the others. In contrast, HMM is also trained to model temporal sequence with 3DCNN, but RTMRBM deals with the visual data simpler and learns better joint representation. In addition, RTMRBM has an improvement compared with non-temporal multimodal deep networks MDAE that is based on MRBM, which means our proposed model learns better feature representation through capturing the temporal information. Note that, for the audiovisual modalities, CRBM makes the modality-specific network connected instead of the joint representation which can capture bet-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality</head><p>Model mean Accuracy A MDAE <ref type="bibr" target="#b12">[13]</ref> 58.40 CRBM <ref type="bibr" target="#b0">[1]</ref> 61.2 RTMRBM 64.41 V Multiscale Spatial Analysis <ref type="bibr" target="#b10">[11]</ref> 44.6 Local Binary Pattern <ref type="bibr" target="#b28">[29]</ref> 58.85 3DCNN-HMM <ref type="bibr" target="#b27">[28]</ref> 59.6 MDAE <ref type="bibr" target="#b12">[13]</ref> 62.10 CRBM <ref type="bibr" target="#b0">[1]</ref> 62.60 RTMRBM 64.63 AV MDAE <ref type="bibr" target="#b12">[13]</ref> 62.90 CRBM <ref type="bibr" target="#b0">[1]</ref> 64.8 RTMRBM 66.04  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Speaker Independent</head><p>In the speaker independent experiments, we compare RTM-RBM mainly with MDAE and MDBN models on AVLetter-s2 and AVDigits. To evaluate our proposed model at different levels of audio noise, we add the white Gaussian noise from -4dB to 12dB SNR to the original clean signal. Table.2 shows the comparison among modalities and models.</p><p>There're three points we should pay attention to. First, on the AVLetters2 dataset, we make a contrast with Active Appearence Model (AAM) <ref type="bibr" target="#b1">[2]</ref> on the visual modality, which learns a mean face template, but it's sensitive to the specific training speakers. The results show that modeling sequence with RTMRBM lower the degree of sensitivity to some extent. Second, on both datasets, we indeed improve the mean accuracy at different levels of SNR by learning both audio and visual modalities instead of one of them. Especially, when audio SNR becomes lower, the accuracy of audiovisual modality has a significant improvement compared with single modality, which ensures the learned joint representation sequence has more discriminative and robust features. We also note that the audiovisual modality performs worse than single audio information in the situation of clean audio signal. This is because the visual modality lower the performance, which is a common situation <ref type="bibr" target="#b12">[13]</ref>. Third, compared with the other multimodal deep networks (MDAE and MDBN), RTMRBM performs better on both datasets. This shows that modeling the multimodal sequences indeed cap-ture temporal information and therefore make the model learn better feature representation across modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Additional Contrast Experiment</head><p>As the RTMRBM is essentially a RNN, it's challenged by the long-term dependencies <ref type="bibr" target="#b2">[3]</ref>. In this experiment, through examining the effectiveness of the proposed model on sub-sequence when modeling the whole audiovisual modalities sequence, we explore the degree of influence. Specifically, the RTMRBM is trained with the whole sequence of speaking examples, but the generated joint representation is equally divided into three parts, former, middle and latter. Feature representation in each part are trained and classified with a linear SVM. And then we make a comparison with MDAE which is treated as the same fashion. <ref type="figure" target="#fig_4">Figure 4</ref> shows the improved mean accuracy of each part compared with MDAE on AVDigits and AVLetters2. The results shows that RTMRBM learns better feature representation on all the three sequences compared with M-DAE, but they have different degree of improvement. On both datasets, the former part of sequence is enhanced more than the latter, which ensures that RTMRBM indeed has difficulty in tackling the long-term dependencies. However, through modeling the multimodal sequence, the temporal information plays an important role in learning the joint representation. RTMRBM can still learn better joint feature representation on the latter part than non-temporal multimodal network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a new architecture RTMRBM for modeling temporal multimodal sequences, which makes the sets of MRBMs model the temporal multimodal data well because the previous robust joint representation is provided for the learning of current MRBM. Meanwhile, the simple connection makes the model easy to train. Our experimental results show that the proposed model can learn temporal joint representation across multiple modalities in the task of AVSR, even when the different levels of audio noise exist. In addition, although the model is truly affected by jointjoint weight matrix in the long-term dependency, it also performs better than non-temporal multmodal networks, which verifies the importance of temporal information and its effectiveness. In the future, we plan to apply the RTMRBM in other temporal multimodal tasks and attempt to reduce the difficulties in learning long-term multimodal sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The sequence of audio and visual frames, where the joint representation across audiovisual modalities in current frame depends on the former. This directed graphical model is proposed to model temporal multimodal sequences in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of the MRBM over audio and visual modality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The structure of the RTMRBM network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The contrast among the improved mean accuracy of three parts (former, middle, and latter) of utterances, compared with MDAE on AVDigits and AVLetters2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>where Z Jtâˆ’1 is the partition function that depends on J tâˆ’1 , ...</figDesc><table>... 

... 
... 

Joint Representation 

Audio Representation 

Visual Representation 

Shared Hidden Units 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Run a Gibbs chain for K steps for each MRBM given J tâˆ’1 at time step t, the sampling probability for b t , a t , and v t follows Eq.9, Eq.10, and Eq.11, respectively. 4: Update the bias of audio and visual layers {b a , b v } follows the Eq.12 and Eq.13. The pairwise matrixes W a and W v are updated according to the rules W a := W a +Î±âˆ† W a F and W v := W v +Î±âˆ† W v F, respectively.</figDesc><table>1: Initialize model parameters. 
2: Compute the joint representation sequence {J t } 

T 

t=1 us-
ing Eq.6. 
3: 5: Update 
{ 
U, b h , b init 
} 
using the learning rules of 
RTRBM [22]. 
6: Repeat above 2-5 steps until convergence or N steps 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The mean accuracy of speech classification on 
AVLetters, RTMRBM and other models are evaluated with 
single audio/visual modality and both of them. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Speech classification performance on AVLetters2 and AVDigits. The results show that, the RTMRBM performs better than MDAE and MDBN under the conditions of different degrees of SNR to the audio signal, also almost better than single modality.ter features across modalities, therefore it's hard for CRBM to learn better joint representation based on previous one. These classification results show the efficient of RTMRBM in generating feature representation on both single modality and multi-modalities.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal fusion using dynamic hybrid models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2014 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="556" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Speakerindependent machine lip-reading with speaker-dependent viseme classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Harvey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult. Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6392</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The challenge of multispeaker lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-J</forename><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSP</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="179" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Binary coding of speech spectrograms using a deep auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1692" to="1695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio-visual deep learning for noise robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7596" to="7599" />
		</imprint>
	</monogr>
	<note>2013 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extraction of visual features for lipreading. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bangham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="213" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured recurrent temporal restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mittelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Audiovisual automatic speech recognition: An overview. Issues in visual and audio-visual speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Information processing in dynamical systems: Foundations of harmony theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved multimodal deep learning with variation of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2141" to="2149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The merging of the senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Meredith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The recurrent temporal restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the convergence properties of contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="789" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two distributed-state models for generating high-dimensional time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1025" to="1068" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training restricted boltzmann machines using approximations to the likelihood gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1064" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">511</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Human Action Recognition Using Deep Probabilistic Graphical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lipreading with local spatiotemporal descriptors. Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>PietikÃ¤inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1254" to="1265" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
