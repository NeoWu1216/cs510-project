<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kernel Approximation via Empirical Orthogonal Decomposition for Unsupervised Feature Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Mukuta</surname></persName>
							<email>mukuta@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1 Hongo Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1 Hongo Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Kernel Approximation via Empirical Orthogonal Decomposition for Unsupervised Feature Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Kernel approximation methods are important tools for various machine learning problems. There are two major methods used to approximate the kernel function: the Nyström method and the random features method. However, the Nyström method requires relatively high-complexity post-processing to calculate a solution and the random features method does not provide sufficient generalization performance. In this paper, we propose a method that has good generalization performance without high-complexity postprocessing via empirical orthogonal decomposition using the probability distribution estimated from training data. We provide a bound for the approximation error of the proposed method. Our experiments show that the proposed method is better than the random features method and comparable with the Nyström method in terms of the approximation error and classification accuracy. We also show that hierarchical feature extraction using our kernel approximation demonstrates better performance than the existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Analyzing data with nonlinearity is one of the main tasks in machine learning. The kernel method maps input data into a high-dimensional feature space and computes the similarity in the feature space without computing the coordinates of data in that space. The computational complexity of the kernel method is determined by the size of data, regardless of the dimension of the feature space. The kernel method is applied to the classifiers and dimensionality reduction techniques, such as the kernel support vector machine (SVM) <ref type="bibr" target="#b5">[6]</ref>, kernel principal component analysis (PCA) <ref type="bibr" target="#b22">[23]</ref>, and kernel canonical correlation analysis (CCA) <ref type="bibr" target="#b11">[12]</ref>. However, the complexity of kernel methods grows quadratically or cubically with the amount of the training data, which makes it difficult to scale directly for large-scale datasets. A method that approximates the kernel function using the inner product of the nonlinear feature functions, which map data into a relatively low-dimensional feature space, is useful because it is compatible with fast linear classifiers.</p><p>There are two major methods for approximating the kernel function: the Nyström method <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> and the random features method <ref type="bibr" target="#b19">[20]</ref>. The Nyström method generates lowrank approximations of the Gram matrix calculated from training data. For the random features method, the kernel function is expressed as the expectation value of the inner product of feature functions, which are randomly sampled from a proper probability distribution. However, the Nyström method requires the calculation of a D × D inverse matrix at the learning phase, where D is the feature dimension, and requires O(D 2 ) post-processing at the classifying phase. Hence, we cannot use a high-dimensional feature when using the Nyström method. To derive greater generalization ability, the random features method requires the feature to have the same number of dimensions as the number of training data, making it difficult to take advantage of the approximation.</p><p>In this paper, we propose a method to closely approximate the kernel function via empirical orthogonal decomposition without post-processing for the features. In the proposed method, the kernel function is decomposed using the probability distribution estimated from training data, which enables it to have a high approximation ability. As the proposed method directly approximates the kernel function, post-processing for the features becomes unnecessary. We show that the spectral norm of the approximation error of the Gram matrix is bounded using eigenvalues and the distance between the true and approximate distributions. We also present the calculation method of the proposed kernel approximation using the Gaussian kernel.</p><p>Kernel approximations are also used to construct a hierarchical image feature by iteratively building kernels between image patches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17]</ref>. We combine our approximation method with convolutional kernel networks (CKN) architecture <ref type="bibr" target="#b16">[17]</ref> and propose a novel method for unsupervised feature learning without a time-consuming optimization process.</p><p>The results of our experiments show that the proposed method is better than the random features method and comparable with the Nyström method in terms of the approximation error and classification accuracy. The proposed method for unsupervised feature learning demonstrates better or comparable accuracy with a shorter learning time than CKN. Our contributions are as follows:</p><p>• We propose a method to construct the feature functions that do not need post-processing by decomposing the kernel function using the probability distribution estimated from training data.</p><p>• We provide a bound for the approximation error of the proposed method and present a calculation method when we assume the Gaussian kernel and a Gaussian distribution.</p><p>• Experimental results on artificial and real datasets show that the proposed methods demonstrate performance that is better than the random features method and comparable with the Nyström method with lower complexity.</p><p>• Experimental evaluations of the unsupervised feature learning method show that the proposed method demonstrates better or comparable accuracy with CKN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There are many kernel approximation methods; however, we briefly introduce the Nyström method and random features method because we shall focus on these methods to approximate the kernel function using the inner product of nonlinear feature functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Nyström method</head><p>The Nyström method approximates the true Gram matrix using kernel similarity to randomly sample data from training examples. Let {x 1 , x 2 , · · · , x D } denote the subset of samples and K D = U ΛU t denote the eigen-decomposition of the Gram matrix generated by the subset of samples, then the Nyström method maps input x in the following way:</p><formula xml:id="formula_0">Λ −1/2 U t (k(x, x 1 ), k(x, x 2 ), · · · , k(x, x D )) t<label>(1)</label></formula><p>To analyze Nyström methods, the bound of the spectral norm of the approximation error is usually calculated. Drineas et al. <ref type="bibr" target="#b7">[8]</ref> showed that the approximation error is O(D −1/2 ). Because Bartlett et al. <ref type="bibr" target="#b0">[1]</ref> showed that the generalization error of the kernel method is O(N −1/2 ), where N is the size of training data, the required number of samples D should be O(N ) to achieve a small approximation error. According to the analysis of Yang et al. <ref type="bibr" target="#b28">[29]</ref>, the number of samples D is reduced to O(N −1/2 ) by assuming that there is a large gap between the eigenvalues. Kumar et al. <ref type="bibr" target="#b10">[11]</ref> provided a detailed comparison of various fixed and adaptive sampling techniques. However, an O(D 3 ) calculation of K −1/2 of the sample Gram matrix is required, and O(D 2 ) post-processing for each datum is required, which is time-consuming when D is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Random features method</head><p>The random features method approximates the kernel function using an inner product of randomly sampled feature functions. </p><formula xml:id="formula_1">k(x, y) = E ω [f ω (x) * f ω (y)] = ∫ dωp(ω)f ω (x) * f ω (y),</formula><p>(2) then a random feature is a method that samples D ω d s i.i.d from p(ω) and maps</p><formula xml:id="formula_2">x → 1 √ D (f ω1 (x), ..., f ωD (x)).</formula><p>If f ω is uniformly bounded, then we can show that we can approximate the original kernel with high probability using a sufficiently large dimension D by applying Hoeffding's inequality.</p><p>Rahimi and Recht <ref type="bibr" target="#b19">[20]</ref> proposed a random feature using trigonometric functions for a shift-invariant kernel in Euclidean space R d . A shift-invariant kernel is a kernel that can be calculated using only the difference between two inputs, such as k(x, y) = φ(x − y). Rahimi and Recht <ref type="bibr" target="#b19">[20]</ref> constructed a random Fourier feature using Bochner's theorem, which connects shift-invariant kernels with probability distributions in Fourier space. Theorem 2.1 (Bochner <ref type="bibr" target="#b21">[22]</ref>). For φ corresponding to a shift-invariant kernel, there is a probability p(ω) on R d that</p><formula xml:id="formula_3">k(x, y) = φ(x − y) = ∫ dωp(ω)e iω(x−y)<label>(3)</label></formula><p>holds.</p><p>According to Bocher's theorem, the shift-invariant kernel is a Fourier transform of some distribution. By sampling ω d from this distribution p(ω), the mapping</p><formula xml:id="formula_4">x → 1 √ D ( e iω1x , ..., e iωDx )<label>(4)</label></formula><p>approximates the original kernel. Additionally, the method that uniformly samples b d from [0, 2π] and maps</p><formula xml:id="formula_5">x → 1 √ D ( √ 2cos (ω 1 x + b 1 ) , ..., √ 2cos (ω D x + b D ) )<label>(5)</label></formula><p>also becomes a random feature, which is used to make the feature value real. We use this form for the experiments. This feature function is uniformly bounded, so it fulfills the condition for Hoeffding's inequality. The framework of Eq. (4) is simple and versatile, but because the feature is random, the feature tends to be verbose. To solve this problem, Hamid et al. <ref type="bibr" target="#b8">[9]</ref> proposed a method that oversamples ω and projects it in a lower-dimensional space, where the projection matrix is also randomly sampled. Yang et al. <ref type="bibr" target="#b26">[27]</ref> proposed a method to use quasi-Monte Carlo instead of i.i.d. random variables. To decrease the complexity, Le et al. <ref type="bibr" target="#b12">[13]</ref> proposed a method to approximate a feature function with complexity O(D log d).</p><p>As an application for data mining, Lopez-Paz et al. <ref type="bibr" target="#b14">[15]</ref> combined a random feature with PCA and CCA and showed that they approximate kernel PCA and kernel CCA. Lu et al. <ref type="bibr" target="#b15">[16]</ref> reported performance comparable with deep learning by combining multiple kernel learning and the composition of kernels. Dai et al. <ref type="bibr" target="#b6">[7]</ref> and Xie et al. <ref type="bibr" target="#b25">[26]</ref> proposed a method that combined a random feature with stochastic gradient descent to construct an online learning method.</p><p>Yang et al. <ref type="bibr" target="#b27">[28]</ref> proposed the random Laplace feature for the kernel k(x, y) = φ(x+y) on a semi-group (R m &gt; 0, +) and applied it to kernels on histogram data, such as bag of visual words.</p><p>However, Rahimi and Recht <ref type="bibr" target="#b20">[21]</ref> reported that the generalization performance using a random feature is O(N −1/2 + D −1/2 ). Thus, we need to sample O(N ) random features to gain sufficient generalization performance, so the complexity does not decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Approximation method</head><p>The Nyström method uses information from input data as the feature function and provides good generalization performance, but requires post-processing of the feature. The random features method approximates the kernel function and does not require post-processing of the feature. The information required to obtain the feature function p(ω), e iωx requires only the kernel function, and hence, it provides lower generalization performance. In this section, we propose a method that approximates the kernel function using information from input data to overcome the limitations of both methods.</p><p>First, from Mercer's theorem <ref type="bibr" target="#b17">[18]</ref>, we can represent the kernel k on domain X with finite measure µ as</p><formula xml:id="formula_6">k(x, y) = ∞ ∑ i=0 λ i ψ i (x)ψ * i (y),<label>(6)</label></formula><p>using eigenvalues λ i and the normalized eigenfunctions ψ i of the positive definite operator T k on L 2 (X) such that</p><formula xml:id="formula_7">(T k f )(·) = ∫ X k(·, x)f (x)dµ(x).<label>(7)</label></formula><p>We can regard the Nyström method as approximating this distribution µ using the histogram of randomly sampled input data. Additionally, using a shift-invariant kernel and a Lebesgue measure, the feature corresponds to a random Fourier feature. Because the Lebesgue measure is not finite, the decomposition is an integral instead of a discrete sum; therefore we need to randomly sample the feature function.</p><p>In this paper, we propose an intermediate approach that approximates the input distribution µ using a distribution for which its eigenfunction decomposition can be solved, and use the eigenfunctions as feature functions. The algorithm is as follows:</p><p>1. Estimate the parameter of some distribution p(x; θ) using training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Solve the eigenfunction decomposition</head><formula xml:id="formula_8">(T k f )(·) = ∫ X k(·, x)f (x)p(x; θ)dx using the estimated distribu- tion p(x; θ).</formula><p>3. Use λ 1/2 i ψ i corresponding to the D largest eigenvalues as feature functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis of the approximation error</head><p>In this section, we evaluate the expectation and highprobability bound for the spectral norm of the approximation error corresponding to the Gram matrix, which is important for the efficiency of the kernel approximation method. We denote the Gram matrix using N data {x 1 , x 2 , ..., x N } by K true , the Gram matrix using the proposed approximation method by K app , and assume that the kernel function is upper bounded by some κ such that k(x, x) ≤ κ for ∀x ∈ X. The following holds when we use the D-dimensional feature:</p><p>Theorem 3.1. Given the true probability density p true (x) and the approximated density as p app , then</p><formula xml:id="formula_9">E xi∼ptrue [∥K true − K app ∥ 2 ] ≤ N ( ∞ ∑ n=D λ n + κ ∫ X |p true (x) − p app (x)|dx ) ,<label>(8)</label></formula><p>holds. Additionally, for a probability larger than 1 − δ,</p><formula xml:id="formula_10">∥K true − K app ∥ 2 ≤ N ( ∞ ∑ n=D λ n + κ ∫ X |p true (x) − p app (x)|dx ) + √ N κ 2 2 log 1 δ ,<label>(9)</label></formula><p>holds.</p><p>Proof. It holds that K diff = K true − K app is also a Gram matrix using kernel k(x, y) = ∑ ∞ i=D λ i ψ i (x)ψ * i (y), so K diff is a symmetric positive semidefinite matrix. Note that this does not hold for a random Fourier feature, which uses an integral instead of discrete sum and does not use its eigenvalues directly. Hence,</p><formula xml:id="formula_11">∥K true − K app ∥ 2 = λ max (∥K diff ∥) (10) ≤ trace∥K diff ∥ = N ∑ i=1 k diff (x i , x i ), holds. Hence, E xi∼ptrue [∥K true − K app ∥ 2 ] ≤ N E x∼ptrue [k diff (x, x)]. Moreover, E x∼ptrue [k diff (x, x)] = ∫ X (p true (x) − p app (x))k diff (x, x)dx + E x∼papp [k diff (x, x)],<label>(11)</label></formula><p>The former is bounded by</p><formula xml:id="formula_12">∫ X |(p true (x) − p app (x))||k diff (x, x)|dx ≤ κ ∫ X |(p true (x) − p app (x)</formula><p>)|dx, and using the property of eigenfunction decomposition,</p><formula xml:id="formula_13">E x∼papp [ψ i (x)ψ * i (y)] = 1,<label>(12)</label></formula><p>the latter becomes ∑ ∞ n=D λ n . Thus, the inequality for the expectation Eq. (8) holds.</p><p>Because 0 ≤ k diff (x, x) ≤ κ, applying Hoeffding's inequality to Eq. (11), we obtain</p><formula xml:id="formula_14">P ( N ∑ i=1 k diff (x i , x i )−NE x∼ptrue [k diffx ]≥N t) ≤ exp ( − 2N t 2 κ 2</formula><p>) .</p><p>(13) Thus, a high probability bound Eq. (9) is obtained.</p><p>From the discussion in the work by Yang et al. <ref type="bibr" target="#b28">[29]</ref>, we require that ∥K true − K app ∥ 2 = O(N 1/2 ) holds for good generalization performance; that is, we require ∑ ∞ n=D λ n ,</p><formula xml:id="formula_15">∫ X |p true (x) − p app (x)|dx to be O(N −1/2 ) for sufficient performance.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gaussian case</head><p>As an example of an analytic solution for eigenfunction decomposition, we consider the Gaussian kernel and a Gaussian distribution as an approximate distribution. If</p><formula xml:id="formula_16">the dimension d = 1, setting k(x, y) = exp(−b(x − y) 2 ), p(x) = N (0, 1 4a )</formula><p>, and using c = √ a 2 + 2ab, A = a + b + c, and B = b/A, the eigensystem is as presented by Zhu et al. <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_17">λ n = √ 2a A B n<label>(14)</label></formula><formula xml:id="formula_18">ψ n (x) = exp(−(c − a)x 2 )H n ( √ 2cx),<label>(15)</label></formula><p>where H n denotes a Hermite polynomial of integer order n and is defined as H n (x) = (−1) n exp(x 2 ) d n dx n exp(−x 2 ). The feature function is localized and better reflects the properties of the Gaussian kernel, for which the similarity diminishes if the data are distant, than a random Fourier feature, which does not attenuate. Additionally, as n increases higher resolutional information can be obtained. There are studies that use this solution for kernel learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>, but to the best of our knowledge, the present research is the first to learn the distribution from data and apply it to unsupervised feature learning.</p><p>When the input dimension d is larger than 1 and the covariance matrix is diagonal, the eigensystem is a product of the above Hermite solution. Even if the covariance is non-diagonal, the solution reduces to the case in which covariance is diagonal by rotating the axis. To evaluate the approximation error, we denote the feature dimension by D and simplify the calculation by assuming a is the same for each dimension. This bounds the general case. Thus, </p><formula xml:id="formula_19">∑ ∞ n=D λ n = ( 2a A ) d/2 (( 1 1−B ) d − ( 1−B D/d 1−B ) d ) ≃ ( 2a A ) d/2 ( 1 1−B ) d dB D/d = dB D/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Gaussian mixture case</head><p>To approximate a more complex distribution, we consider a Gaussian mixture. The analytic solution using this Gaussian mixture is not known, so we consider approximating it using the result for a Gaussian distribution. We denote the number of components by K and set p(x) = ∑ K k=1 γ k N (µ k , Σ k ). Let (λ k n , ψ k n ) be the eigensystem for N (µ k , Σ k ). Because the kernel can be decomposed as</p><formula xml:id="formula_20">k(x, y) = K ∑ k=1 ω k k(x, y) = K ∑ k=1 ∞ ∑ n=0 ω k λ k n ψ k n (x)ψ k * n (y),<label>(16)</label></formula><p>we consider using (ω k λ k n ) 1/2 ψ k n (x) for larger ω k λ k n as feature functions.</p><p>Next we analyze the performance of this method. As the feature function is not the true eigenfunction, the above discussion does not hold in its current form. However, we can see that K diff is a symmetric positive semidefinite matrix, and we have only to bound E x∼p [k diff (x, x)]. To simplify this, we assume that ω k = 1 K and a, b are the same for each distribution and dimension, and each µ k is well separated such that E x∼N (µ k ,Σ k ) [ψ k ′ (x)ψ k * (y)] &lt; R for some R. In this case, using the result from the previous section,</p><formula xml:id="formula_21">E x∼p [k diff (x, x)] &lt; (1+(k −1)R)dB D dk</formula><p>is obtained. Thus, we infer that this method has an exponential gain in performance with D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Relation to kernel PCA</head><p>When we apply kernel methods, we often use PCA in the projected high-dimensional space to obtain uncorrelated useful features. The proposed methods, which use eigenfunctions, are automatically projected in the feature spaces, so correlations between features are small. Thus, it is expected that our methods demonstrate a similar effect to PCA.</p><p>When we use the proposed method with Gaussian distribution, we rotate the input space so that each input element is uncorrelated. Then, the axis with a large variance has large B, so even high-order eigenfunctions with a high resolution are used. Conversely, the axis with a small variance makes a small contribution to the feature vector. In particular, the axis on which only the 0-th order eigenfunction is used only contributes to the norm of the feature, thus we can ignore it when, for example, the features are normalized. Thus, we can say that the proposed method also applies dimensionality reduction in the input space. When the input space is d dimensional, the number of substantial dimensions is d ′ , and we extract the D dimensional feature, the complexity of the proposed method is O(dd ′ ) for rotation plus an O(d ′ D) calculation of Hermite polynomials. We replace d ′ with d when we use the full input vector. In all cases, it is much smaller than the Nyström method, which requires an O(D) calculation of kernel values between d dimension vectors plus an O(D 2 ) whitening step when D is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Application to unsupervised feature learning</head><p>We can combine the proposed method with CKN <ref type="bibr" target="#b16">[17]</ref> architecture for unsupervised feature learning. The CKN hierarchically defines the kernel between image patches as the summation of kernels between 2-d positions of points in the patches multiplied by the kernels between feature vectors of points. The kernel value between patches Ω, Ω ′ can be represented as follows:</p><formula xml:id="formula_22">K(Ω, Ω ′ ) = ∑ z∈Ω ∑ z ′ ∈Ω ′ ∥φ(z)∥∥φ ′ (z ′ )∥e − 1 2β 2 ∥δz∥ 2 e − 1 2σ 2 ∥δφ∥ 2 ,<label>(17)</label></formula><p>where z and z ′ are positions of the points, φ(z) and φ ′ (z ′ ) denote feature vectors of the points, and δz and δφ denote z −z ′ and φ(z)−φ ′ (z ′ ) respectively. When we approximate the kernel between positions as ξ pos (z) T ξ pos (z ′ ) and the kernel between feature vectors as ξ feat (φ(z)) T ξ feat (φ ′ (z ′ )), the convolutional kernel is approximated as the linear inner product of</p><formula xml:id="formula_23">∑ z∈Ω ∥φ(z)∥ξ pos (z) ⊗ ξ feat (φ(z)),<label>(18)</label></formula><p>where ⊗ denotes the Kronecker product. CKN hierarchically applies this mapping and uses the feature vector of the final layer for recognition. The original CKN uses the approximated feature for the kernel between positions as ξ pos (z) = e , and then learns η d , w d so that the reconstruction error of kernel values</p><formula xml:id="formula_24">n ∑ i=1 ( e − 1 2σ 2 ∥δφ∥ 2 − D ∑ d=1 η d e 1 σ 2 ∥φ(zi)−w d ∥ 2 e 1 σ 2 ∥φ(z ′ i )−w d ∥ 2 ) 2 ,<label>(19)</label></formula><formula xml:id="formula_25">is minimized, where (z i , z ′ i ) n i=1</formula><p>are patch pairs sampled from training data and D is the dimension of the feature.</p><p>Instead of learning feature with a gradient descent, we can use other kernel approximation methods. We propose using eigenfunctions with distribution learned from z i , z ′ i as an approximation function. The proposed method does not experience a long optimization time and local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To test the efficiency of our methods, we compared the approximation error of the Gram matrices, the classification accuracy, and performance for unsupervised feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Approximation error of the Gram matrices</head><p>First, we evaluated the approximation performance using synthesized data. We set the dimensionality of the input data to d = 10, number of samples to N = 5000, and kernel parameter to b = 1 2d , and compared the Nyström method (Nyström), random Fourier feature (Random), and proposed methods (Proposed) using data sampled from the Gaussian distribution with mean equal to 0 and a covariance identity matrix, from the Laplace distribution with location parameter equal to 0 and scale parameter equal to 1 as a super-Gaussian distribution, and from a uniform distribution from [-1,1] as a sub-Gaussian distribution. For each method, we evaluated the normalized spectral norm of the error matrix ∥Ktrue−Kapp∥2 ∥Ktrue∥2</p><p>. We set the feature dimension D = 40, 160, 640, 2560, and the number of mixture components to <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">64</ref>. Note that if the number of mixture components is 1, the situation is equivalent to the Gaussian case. To estimate the parameter of the data distribution, we used another set of N data sampled from the same distribution. For preprocessing, we rotated the data so that the estimated covariance was diagonal and used the diagonal Gaussian mixture. This rotation did not change the kernel value. We performed the experiment 10 times for each setting and calculated the mean value. <ref type="figure">Figure 1</ref> shows the results. The number in the "Proposed" label indicates the number of mixture components. The figure shows that for each distribution, the proposed method that assumed a Gaussian distribution yielded the best approximation performance if the dimension was low. Even if the dimension was high, the proposed method yielded a performance comparable with the  Nyström method. Better performance for low feature dimensions occurred because the rough estimation of the proposed method approximated the true distribution better than the estimation using a small sample histogram from the Nyström method. The random Fourier feature demonstrated similar performance for each distribution, which agrees with the fact that random features method does not use distribution information. By contrast, when we assumed a Gaussian mixture, the performance was lower and the performance gain was also smaller than the case that assumed a Gaussian distribution in each case. The uncertainty associated with the parameter estimation and the decrease of decay speed of eigenvalues influenced the performance more than the approximation accuracy of the true distribution. We then compared the performance of the proposed kernel approximation method assuming Gaussian distribution using the ILSVRC2015 classification dataset. The dataset contained approximately 1,200,000 images and we used the output of the global average pooling layer of GoogLeNet as input features, which are 1,024 dimensional per image. We used 100,000 samples for model inference and evaluated the normalized spectral norm of the error matrix ∥Ktrue−Kapp∥2 ∥Ktrue∥2 using 5,000 randomly chosen samples. We plotted the mean of five trials. We set the feature dimension D = 40, 160, 640, 2560, 10240 <ref type="figure">Figure 2</ref> shows that the proposed approximation method demonstrates better performance even for a real image dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification Accuracy</head><p>We compared regression performance and classification accuracy using real data. We used the data from the LIB-SVM site 1 . We scaled each element of the input data to [0,1] for the classification task and [-1,1] for the regression task. <ref type="table">Table 1</ref> shows the statistics for the datasets. We set the kernel parameter b = 1 2d and used LIBLIN-EAR 2 with C = 100 to compare the classification accuracy of the test data for classification tasks and ridge regression min w ∥Ψ t w − t∥ 2 2 + λ∥w∥ 2 2 with λ = 0.01 to compare the mean squared error of the test data 1 n ∑ n i=1 ∥t i − w t ψ(x i )∥ 2 2 for regression tasks. We set the feature dimension D = 40, 160, 640, 2560 for CPUSMALL, CA-DATA, ADULT and IJCNN1, and 40, 160, 640 for the larger datasets YEARMSD and COVTYPE. We set the number of mixture components to 1, 4, 16. For parameter estimation, we sampled 1000 data for CPUSMALL, CADATA, and 10000 data for ADULT, IJCNN1, YEARMSD and COV-TYPE. For each setting, we conducted 10 experiments and calculated the mean. <ref type="figure" target="#fig_3">Figure 3</ref> and <ref type="figure" target="#fig_4">Figure 4</ref> show the results. The result for Nyström is overlapped by that for Proposed1 in CPUS-MALL. We omitted the results for Proposed4 and Pro-posed16 in YEARMSD because, in some cases, they had a mean squared error that was too large. The figures show that the proposed method assuming a Gaussian distribution demonstrated better performance than the random Fourier feature, especially when the dimension was small. Additionally, they demonstrated comparable performance with the Nyström method for each dimension. Because the Nyström method requires O(D 2 ) post-processing for each feature, our method is more efficient considering the computation complexity. Generally, the proposed method as-  suming a Gaussian mixture demonstrated poorer performance than the other methods. However, the differences between each performance were small when the dimension was high, and Proposed16 demonstrated the best performance in ADULT. The proposed methods assuming the mixture model work well if the feature dimension is not small and the model fits the data distribution. Generally, when assuming a Gaussian distribution, the data distribution was sufficiently approximated and our method demonstrated comparable performance with the Nystöm method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Unsupervised feature learning</head><p>Next, we used the random features method, Nyström method, and proposed method with Gaussian distribution, which demonstrated good performance in previous experiments as kernel approximation methods for CKN architecture, and compared the accuracy with the original CKN.</p><p>We used MNIST <ref type="bibr" target="#b13">[14]</ref>, CIFAR-10 <ref type="bibr" target="#b9">[10]</ref>, CIFAR-100 <ref type="bibr" target="#b9">[10]</ref>, and SVHN <ref type="bibr" target="#b18">[19]</ref> as datasets and adopted similar network architectures to those of Mairal et al. <ref type="bibr" target="#b16">[17]</ref>. We denote the detail of the architectures in the Appendix. We used 300,000 patch pairs for feature learning, LIBLINIEAR as a linear classifier and determined the regularization parameter using 5-fold crossvalidation from 2 i ,i = −15, ..., 15.</p><p>We show the results in <ref type="table">Table 2</ref>. In most cases, the proposed method demonstrated better performance than the original CKN. This is because the proposed method did not experience local minima of the optimization and could use the input information more efficiently. The proposed method did not require a time-consuming optimization phase, therefore it was a good choice to adopt the proposed kernel approximation method for CKN architecture. Additionally, the proposed method demonstrated better performance than the random features method and comparable or better performance than the Nyström method in most settings, which demonstrated a similar tendency to that of previous experiments. This indicates the effectiveness of hierarchically approximating the kernel, and the proposed method is reasonable and effective.</p><p>Additionally, we showed the covariance of the rescaled first 400-dimensional feature in the final layer learned from CIFAR-10 with setting 1 in <ref type="figure" target="#fig_5">Figure 5</ref>. The figure shows that while CKN had relatively large non-diagonal covariance, the proposed method demonstrated uniformly small non-diagonal covariance, which agrees with the argument that the proposed method demonstrated a similar effect to PCA.</p><p>Additionally, we varied the number of feature maps in the final layer to 200, 400, 600, 800 and evaluated the performance using CIFAR-10 with setting 1. We show the result in <ref type="figure" target="#fig_6">Figure 6</ref>. As the dimension decreased, the method using random features demonstrated poorer performance  than the proposed method and Nyström method, which illustrates the importance of using input information for approximation. Conversely, these three methods demonstrated similar performance when the dimension was 200. This suggests that when the dimension was very small, the input information was not sufficient and we needed to include discriminative information in learning. In any case, the proposed method demonstrated much better performance than the original CKN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed an approximation method that first inferred the distribution of input data and then used the eigenfunctions of the operator using the kernel function and the approximated distribution as feature functions. The proposed method used information from the input data for the feature function and therefore exhibited better performance than the random features method, which only used information from the kernel. Additionally, because the proposed method directly approximated the kernel function instead of the Gram matrix, no post-processing of the feature was required. The approximation error for the proposed method can be bounded using the divergence of the true and approximated distribution and the eigenvalue of the integral operator using the kernel. We showed how to calculate the feature function for a Gaussian kernel and the approximation distribution was a Gaussian distribution or Gaussian mixture. Experiments using synthesized and real data demonstrated that our proposed method yielded a performance that was better than the random features method while comparable with the Nyström method.</p><p>There are two alternatives to expand this research. The first is to apply our method to another kernel. In the Gaussian case, we can obtain an analytic solution, but generally, the solution is not known. However, if the kernel and distribution can be decomposed into a product of functions for each dimension, we only need to consider a 1-dimensional case. Additionally, if the input dimension is high, the degree of the eigenfunction does not need to be high; hence, we can approximate the eigenfunction using, for example, series expansions. The second alternative is to approximate more accurately the eigenfunction using a mixture distribution. In this paper, we assumed that each distribution was sufficiently distant and used eigenfunctions for each distribution. However, we need to correct the original eigenfunction, for example, by adding another eigenfunction for a better approximation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2 . 1 .</head><label>21</label><figDesc>For kernel k on domain X, if there are functions f ω parameterized by ω and parameter distribution p(ω) that fulfill the equality</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Comparison of the approximation error for the Gram matrix on synthesized data sampled from (left) a Gaussian distribution, (center) Laplace distribution, and (right) uniform distribution. Comparison of the approximation error for the Gram matrix on GoogLeNet features extracted from the ILSVRC2015 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of the mean squared error for the datasets (left) CPUSMALL, (middle) CADATA, and (right) YEARMSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of the classification accuracy for the datasets (left) ADULT, (middle) IJCNN1, and (right) COVTYPE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Covariance of learned feature for (top left) CKN, (top right) Random, (bottom left) Nyström, and (bottom right) Proposed. The proposed method shows less non-diagonal covariance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Accuracy of CKN with fewer feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>d and we can see that the error decreases exponentially with D.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 2. Classification accuracy for MNIST, CIFAR-10, CIFAR-100, STL-10, and SVHN.</figDesc><table>Setting CKN Random Nyström Proposed 
MNIST 
1 
99.34 
99.49 
99.38 
99.36 
2 
99.28 
99.46 
99.48 
99.43 
3 
99.42 
99.45 
99.47 
99.51 
CIFAR-10 
1 
74.59 
75.93 
75.72 
76.00 
2 
79.19 
80.73 
81.52 
81.27 
3 
77.41 
77.68 
78.57 
78.29 
CIFAR-100 
1 
43.25 
43.64 
43.36 
43.66 
2 
53.37 
55.20 
54.44 
55.01 
3 
50.41 
51.02 
50.53 
51.04 
SVHN 
1 
91.80 
91.54 
91.96 
91.98 
2 
90.79 
90.75 
91.18 
91.36 
3 
85.52 
85.60 
85.88 
86.05 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/ 2 https://www.csie.ntu.edu.tw/ cjlin/liblinear/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by CREST, JST.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Local rademacher complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1497" to="1537" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object recognition with hierarchical kernel descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kernel descriptors for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On invariance in hierarchical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bouvrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-margin classification in infinite neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable kernel methods via doubly stochastic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the nyström method for approximating a gram matrix for improved kernel-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2153" to="2175" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compact random feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gittens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sampling methods for the nyström method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="981" to="1006" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kernel and nonlinear canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fyfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fastfood-approximating kernel expansions in loglinear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Randomized nonlinear component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Garakani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4000</idno>
		<title level="m">How to scale up kernel methods to be as good as deep neural nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Functions of positive and negative type, and their connection with the theory of integral equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="446" />
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fourier analysis on groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The effect of the input density distribution on kernel-based classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using the nyström method to speed up kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scale up nonlinear component analysis with doubly stochastic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.03655</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Quasimonte carlo feature maps for shift-invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Random laplace feature maps for semigroup kernels on histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nyström method vs random fourier features: A theoretical and empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gaussian regression and optimal finite dimensional linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rohwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morciniec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
