<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-Local Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tel Aviv University</orgName>
								<orgName type="institution" key="instit2">University of Haifa</orgName>
								<orgName type="institution" key="instit3">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Treibitz</surname></persName>
							<email>ttreibitz@univ.haifa.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tel Aviv University</orgName>
								<orgName type="institution" key="instit2">University of Haifa</orgName>
								<orgName type="institution" key="instit3">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
							<email>avidan@eng.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tel Aviv University</orgName>
								<orgName type="institution" key="instit2">University of Haifa</orgName>
								<orgName type="institution" key="instit3">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Non-Local Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Haze limits visibility and reduces image contrast in outdoor images. The degradation is different for every pixel and depends on the distance of the scene point from the camera. This dependency is expressed in the transmission coefficients, that control the scene attenuation and amount of haze in every pixel. Previous methods solve the single image dehazing problem using various patch-based priors. We, on the other hand, propose an algorithm based on a new, non-local prior. The algorithm relies on the assumption that colors of a haze-free image are well approximated by a few hundred distinct colors, that form tight clusters in RGB space. Our key observation is that pixels in a given cluster are often non-local, i.e., they are spread over the entire image plane and are located at different distances from the camera. In the presence of haze these varying distances translate to different transmission coefficients. Therefore, each color cluster in the clear image becomes a line in RGB space, that we term a haze-line. Using these haze-lines, our algorithm recovers both the distance map and the haze-free image. The algorithm is linear in the size of the image, deterministic and requires no training. It performs well on a wide variety of images and is competitive with other stateof-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Outdoor images often suffer from low contrast and limited visibility due to haze, small particles in the air that scatter the light in the atmosphere. Haze is independent of scene radiance and has two effects on the acquired image: it attenuates the signal of the viewed scene, and it introduces an additive component to the image, termed the ambient light, or airlight (the color of a scene point at infinity). The image degradation caused by haze increases with the distance from the camera, since the scene radiance decreases and the airlight magnitude increases. Thus, hazy images can be modeled as a per-pixel convex combination of a haze-free image and the global airlight.</p><p>Our goal is to recover the RGB values of the haze-free image and the transmission (the coefficient of the convex combination) for each pixel. This is an ill-posed problem that has an under-determined system of three equations and at least four unknowns per pixel, with inherent ambiguity between haze and object radiance. To handle this ambiguity, some previous works used additional information such as more images, while others assumed an image prior to solve the problem from a single image (see Sec. 2).</p><p>Here, we use the observation that colors of a haze-free image can be well approximated by a few hundred distinct colors <ref type="bibr" target="#b13">[14]</ref>. This implies that pixels in a hazy image can be modeled by lines in RGB space that pass through the airlight coordinates. We term these lines haze-lines to stress this characteristic <ref type="figure" target="#fig_0">(Figs. 1, 2</ref>). Pixels along a haze-line come from objects that have similar radiance colors, located over the entire image plane. These objects can be and indeed are located at different distances from the camera. Since their acquired color can be modeled by a convex combination of the radiance color and the airlight color, such objects will span a line in RGB space. We use these lines to estimate the per-pixel transmission based on the pixel's position along the line it belongs to.</p><p>As opposed to recent state-of-the-art methods our Haze-lines (ours) vs. Color lines <ref type="bibr" target="#b2">[3]</ref>. (a) An input hazy image. Six pixels belonging to similar objects in different distances are marked by round color markers, and a local patch is marked by an orange frame. (b) The color coordinates of the pixels depicted in (a) with round markers are shown in RGB color space, with a corresponding color coding. They are distributed over a haze-line, as identified by our method. The line passes through the airlight, marked in black. The other end of the line is the haze-free color of these pixels, not the origin. (c) As opposed to our method, the pixels in the local patch marked by an orange square are distributed along a color line <ref type="bibr" target="#b2">[3]</ref> that intersects the vector from the origin to the airlight.</p><p>method is global and does not divide the image to patches. Patch-based methods take great care to avoid artifacts by either using multiple patch sizes <ref type="bibr" target="#b18">[19]</ref> or taking into consideration patch overlap and regularization using connections between distant pixels <ref type="bibr" target="#b2">[3]</ref>. In our case, the pixels that form the haze-lines are spread across the entire image and therefore capture a global phenomena that is not limited to small image patches. Thus, our prior is more robust and significantly more efficient in run-time.</p><p>We propose an efficient algorithm that is linear in the size of the image. We automatically detect haze-lines and use them to dehaze the image. We also conduct extensive experiments to validate our assumptions and report quantitative and qualitative results on many outdoor images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>A variety of approaches have been proposed to solve image dehazing. Several methods require additional information to dehaze images, such as multiple images taken under different weather conditions <ref type="bibr" target="#b10">[11]</ref>, or two images with different polarization states <ref type="bibr" target="#b15">[16]</ref>. Alternatively, the scene geometry is used <ref type="bibr" target="#b5">[6]</ref>. Single image dehazing methods assume only the input image is available and rely on image priors.</p><p>Haze reduces the contrast in the image, and various methods rely on this observation for restoration. Tan <ref type="bibr" target="#b17">[18]</ref> maximizes the contrast per patch, while maintaining a global coherent image. In <ref type="bibr" target="#b14">[15]</ref> the amount of haze is estimated from the difference between the RGB channels, which decreases as haze increases. This assumption is problematic in gray areas. In <ref type="bibr" target="#b20">[21]</ref> the haze is estimated based on the observation that hazy regions are characterized by high brightness and low saturation.</p><p>Some methods use a prior on the depth of the image. A smoothness prior on the airlight is used in <ref type="bibr" target="#b19">[20]</ref>, assuming it is smooth except for depth discontinuities. Nishino et al. <ref type="bibr" target="#b11">[12]</ref> assume the scene albedo and depth are statisti-cally independent and jointly estimate them using priors on both. The prior on the albedo assumes the distribution of gradients in images of natural scenes exhibits a heavy-tail distribution, and it is approximated as a generalized normal distribution. The depth prior is scene-dependent, and is chosen manually, either as piece-wise constant for urban scenes or smoothly varying for non-urban landscapes.</p><p>Several methods assume that transmission and radiance are piece-wise constant, and employ a prior on a patch basis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. The dark channel prior <ref type="bibr" target="#b4">[5]</ref> assumes that within small image patches there will be at least one pixel with a dark color channel and use this minimal value as an estimate of the present haze. This prior works very well, except in bright areas of the scene where the prior does not hold.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, color ellipsoids are fitted in RGB space per-patch. These ellipsoids are used to provide a unified approach for previous single image dehazing methods, and a new method is proposed to estimate the transmission in each ellipsoid.</p><p>In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>, color lines are fitted in RGB space per-patch, looking for small patches with a constant transmission. This prior is based on the observation <ref type="bibr" target="#b12">[13]</ref> that pixels in a hazefree image form color lines in RGB space. These lines pass through the origin and stem from shading variations within objects. As shown in <ref type="bibr" target="#b2">[3]</ref>, the color lines in hazy images do not pass through the origin anymore, due to the additive haze component. In small patches that contain pixels in a uniform distance from the camera, these lines are shifted from the origin by the airlight at that distance. By fitting a line to each such patch, the transmission in the patch is estimated using the line's shift from the origin.</p><p>The variety of priors has led to the work of <ref type="bibr" target="#b18">[19]</ref>, that examines different patch features in a learning framework.</p><p>Instead of small patches, in <ref type="bibr" target="#b6">[7]</ref> the image is segmented to regions with similar distances, and the contrast is stretched within each segment. This may create artifacts at the boundaries between segments.</p><p>While our haze-lines might seem similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, they are inherently different. The differences are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In <ref type="bibr" target="#b2">[3]</ref>, lines are defined by the pixels of small patches in the image plane assuming constant transmission, with intensity differences caused by shading, and therefore relatively small <ref type="figure" target="#fig_1">(Fig. 2c</ref>). This is a local phenomena that does not always hold and indeed, in <ref type="bibr" target="#b2">[3]</ref> care is taken to ensure only patches where the assumption holds are considered. We, on the other hand, look at lines that are formed by individual pixels that are scattered over the entire image. These pixels usually have large intensity differences that are caused by changes in transmission and not local shading effects, as demonstrated in <ref type="figure" target="#fig_1">Fig. 2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Non-Local Colors in Hazy Images</head><p>We first present the haze model and then describe how we use non-local haze-lines for image dehazing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Haze Model</head><p>The common hazy image formation model is <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_0">I(x) = t(x) · J (x) + [1 − t(x)] · A ,<label>(1)</label></formula><p>where x is the pixel coordinates, I is the observed hazy image, and J is the true radiance of the scene point imaged at x. The airlight A is a single color representing the airlight in image areas where t = 0. The scene transmission t(x) is distance-dependent:</p><formula xml:id="formula_1">t(x) = e −βd(x) ,<label>(2)</label></formula><p>where β is the attenuation coefficient of the atmosphere and d(x) is the distance of the scene at pixel x. Generally, β is wavelength dependent and therefore t is different per color channel <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>. This dependency has been assumed negligible in previous single image dehazing methods to reduce the number of unknowns and we follow this assumption. The transmission t(x) acts as a matting coefficient between the scene J and the airlight A. Thus, per-pixel x, Eq. (1) has three observations I(x) and four unknowns: J (x) and t(x), resulting in an under-determined estimation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Prior</head><p>Our method is based on the observation that the number of distinct colors in an image is orders of magnitude smaller than the number of pixels <ref type="bibr" target="#b13">[14]</ref>. This assumption has been used extensively in the past and is used for saving color images using indexed colormaps. We validate and quantify it on the Berkeley Segmentation Dataset (BSDS300). This is a diverse dataset of clear outdoor natural images and thus represents the type of scenes that might be degraded by haze. We clustered the RGB pixel values of each image using K-means to a maximum of 500 clusters, and replaced every pixel in the image with its respective cluster center. The result is an image with 500 different RGB values at  most (two orders of magnitude smaller than image size). The PSNR of the images generated with the reduced color set, compared to the original ones, were high and ranged from 36.6dB to 52.6dB. A histogram of the obtained PSNR values is shown in <ref type="figure" target="#fig_3">Fig 3,</ref> as well as the image that had the worst PSNR, before and after color quantization. The observation regarding a small number of distinct colors holds for haze-free images. In the presence of haze, object points that belong to the same color cluster end up with different acquired colors, since they are located in disparate image areas and thus have different distances from the camera. This prior suggests that pixels clustered together in a haze-free image form a line in RGB space in a hazy image. Based on Eq. (1), the two end points of the line are the original color J and the airlight A. These are the haze-lines.</p><p>This prior is demonstrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. A haze-free image is clustered using K-means to 500 clusters. The pixels belonging to four of these clusters are marked by different color markers in <ref type="figure" target="#fig_0">Fig. 1a</ref> and their RGB coordinates are plotted in <ref type="figure" target="#fig_0">Fig. 1b</ref>, demonstrating tight clusters. Note that the clusters include pixels distributed over the entire image that come from objects with different distances from the camera. A synthetic hazy image was generated from the clear image <ref type="figure" target="#fig_0">(Fig. 1c)</ref> by <ref type="bibr" target="#b2">[3]</ref>. The same pixels as in <ref type="figure" target="#fig_0">Fig. 1a</ref> are marked. However, now, colors of pixels that belonged to the same color cluster are no longer similar. This is depicted in RGB space in <ref type="figure" target="#fig_0">Fig. 1d</ref>, where the color coordinates of these pixels are distributed along a haze-line spanned by the original color and the airlight. The pixels marked by purple circles (originating from the sand patch) are located in similar distances, so their distribution along the haze-line is rather tight. However, the pixels marked by orange triangles (grassy areas) are found at different locations in the real world, so they are distributed along the haze-line. <ref type="figure" target="#fig_1">Fig. 2</ref> demonstrates the haze-lines prior on a hazy outdoor image. Six different pixels identified by our method as belonging to the same haze line are circled. All of them are on shaded tree trunks and branches, and are likely to have similar radiance J . However, their observed intensity I is quite different, as shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>, where these pixels form a haze-line in RGB space that passes through the airlight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Haze Removal</head><p>Our algorithm is composed of four essential steps: clustering the pixels into haze-lines, estimating an initial transmission map, regularization, and dehazing (see Alg. 1). Finding Haze-Lines: We estimate A using one of the previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref>. Let us define I A as:</p><formula xml:id="formula_2">I A (x) = I(x) − A ,<label>(3)</label></formula><p>i.e., the 3D RGB coordinate system is translated such that the airlight is at the origin. Following Eq. <ref type="formula" target="#formula_0">(1)</ref>,</p><formula xml:id="formula_3">I A (x) = t(x) · [J (x) − A] .<label>(4)</label></formula><p>We express I A (x) in spherical coordinates:</p><formula xml:id="formula_4">I A (x) = [r(x), θ(x), φ(x)] .<label>(5)</label></formula><p>Here r is the distance to the origin (i.e., I − A )), θ and φ are the longitude and latitude, respectively. The colors of the pixels are now represented in a spherical coordinate system around the airlight. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the histogram of the Forest image <ref type="figure" target="#fig_1">(Fig. 2a)</ref> projected onto a sphere. The color represents the number of pixels pointing at each direction. The equator (φ = 0) is marked by a bold dashed blue line, while the longitudes θ = 0, π 2 are marked by dotted blue lines. The triangulation is explained later. The color-mapping is logarithmic for illustration purposes. The histogram indicates that the pixels are highly concentrated in terms of their longitude and latitude.</p><p>Let us look at Eq. (4). For given values of J and A, scene points at different distances from the camera differ only in the value of t. In the spherical coordinate system we defined, changes in t affect only r(x) without changing either φ(x) or θ(x). In other words, pixels x and y have similar RGB values in the underlying haze-free image if their [φ, θ] are similar: Note that there is inherent ambiguity between color and haze for colors which are collinear with the airlight:</p><formula xml:id="formula_5">J (x) ≈ J (y) ⇒ {φ(x) ≈ φ(y) , θ(x) ≈ θ(y)},</formula><formula xml:id="formula_6">J 1 − A = α(J 2 − A) ⇒ J 1 = (1 − α)A + αJ 2 ,<label>(7)</label></formula><p>where α is a scale factor. In this case all single image dehazing methods will correct J 1 and J 2 to the same color. This is the only case in our method when two color clusters will be mapped to the same haze-line.</p><p>In order to determine which pixels are on the same hazeline, pixels should be grouped according to their angles [φ, θ]. A 2-D histogram binning of θ and φ with uniform edges in the range [0, 2π] × [0, π] will not generate a uniform sampling of a sphere. Instead, the samples will be denser near the poles <ref type="bibr" target="#b7">[8]</ref>, since the distance on the sphere is relative to sin(θ). Therefore, we sample the unit sphere uniformly, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, where each vertex is a sample point. Each vertex corresponds to a haze-line. For clarity of display, the number of samples in <ref type="figure" target="#fig_4">Fig. 4</ref> is smaller than the actual number we use. We group the pixels based on their [φ(x), θ(x)] values, according to the closest sample point on the surface. This can be implemented efficiently by building a KD-Tree from the pre-defined tessellation and querying the tree for each pixel. This is much faster than running a clustering algorithm such as k-means.</p><p>Based on the analysis described in section 3.2, several hundreds of haze-lines represent an image with a good approximation. <ref type="figure" target="#fig_6">Fig. 5a</ref> depicts the layout of two different haze-lines in the image plane for the Forest image.</p><p>Estimating Initial Transmission: For a given haze-line defined by J and A, r(x) depends on object distance:</p><formula xml:id="formula_7">r(x) = t(x) J (x) − A , 0 ≤ t(x) ≤ 1 .<label>(8)</label></formula><p>Thus, t = 1 corresponds to the largest radial coordinate:</p><formula xml:id="formula_8">r max def = J − A .<label>(9)</label></formula><p>Combining Eqs. <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b8">9)</ref> results in an expression for the transmission based on radii in the haze-line:</p><formula xml:id="formula_9">t(x) = r(x)/r max .<label>(10)</label></formula><p>Now, the question is how to find an estimater max for the maximal radius? If a haze-line H contains a haze-free pixel, thenr max is the maximal radius of that haze-line:</p><formula xml:id="formula_10">r max (x) = max x∈H {r(x)} ,<label>(11)</label></formula><p>where the estimation is done per haze-line H. <ref type="figure" target="#fig_6">Fig. 5b</ref> displays the radii histograms of the two clusters shown in <ref type="figure" target="#fig_6">Fig. 5a</ref>. We assume that the farthest pixel from the airlight is haze free, and that such a pixel exists for every haze-line. This assumption does not hold for all of the haze-lines in an image, however the regularization step partially compensates for it. Combining Eqs. (10,11) results in a per-pixel estimation of the transmission:</p><formula xml:id="formula_11">t(x) = r(x)/r max (x) .<label>(12)</label></formula><p>Regularization: Since the radiance J is positive (i.e., J ≥ 0 ), Eq. (1) gives a lower bound on the transmission:</p><formula xml:id="formula_12">t LB (x) = 1 − min c∈{R,G,B} {I c (x)/A c } .<label>(13)</label></formula><p>In <ref type="bibr" target="#b4">[5]</ref>, the transmission estimate is based on an eroded version of t LB . We impose this bound on the estimated transmission, per-pixel:</p><formula xml:id="formula_13">t LB (x) = max{t(x), t LB (x)} .<label>(14)</label></formula><p>The estimation in Eq. <ref type="formula" target="#formula_0">(12)</ref> is performed per-pixel, without imposing spatial coherency. This estimation can be inaccurate if a small amount of pixels were mapped to a particular haze-line, or in very hazy areas, where r(x) is very small and noise can affect the angles significantly. The transmission map should be smooth, except for depth discontinuities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>. We seek a transmission map t(x) that is similar tot LB (x) and is smooth when the input image is smooth. Mathematically, we minimize the following function w.r.t.t(x):</p><formula xml:id="formula_14">x t (x) −t LB (x) 2 σ 2 (x) + λ x y∈Nx t (x) −t(y) 2 I(x) − I(y) 2 ,<label>(15)</label></formula><p>where λ is a parameter that controls trade-off between the data and the smoothness terms, N x denotes the four nearest neighbors of x in the image plane and σ(x) is the standard deviation oft LB , which is calculated per haze-line. σ(x) plays a significant role since it allows us to apply our estimate only to pixels where the assumptions hold. When the variance is high, the initial estimation is less reliable. σ(x) increases as the number of pixels in a haze line decreases. When the radii distribution in a given haze-line is small, our haze-line assumption does not hold since we do not observe pixels with different amounts of haze. In such cases, σ(x) increases as well.</p><p>Dehazing: Oncet(x) is calculated as the minimum of Eq. (15), the dehazed image is calculated using Eq. (1):</p><formula xml:id="formula_15">J (x) = I(x) − 1 −t(x) A t (x) .<label>(16)</label></formula><p>The method is summarized in Alg. 1 and demonstrated in <ref type="figure" target="#fig_8">Fig. 6. Fig. 6a</ref> shows the input hazy image. The final, dehazed image is shown in <ref type="figure" target="#fig_8">Fig. 6b. Fig. 6c</ref> shows the distance in RGB space of every pixel in the hazy image to the airlight. Note that this distance decreases as haze increases. <ref type="figure" target="#fig_8">Fig. 6d</ref> shows the maximum radiir max (x) per haze-line. Observe that <ref type="figure" target="#fig_8">Fig. 6d</ref> is much brighter than <ref type="figure" target="#fig_8">Fig. 6c</ref>. Since larger values are represented by brighter colors, this indicates that the distance to the airlight is increased. The pixels with the maximum radius in their haze-line are marked on the hazy image in <ref type="figure" target="#fig_8">Fig. 6e</ref>. Note that these pixels are mostly at the foreground, where indeed there is a minimal amount of haze. We filtered out pixels that had a maximum radius in the haze line, yet had a σ &gt; 2, since the model assumptions do not hold for these haze lines. The aforementioned pixels are found in the sky, since the distance to the airlight in RGB space is very short. Therefore, clustering them according to their angles is not reliable due to noise. In the regularization step this fact is taken into consideration through the dataterm weight 1 σ 2 (x) , which is shown in <ref type="figure" target="#fig_8">Fig. 6f</ref> (warm colors depict high values). The ratio of Figs. 6c and 6d yields the initial transmissiont(x) that is shown in <ref type="figure" target="#fig_8">Fig. 6g</ref>. The trans-(a) Hazy image I(x) mission map after regularization is shown in <ref type="figure" target="#fig_8">Fig. 6h</ref>. Whilẽ t(x) contains fine details even in grass areas that are at the same distance from the camera,t(x) does not exhibit this behavior. This indicates the regularization is necessary.</p><formula xml:id="formula_16">(b) Dehazed imageĴ (x) (c) r(x) (d)rmax(x) (e) pixels {x|r(x) =rmax(x)} (f) 1 σ 2 (x) , colomapped (g) Initial trans.t(x) (h) Regularized trans.t(x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We evaluate our method on a large dataset containing both natural and synthetic images and compare our performance to state-of-the-art algorithms. We assume A is given, by using the airlight vector A calculated by <ref type="bibr" target="#b16">[17]</ref>. We use the same parameters for all of the images: in Eq. (15) we set λ = 0.1 and we scale 1/σ 2 (x) to be in the range [0, 1] in order to avoid numeric issues. In order to find the haze lines, we sample uniformly 1000 points on the unit sphere <ref type="figure" target="#fig_4">(Fig. 4</ref> shows only 500 for clarity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative results</head><p>A synthetic dataset of hazy images of natural scenes was introduced by <ref type="bibr" target="#b2">[3]</ref>, and is available online. The dataset contains eleven haze free images, synthetic distance maps   <ref type="table" target="#tab_0">Table 1</ref> summarizes the L 1 errors on non-sky pixels (same metric used in <ref type="bibr" target="#b2">[3]</ref>) of the transmission maps and the dehazed images. Our method is compared to the method of <ref type="bibr" target="#b2">[3]</ref> and an implementation of <ref type="bibr" target="#b4">[5]</ref> by <ref type="bibr" target="#b2">[3]</ref>. For five images out of this dataset, results of both clear and noisy images are provided by <ref type="bibr" target="#b2">[3]</ref> 1 .</p><p>Our method outperforms previous methods in most cases, and handles the noise well. As expected, our performance degrades when the noise variance increases. However, our method maintains its ranking, with respect to other methods, regardless of the amount of noise. This shows that our algorithm is quite robust to noise, despite being pixelbased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative results</head><p>Figs. 7 and 8 compare our results to state-of-the-art single image dehazing methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>. As previously noted by <ref type="bibr" target="#b4">[5]</ref>, the image after haze removal might look dim, since the scene radiance is usually not as bright as the airlight. For display, we perform a global linear contrast stretch on the output, clipping 0.5% of the pixel values both in the shadows and in the highlights. Pixels whose radius is maximal in their haze-line are marked in pink on the hazy input. We marked only pixels x for which σ(x) &lt; 2 and for clarity, only ones that belong to large clusters.  The method of <ref type="bibr" target="#b0">[1]</ref> leaves haze in the results, as seen in the areas circled in yellow in <ref type="figure">Fig. 7</ref>. In the result of <ref type="bibr" target="#b6">[7]</ref> there are artifacts in the boundary between segments (pointed by arrows). The method of <ref type="bibr" target="#b11">[12]</ref> tends to oversaturate (e.g., House). The methods of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref> produce excellent results in general but lack some micro-contrast when compared to <ref type="bibr" target="#b2">[3]</ref> and to ours. This is evident in the zoomed-in buildings shown in Cityscape results, where in our result and in <ref type="bibr" target="#b2">[3]</ref> the windows are sharper than in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref> (best viewed on a monitor). The result of <ref type="bibr" target="#b3">[4]</ref> was not enlarged as it has a low resolution. Results of <ref type="bibr" target="#b2">[3]</ref> are sometimes clipped, e.g., the leaves in House and in the sky in Forest.</p><p>Our assumption regarding having a haze-free pixel in each haze-line does not hold in Cityscape, as evident by several hazy pixels that set a maximum radius, e.g. the red buildings. Despite that, the transmission in those areas is estimated correctly due to the regularization that propagates the depth information spatially from the other haze-lines. <ref type="figure" target="#fig_10">Fig. 8</ref> compares both the transmission maps and the dehazed images. It shows our method is comparable to other methods, and in certain cases works better. For example, The two rows of trees are well separated in our result when compared to <ref type="bibr" target="#b4">[5]</ref>.</p><p>The main advantage of the global approach is the ability to cope well with fast variations in depth, when the details are smaller than the patch size. <ref type="figure" target="#fig_11">Fig. 9</ref> shows an enlarged  <ref type="bibr" target="#b4">[5]</ref>. (c) The result of <ref type="bibr" target="#b2">[3]</ref>. (d) Our result. Note the artifacts around the leaves and the branch at (c). This is a result of the patch-based method. While <ref type="bibr" target="#b4">[5]</ref> is also a patch-based and does not exhibit these artifacts, this method under-estimates the haze in this image, so the depth gap is not pronounced. Note the lacking details in the tree trunk of (b) compared to (c) and (d).  portion of an image, where clear artifacts are visible in the result of <ref type="bibr" target="#b2">[3]</ref>, around the leaves and at the boundary between the trunk and the background. A patch-based method is less likely to estimate the distance of such scenes accurately. The result of <ref type="bibr" target="#b4">[5]</ref> does not exhibit these artifacts in <ref type="figure" target="#fig_11">Fig. 9</ref>, since the dehazing is less effective in this image and the details are less clear (e.g, the circled trunk). This phenomena is also visible in <ref type="figure">Fig. 7</ref> in the dehazed Cityscape image of <ref type="bibr" target="#b3">[4]</ref>, where a halo between the trees in the foreground and the background is visible, and also in the train output of <ref type="bibr" target="#b2">[3]</ref> around the pole (marked by a yellow square).</p><p>Using a fixed tessellation of the unit sphere might raise a concern that fine tones will not be distinguished. <ref type="figure" target="#fig_0">Fig. 10</ref> demonstrates this is not the case. The pumpkins (a crop of <ref type="figure" target="#fig_8">Fig. 6a</ref>) are lit from above, and therefore are brighter at the top and gradually become darker towards the ground ( <ref type="figure" target="#fig_0">Fig. 10 left)</ref>. <ref type="figure" target="#fig_0">Fig. 10</ref> right depicts the cluster map -each color symbolizes a different haze-line. The gradual tone change is evident in the cluster map.</p><p>The Weather and ILlumination Database (WILD) <ref type="bibr" target="#b9">[10]</ref> contains multiple images of the same scene and the ground truth depth. <ref type="figure" target="#fig_0">Fig. 11</ref> shows on the top row a clear day image of the scene as well as the depth. Below are images taken under bad weather conditions and our results. The image at the middle row was taken in light rain and mist, and our method restores the visibility while estimating a rough depth map. The image at the bottom left was taken under heavy mist and limited visibility. There, the pixels are not correctly clustered into haze-lines. The result is an inaccurate transmission and artifacts in the restored image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Complexity analysis</head><p>Our method is linear in N , the number of pixels in the image, and therefore fast. The clustering is done using a nearest neighbor search on a KD-Tree with a fixed number of points. Estimating the radius within each cluster is linear in N . Therefore, the initial radius estimation is O(N ). Seeking the minimum of Eq. (15) requires solving a sparse linear system, which is also O(N ). Restoring the dehazed image from the transmission map is O(N ) as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We introduced a novel non-local method for single image dehazing. The method is based on the assumption that an image can be faithfully represented with just a few hundreds of distinct colors. In RGB space, this corresponds to a few hundreds tight color clusters. We showed that in a hazy image, these tight color clusters change because of haze and form lines in RGB space that pass through the airlight coordinate. We proposed an efficient algorithm to identify these haze-lines and estimate a per-pixel transmission based on them. We take into consideration the variance of our estimation in the regularization process, so only pixels that comply with the model assumptions contribute to the result. Our method may fail in scenes where the airlight is significantly brighter than the scene. In such cases, most pixels will point in the same direction and it will be difficult to detect the haze lines.</p><p>In contrast to previous methods our algorithm is pixelbased and not patch-based. This makes our algorithm faster, more robust and less prone to issues such as the choice of patch size, patch tiling, and patches with non-uniform content. Our method was tested and found to work well on many real-world images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Non-Local Image Dehazing. (a) Pixels of a haze free color image are clustered using K-means. Pixels belonging to four of the clusters are marked. Note that the pixels are non-local and are spread all over the image plane. (b) The four color clusters are depicted in RGB space. Colors of the clusters correspond to the highlighted pixels in (a). (c) Synthetic haze is added to (a). The same clustered pixels are marked, but their observed colors are affected by different amounts of haze. (d) The hazy pixels depicted in RGB color space. They are distributed along lines, termed haze-lines, passing through the airlight, marked in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>hazy image: Forest. (b) The circled pixels form a (c) The pixels in the orange square Note pixels marked in circles haze-line in RGB color space form a color-line in RGB color space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Prior validation: (a) A PSNR histogram of the quantization errors on the Berkeley Segmentation Dataset (BSDS300): The RGB values of each image were clustered using K-means to 500 clusters and replaced by the cluster center. The histogram shows the PSNRs measured on the entire dataset. (b,d) The image that had the worst PSNR, 36.64dB, before (b) and after (d) color quantization. (c) Absolute difference image to color-quantized version (the contrast was stretched for display, note that the maximal difference was 18 out of 256).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Airlight centered spherical representation. The sphere was sampled uniformly using 500 points. The color at each point [φ, θ] indicates the number of pixels x with these angles when writing IA(x) in spherical coordinates (image size 768 × 1024).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>pixels belong to the same haze-line if their [φ(x), θ(x)] values are similar. Each point on the sphere in Fig. 4 represents a haze-line, in which all the pixels have approximately the same angles [φ(x), θ(x)]. The pixels on each haze-line have similar values in the non-hazy image J with high probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Distance distribution per haze-line: (a) Pixels belonging to two different haze-lines are depicted in green and blue, respectively. (b) A histogram of r(x) within each cluster. The horizontal axis is limited to the range [0, A ], as no pixel can have a radius outside that range in this particular image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>I A to spherical coordinates to obtain [r(x), φ(x), θ(x)] 3: Cluster the pixels according to [φ(x), θ(x)].Each cluster H is a haze-line. 4: for each cluster H do5:    Estimate maximum radius: r max (x) = max x∈H {r(the dehazed image using Eq.<ref type="bibr" target="#b15">(16)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Intermediate and final results of our method: (a) An input hazy image; (b) The output image; (c) The distance r(x) of every pixel of the hazy image to the airlight; (d) the estimated radiî rmax(x) calculated according to Eq. (11); (e) The input image is shown, with the pixels x for which r(x) =rmax(x) marked by cyan circles; (f) The data term confidence in Eq. (15) colormapped (warm colors show the larger values); (g) The estimated transmission mapt(x) before the regularization; (h) The final transmission mapt(x) after regularization. (g) and (h) are colormapped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>simulated haze images. An identicallydistributed zero-mean Gaussian noise with three different noise level: σ n = 0.01, 0.025, 0.05 was added to these images (with image intensity scaled to [0, 1]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Comparison of transmission maps and dehazed images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>(a) Hazy input. (b) The result of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .</head><label>10</label><figDesc>Color Clustering: Left: a crop of Fig. 6a). Right: a cluster map -each color represents a different haze-line. The gradual tone change of the pumpkins is preserved in the clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Top row: clear day image and ground-truth depth of the scene. Two bottom rows, from left to right: an image taken in bad weather conditions, our result and the transmission map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>T.T. was funded by The Leona M. and Harry B. Helmsley Charitable Trust and The Maurice Hatter Foundation. This research was supported in part by ISF grant 1917/15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Comparison of L1 errors over synthetic hazy images with various amount of noise. The noise standard deviation is given and the images are scaled to the range [0, 1]. The table compares the L1 errors of the estimated transmission maps (left value) and the dehazed images (right value).</figDesc><table>σ 
[5] 
[3] 
ours 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Comparison on natural images: [Left] Input with pixels that set the maximum radius in their haze-line circled in pink.[Right] Our result. Middle columns display results by several methods, since each paper reports results on a different set of images.</figDesc><table>hazy image: House 

He et al. [5] 
Gibson and Nguyen [4] Nishino et al. [12] 
Fattal [3] 
Ours 

hazy image: Train 
He et al. [5] 
Luzón-González et al. [7] Ancuti and Ancuti [1] 
Fattal [3] 
Ours 

hazy image: Cityscape 
He et al. [5] 
Gibson and Nguyen [4] Tang et al. [19] 
Fattal [3] 
Ours 

Figure 7. Top to bottom: 
hazy image: Forest 
He et al. [5] results 
Fattal [3] results 
our results 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A complete summary of results and more is available on the project's website.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single image dehazing by multiscale fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3271" to="3282" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of single image defogging methods using a color ellipsoid framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep photo: Model-based photograph enhancement and viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recovering of weather degraded images based on RGB response ratio constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luzón-González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Nieves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Choosing a point from the surface of a sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marsaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1972</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Vision through the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E K</forename><surname>Middleton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
			<publisher>University of Toronto Press</publisher>
			<pubPlace>Toronto</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">All the Images of an Outdoor Scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002-05" />
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="page" from="148" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chromatic framework for vision in bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian defogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="278" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Color lines: Image specific color representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Color quantization of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2677" to="2690" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast single image dehazing using characteristics of RGB channel of foggy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1793" to="1799" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Instant dehazing of images using polarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic recovery of the atmospheric light in hazy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sulami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Geltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCP</title>
		<meeting>IEEE ICCP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Investigating haze-relevant features in a learning framework for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast visibility restoration from a single color or gray level image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="2201" to="2208" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single image dehazing using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
