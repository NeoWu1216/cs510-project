<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting Motivations of Actions by Leveraging Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
							<email>vondrick@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology †University of Maryland</orgName>
								<address>
									<settlement>Baltimore County</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Oktay</surname></persName>
							<email>denizokt@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology †University of Maryland</orgName>
								<address>
									<settlement>Baltimore County</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology †University of Maryland</orgName>
								<address>
									<settlement>Baltimore County</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
							<email>torralba@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology †University of Maryland</orgName>
								<address>
									<settlement>Baltimore County</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting Motivations of Actions by Leveraging Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding human actions is a key problem in computer vision. However, recognizing actions is only the first step of understanding what a person is doing. In this paper, we introduce the problem of predicting why a person has performed an action in images. This problem has many applications in human activity understanding, such as anticipating or explaining an action. To study this problem, we introduce a new dataset of people performing actions annotated with likely motivations. However, the information in an image alone may not be sufficient to automatically solve this task. Since humans can rely on their lifetime of experiences to infer motivation, we propose to give computer vision systems access to some of these experiences by using recently developed natural language models to mine knowledge stored in massive amounts of text. While we are still far away from fully understanding motivation, our results suggest that transferring knowledge from language into vision can help machines understand why people in images might be performing an action.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing human actions is an important problem in computer vision. However, recognizing actions is only the first step of understanding what a person is doing. For example, you can probably tell that the people in <ref type="figure">Figure 1</ref> are riding bicycles. Can you determine why they are riding bicycles? Unfortunately, while computer vision systems today can recognize actions well, they do not yet understand the intentions and motivations behind people's actions.</p><p>Humans can often infer why another person performs an action, in part due to a cognitive skill known as the theory of mind <ref type="bibr" target="#b38">[39]</ref>. This capacity to infer another person's intention may stem from the ability to impute our own beliefs onto others <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. For example, if we needed to commute to work, we might choose to ride our bicycle, similar to the top right in <ref type="figure">Figure 1</ref>. Since we would be commuting to work in that situation, we might assume others in a similar situation would do the same.</p><p>to sell ice cream to commute to work to answer emergency call to win race Why are they doing that? <ref type="figure">Figure 1</ref>: Understanding Motivations: You can probably recognize that all of these people are riding bikes. Can you tell why they are riding their bikes? In this paper, we learn to predict the motivations of people's actions by leveraging large amounts of text.</p><p>In this paper, we seek to predict the motivation behind people's actions in images. To our knowledge, inferring why a person is performing an action from images has not yet been extensively explored in computer vision. We believe that predicting motivations can help understand human actions, such as anticipating or explaining an action.</p><p>To study this problem, we first assembled an image dataset of people (about 10, 000 people) and annotated them with their actions, motivations, and scene. We then combine these labels with state-of-the-art image features <ref type="bibr" target="#b40">[41]</ref> to train classifiers that predict a person's motivation from images. However, visual features alone may not be sufficient to automatically solve this task. Humans can rely on a lifetime of experiences to predict motivations. How do we give computer vision systems access to similar experiences?</p><p>We propose to transfer knowledge from unlabeled text into visual classifiers in order to predict motivations. Us-ing large-scale language models <ref type="bibr" target="#b10">[11]</ref> estimated on billions of web pages <ref type="bibr" target="#b4">[5]</ref>, we can acquire knowledge about people's experiences, such as their interactions with objects, their environments, and their motivations. We present an approach that integrates these signals from text with computer vision to better infer motivations. While we are still a long way from incorporating human experiences into a computer system, our experiments suggest that we can predict motivations with some success. By transferring knowledge acquired from text into computer vision, our results suggest that we can predict why a person is engaging in an action better than a simple vision only approach.</p><p>The primary contribution of this paper is introducing the problem of predicting the motivations of actions to the computer vision community. Since humans are able to reliably perform this task, we believe that answering "why" for human actions is an interesting research problem to work on. Moreover, predicting motivations has several applications in understanding and forecasting actions. Our second contribution is to use knowledge mined from text on the web to improve computer vision systems. Our results suggest that this knowledge transfer may be beneficial for predicting human motivation. The remainder of this paper describes this approach in detail. Section 2 first reviews related work. Section 3 then introduces a new dataset for this task. Section 4 describes our model that uses a factor graph composed of visual classifiers and pairwise potentials estimated from text. Section 5 presents experiments to analyze the approaches to predict motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Motivation in Vision: Perhaps the most related to our paper is work that predicts the persuasive motivation of the photographer who captured an image <ref type="bibr" target="#b13">[14]</ref>. However, our paper is different because we seek to infer the motivation of the person inside the image, and not the motivation of the photographer.</p><p>Action Prediction: There have been several works in robotics that predicts a person's imminent next action from a sequence of images <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. In contrast, we wish to deduce the motivation of actions in a single image, which may be related to what will happen next. There also has been work in forecasting activities <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37]</ref>, inferring goals <ref type="bibr" target="#b39">[40]</ref>, and detecting early events <ref type="bibr" target="#b11">[12]</ref>, but they are interested in predicting the future in videos while we wish to explain the motivations of actions of people in images. We believe insights into motivation can help further progress in action prediction.</p><p>Action Recognition: There is a large body of work studying how to recognize actions in images <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1]</ref>. Our problem is related since in some cases the motivation can be seen as a high-level action. However, we are interested in understanding the motivation of the person engaging in an action rather than the recognizing the action itself. Our work complements action recognition because we seek to infer why a person is performing an action.</p><p>Commonsense Knowledge: There are promising efforts in progress to acquire commonsense sense for use in computer vision tasks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42]</ref>. In this paper, we also seek to put commonsense knowledge into computer vision, but we instead attempt to extract it from written language.</p><p>Language in Vision: The community has recently been incorporating natural language into computer vision, such as generating sentences from images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36]</ref>, producing visual models from sentences <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b37">38]</ref>, and aiding in contextual models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22]</ref>. In our work, we seek to mine language models trained on a massive text corpus to extract some knowledge that can assist computer vision systems.</p><p>Visual Question Answering: There have been several efforts to develop visual question and answering systems in both images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref> and videos. One could view answering why a person performs an action as a subset of the more general visual QA problem. However, we believe understanding motivations is an important subset to study specifically since there are many applications, such as action forecasting. Moreover, our approach is different from most visual question answering systems, as it jointly infers the actions with the motivations, and also provides a structured output that more suitable for machine consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>On the surface, it may seem difficult to collect data for this task because people's motivations are private and not directly observable. However, humans do have the ability to think about other people's thinking <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. Consequently, we instruct crowdsourced workers to examine images of people and predict their motivations, which we can use as both training and testing data.</p><p>We compiled a dataset of images of people by selecting 10, 191 people from Microsoft COCO <ref type="bibr" target="#b22">[23]</ref>, and annotating motivations with Mechanical Turk. In building this dataset, we found there were important choices for collecting good annotations of motivations. We made sure that these images did not have any person looking at the camera (using <ref type="bibr" target="#b30">[31]</ref>), as otherwise the dominant motivation would be "to take photo." We wish to study natural motivations, and not ones where the person is aware of the photographer. We instructed workers on Amazon Mechanical Turk to annotate each person with their current action, the scene, and their motivation. We originally required workers to pick actions from a pre-defined vocabulary, but we found this was too restrictive for workers. We had difficulty coming up with a vocabulary of actions, possibly because the set of human actions may not be well-defined. Consequently, we decided to allow workers to write short phrases for each concept. Specifically, we had workers fill in the blanks for focusing on a frisbee to block it brushing their hair in order to look nice bending over in order to to ride a skateboard holding his arm up in order to give a toast putting candles in in order to prepare for birthday sitting down in order to watch the dogs holding string in order to fly a kite skiing down a hill in order to win the race running forward in order to grab a ball laying down in order to sleep holding a controller in order to play wii shouting in order to celebrate standing at a register in order to purchase bakery raising hands in order to catch a frisbee holding a container in order to sell meat holding a phone in order to take picture bending in order to blow candles bending over in order to pick up something swinging a racket in order to hit the ball raising his hand in order to feed the giraffe <ref type="figure">Figure 2</ref>: Motivations Dataset: We show some example images, actions, and motivations from our dataset. Below each image we write a sentence in the form of "action in order to motivation." We use this dataset to both train and evaluate models that predict people's motivations. The dataset consists of around 10, 000 people. Notice how the motivations are often outside the image, either in space or time.</p><p>two sentences: a) "the person is [type action] in order to [type motivation]" and b) "the person is in a [type scene]." After data collection, we manually corrected the spelling.</p><p>We show examples from our dataset in <ref type="figure">Fig.2</ref>. The images in the dataset cover many different natural settings, such as indoor activities, outdoor events, and sports. Since workers could type in any short phrase for motivations, the We calculate the probability of a motivation conditioned on the action, and plot the entropy for each action. If motivations could be perfectly predicted from actions, the curve would be a straight line at the bottom of the graph (entropy would be 0). If motivations were unpredictable from actions, the curve would be at the top (maximum entropy of 8). This plot suggests that actions are correlated to the motivations, but it is not possible to predict the motivations only given the action. To predict motivations, we likely need to reason about the full scene.</p><p>motivations in our dataset vary. In general, the motivations tend to be high-level activities that people do, such as "celebrating" or "looking nice". Moreover, while the person's action is usually readily visible, people's motivations are often outside of the image, either in space or time. For example, many of the motivations have not happened yet, such as raising one's hands because they want to catch a ball.</p><p>Since we instructed workers to type in simple phrases, workers frequently wrote similar sentences. To merge these,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All Five Workers Disagree</head><p>All Five Workers Agree we cluster each concept. We first embed each concept into a feature space with skip-thoughts <ref type="bibr" target="#b16">[17]</ref>, and cluster with kmeans. For actions and scenes, we found k = 100 to be reasonable. For motivations, we found k = 256 to be reasonable. After clustering, we use the member in each cluster that is closest to the center as the representative label for a cluster. <ref type="figure">Fig.3</ref> shows the distribution of motivations in our dataset. This class imbalance shows one challenge of predicting motivations because we need to acquire knowledge for many categories. Since collecting such knowledge manually with images (e.g. via annotation) would be expensive, we believe language is a promising way to acquire some of this knowledge.</p><p>We are interested in analyzing the link between actions and motivations. Can motivations be predicted from the action alone? To explore this, we calculate the distribution of motivations conditioned on an action, and plot the en-Relationship Query to Language Model action the person is action motivation the person wants to motivation scene the person is in a scene action + motivation the person is action in order to motivation action + scene the person is action in a scene motivation + scene the person wants to motivation in a scene action + motivation + scene the person is action in order to motivation in a scene <ref type="table">Table 1</ref>: Templates for Language Model:</p><p>We show examples of the queries we make to the language model. We combinatorially replaced tokens with words from our vocabulary to score the relationships between concepts.</p><p>tropy of these distributions in <ref type="figure">Fig.4</ref>. If motivations were predictable given the action, then the entropy would be zero. On the other extreme, if motivations were uncorrelated with actions, then the entropy would be maximum (i.e., − log 2 (256) = 8). Interestingly, the motivations in our dataset lie between these two extremes, suggesting that motivations are related to actions, but not the same.</p><p>Finally, we split the dataset into 75% for training, and the rest for testing. To check human consistency at this task, we annotated the test set 5 times. Two workers agreed on the motivation 65% of the time, and three workers agreed 20% of the time. We compare this to the agreement if workers were to annotate random motivations: two random labels agree 6% of the time, and three random labels agree less than 1% of the time. This suggests there is some structure in the data that the learning algorithm can utilize. However, the problem may also emit multi-modal solutions (people can have several motivations in an image). We show example images where workers agree and disagree in <ref type="figure" target="#fig_1">Fig.5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Predicting Motivations</head><p>In this section, we present our approach to predict the motivations behind people's actions. We first describe a vision-only approach that estimates motivation from image features. We then introduce our main approach that combines knowledge from text with visual recognition to infer motivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Vision Only Model</head><p>Given an image x and a person of interest p, a simple method can try to predict the motivation using only image features. Let y ∈ {1 . . . M } represent a possible motivation for the person. We experimented with using a linear model to predict the most likely motivation:</p><formula xml:id="formula_0">argmax y∈{1,...,M } w T y φ(x, p)<label>(1)</label></formula><p>where w y ∈ R D is a classifier that predicts the motivation y from image features φ(x) ∈ R D . We can estimate w y by training an M -way linear classifier on annotated motivations. We use one versus rest for multi-class classification.</p><p>In our experiments, we use this model as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Extracting Commonsense from Text</head><p>We seek to transfer some knowledge from text into the visual classifier to help predict motivation. Our main idea is to create a factor graph over several concepts (actions, motivations, and scenes). The unary potentials come from visual classifiers, and the potentials for the relationships between concepts can be estimated from large amounts of text.</p><p>Let x be an image, p be a person in the image, and y i ∈ {1 . . . k i } be its corresponding labels for 1 ≤ i ≤ K. In our case, K = 3 because each image is annotated with a scene, action, and motivation. We score a possible labeling configuration y of concepts with the function:</p><formula xml:id="formula_1">Ω(y|x, p; w, u) = K i w T yi φ i (x, p) + K i u i L i (y i ) + K i&lt;j u ij L ij (y i , y j ) + K i&lt;j&lt;k u ijk L ijk (y i , y j , y k )<label>(2)</label></formula><p>where w yi ∈ R Di is the unary term for the concept y i under visual features φ i (·), and L(y i , y j , y k ) are potentials that scores the relationship between the visual concepts y i , y j , and y k . The terms u ijk ∈ R calibrate these potentials with the visual classifiers. We will learn both w and u, while L is estimated from text. Our model forms a third order factor graph, which we visualize in <ref type="figure">Fig.6</ref>.</p><p>In order to learn about the relationships between concepts, we mine large amounts of text. Recent progress in natural language processing has created large-scale language models that are trained on billions of web-pages <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. These models work by ultimately calculating a probability that a sentence or phrase would exist in the training corpus. Since people usually do not write about scenarios that are rare or impossible, we can query these language models to score the relationship between concepts. <ref type="figure">Fig.2</ref> shows some pairs of actions and motivations, sorted by the score from the language model. For example, the language  model that we use predicts that "reading in order to learn" is more likely than "reading in order to cut wedding cake," likely because stories about people reading to cut wedding cake is uncommon. Specifically, to estimate L(·) we "fill in the blanks" for sentence templates. Tab.1 shows some of the templates we use. For example, to score the relationships between different motivations and actions, we query the language model for "the person is action in order to motivation" where action and motivation are replaced with different actions and motivations from our dataset. Since querying is automatic, we can efficiently do this for all combinatoric pairs. In the most extreme case, we query for tertiary terms for all possible combinations of motivations, actions, and scenes. In our experiments, we use a 5-gram language model that outputs the log-probabilities of each sentence.</p><p>Note that, although ideally the unary and binary potentials would be redundant with the ternary language potentials, we found including the binary potentials and learning a weight u for each improved results. We believe this is the case because the binary language model potentials are not true marginals of the ternary potentials as they are built by a limited number of queries. Moreover, by learning extra weights, we increase the flexibility of our model, so we can weakly adapt the language model to our training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference</head><p>Joint prediction of all concepts including motivation corresponds to calculating the most likely configuration y given an image x and learned parameters w and u over the factor graph:</p><formula xml:id="formula_2">y * = argmax y Ω(y|x, p; w, u)<label>(3)</label></formula><p>m a s <ref type="figure">Figure 6</ref>: Factor Graph Relating Concepts: We visualize the factor graph for our model. a refers to action, s for scene, and m for motivation. We use language to estimate potentials.</p><p>We often require the n-best solutions, which can be done efficiently with approximate approaches such as n-best MAP estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref> or sampling techniques <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b24">25]</ref>. However, we found that, in our experiments, it was tractable to evaluate all configurations with a simple matrix multiplication, which gave us the exact n-best solutions in a few seconds on a high-memory server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Learning</head><p>We wish to learn the parameters w for the visual features and u for the language potentials using training data of images and their corresponding labels, {x n , y n }. Since our scoring function in Eqn.2 is linear on the model parameters θ = [w; u], we can write the scoring function in the linear form Ω(y|x, p, w, u) = θ T ψ(y, x, p). We want to learn θ such that the labels matching the ground truth score higher than incorrect labels. We adopt a max-margin structured prediction objective:</p><formula xml:id="formula_3">argmin θ,ξ n ≥0 1 2 ||θ|| 2 + C n ξ n s.t. θ T ψ(y n , x n , p n ) − θ T ψ(h, x n , p n ) ≥ 1 − ξ n ∀ n , ∀ h =y n<label>(4)</label></formula><p>The linear constraints state that the score for the correct label y n should be larger than that of any other hypothesized label h n by at least 1. We use a standard 0-1 loss function that incurs a penalty if any of the concepts do not match the ground truth. This optimization is equivalent to a structured SVM and can be solved by efficient off-the-shelf solvers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>. Note that one could use hard-negative mining to solve this optimization problem for learning. However, in our experiments, it did not improve results, possibly because we train the model on a high-memory machine.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Indicating Person of Interest</head><p>Finally, to predict somebody's motivation, we must specify the person of interest. To do this, we crop the person, and extract visual features for the close-up crop. We then concatenate these features with the full image features. We use this concatenation to form φ(x, p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the performance at inferring motivations in images from our models. We first describe our evaluation setup, then we present our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>We designed our experiments to evaluate how well we can predict the motivation of people from our dataset. We assumed the person-of-interest is specified since the focus of this work is not person detection. We computed features from the penultimate layer (fc7) in the VGG-16 convolutional neural network trained on Places <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b32">33]</ref> due to their strong performance on other visual recognition problems <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b7">8]</ref>. We concatenate the features for both the full image and a close-up crop of the person of interest (giving a 8, 192 dimensional feature vector). We experimented with a few different C values, and report the best performing (0.001). To compute the chance performance, we evaluate median rank against a list sorted by the frequency of motivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation</head><p>We evaluate our approach on an image by finding the median rank of a ground truth motivation in the max-marginals on all states for the motivation concept. This is equivalent to the rank of ground truth motivation in the list of motivation categories, sorted by their best score among all possible configurations. We use median rank because motivation prediction may have multi-modal solutions, and we seek an evaluation metric that allows for multiple predictions. We report the median rank of our full approach and the baseline in Tab.3.</p><p>Our results suggest that incorporating knowledge from text can improve performance at predicting motivations, compared to a vision-only approach. Our interpretation is that text acts as a regularizer that helps the model perform better on novel images during inference time. The baseline appears prone to over-fitting, since the baseline performs slightly worse when we specify the person. Moreover, our approach is significantly better than chance, suggesting that the learning algorithm is able to use the structure in the data to infer motivations. As one might expect, performance for our model improves when we specify the person of interest in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Diagnostics</head><p>For diagnostic purposes, the bottom of Tab.3 shows the performance of our approach if we had ideal recognition systems for each visual concept. To give ideal detectors to our system, we can constrain the unary potentials for the corresponding concept to the ground truth. Our results suggest that if we had perfect vision systems for actions and scenes, then our model would only slightly improve, and it would still not solve the problem. This suggests that motivations are not the same as scenes or actions. Rather, in order to improve performance further, we hypothesize integrating additional visual cues such as human gaze <ref type="bibr" target="#b30">[31]</ref> and clothing will yield better results, motivating work in highlevel visual recognition.</p><p>To evaluate the impact of the language model, we also experimented with using an alternative language model trained on text from Wikipedia instead of the web. Here, the language model is estimated with much less data. Interestingly, using the Wikipedia language model performs siting at a table in order to eat in a restaurant holding a cake in order to talk to someone in a dining room drinking wine in order to eat dinner in a dining room swinging a bat in order to hit a ball in a baseball field hitting a ball in order to hit a ball in a baseball field holding a bat in order to strike out in a baseball field standing in order to play video game in a living room holding controller in order to play video game in a house playing video game in order to get information in a dining room sitting at a table in order to get information in a dining room talking in order to eat in a living room sitting down in order to rearrange food in a classroom watching the ball in order to hit the ball in a tennis court doing a shove-it in order to play tennis in a baseball field swinging a racket in order to hit the ball in a baseball stadium standing in order to buy fruit in a market tieing in order to see what is happening in a street prepping food in order to sell them in a sidewalk standing with co-worker in order to pickup luggage in airport waiting in order to juggle in a ballpark bending down in order to have fun in a skatepark holding scissors in order to make origami in a kitchen sitting down in order to eat in a living room holding dog in order to advertise something in a living room <ref type="figure">Figure 7</ref>: Example Results: We show the top three predictions for some people in our evaluation set. The sentences are shown only to visualize results: we do not generate free-form captions. Even when the model is wrong, it usually predicts concepts that have plausible relations to other, which is possible because of the language model. slightly worse at predicting motivations (by one point), suggesting that a) leveraging more unlabeled text may help, and b) better language models can help computer vision tasks. We believe that advances in natural language processing can help computer vision systems recognize concepts, especially higher-level concepts such as motivations.</p><p>We show a few examples of successes and failures for our approach in <ref type="figure">Fig.7</ref>. We hypothesize that our model often produces more sensible failures because it leverages some of the knowledge available in unlabeled text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>While computers can recognize the actions of people in images with good performance, predicting the motivations behind actions is a relatively less explored problem in computer vision. This problem is important both for developing a full understanding of actions, as well as in applications, such as anticipating future actions. We have released a new dataset to study this problem, and we have investigated how to transfer some knowledge from text to help predict motivations.</p><p>Our experiments indicate that there is still significant room for improvement. We suspect that advances in highlevel visual recognition can help this task. However, our results suggest that visual information alone may not be sufficient for this challenging task. We hypothesize that incorporating common knowledge from other sources can help, and our results imply that written language is one valuable source. Our framework only transfers some of the com-monsense knowledge into vision models, and we hypothesize that continued work in commonsense reasoning for vision will help machines infer motivations. We believe that progress in natural language processing is one way to advance such high-level reasoning in computer vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Statistics of Dataset: We show a histogram of frequencies of the actions, motivations, and scenes in our dataset. There are 100 actions, 256 motivations, and 100 scenes. Notice the class imbalance. On the vertical axis, not all categories are shown due to space restrictions. Are motivations predictable from just actions?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Which images have consistent motivations? On the left, we show some images from our test set where all workers disagreed on the motivation. On the right, we show images where all workers agreed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Example</figDesc><table>Language Potentials: By mining bil-
lions of web-pages, we can extract some knowledge about 
the world. This table shows some pairs of concepts, sorted 
by the score from the language model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Evaluation</figDesc><table>of Median Rank: We show the median rank of the ground truth motivations in the predicted motivations, 
comparing several methods. Lower is better with 1 being perfect. There are 256 motivations, 100 actions, and 100 scenes. 
In the bottom of the table, we show diagnostic experiments, where we give the classifier access to the ground truth of other 
concepts, and evaluate how well it can infer the remaining concepts. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We thank Lavanya Sharan for important discussions, Adria Recasens for help with the gaze dataset, and Kenneth Heafield and Christian Buck for help with transferring the 6 TB language model across the Atlantic ocean. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research. This work was supported by NSF grant IIS-1524817, and by a Google faculty research award to AT, and a Google PhD fellowship to CV.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Action understanding as inverse planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diverse m-best solutions in markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">N-gram counts and language models from the common crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ooyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>LREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neil: Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning everything about anything: Webly-supervised visual concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning intentions for improved human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elfring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van De Molengraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting object dynamics in scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2027" to="2034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="27" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual persuasion: Inferring communicative intents of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Steen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep networks for predicting human intent with respect to objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wigand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Human-Robot Interaction</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3276" to="3284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<title level="m">Activity forecasting. In ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A procedure for computing the k best solutions to discrete optimization problems and its application to the shortest path problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Lawler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting language models to recognize unseen actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human intent prediction using markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nasir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Atkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Infotech@ Aerospace</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Grante: Inference and estimation for discrete factor graph model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language for learning complex human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Miro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cˆ4: Exploring multiple solutions in graphical models by cluster sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Porway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dual coordinate solvers for large-scale structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.1743</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Where are they looking?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">People thinking about thinking people: the role of the temporo-parietal junction in theory of mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanwisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Predicting human intention in visual observations of hand/object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papazov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02902</idno>
		<title level="m">Movieqa: Understanding stories in movies through question-answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Patch to the future: Unsupervised visual prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning models for object recognition from natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children&apos;s understanding of deception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inferring&quot; dark matter&quot; and&quot; dark energy&quot; from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reasoning about object affordances in a knowledge base representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="408" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bringing semantics into focus using visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
