<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional 1 salient object detection models often use hand-crafted features to formulate contrast and various prior knowledge, and then combine them artificially. In this work, we propose a novel end-to-end deep hierarchical saliency network (DHSNet) based on convolutional neural networks for detecting salient objects. DHSNet first makes a coarse global prediction by automatically learning various global structured saliency cues, including global contrast, objectness, compactness, and their optimal combination. Then a novel hierarchical recurrent convolutional neural network (HRCNN) is adopted to further hierarchically and progressively refine the details of saliency maps step by step via integrating local context information. The whole architecture works in a global to local and coarse to fine manner. DHSNet is directly trained using whole images and corresponding ground truth saliency masks. When testing, saliency maps can be generated by directly and efficiently feedforwarding testing images through the network, without relying on any other techniques. Evaluations on four benchmark datasets and comparisons with other 11 state-of-the-art algorithms demonstrate that DHSNet not only shows its significant superiority in terms of performance, but also achieves a real-time speed of 23 FPS on modern GPUs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection aims at accurately and uniformly detecting objects that grab human attention in images. In recent years, researchers have developed many computational models for salient object detection and applied them to benefit many other applications, such as image summarization <ref type="bibr" target="#b0">[1]</ref>, segmentation <ref type="bibr" target="#b1">[2]</ref>, retrieval <ref type="bibr" target="#b2">[3]</ref>, and editing <ref type="bibr" target="#b3">[4]</ref>.</p><p>Traditional saliency detection methods rely on various saliency cues. The most widely explored one is contrast, which aims at evaluating the distinctiveness of each image  Corresponding author. region or image pixel with respect to local contexts or global ones. Local contrast based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> typically tend to highlight object boundaries while often miss object interiors. On the contrary, global contrast based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> are capable of highlighting object interiors uniformly. This kind of methods are better, but still unsatisfactory. On one hand, they usually fail to preserve object details. On the other hand, they are often difficult to detect salient objects with large sizes and complex textures, especially when image backgrounds are also cluttered or have similar appearances with foreground objects (see column (b) in <ref type="figure" target="#fig_0">Figure 1</ref>). Furthermore, conventional methods usually model contrast via hand-crafted features (e.g., intensity, color, and edge orientation <ref type="bibr" target="#b4">[5]</ref>) and human designed mechanisms (e.g., the "Difference of Gaussians" (DoG) operator <ref type="bibr" target="#b4">[5]</ref>) based on limited human knowledge on visual attention. Thus they may not generalize well in different scenarios.</p><p>Some recent works also utilize various prior knowledge as informative saliency cues. Background prior <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> hypothesizes that regions near image boundaries are probably backgrounds. However, it often fails when salient objects touch image boundaries or have similar appearance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nian Liu Junwei Han * School of Automation, Northwestern Polytechnical University</head><p>Xi'an, 710072, P. R. China {liunian228, junweihan2010}@gmail.com with backgrounds (see column (c) in <ref type="figure" target="#fig_0">Figure 1</ref>). Compactness prior <ref type="bibr" target="#b11">[12]</ref> advocates that salient object regions are compact and perceptually homogeneous elements. Objectness prior <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> tends to highlight an image region which is likely to contain an object of a certain class. Although these priors can further provide informative information for salient object detection, they are usually explored empirically and modelled by hand-designed formulations.</p><p>Various saliency cues are also combined in some works to incorporate their complimentary interactions. Nevertheless, these works usually resort to simple combination schemes (e.g., simple arithmetic) or shallow learning models (e.g., CRF used in <ref type="bibr" target="#b14">[15]</ref>), which are hard to mine complicated joint interactions between diverse saliency cues. Moreover, to preserve object details and subtle structures, many traditional methods adopt over-segmentations of images (e.g., superpixels used in <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> and object proposals used in <ref type="bibr" target="#b13">[14]</ref>) either as the basic computational units to predict saliency or as the post-processing methods to smooth saliency maps. Although these methods can further improve saliency detection results, they are usually very time-consuming, becoming the bottleneck of the computational efficiency of a salient object detection algorithm.</p><p>From the discussions above, we can see that, how to build real meaningful feature representations, how to simultaneously explore all potential saliency cues, how to find the optimal integration strategy, and how to efficiently preserve object details become the most intrinsic problems for further promoting salient object detection methods.</p><p>To solve these problems, we propose a novel end-to-end deep hierarchical saliency detection framework, i.e., the DHSNet, via convolutional neural networks (CNN) <ref type="bibr" target="#b19">[20]</ref>. DHSNet takes the whole images as the inputs and outputs saliency maps directly, hierarchically detecting salient objects from the global view to local contexts, from coarse scale to fine scales (see <ref type="figure">Figure 2</ref>). In details, we first adopt a CNN over the global view (GV-CNN) to generate a coarse global saliency map ( G Sm ) to roughly detect and localize salient objects. With the supervision of the global structured loss, the GV-CNN can automatically learn feature representations and various global structured saliency cues, such as global contrast, objectness, compactness, and their optimal combination. Consequently, the GV-CNN can obtain optimal global salient object detection results, being robust to complex foreground objects and cluttered backgrounds, even if they are very similar in appearance (see column (d) in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>The generated G Sm is much coarser than the input image since some detailed information, such as accurate object boundaries and subtle structures, are gradually discarded in the GV-CNN. To address this problem, we further propose to adopt a novel hierarchical recurrent convolutional neural network (HRCNN) to refine saliency maps in details by incorporating local contexts. The HRCNN is composed of several recurrent convolutional layers (RCL) <ref type="bibr" target="#b20">[21]</ref> and upsampling layers (see <ref type="figure">Figure 2</ref>). RCLs incorporate recurrent connections into each convolutional layer, thus enhancing the capability of the model to integrate context information, which is very important for saliency detection models. In HRCNN, we refine the saliency map in several steps hierarchically and successively. In each step, we adopt a RCL to generate a finer saliency map by integrating the upsampled coarse saliency map predicted at the last step and the finer feature maps from the GV-CNN. The RCL in each step boosts the details for the former step, and provides a good initialization for the next step. As the scales of intermediate saliency maps become finer and finer and the receptive fields of the combined feature maps become smaller and smaller, the image details can be rendered step by step, without relying on image over-segmentations (see the final results in column (e) in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>The contributions of this paper can be summarized as follows:</p><p>(1) We propose a novel end-to-end saliency detection model, i.e., the DHSNet, to detect salient objects. DHSNet can simultaneously learn powerful feature representations, informative saliency cues (for instance, global contrast, objectness, and compactness), and their optimal combination mechanisms from the global view, and subsequently learn to further refine saliency map details.</p><p>(2) We propose a novel hierarchical refinement model, i.e., the HRCNN, which can hierarchically and progressively refine saliency maps to recover image details by integrating local context information without using over-segmentation methods. The proposed HRCNN can significantly and efficiently improve saliency detection performance. Furthermore, it can also be used in other pixel-to-pixel tasks, such as scene labeling <ref type="bibr" target="#b21">[22]</ref>, semantic segmentation <ref type="bibr" target="#b22">[23]</ref>, depth estimation <ref type="bibr" target="#b23">[24]</ref> and so on.</p><p>(3) Experimental results on four benchmark datasets and comparisons with other 11 state-of-the-art approaches demonstrate the great superiority of DHSNet on the salient object detection problem, especially on complex datasets. Furthermore, DHSNet is very fast on modern GPUs, achieving a real-time speed of 23 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional Neural Networks</head><p>Recently, CNNs have achieved great successes in many computer vision tasks, including image classification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, object detection and localization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, face recognition <ref type="bibr" target="#b28">[29]</ref> and so on. CNNs have also been successfully applied in many pixel-wise prediction tasks <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. Here we briefly review several works related to this paper.</p><p>Many works adopt diverse deep architectures to preserve details in pixel-wise tasks. For depth map prediction, Eigen et al. <ref type="bibr" target="#b23">[24]</ref> first trained a CNN for making a coarse global prediction based on the entire image, then another CNN was used to refine this prediction locally. For semantic segmentation, <ref type="bibr" target="#b29">[30]</ref> utilized deconvolutional layers and unpooling layers to gradually enlarge the resolutions of feature maps to predict a fine semantic segmentation results. Similarly, <ref type="bibr" target="#b30">[31]</ref> utilized several "upconvolutional" layers which consisted of deconvolutional layers and unpooling layers to refine the optical flow predictions layer by layer. These two works share similar ideas of gradually refining feature maps or prediction results from coarse to fine with our model. However, the unpooling layers adopted in their models selectively transferred information from a coarser layer to a finer layer yet limitted the transferred information. Besides, their heavy decoder architectures introduced lots of parameters to learn and made their network hard to train. Last but not least, we embedded RCLs <ref type="bibr" target="#b20">[21]</ref> in each refinement step, thus enhancing the capability of the model to integrate context information with limited parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Convolutional Neural Networks for Saliency Detection</head><p>Some researchers have already applied deep neural networks to saliency detection, which includes two branches, i.e., eye fixation prediction <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> and salient object detection <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. Here we briefly review the works about the latter which are related to our work.</p><p>For salient object detection, Wang et al. <ref type="bibr" target="#b34">[35]</ref> used a CNN to predict saliency score for each pixel in local context first, then they refined the saliency score for each object proposal over the global view. Li and Yu <ref type="bibr" target="#b33">[34]</ref> predicted the saliency score for each superpixel by using multiscale CNN features. Similarly, Zhao et al. <ref type="bibr" target="#b35">[36]</ref> predicted the saliency score for each superpixel by incorporating local context and global context simultaneously in a multi-context CNN. These three methods all achieved better results than traditional methods. However, none of them considered global context preferentially. Furthermore, they processed local regions (superpixels, and object proposals) separately, thus the correlation of the regions in different spatial locations was not utilized. These two weaknesses make their networks hard to learn enough global structures, thus their results are often distracted by local salient patterns in cluttered backgrounds and are not able to highlight salient objects uniformly. On the contrary, DHSNet adopts the whole image as the computational unit and propagates the global context information to local contexts hierarchically and progressively, being able to perceive global properties and avoid the distraction of local interferences from the beginning. Last but not least, all these three methods relied on image over-segmentations, making their algorithms very time-consuming. While DHSNet only needs to feedforward each testing image through the network, thus is much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DHSNet for Salient Object Detection</head><p>As shown in <ref type="figure">Figure 2</ref>, DHSNet is composed of the GV-CNN and the HRCNN. The GV-CNN first coarsely detects salient objects in a global perspective, then the HRCNN hierarchically and progressively refines the details of the saliency map step by step. DHSNet is trained end-to-end. When testing we just feedforward the input <ref type="figure">Figure 2</ref>: The architecture of the proposed DHSNet method. The spatial size of each image or feature map is given. In the VGG net, the names of the layers whose features are utilized in the HRCNN are shown. The name of each step-wise saliency map is also shown.</p><p>image through the network, without using any post-processing and image over-segmentation method, thus making DHSNet not only effective, but also efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">GV-CNN for Coarse Global Prediction</head><p>As shown in <ref type="figure">Figure 2</ref>, the GV-CNN consists of 13 convolutional layers of the VGG net <ref type="bibr" target="#b24">[25]</ref>, a subsequent fully connected layer, and a reshape layer. For an input image wrapped to size 224×224, the 13 convolutional layers of the VGG 16-layer network are first adopted to extract deep features. Afterwards, on top of the last convolutional layer (i.e., the third sublayer in the fifth group of convolutional layers, denoted as Conv5_3. The other convolutional layers in the VGG net can also be denoted by this analogy.) with size 14×14×512, a fully connected layer with sigmoid activation function and 784 nodes is deployed. Finally this layer is reshaped to size 28×28 as the coarse global saliency map G Sm . Supervised by the global structured loss, i.e., the averaged pixel-wise cross entropy loss between G Sm and the ground truth saliency mask, the fully connected layer learns to detect and localize salient objects of the input image from the feature maps ahead by integrating various saliency cues. As <ref type="bibr" target="#b36">[37]</ref> pointed out, convnet features can localize at a much finer scale than their receptive field sizes. Thus the GV-CNN can generate a relatively large saliency map (28×28) even though the size of layer Conv5_3 is small (14×14). The experiments in Section 4.5 show the effectiveness of the GV-CNN and its learned saliency cues.</p><p>Although the GV-CNN can coarsely detect and localize salient objects, the image details in G Sm , e.g., object boundaries and subtle structures, are still missing. The reasons are two-folds. First, the 4 max-pooling layers in the VGG net abandon some spatial information, making the layer Conv5_3 hard to reserve local details. Second, the amount of the parameters in the fully connected layer increases linearly with the enlargement of the size of G Sm , making the training difficult. Thus we have to choose a small size for G Sm . As a result, G Sm is not satisfactory enough, both quantitatively and visually, and further refinements are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">HRCNN for Hierarchical Saliency Map Refinement</head><p>To further improve G Sm in details, we propose a novel architecture, i.e., the HRCNN, to hierarchically and progressively render image details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Convolutional Layer.</head><p>The core of the HRCNN is the RCL which was proposed by <ref type="bibr" target="#b20">[21]</ref>. RCL incorporates recurrent connections into each convolutional layer. For a unit located at (i, j) on the kth feature map in an RCL, its state at time step t is given by:</p><formula xml:id="formula_0">( ) ( ( ( ))), ijk ijk x t g f z t  (1)</formula><p>where f is the ReLU <ref type="bibr" target="#b25">[26]</ref> activation function and g is the local response normalization (LRN) function <ref type="bibr" target="#b25">[26]</ref> to prevent the states from exploding:</p><formula xml:id="formula_1">min( , / 2) 2 max(0, / 2) () ( ( )) , 1 ( ) ijk ijk K k N ijk k k N ft g f t f N                (2) where ( ( )) ijk f z t is abbreviated as () ijk</formula><p>ft , K is the total number of feature maps, N is the size of the local neighbor feature maps which are involved in the normalization, α and β are constants to modulate the normalization.</p><p>In Eq. (1), () ijk ztis the input of the unit, which incorporates a feedforward connection and a recurrent connection:</p><formula xml:id="formula_2">( ) ( ) ( ) ( ) ( ) ( 1) . f T i, j r T i, j ijk k k k z t t b     w u w x (3) where () i, j u and () ( 1) i, j t  x</formula><p>are the feedforward input from the previous layer and the recurrent input from the current layer at time step t-1, respectively. f k w and r k w are the feedforward weights and the recurrent weights, respectively. k b is the bias.</p><p>A RCL with T time steps can be unfolded to a feed-forward subnetwork of depth T + 1. We follow <ref type="bibr" target="#b20">[21]</ref> to set T = 3 and show the unfolded RCL in the blue dotted box in <ref type="figure" target="#fig_1">Figure 3</ref>. We can see that multiple recurrent connections make the subnetwork has multiple paths from the input layer to the output layer, which facilitates the learning. Besides, the effective receptive field of an RCL unit expands when the time step increases, making the units to be able to "see" larger and larger contexts without increasing the number of network parameters. Thus RCLs can help to incorporate local contexts efficiently in HRCNN to refine saliency maps. The experiment in Section 4.5 demonstrates the superiority of RCLs over traditional convolutional layers.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we use 64 feature maps in each RCL empirically to save computational costs and follow <ref type="bibr" target="#b20">[21]</ref>  to use feed-forward and recurrent filters with size 3×3. The hyper-parameters of LRN in Eq. (2) are set as α = 0.001, β = 0.75 and N = 7. Different from <ref type="bibr" target="#b20">[21]</ref>, we do not adopt dropout <ref type="bibr" target="#b20">[21]</ref> in RCLs. <ref type="figure">Figure 2</ref>, we first combine G Sm with layer Conv4_3 of the VGG net and adopt a RCL to generate a finer saliency map (as this saliency map is obtained by adopting a RCL over the local features in Conv4_3, we denote it as <ref type="bibr">4 RCL</ref> Sm and the subsequent further refined saliency maps are denoted in the same way). As </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Saliency Map Refinement. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sm</head><p>, which is the final saliency map. In <ref type="figure" target="#fig_1">Figure 3</ref>, we show the detailed framework of a refinement step, i.e., combining a coarse saliency map with a convolutional layer from the VGG net to generate a finer saliency map. We first use a convolutional layer with 64 1×1 convolutional kernels and sigmoid activation function to squash the features of the VGG layer. The reasons are two folds. First, we decrease the number of feature maps of the VGG layer to save computational costs. Second, by using sigmoid activation function, we squash the range of the activation values of the neurons to be [0,1], which is as the same as the combined saliency map. Without doing this, the combined saliency map will be overwhelmed since the activation values in each layer of the VGG net are usually very large with ReLU activation functions.</p><p>Next, the squashed VGG layer is concatenated with the upsampled coarse saliency map (except that G Sm is directly concatenated with layer Conv4_3 without upsampling), resulting in 65 feature maps. Then we adopt a RCL to combine the coarse saliency map and the local features in the VGG layer. At last, the refined saliency map can be generated by adopting a convolutional layer with 1×1 kernels and sigmoid activation function. <ref type="table">Table 1</ref> shows the size of each step-wise saliency map and the sizes of the receptive fields from which they are induced. From </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sm</head><p>, the sizes of step-wise saliency maps are gradually enlarged but the receptive fields gradually shrink (note that the receptive field of G Sm is the whole image). Thus HRCNN refines the saliency maps in a coarse to fine and global to local manner. Consequently, image details can be rendered step by step by incorporating finer and finer features. The experiments in Section 4.5 also verify the effectiveness of the hierarchical refinement scheme.</p><p>To facilitate learning, we also adopt the deep supervision <ref type="bibr" target="#b37">[38]</ref> scheme. To be specific, as shown in <ref type="figure">Figure 2</ref>, we resize the ground truth saliency mask to sizes ranging from 224 to 28 to supervise the corresponding learning of each step-wise saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conducted evaluations on four widely used salient object benchmark datasets. ECSSD <ref type="bibr" target="#b17">[18]</ref> includes 1,000 semantically meaningful but complex images. MSRA10K <ref type="bibr" target="#b6">[7]</ref> contains 10,000 images with various objects. Most images contain only one salient object and the backgrounds are usually clear. DUT-OMRON <ref type="bibr" target="#b9">[10]</ref> includes 5,168 images with one or more salient objects and relatively complex backgrounds. PASCAL-S <ref type="bibr" target="#b13">[14]</ref> contains 850 real-world images selected from the PASCAL VOC segmentation dataset. Many images in this dataset have highly cluttered backgrounds and multiple complex foreground objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation metrics</head><p>We first adopted the precision-recall (PR) curve to evaluate DHSNet. Specifically, saliency maps were binarized using different thresholds varying from 0 to 1. Then compared with the ground truth, a series of precision-recall values can be obtained at different thresholds to plot the PR curve. We also adopted the F-measure <ref type="bibr" target="#b7">[8]</ref> score to comprehensively consider precision and recall. By using an image adaptive threshold, we can obtain Precision and Recall , then F-measure is given by:</p><formula xml:id="formula_3">2 2 (1 ) . Precision Recall F Precision Recall         (4)</formula><p>where 2  is set to 0.3 and the adaptive threshold is set to twice the mean saliency value of each saliency map as suggested in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>We randomly selected 6,000 images from MSRA10K dataset and 3,500 images from DUT-OMRON dataset as the training set, and another 800 images from MSRA10K and 468 images from DUT-OMRON as the validation set.  <ref type="table" target="#tab_2">Size  28  28  56  112  224  RF  224  156  72  30  13   Table 1</ref>: The sizes of the step-wise saliency maps and the receptive fields (RF) from which they are induced.</p><p>Then we tested our model on the rest images and other datasets.</p><p>During training, we did image augmentation by horizontal-flipping and image cropping to relieve overfitting. In details, for each training image and the corresponding ground truth, we cropped out the most top, bottom, left, right, and middle 9/10 image as training samples. In addition to the original images and the horizontally-flipped ones, we increased the training set by 12 times. When fed into DHSNet, each image was first wrapped to size 224×224 and subtracted a mean pixel provided by VGG net at each position.</p><p>With the RCLs unfolded through time steps, the whole network was trained end-to-end by using back propagation algorithm <ref type="bibr" target="#b38">[39]</ref>. The upsampling layers were simply implemented using the nearest-neighbor interpolation method. To facilitate training, we first trained the GV-CNN alone with the 13 convolutional layers initialized by the VGG net and the fully connected layer randomly initialized. We used a minibatch size of 12 and 40,000 iteration steps, and set the learning rate to 0.015 in the last layer and a 1/10 smaller one in the VGG layers. We also halved the learning rate every 4,000 iterations. Besides, we set momentum to 0.9 and weight decay factor to 0.0005. Then we trained the whole DHSNet with the GV-CNN part initialized by the pretrained model and the HRCNN part initialized randomly. Here we set the minibatch size to 5 and kept the 40,000 iteration steps, and set the learning rate to 0.03 in the HRCNN part and a 1/1000 smaller one in the GV-CNN layers. The learning rate decay policy, the momentum and the weight decay factor were kept as the same as those when training GV-CNN. We tested the cross entropy loss on the validation set every 2000 iteration steps and selected a model with the lowest validation loss as the best model to do testing.</p><p>We implemented DHSNet using caffe <ref type="bibr" target="#b39">[40]</ref> toolbox. The testing codes were implemented using Matlab. A GTX Titan X GPU was used both in training and testing for acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>We compared DHSNet with other 11 state-of-the-art models, including RC <ref type="bibr" target="#b6">[7]</ref>, GS <ref type="bibr" target="#b10">[11]</ref>, HS <ref type="bibr" target="#b17">[18]</ref>, GBMR <ref type="bibr" target="#b9">[10]</ref>, DRFI <ref type="bibr" target="#b18">[19]</ref>, wCtr <ref type="bibr" target="#b40">[41]</ref>, HDCT <ref type="bibr" target="#b16">[17]</ref>, CPMC-GBVS <ref type="bibr" target="#b13">[14]</ref>, LEGS <ref type="bibr" target="#b34">[35]</ref>, MDF <ref type="bibr" target="#b33">[34]</ref> and MCDL <ref type="bibr" target="#b35">[36]</ref> 2 .</p><p>For quantitative evaluation, we show comparison results with PR curves and F-measure scores in <ref type="figure" target="#fig_4">Figure 4</ref>. We can see that, DHSNet outperforms all other methods by a large margin, especially on complex datasets, i.e., ECSSD, DUT-OMRON, and PASCAL-S. In terms of PR curves, as shown in the top row in <ref type="figure" target="#fig_4">Figure 4</ref>, DHSNet achieves much higher curves than all other methods from the beginning to the end on all the 4 datasets, indicating that DHSNet can achieve both the highest precision and the highest recall. From the bottom row in <ref type="figure" target="#fig_4">Figure 4</ref>, we can see that DHSNet almost achieves all of the highest precision, recall and F-measure scores under the adaptive threshold on all the 4 2 LEGS didn't publish their results on DUT-OMRON. MDF and LEGS were trained on the MSRA-B dataset <ref type="bibr" target="#b14">[15]</ref>, which is covered by MSRA10K, and MCDL was trained on MSRA10K. Thus we didn't compare our model with these three models on this dataset. datasets, except that on PASCAL-S, the precision of MDF is slightly better than DHSNet, however its recall and F-measure score are much lower than those of DHSNet. It is worth noting that DHSNet also outperforms the other three CNN-based methods (i.e., LEGS, MDF, and MCDL) a lot. The improvement DHSNet achieved with respect to these three methods is almost equivalent to their improvement with respect to traditional approaches, demonstrating the superiority of DHSNet.</p><p>We showed visual comparison in <ref type="figure" target="#fig_5">Figure 5</ref>. As we can see, DHSNet not only detects and localizes salient objects accurately, but also preserves object details subtly. It can handle various situations well, including foreground objects being very big (row 1 and 6 in <ref type="figure" target="#fig_5">Figure 5</ref>) or very small (row 8), images with multiple foreground objects (row 3 and 8), cluttered backgrounds and complex foregrounds (row 4, 5 and 7), and salient objects touching image boundaries (row 1, 2 and 6). Especially in row 1, 2 and 5, the foreground objects have similar appearance with backgrounds, which confuses most other methods, while DHSNet works well. We can also see that LEGS, MDF, and MCDL often are distracted by local salient patterns in cluttered backgrounds and are not able to highlight salient objects uniformly. In contrast, DHSNet can overcome these difficulties well.</p><p>We also evaluated the runtime of each method in <ref type="table" target="#tab_2">Table 2</ref>. These evaluations were conducted on a machine with 2 2.8GHz 6-core CPUs and 32GB memory. LEGS, MDF, MCDL, and DHSNet were accelerated by a GTX Titan X GPU. We can see that DHSNet is the fastest, achieving a real-time speed of 23 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Model Component Analysis</head><p>The effectiveness of the hierarchical refinement scheme of HRCNN. We show the F-measure scores and the corresponding precision and recall of the step-wise saliency maps in <ref type="table" target="#tab_4">Table 3</ref>. We can see that, from G Sm to <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RCL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sm</head><p>, all the three metrics, i.e., precision, recall and F-measure are progressively enhanced, except that the recall saturates and fluctuates slightly after 3 RCL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sm</head><p>. Qualitative results are shown in <ref type="figure" target="#fig_6">Figure 6</ref>. We can see that the hierarchical refinement scheme can progressively improve the details of saliency maps, not only eliminating false positive highlights,   but also redetecting missing parts (e.g., the arm of the man in the top row).</p><p>The effectiveness of the GV-CNN. In <ref type="figure" target="#fig_7">figure 7</ref>, we show some saliency maps from G Sm as intuitive examples of the saliency cues learned in GV-CNN. G Sm in (a) just highlights the dog despite of the small-scale high-contrast patterns in the background (i.e., the flowers), which indicates that GV-CNN learned global contrast. In (b), although the squirrel has similar appearance with the background, GV-CNN can still detect it accurately due to the learned objectness. In (c), the successful detection of all the sailing ships indicates that GV-CNN is robust to scales and locations of foreground objects. Furthermore, we can see that the highlighted regions are basically compact and homogeneous, demonstrating that GV-CNN also follows the compactness prior well.</p><p>To further verify the effectiveness of GV-CNN quantitatively, we also adopted a fully connected network (FCN) <ref type="bibr" target="#b22">[23]</ref> to substitute GV-CNN in DHSNet. The utilized FCN was implemented by adopting the "hole algorithm" <ref type="bibr" target="#b41">[42]</ref> to the Conv5 layers and using a convolutional layer with 1 3×3 sized kernel and sigmoid activation function to generate the coarse saliency map with size 28×28 at last. The receptive field size of the units in the output layer is 228. However, only the pixels around the image center can perceive the whole image, and other pixels can only "watch" part of the image, which is the limitation of the FCN. As a result, although the performance of the coarse prediction the FCN made (denoted as FCN G Sm in <ref type="table" target="#tab_4">Table 3</ref>) approximates that of G Sm , the final refined results of FCN (FCN 1 RCL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sm</head><p>in <ref type="table" target="#tab_4">Table 3</ref>) are still worse than those of DHSNet with GV-CNN, which is probably because the incorrect detections in FCN G Sm are magnified by HRCNN.</p><p>The effectiveness of the RCLs. To demonstrate the effectiveness of the deployed RCLs, we followed <ref type="bibr" target="#b20">[21]</ref> to substitute RCLs with traditional convolutional layers with more feature maps (128 feature maps were used to keep the number of parameters in each refinement step approximately unchanged). The results in <ref type="table" target="#tab_4">Table 3</ref> show that when RCLs are substituted, all the precision, recall and F-measure will drop, which demonstrates the superiority of adopting RCLs in the salient object detection problem.</p><p>Comparison with other encoder-decoder networks. We also compared the proposed DHSNet with the DeconvNet <ref type="bibr" target="#b29">[30]</ref>. The results in the last row of <ref type="table" target="#tab_4">Table 3</ref> demonstrate the superiority of DHSNet over traditional encoder-decoder networks on the salient object detection task. Further analysis will be done in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed DHSNet as a novel end-to-end salient object detection model. It first detected salient objects coarsely from a global view, then hierarchically and progressively improve image details by integrating local contexts. DHSNet not only obtained state-of-the-art results, but also achieves real-time speed.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of results by different kinds of methods. For images in (a), we show the salient object detection results of a global contrast based method in (b), a background prior based method in (c), the results of the GV-CNN in (d), the final refined results of DHSNet in (e), and the ground truth in (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The detailed framework of a refinement step. The RCL is unfolded along with the time steps in the blue dotted box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Quantitative model comparisons. We show PR curves (top) and F-measure scores (bottom) on 4 benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative model comparisons. The ground truth (GT) is shown in the last column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the step-wise saliency maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Intuitive examples of the saliency cues learned in GV-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Runtime of each method. *: GPU time.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>: This work was supported in part by the National Science Foundation of China under Grants 61522207 and 61473231. DHSNet with the GV-CNN substituted by a FCN FCN</figDesc><table>Settings 
Precision 
Recall 
F-measure 
step-wise results of DHSNet 

G 

Sm 

0.8063 
0.8352 
0.7941 

4 
RCL 

Sm 

0.8154 
0.8451 
0.8074 

3 
RCL 

Sm 

0.8350 
0.8710 
0.8277 

2 
RCL 

Sm 

0.8496 
0.8757 
0.8425 

1 
RCL 

Sm 

0.8753 
0.8720 
0.8645 
G 

Sm 

0.8067 
0.8300 
0.7923 

FCN 

1 
RCL 

Sm 

0.8615 
0.8682 
0.8516 
DHSNet with RCLs substituted by traditional 
convolutional layers 
DHSNet(w/o 
RCLs) 
0.8622 
0.8685 
0.8516 

Comparison with other encoder-decoder networks 
DeconvNet 
[30] 
0.8493 
0.8661 
0.8396 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The results for component analysis on ECSSD dataset. The best results are shown in bold face.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Saliency driven total variation segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual-textual joint relevance learning for tag-based social image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bordeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autocollage. ACM Trans. Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="847" to="852" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Center-surround divergence of feature statistics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Background Prior-Based Salient Object Detection via Deep Reconstruction Residual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. CSVT</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fusing generic objectness and visual saliency for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Saliency Propagation from Simple to Difficult. in CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Salient region detection via high-dimensional color transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent Convolutional Neural Network for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks. in NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">FlowNet: Learning Optical Flow with Convolutional Networks. in ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Predicting eye fixations using convolutional neural networks. in CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Gaze I: Boosting saliency prediction with feature maps trained on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual Saliency Based on Multiscale Deep Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Networks for Saliency Detection via Local Estimation and Global Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Do Convnets Learn Correspondence? in NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="538" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs. in ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
