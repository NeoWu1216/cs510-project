<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Language Object Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
							<email>ronghang@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<email>rohrbach@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">ICSI</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@cs.uml.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Lowell</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<email>trevor@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Language Object Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Significant progress has been made in object detection in recent years; with the help of Convolutional Neural Networks (CNNs), it is possible to detect a predefined set of object categories with high accuracy <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, and the number of categories in object detection has grown over 10K to 100K with the help of domain adaptation <ref type="bibr" target="#b11">[12]</ref> and hashing <ref type="bibr" target="#b1">[2]</ref>. However, in practical application scenarios, instead of using a predefined fixed set of object categories, one would often prefer to refer to an object with natural language rather than use a predefined category label. Such natural language query can include different types of phrases such as categories, attributes, spatial configurations and interactions with other objects, such as "the young lady in a white dress  <ref type="figure">Figure 1</ref>. Overview of our method. Given an input image, a text query and a set of candidate locations (e.g. from object proposal methods), a recurrent neural network model is used to score candidate locations based on local descriptors, spatial configurations and global context. The highest scoring candidate is retrieved. sitting on the left" or "white car on the right" in <ref type="figure">Figure 1</ref>.</p><p>In this paper, we address the problem of natural language object retrieval: given an image and a natural language description of an object as query, we want to retrieve the object by localizing the object in the image. Natural language object retrieval can be seen as a generalization of generic object detection and has a wide range of applications, such as handling natural language commands in robotics where the user may ask to a robot to pick up "the TV remote control on the shelf".</p><p>We frame natural language object retrieval as a retrieval task on a set of candidate locations in a given image in this paper, as shown in <ref type="figure">Figure 1</ref>, where candidate locations can come from object proposal methods such as EdgeBox <ref type="bibr" target="#b32">[33]</ref>. We observe that simply applying text-based image retrieval systems on the image regions cropped from candidate locations for this task leads to inferior performance, as natural language object retrieval involves spatial configurations of objects and the global scene as context. For example, to decide how likely an object in a scene corresponds to "the man in a blue jacket sitting on the right in front of the house", one needs to look at both the object to determine whether it is "the man" (category), "in blue jacket" (attribute) and "sitting" (action), and its spatial configuration within the scene to determine whether it is "on the right", and the whole image as global contextual information to determine whether it is "in front of the house". Although both text-based image retrieval and natural language object retrieval involve jointly modeling images and text, they are different vision and language domains with domain shift from whole images to bounding boxes.</p><p>To address these issues, we propose the Spatial Context Recurrent ConvNet (SCRC) model to learn a scoring function that takes the text query, candidate regions, their spatial configurations and global context as input and outputs scores for candidate regions. Inspired by the Long-term Recurrent Convolutional Network (LRCN) <ref type="bibr" target="#b3">[4]</ref>, an effective recurrent architecture for both image captioning and image retrieval, we use a two-layer LSTM network structure where the embedded text sequence and visual features serve as input to the first layer and the second layer, respectively. However, we note that it is possible to build our model on other recurrent network architectures such as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Compared with other types of visual-linguistic models such as bag-of-words <ref type="bibr" target="#b26">[27]</ref>, one of the advantages of using a recurrent neural network as scoring function is that the whole model can be easily learned end-to-end via simple back propagation, allowing visual feature extraction and text sequence embedding to be adapted to each other, and we show that it significantly outperforms a previous method using bag-of-words. Another advantage is that it is easy to utilize relatively large scale image-text datasets from other domains like image captioning (e.g. MSCOCO <ref type="bibr" target="#b22">[23]</ref>) to learn a vision-language model, by first pretraining the model on the image captioning task, and then adapting it to natural language object retrieval task through fine-tuning. One of the main challenges for natural language object retrieval is the lack of large scale datasets with annotated object bounding box and description pairs. To address this issue, we show that it allows us to transfer visual-linguistic knowledge learned from the former task to the latter one by first pretraining on the image caption domain and then adapting it to the natural language object retrieval domain. This pretraining and adaptation procedure improves the performance and avoids over-fitting, especially when the object retrieval training dataset is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Natural language object retrieval. Based on a bag of words sentence model and embeddings derived from Ima-geNET classifiers, <ref type="bibr" target="#b9">[10]</ref> addresses a similar problem as ours and localizes an object within an image based on a text query. Given a set of candidate object regions, <ref type="bibr" target="#b9">[10]</ref> generates text from those candidates represented as bag-of-words using category names predicted from a large scale pretrained classifier and compares the word bags to the query text. Other methods generate visual features from query text and match them to image regions, e.g. through a text-based image search engine <ref type="bibr" target="#b0">[1]</ref> or learn a joint embedding of text phrases and visual features. Concurrent with our work, <ref type="bibr" target="#b23">[24]</ref> also proposes a recurrent network model to localize objects from given descriptions.</p><p>Grounding Objects from Image Descriptions. Given an image and its description sentence, <ref type="bibr" target="#b17">[18]</ref> aligns sentence fragments to image regions by embedding the detection results from a pretrained object detector and the dependency tree from a parser with a ranking loss. <ref type="bibr" target="#b16">[17]</ref> builds on <ref type="bibr" target="#b17">[18]</ref> and replaces the dependency tree with a bidirectional RNN. Canonical Correlation Analysis (CCA) is used in <ref type="bibr" target="#b25">[26]</ref> to learn a joint embedding of image regions and text snippets to localize each object mentioned in the caption. <ref type="bibr" target="#b21">[22]</ref> uses a structure prediction model to align text to image and reasons about object co-reference in text for 3D scene parsing. Concurrent with this paper, <ref type="bibr" target="#b27">[28]</ref> uses an attention model to ground referential phrases in image descriptions by attending to regions where the phrases can be best reconstructed.</p><p>Image Captioning. Image captioning methods take an input image and generate a text caption describing it. Recently, methods based on recurrent neural networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4]</ref> have shown to be effective on this task. LRCN <ref type="bibr" target="#b3">[4]</ref> is one of these recent successful methods and involves a two-layer LSTM network with the embedded word sequence and image features as input at each time step. We use LRCN as our base network architecture in this work and incorporate spatial configurations and global context into the recurrent model for natural language object retrieval.</p><p>Image Retrieval. Text-based image retrieval systems select from a set of images an image that best matches the query text. In image retrieval, a ranking function is learned through a recurrent neural network <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4]</ref>, metric learning <ref type="bibr" target="#b12">[13]</ref>, correlation analysis <ref type="bibr" target="#b20">[21]</ref> and other methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. It was shown in <ref type="bibr" target="#b3">[4]</ref> that a probabilistic image captioning model such as LRCN can also be used as an image retriever by using the probability of the query text sequence conditioned on the image p(S query |I) generated by image captioning model as a score for retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our model</head><p>In this section, we describe our Spatial Context Recurrent ConvNet (SCRC) model for natural language object retrieval and the training procedure in details. At test time, an image, a natural language object query and a set of candidate bounding boxes (e.g. from object proposal methods such as EdgeBox <ref type="bibr" target="#b32">[33]</ref>) are provided. The system needs to select from the candidate set a subset of bounding boxes that match the query text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial Context Recurrent ConvNet</head><p>Inspired by the architecture of LRCN <ref type="bibr" target="#b3">[4]</ref>, our Spatial Context Recurrent ConvNet (SCRC) model for natural language object retrieval consists of several components as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The model has three Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">[11]</ref> units denoted by LSTM language , LSTM local and LSTM global , a local and a global Convolutional Neural Network (CNN), a word embedding layer and a word prediction layer. At test time, given an image I, a query text sequence S and a set of candidate bounding boxes {b i } in I, the network outputs a score s i for the i-th candidate box b i based on local image descriptors x box on b i , spatial configuration x spatial of the box with respect to the scene, and global contextual feature x context .</p><p>In this work, the local descriptor x box is extracted by CNN local from local region I box on b i , and we use feature extracted by another network CNN global on the whole image I im as scene-level contextual feature x context . The spatial configuration of b i is an 8-dimensional representation</p><formula xml:id="formula_0">x spatial = [x min , y min , x max , y max , x center , y center , w box , h box ]</formula><p>(1) where w box and h box are the width and height of b i . We nor-malize image height and width to be 2 and place the origin at the image center, so that coordinates range from −1 to 1.</p><p>The words {w t } in the query text sequence S are represented as one-hot vectors and embedded through a linear word embedding matrix as Ew t , and processed by LSTM language as the input time sequence. At each time</p><formula xml:id="formula_1">step t, LSTM local takes in [h (t) language , x box , x spatial ] (con- catenation of the three vectors, where h (t)</formula><p>language is the hidden state from LSTM language ), and LSTM global takes in</p><formula xml:id="formula_2">[h (t) language , x context ]. Finally, based on h (t) local and h (t)</formula><p>global , a word prediction layer predicts the conditional probability distribution of the next word based on local image region I box , whole image I im , spatial configuration x spatial and all previous words it have seen so far, as</p><formula xml:id="formula_3">p(w t+1 |w t , · · · , w 1 , I box , I im , x spatial ) = Softmax(W local h (t) local + W global h (t) global + r) (2)</formula><p>where W local and W global are weight matrices for word prediction and r is a bias vector. Softmax(·) is a softmax function over a vector to output a probability distribution.</p><p>We note that when setting W local = 0 in Eqn. 2, our Spatial Context Recurrent ConvNet (SCRC) model is equivalent to the LRCN model <ref type="bibr" target="#b3">[4]</ref> for image captioning and image retrieval by only modeling p(S|I im ) to predict a text sequence S based on the whole image I im while ignoring I box and x spatial . This makes it possible to pretrain the model on the image captioning in Section 3.2 to obtain a good parameter initialization for visual-linguistic modeling, and transfer knowledge from large image captioning datasets.</p><p>We use VGG-16 net <ref type="bibr" target="#b29">[30]</ref> trained on ILSVRC-2012 dataset <ref type="bibr" target="#b28">[29]</ref> as the CNN architecture for CNN local and CNN global and extract 1000-dimensional fc8 outputs as x box and x context , and use the same LSTM implementation as in <ref type="bibr" target="#b3">[4]</ref>, where the gates are computed as</p><formula xml:id="formula_4">i t = σ(W xi x t + W hi h t−1 + b i ) (3) f t = σ(W xf x t + W hf h t−1 + b f ) (4) o t = σ(W xo x t + W ho h t−1 + b o ) (5) g t = tanh(W xg x t + W hg h t−1 + b g )<label>(6)</label></formula><p>All the three LSTM units have 1000-dimensional state h t . At test time, given an input image I, a query text S and a set of candidate bounding boxes {b i }, the query text S is scored on i-th candidate box using the likelihood of S conditioned on the local image region, the whole image and the spatial configuration of the box, computed as</p><formula xml:id="formula_5">s = p(S|I box , I im , x spatial ) = wt∈S p(w t |w t−1 , · · · , w 1 , I box , I im , x spatial )(7)</formula><p>and the highest scoring candidate boxes are retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Knowledge transfer from image captioning</head><p>To exploit paired image-text data in image captioning datasets, and to learn a good initialization of parameters in word embedding, word prediction and three LSTM units, we first pretrain our model on an image captioning dataset, by restricting W local = 0 in Eqn. 2, which is equivalent to training a LRCN model <ref type="bibr" target="#b3">[4]</ref>. We follow the procedure in <ref type="bibr" target="#b3">[4]</ref> for pretraining on image captioning. During pretraining, the probability of ground truth image caption p(S gt |I im ) is maximized over the training image-sentence pairs, and the whole network is optimized with standard Stochastic Gradient Descent (SGD). We refer to <ref type="bibr" target="#b3">[4]</ref> for the training details on image captioning.</p><p>Since we restrict W local = 0 in Eqn. 2 during pretraining, the parameters in LSTM local are not learned. To obtain a good initialization of this unit, we copy those weights in Eqn. 3 -6 from LSTM global to LSTM local . The weights over the extra 8 dimensions of x spatial are initialized with zero. We also copy W global to W local to initialize word prediction weights.</p><p>After pretraining on the image captioning task, the parameters in our model already encode useful knowledge of word embedding and decoding and sequence prediction based on image features. The knowledge is transferred to the natural language object retrieval task in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training for object retrieval</head><p>After pretraining, we adapt the SCRC model to natural language object retrieval. In this paper, we assume that the training dataset consists of N images, with each image containing M i (i = 1, · · · , N ) annotated objects, and each object annotated by a bounding box and K i,j (i = 1, · · · , N , j = 1, · · · , M i ) text descriptions (an object can be described more than once with different descriptions). At training time, each instance is an image-bounding box-description tuple (I i , b i,j , S i,j,k ), where I i is the whole image, b i,j = [x min , y min , x max , y max ] is the bounding box of the j-th object and S i,j,k is a description text in natural language such as "the black and white cat".</p><p>Our model for natural language object retrieval can be trained via maximizing the probability of the object description text in ground truth annotations conditioned on the local image region I box and the whole image I im as context, which is analogous to training a generic object detection system. Many state-of-the-art generic object detectors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> are built by turning object detection into a classification problem on candidate bounding boxes produced either from a sliding window or an object proposal mechanism, and a classifier is trained by maximizing the probability of ground truth object category label. In natural language object retrieval, the description text of an object can be seen as a generalized "label" of the object, and maximizing its conditional probability is similar to training a "generalized classifier" whose output is a sequence of word labels rather than a single category label.</p><p>Given a natural language object retrieval dataset, we construct all tuples (I i , b i,j , S i,j,k ) from the ground truth annotations as training instances (multiple tuples are constructed if there are multiple descriptions for the same object). For each annotated object in the training set, an image patch I box is cropped from the whole image I im based on bounding box of that object region, with its spatial configuration x spatial constructed through Eqn. 1. We define the loss function during training as</p><formula xml:id="formula_6">L = − N i=1 Mi j=1</formula><p>Ki,j k=1 log(p(S i,j,k |I boxi,j , I imi , x spatial i,j )) <ref type="bibr" target="#b7">(8)</ref> where N is the number of images, M i is the number of annotated objects in i-th image, K i,j is the number of natural language descriptions associated with the j-th object in that image, and p(S i,j,k |I boxi,j , I imi , x spatial i,j ) is computed by Eqn. 7.</p><p>During training, the model parameters are initialized from the pretrained network in Section 3.2, and fine-tuned using SGD with a smaller learning rate, allowing the network to adapt to natural language object retrieval domain. The whole network is trained end-to-end via back propagation. Our model is implemented using Caffe <ref type="bibr" target="#b15">[16]</ref> and our code and data are available at http://ronghanghu. com/text_obj_retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on different datasets from small scale to relatively large scale. More experimental results can be found in the supplementary material or in <ref type="bibr" target="#b14">[15]</ref>. Since <ref type="bibr" target="#b9">[10]</ref> solves a similar problem to our paper, we adopt it as our baseline. In <ref type="bibr" target="#b9">[10]</ref>, a large scale fine-grained classifier of 7K object classes is trained on ImageNET <ref type="bibr" target="#b2">[3]</ref>. Each box in the candidate set is classified into one of the 7K classes, and a bag of words is extracted from the predicted object class based on its ImageNET <ref type="bibr" target="#b2">[3]</ref> synset containing category name and synonyms. Then, the word bag is projected to a vector space, and matched to the projected query text using cosine distance to obtain a score. The sentence projection (embedding) in <ref type="bibr" target="#b9">[10]</ref> is predefined and the only training involved in training the 7K object classifier. Note that <ref type="bibr" target="#b9">[10]</ref> also proposes an instance match model that relies on online APIs at test time. As in this work we assume a self-contained system without resorting to other APIs, we only use the category model (CAFFE-7K) in [10] as our baseline.</p><p>As our recurrent architecture is inspired by LRCN <ref type="bibr" target="#b3">[4]</ref>, which is shown to be effective for both image captioning and image retrieval, we also compare our model to LRCN. We use the LRCN model trained on MSCOCO <ref type="bibr" target="#b22">[23]</ref> for image captioning task as an object retriever by evaluating it on candidate bounding boxes. Given an image I with a set of candidate boxes and a query text S query , we compute p(S query |I box ), the probability of the query text S query conditioned on the local image region I box outputted by LRCN as a score for each box in the candidate set, and retrieve highest scoring candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object retrieval evaluation on ReferIt dataset</head><p>The ReferIt dataset <ref type="bibr" target="#b18">[19]</ref> is the biggest publicly available dataset containing image regions with descriptions at the time of writing. It contains 20,000 images from IAPR TC-12 dataset <ref type="bibr" target="#b8">[9]</ref>, together with segmented image regions from SAIAPR-12 dataset <ref type="bibr" target="#b4">[5]</ref>, and 120K annotated descriptions for the image regions collected in a two-player game that aims to make the image region identifiable from the annotation. The ReferIt dataset also contains some ambiguous (e.g. "anywhere") and mistakenly annotated examples where the annotation does not correspond to any object. To evaluate on this dataset, we split the 20,000 images (together with their annotations) into 10,000 for training and validation and 10,000 for test, and construct image-bounding box-description tuples on all annotated image regions as training instances. There are 59,976 (image, bounding box, description) tuples in the trainval set and 60,105 in the test set. In our experiments on this dataset, we only use the bounding boxes of annotated regions during training and evaluation. The bounding boxes are obtained from the segmentation regions in SAIAPR-12 dataset corresponding to the clicks by annotators. Note that although <ref type="bibr" target="#b18">[19]</ref> introduces the ReferIt dataset, it does not propose a baseline method for object retrieval based on text query.</p><p>As described in Section 3, we first pretrain a SCRC model on MSCOCO dataset <ref type="bibr" target="#b22">[23]</ref> for image captioning. The training details such as hyper-parameters of SGD follow <ref type="bibr" target="#b3">[4]</ref>. After pretraining, we copy the weights in LSTM and the word prediction layer to the local part of the network as mentioned in Section 3.2. Then the pretrained SCRC model is adapted to the natural language object retrieval task following the procedure in Section 3.3. The model is finetuned on image-bounding box-description tuples in ReferIt trainval set with back propagation.</p><p>Ablations. To test the effect of incorporating spatial configurations x spatial and scene-level contextual feature x context , we evaluate different setups during fine-tuning on ReferIt. By setting x spatial and W global to 0 during finetuning and testing, the model can only learn to score a box based on local image descriptors x box from candidate boxes, denoted by SCRC (w/o context, spatial). Similarly, by setting W global to 0, the model can learn a scoring function on x box and x spatial but cannot utilize scene-level context, denoted by SCRC (w/o context).</p><p>As a comparison, we directly trained a SCRC model on ReferIt without first pretraining on MSCOCO, and set x spatial and W global to 0 during training and testing, denoted by SCRC (w/o context, spatial, transfer). The CNN parameters in the model are initialized from VGG-16 net <ref type="bibr" target="#b29">[30]</ref> and other parameters are randomly initialized. In all the training above, the whole SCRC model is trained endto-end with SGD, allowing visual feature extraction and textual sequence prediction to be optimized jointly.</p><p>At test time, all the 4 SCRC models mentioned above, the bag-of-words model (CAFFE-7K) in <ref type="bibr" target="#b9">[10]</ref> and LRCN <ref type="bibr" target="#b3">[4]</ref> as an object retriever on candidate boxes are compared on the ReferIt test set. The LRCN model is trained on MSCOCO dataset for image captioning as described in <ref type="bibr" target="#b3">[4]</ref> to learn a probabilistic generative model p(S|I), and we use it to score a candidate region I box based on a text query S query by computing the probability of the text conditioned on the local region, i.e. p(S query |I box ) as a baseline.</p><p>We evaluate with two testing scenarios: In the first scenario similar to the experiment in <ref type="bibr" target="#b9">[10]</ref>, given an image and a text query, the model is asked to retrieve the corresponding image region from all annotated regions in that image. In the second scenario, which is a harder task but closer to real applications, given a text query the model retrieves an image region from a set of candidate bounding boxes produced by object proposal methods. A retrieved region is considered as correct if it overlaps with ground truth bounding box by at least 50% IoU. In this experiment, we use top 100 proposals from EdgeBox <ref type="bibr" target="#b32">[33]</ref> as our candidate box set.</p><p>Results. <ref type="table">Table 1</ref> shows the top-1 precision (the percentage of the highest-scoring region being correct) in the first scenario where the candidate set is all annotated boxes in the image. Note that CAFFE-7K cannot return informative results when none of the words in query are in its category names (leading to an empty bag and same score for all regions), whereas our SCRC model can always return deterministic result since it can represent unknown words with "&lt;unk&gt;". Similar to <ref type="bibr" target="#b9">[10]</ref>, we evaluate with "P@1-NR" corresponding to non-random top-1 precision computed on the those informative results and "P@1" corresponding to top-1 precision on all cases including non-informative results, where random guess is used. Results show that our full SCRC model achieves the highest top-1 precision. In <ref type="table">Table 1</ref>, it can be seen that pretraining on image captioning, adding spatial configuration, and adding scene-level context all improve the performance, with adding spatial configuration x spatial leading to the most significant performance boost. This is no surprise, as spatial configuration not only benefits in cases where spatial relationship is directly involved in the query (e.g. "the man on the left"), but also enables the network to learn a prior distribution of object locations (e.g. "sky" is usually at the top of the scene while "ground" is usually at the bottom). <ref type="table">Table 2</ref> shows the result of the second scenario on 100 EdgeBox proposals, where "R@1" is the recall of the highest scoring box (the percentage of the highest scoring box being correct), and "R@10" is the percentage of at least one of the 10 highest scoring proposals being correct. We also report "Oracle" (or equivalently "R@100"), the percentage of at least one of all 100 proposals being correct, as an upper-bound of all object retrieval systems in this scenario. It can be seen that results in <ref type="table">Table 2</ref> follow the same trend as in <ref type="table">Table 1</ref>, with our full SCRC model achieving the highest recall. <ref type="figure" target="#fig_3">Figure 4</ref> shows examples of correctly retrieved objects at top-1 using 100 EdgeBox proposals, where the highest scoring candidate region from our SCRC model overlaps with ground truth annotation by at least 50% IoU, and Figure 5 shows some failure cases, where retrieved top-1 candidate region fails to match ground truth.</p><p>By comparing "SCRC (w/o context, spatial)" and "SCRC (w/o context, spatial, transfer)" in <ref type="table">Table 1</ref> and Table 2, it can also be seen that the pretraining and adaptation procedure described in Section 3 outperforms directly training on retrieval dataset, showing that pretraining allows the model to transfer useful visual-linguistic knowledge from image captioning dataset. Also, our SCRC model outperforms the bag-of-words CAFFE-7K model and LRCN model significantly. Compared with our model, CAFFE-7K method suffers from information loss by first projecting image region to category names and limited vocabulary drawn from predefined object category names, and is not end-to-end trainable. Although LRCN model trained on MSCOCO for image captioning task is effective for text-based image retrieval as shown in <ref type="bibr" target="#b3">[4]</ref>, directly running it as an object retriever on a set of candidate boxes results in inferior performance. This is because object retrieval and image retrieval are different domains, and LRCN model as a object retriever does not encode spatial configuration or global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object retrieval evaluation on Kitchen dataset</head><p>We also evaluate and compare our method with the baseline model <ref type="bibr" target="#b9">[10]</ref> on the same Kitchen dataset as used in <ref type="bibr" target="#b9">[10]</ref>. Kitchen is a dataset with 606 images sampled from the kitchen/household sub-tree of ImageNET hierarchy <ref type="bibr" target="#b2">[3]</ref>, with 10 different descriptions annotated for each image. Since objects in this dataset almost occupy the entire images, instead of using retrievals on candidate object proposals boxes, in <ref type="bibr" target="#b9">[10]</ref> the performance of the object retrieval is evaluated at image-level. During testing, for each query text, the candidate set consists of 11 images with ground truth and 10 distractors. The distractors are sampled either from the same Kitchen dataset ("Kitchen" experiment) or from the whole ImageNET ("ImageNET" experiment), with the latter being an easier task. Performance of object retrieval is evaluated using top-1 precision.</p><p>To evaluate our method on this dataset, we split the dataset into two parts, with 300 images as trainval set and 306 images as test set. Similar to Section 4.1, we first pretrain a SCRC model on MSCOCO dataset <ref type="bibr" target="#b22">[23]</ref> for image captioning, and then fine-tune the model on the trainval set. The our model is tested through image-level retrieval on the candidate set of ground truth and 10 distractors, where we use the feature extracted from the entire image as x box . Since the dataset involves no spatial configurations or scene-level contextual information, we set x spatial and W global in Eqn. 2 to zero during fine-tuning and testing, so query='whisk with red tipped handle' query='mobile phone the pink color' the model can only learn to score a candidate based x box . As this dataset is a much smaller than ReferIt, we observe that transferring knowledge from MSCOCO significantly boosts the performance and avoids overfitting.</p><p>Results. <ref type="table" target="#tab_2">Table 3</ref> shows the top-1 precision (P@1) of our method together with the baseline on the test set. The first column "Kitchen" corresponds to sampling the 10 distractors from the same Kitchen dataset, while the second column corresponds to sampling distractors from the whole ImageNET 7K dataset <ref type="bibr" target="#b2">[3]</ref>. Similar to Section 4.1, LRCN refers to directly running LRCN model on the candidate images as a retriever. SCRC (w/o context, spatial, transfer) refers to the SCRC model directly trained on the trainval part of the Kitchen dataset, with convolutional layer initialized from VGG-16 net, and LSTM unit, word embedding and word prediction weights randomly initialized. SCRC (w/o context, spatial) corresponds to first pretraining on MSCOCO and then fine-tuning on Kitchen trainval set as described in Section 3. As the dataset contains no spatial configuration or scene-level context information, we cannot test our full SCRC model on it. It can be seen from <ref type="table" target="#tab_2">Table  3</ref> that in both scenarios, pretraining on image captioning and fine-tune on natural language object retrieval leads to the best performance, outperforming the baseline bag-ofwords model CAFFE-7K and LRCN. <ref type="figure" target="#fig_2">Figure 3</ref> shows some correctly retrieved object examples from Kitchen dataset, where the highest scoring candidate matches the ground truth. Both the ground truth and the 10 distractor images are sampled from the same Kitchen dataset in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Moreover, as Kitchen dataset has only 606 objects and is more than 100 times smaller than ReferIt dataset, "SCRC (w/o context, spatial)" has significantly higher accuracy than "SCRC (w/o context, spatial, transfer)". This shows that pretraining on MSCOCO for image captioning dataset improves the performance of natural language object retrieval significantly on this relatively smaller dataset, by transferring the visual-linguistic knowledge from the former task to the latter task. As a reference, we note that <ref type="bibr" target="#b9">[10]</ref> also uses an instance model and achieves higher overall performance. The instance model sends the query and candidate image regions to online APIs such as Google Image Search and FreeBase on the fly at test time. As in this</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Kitchen ImageNet CAFFE-7K <ref type="bibr" target="#b9">[10]</ref> 51.34% 57.50% LRCN <ref type="bibr" target="#b3">[4]</ref> 40.35% 63.22% SCRC (w/o context, spatial, transfer) 54.02% 74.08% SCRC (w/o context, spatial) 61.62% 81.15% work we assume a self-contained system that can be applied without resorting to Internet APIs on the fly, we only compare with the category model CAFFE-7K in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we address natural language object retrieval with Spatial Context Recurrent ConvNet (SCRC), a recurrent neural network model that scores a candidate box based on local image descriptors, spatial configurations and global scene-level context. We show that incorporation of spatial configuration and global context improves the performance of natural language object retrieval significantly. The recurrent network model used in our method leads to an end-to-end trainable scoring function, which significantly outperforms baseline methods.</p><p>Also, we demonstrate that natural language object retrieval can benefit from transferring knowledge learned on image captioning through pretraining and adaptation. As one of the difficulties for natural language object retrieval systems is the lack of large datasets with object-level annotation, we show that this problem can be alleviated by exploiting datasets with image-level annotations, which are often easier to collect than object-level descriptions. As follow up to this work we show successful results by encoding the phrase rather than scoring it <ref type="bibr" target="#b27">[28]</ref> and also predicting image segmentations instead of bounding boxes <ref type="bibr" target="#b13">[14]</ref>.</p><p>Acknowledgments. M. Rohrbach was supported by a fellowship within the FITweltweit-Program of the German Academic Exchange Service (DAAD). J. Feng was supported by NUS startup grant R263000C08133. This work was supported by DARPA, AFRL, DoD MURI award N000141110688, NSF awards IIS-1427425 and IIS-1212798, and the Berkeley Vision and Learning Center.  query='man on right blue gloves' query='the piece with no shadows' query='face' query='rock' query='water on the right only' query='dirt patch next to car (right side)' <ref type="figure">Figure 5</ref>. Failure cases (IoU &lt; 0.5) on ReferIt with EdgeBox. Ground truth in yellow and incorrectly retrieved box in red. Some failures cases are caused by ambiguity of the query and some due to wrong annotations in the dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>man in middle with blue shirt and blue shorts' score box =p(S='man in middle with blue shirt and blue shorts' man in middle with blue shirt and blue shorts' Our Spatial Context Recurrent ConvNet (SCRC) for natural language object retrieval. The recurrent network in our model contains three LSTM units. Two CNN's are used to extract local image descriptors and global scene-level contextual feature respectively. Parameters in word embedding, word prediction and three LSTM units are initialized by pretraining on image captioning dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Correctly retrieved examples in Kitchen dataset, where the highest scoring object (green) matches ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Correctly localized examples (IoU ≥ 0.5) on ReferIt with EdgeBox. Ground truth in yellow and correctly retrieved box in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Performance of different methods on the Kitchen dataset. See Section 4.2 for details.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>query='man squatting' query='standing guy' query='bike wheels' query='white hat' query='Window with closed curtains' query='right lake' query='bird on the left' query='leaves of left tree' query='pillar building in the middle'</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple queries for large scale specific object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast, accurate detection of 100,000 object classes on a single machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1814" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The segmented and annotated iapr tc-12 benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Villaseñor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="428" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The iapr tc-12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop OntoImage</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open-vocabulary object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lsda: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3536" to="3544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning distance metrics with contextual constraints for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2072" to="2078" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06180</idno>
		<title level="m">Segmentation from natural language expressions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04164</idno>
		<title level="m">Natural language object retrieval</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fisher vectors derived from hybrid gaussian-laplacian mixture models for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7399</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What are you talking about? text-to-image coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image question answering: A visual semantic embedding model and a new dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03745</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
