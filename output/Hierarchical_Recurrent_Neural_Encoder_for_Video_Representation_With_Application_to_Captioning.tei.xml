<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University † University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
							<email>zhongwen.s.xu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University † University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University † University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<email>wufei@cs.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University † University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@cs.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University † University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep learning approach, especially deep Convolutional Neural Networks (ConvNets), have achieved overwhelming accuracy with fast processing speed for image classification. Incorporating temporal structure with deep ConvNets for video representation becomes a fundamental problem for video content analysis. In this paper, we propose a new approach, namely Hierarchical Recurrent Neural Encoder (HRNE), to exploit temporal information of videos. Compared to recent video representation inference approaches, this paper makes the following three contributions. First, our HRNE is able to efficiently exploit video temporal structure in a longer range by reducing the length of input information flow, and compositing multiple consecutive inputs at a higher level. Second, computation operations are significantly lessened while attaining more nonlinearity. Third, HRNE is able to uncover temporal transitions between frame chunks with different granularities, i.e. it can model the temporal transitions between frames as well as the transitions between segments. We apply the new method to video captioning where temporal information plays a crucial role. Experiments demonstrate that our method outperforms the state-of-the-art on video captioning benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Incorporating temporal information into video representation has long been a fundamental problem in computer vision. Earlier works such as Dense Trajectories <ref type="bibr" target="#b38">[39]</ref> and improved Dense Trajectories (iDT) <ref type="bibr" target="#b39">[40]</ref> typically utilize optical flow to extract temporal information and hand-crafted features to model appearances and motions. With the recent success of deep Convolutional Neural Networks (Con-vNets) both in efficiency and efficacy, we have witnessed a new trend in leveraging ConvNets to infer video representation. Xu et al. <ref type="bibr" target="#b40">[41]</ref> propose to utilize Vector of Locally Ag-gregated Descriptors (VLAD) <ref type="bibr" target="#b12">[13]</ref> to aggregate frame level ConvNet for video representation, which is unable to capture temporal structure. Simonyan and Zisserman <ref type="bibr" target="#b25">[26]</ref> combine stacked optical flow frames and RGB streams to train ConvNets for video classification, which achieves comparable performance to iDT in action recognition. A limitation of two-stream ConvNets <ref type="bibr" target="#b25">[26]</ref> and iDT <ref type="bibr" target="#b39">[40]</ref> is that both algorithms require optical flow as input, which is expensive to extract (it takes usually 0.06 seconds to extract optical flow between a pair of frames <ref type="bibr" target="#b25">[26]</ref>), but is only able to capture temporal information in video clips of short duration.</p><p>To avoid extracting optical flow, 3D ConvNets are proposed in <ref type="bibr" target="#b34">[35]</ref> to generate a video representation, with emphasis on efficiency improvement. This approach, however, can only cope with 16 frames or so each time <ref type="bibr" target="#b34">[35]</ref>. Very recently, Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">[11]</ref> has been applied to video analysis <ref type="bibr" target="#b19">[20]</ref>, inspired by the general recurrent encoder-decoder framework <ref type="bibr" target="#b30">[31]</ref>. A plausible feature of LSTM is that LSTM is capable of modeling data sequences. However, as this paper tries to cope with, there are still a few challenges remain unaddressed.</p><p>First, a large number of long-range dependencies are usually difficult to capture. Even though LSTM can deal with long video clips in principal, it has been reported that the favorable length of video clips to be feed into LSTM falls in the range of 30 to 80 frames <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>. In order to model longer video clips while attaining similar good performance as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>, we propose to divide a long video clip into a few short frame chunks, feed the chunks into LSTM, and composite the LSTM outputs of the frame chunks into one vector, which can then be fed into another LSTM at a higher level to uncover the temporal information among the composited vectors over a longer duration. Such a hierarchical structure significantly reduces the length of input information flow but is still capable of exploiting temporal information over longer time afterwards at a higher level.</p><p>Second, additional non-linearity has been demonstrated helpful for improving model training for visual tasks such as image and video classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref>. A straightforward way of adding non-linearity into LSTM is stacking <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20]</ref>. Despite of the improved performance, a major disadvantage of stacking is that it introduces a long path from the input to the output video vector representation, thereby resulting in heavier computational cost. As we will discuss in details later, Hierarchical Recurrent Neural Encoder (HRNE) proposed in this paper dramatically shortens the path with the capability of adding non-linearity, providing a better trade-off between efficiency and effectiveness.</p><p>Third, video temporal structures are intrinsically layered. Suppose a video of birthday party consists of three actions, e.g., blowing candles, cutting cake, and eating cake. As the three actions usually take place sequentially, i.e., there are strong temporal dependencies among them, we need to appropriately model the temporal structure among the three actions. In the meantime, the temporal structure within each action should also be exploited. To this end, we need to model video temporal structure with multiple granularities. Unfortunately, straightforward implementation of LSTM can not achieve this goal.</p><p>The proposed HRNE framework models video temporal information using a hierarchical recurrent encoder and can effectively deal with the three aforementioned challenges. While HRNE is a generic video representation, we apply it to video captioning to test the performance, because temporal information plays a key role in video captioning. Two widely-used video captioning datasets, the Microsoft Research Video Description Corpus (MSVD) <ref type="bibr" target="#b4">[5]</ref> and the Montreal Video Annotation Dataset (M-VAD) <ref type="bibr" target="#b33">[34]</ref>, are used in our experiments, which demonstrate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Dense Trajectories <ref type="bibr" target="#b38">[39]</ref> and its improved version: improved Dense Trajectories <ref type="bibr" target="#b39">[40]</ref> have dominated the filed of action recognition and general video classification tasks such as complex event detection. Dense Trajectories applies dense sampling to get the interest points along the video and then tracks the points in a short time period. Local descriptors such as HOG, HOF and MBH are extracted along the tracklets. Bag-of-Words (BoWs) <ref type="bibr" target="#b27">[28]</ref> and Fisher vector encoding <ref type="bibr" target="#b24">[25]</ref> are then applied to accumulate the local descriptors and generate the video representation.</p><p>Besides the hand-crafted visual features like Dense Trajectories, researchers have started exploring the Convolutional Neural Networks (ConvNets) on video representation recently. Karpathy et al. <ref type="bibr" target="#b14">[15]</ref> first introduce Con-vNets which are similar with Krizhevsky et al. <ref type="bibr" target="#b16">[17]</ref> into video classification, and different fusion strategies are explored to combine information over the temporal domain in this work. In order to better capture temporal information in action recognition, Simonyan and Zisserman <ref type="bibr" target="#b25">[26]</ref> propose to utilize stacked optical flow frames as inputs to train the ConvNets, which, together with the RGB stream, achieves comparable performance as the state-of-the-art hand-crafted features <ref type="bibr" target="#b39">[40]</ref> on action recognition. Tran et al. <ref type="bibr" target="#b34">[35]</ref> utilize 3D ConvNets to learn temporal information without optical flows, which is inspired by Ji et al. <ref type="bibr" target="#b13">[14]</ref> and Simonyan and Zisserman <ref type="bibr" target="#b26">[27]</ref>. Xu et al. <ref type="bibr" target="#b40">[41]</ref> propose to utilize VLAD <ref type="bibr" target="#b12">[13]</ref> aggregation on frame-level ConvNet features and it directly adapts ImageNet pretrained image classification model to video representation.</p><p>All these works mentioned above utilize either average pooling or encoding methods such as Fisher vector and VLAD over time to generate a global video feature from a set of local features. However, time dependency information is lost since average pooling and encoding methods always ignore the order of the input sequences, i.e., taking the local features as a set rather than a sequence. To tackle this problem, Ng et al. <ref type="bibr" target="#b19">[20]</ref> introduce Long Short-Term Memory (LSTM) to model the temporal order, inspired by the general sequence to sequence learning neural model proposed by Sutskever et al. <ref type="bibr" target="#b30">[31]</ref>. Stacked LSTM is applied in <ref type="bibr" target="#b19">[20]</ref>, where each layer of the LSTM retains the same time scale. Different from the standard approach which stackes LSTM layers into multilayered one and simply aims to introduce more non-linearity into the neural model, the Hierarchical Recurrent Neural Encoder proposed in this work aims to abstract the visual information at different time scales, and learns the visual features with multiple granularities.</p><p>In the application of video captioning, Donahue et al. <ref type="bibr" target="#b8">[9]</ref> introduce the LSTM into this task by feeding the Conditional Random Field (CRF) outputs of objects, subjects, and verbs into the LSTM to generate video description. The same as <ref type="bibr" target="#b8">[9]</ref>, other works such as <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b20">21]</ref> utilize the LSTM essentially as a recurrent neural network language model to generate video descriptions, which conditions on either the average pooled frame-level features or the context vector linearly blended by the attention mechanism <ref type="bibr" target="#b0">[1]</ref>. In contrast to these works, we study better video content understanding from the visual feature aspects instead of language modeling ones. Based on stacked LSTM, Venugopalan et al. <ref type="bibr" target="#b36">[37]</ref> is the only attempt to utilize LSTMs as both visual encoder and language decoder in the video captioning task, which is inspired by the general neural encoder-decoder framework <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7]</ref> as well.</p><p>In the area of query suggestion, Sordoni et al. <ref type="bibr" target="#b28">[29]</ref> propose a hierarchical recurrent neural network for contextaware query suggestion in a search engine. In this model, the text query in a session is firstly abstracted by one RNN layer into the query-level state, another RNN layer is used to learn session-level dependency and then, the session-level hidden states is utilized to make suggestions for users.</p><p>Contemporary to this work, Yu et al. <ref type="bibr" target="#b42">[43]</ref> introduce a hierarchical RNN decoder, specifically Gated Recurrent Unit (GRU) <ref type="bibr" target="#b6">[7]</ref>, into the video captioning system. A sentence generator consisting of a GRU layer conditions on visual feature, and then a paragraph generator accepts sentence vector and the context to generate paragraph level description, which essentially learns the time dependencies between sentences, and works on the language processing aspects. In contrast, this work is focusing on learning good visual feature, i.e., the encoder part, but not the language processing, i.e., the decoder part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Approach</head><p>We propose a Hierarchical Recurrent Neural Encoder (HRNE) model for video processing tasks. Assume we have n frames in the video, based on the HRNE model, we develop a general video encoder which takes the frame-level visual features from a video sequence (x 1 , x 2 , ..., x n ) as input and outputs a single vector v as the representation for the whole video. For the video captioning task specifically, we keep the single layer LSTM decoder as a recurrent neural network language model <ref type="bibr" target="#b30">[31]</ref>, which conditions on the video feature vector v, similar to previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Recurrent Neural Network</head><p>The recurrent neural network is a natural extension of feedforward neural networks on modeling sequence. Given an input sequence (x 1 , x 2 ..., x n ), a standard RNN computes the output sequence (z 1 , z 2 ..., z n ) by iterating the following equations:</p><formula xml:id="formula_0">h t = tanh(W hx x t + W hh h t−1 ), (1) z t = W zh h t ,<label>(2)</label></formula><p>The RNN can map the inputs to the outputs whenever the alignment between inputs and outputs is provided. The standard RNN would work principally, but it is really difficult to train the standard RNN due to the vanishing gradient problem <ref type="bibr" target="#b2">[3]</ref>. The Long Short-Term Memory (LSTM) is known to learn patterns with wider range temporal dependencies. We now introduce the LSTM model.</p><p>The core of the LSTM model is a memory cell c t which records the history of the inputs observed up to that time step. c t is a summation of the previous memory cell c t−1 modulated by a sigmoid gate f t , and g t , a function of previous hidden state and the current input modulated by another sigmoid gate i t . The sigmoid gates can be thought as knobs that LSTM learns to selectively forget its memory or accept current input. The cell has three gates. The input i t gate controls whether the LSTM will consider current input x t .</p><p>The forget gate f t is used to control whether LSTM will forget the previous memory c t−1 . The output gate o t controls how much information will be transferred from memory c t to hidden state h t . There are several widely used LSTM variants and we use the LSTM unit described in <ref type="bibr" target="#b43">[44]</ref> in our model, which iterates as follows:</p><formula xml:id="formula_1">i t = σ(W ix x t + W ih h t−1 + b i ), (3) f t = σ(W f x x t + W f h h t−1 + b f ), (4) o t = σ(W ox x t + W oh h t−1 + b o ), (5) g t = φ(W gx x t + W gh h t−1 + b g ), (6) c t = f t ⊙ c t−1 + i t ⊙ g t , (7) h t = o t ⊙ φ(c t ),<label>(8)</label></formula><p>where σ is the sigmoid function, φ is the hyperbolic tangent function tanh, ⊙ donates element-wise product, W * x is the transform from the input to LSTM states, W * h is the recurrent transformation matrix between the hidden states and b * is the biases vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Recurrent Neural Encoder</head><p>It has been reported that adding more non-linearity is helpful for vision tasks <ref type="bibr" target="#b26">[27]</ref>. The performance of LSTM can be improved if additional non-linearity is added. A straightforward way is stacking multiple layers, which, however, will increase computation operations. Inspired by the Con-vNet operations in spatial domain, we propose a Hierarchical Recurrent Neural Encoder (HRNE) model. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in a ConvNet model, a filter is used to explore the spatial visual information of an image by performing convolution calculation between image patch matrix I and a learnable filter matrix H:</p><formula xml:id="formula_2">y = h ∑ i=1 w ∑ j=1 I i,j H i,j ,<label>(9)</label></formula><p>where w denotes the number of columns of the filter matrix, h denotes the number of rows, H i,j denotes the matrix item located in the i-th row and j-th column and y is the convolution calculation result. The filter is applied over the whole image to generate the filtered image, which is further forwarded into the next layer. Similarly, in temporal domain, we introduce an additional layer, instead of stacking, by which only short LSTM chains need to be dealt with. The filters in ConvNet's convolutional layer are well suited for exploring local spatial structure. Analogously, using temporal filter to explore the local temporal structure is presumed to be beneficial since videos always consist of several incoherence clips.</p><p>The main difficulty of introducing additional layers into temporal modeling is finding a proper temporal filter. In spatial domain, the output of filter is independent from spatial location, and a matrix can be used as a filter. Differently, in temporal domain, there is certain temporal dependencies between consecutive items. As a result, a matrix is not sufficient to be used as a temporal filter. Since RNN is well In ConvNet a learnable filter is applied to each location to generate a filtered image which is further forwarded to the next layer. In HRNE, a learnable filter (i.e., LSTM) along with attention mechanism is applied to each temporal time step to generate a sequence of video chunk vectors, which are further forwarded to the next layer.</p><p>suited for temporal dependency modeling, we adopt short RNN chains as the temporal filters in our HRNE model. Specifically, we use LSTM chains in this paper and take the mean of all LSTM chain's hidden states as the filtering result.</p><p>We first divide an input sequence (</p><formula xml:id="formula_3">x 1 , x 2 , . . . , x T ) into several chunks (x 1 , x 2 , ..., x n ), (x 1+s , x 2+s , . . . , x n+s ), . . . , (x T −n+1 , x T −n+2 , . . . , x T ),</formula><p>where s is stride and it denotes the number of temporal units two adjacent chunks are apart. After inputting these subsequences into the LSTM filter, we will get a sequence of feature vectors h 1 , h 2 , .., h ⌈T /n⌉ , where ⌈x⌉ denotes the least integer among those integers which are larger than x. Each feature vector in h 1 , h 2 , .., h ⌈T /n⌉ gives a proper abstract of its corresponding clip. To get the feature vector of the whole video, we propose to use another LSTM layer to summarize all these feature vectors. We combine these two LSTM layers and build our HRNE model. The first LSTM layer serves as a filter and it is used to explore local temporal structure within subsequences. The second LSTM learns the temporal dependencies among subsequences. We note that more complex HRNE model could be adding more layers to build multiple time-scale abstraction of the visual information.</p><p>A large number of long-range dependencies are usually difficult to capture. Even though LSTM can deal with long video clips in principal, we compare HRNE with stacked multilayered LSTM in <ref type="figure" target="#fig_2">Figure 2</ref>. The red line in the <ref type="figure" target="#fig_2">Figure 2</ref> shows how the input at t = 1 flows though the model to the final output. We are used to set the stride to be the same as the LSTM filter length. For an input sequence of length T and a LSTM filter of length n, the red line in HRNE model goes through n + ⌈T /n⌉ LSTM units, which means the input at t = 1 will only flow through n + ⌈T /n⌉ steps to the output rather than T + 1 steps if stacked RNN is used. If T = 1, 000 and we set n to be 30, then HRNE will only go through 64 steps rather than 1,001 steps. Fewer steps an in-put will go through before it reaches the output means that it's easier to backtrack, so our HRNE is easier for stochastic gradient methods via Back-propagation Through Time (BPTT) to train.</p><p>Since the recently proposed soft attention mechanism from <ref type="bibr" target="#b0">[1]</ref> has achieved great success in several sequence modeling tasks, we integrate the attention mechanism into our HRNE model. We next introduce the attention mechanism part.</p><p>The core of the soft attention mechanism is that instead of just inputting the original sequence (x 1 , x 2 , ..., x n ) into a LSTM layer, dynamic weights are used to generate a new sequence</p><formula xml:id="formula_4">(v 1 , v 2 , ..., v m ): v t = n ∑ i=1 α (t) i x i ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_5">∑ n i=1 α (t) i = 1 and α (t) i</formula><p>will be calculated by an attention neural network at each time step t = 1, 2, . . . , m.</p><p>The attention weight α (t) i actually measures the relevance between the i-th element x i of the input sequence and the history information recorded by the LSTM h t−1 . Hence a function is needed to calculate the relevance score:</p><formula xml:id="formula_6">e (t) i = w ⊤ tanh(W a x i + U a h t−1 + b a ),<label>(11)</label></formula><p>where w, W a , U a , b a are all parameters and h t−1 is the hidden state of the LSTM at (t−1)-th time step. We need to calculate e </p><p>The attention mechanism could make the LSTM pay attention to different temporal locations of the input sequence according to its backprop information, and when the input sequence and the output sequence are not aligned strictly, attention would especially be helpful. We add attention units in three different positions in our video caption model: between the visual input and the LSTM filter, between the output of the filter and the second LSTM layer, between the output of our HRNE and the description decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Video Captioning</head><p>Our HRNE can be applied to several video processing tasks where feature vectors are required to represent videos. In this paper, we use video captioning, where temporal information plays an important role, to showcase the advantage of the proposed method.</p><p>We develop our video captioning model based on the general sequence to sequence model <ref type="bibr" target="#b30">[31]</ref>, i.e., encoderdecoder framework, which is same as the previous works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37]</ref>. We use the general video encoder to map video sequences to feature vectors and then one-layer LSTM decoder conditioned on the video feature vector to generate description for the video.</p><p>The overall objective function we are optimizing is the log-likelihood over the whole training set,</p><formula xml:id="formula_8">max Θ T ∑ t=1 log Pr(y t |z, y t−1 ; Θ),<label>(13)</label></formula><p>where y t is a one-hot vector (1-of-N coding, where N is the size of the word vocabulary) used to represent the word at the t-th time step, z is the feature vector output by the video encoder and Θ represents the video captioning model's parameters. Similar to most recurrent neural network language models, we utilize a softmax layer to model the probability distribution of the next word over the word space, i.e.,</p><formula xml:id="formula_9">Pr(y t |z, y t−1 ; Θ) ∝ exp(y ⊤ t W y s t ),<label>(14)</label></formula><p>where</p><formula xml:id="formula_10">s t = tanh(W z z + W h h t + W e y t−1 + b),<label>(15)</label></formula><p>and W y , W z , W h , W e and b are all the parameters. Eqn <ref type="formula" target="#formula_10">(15)</ref> is an instance of deep output layer proposed in Pascanu et al. <ref type="bibr" target="#b22">[23]</ref> and we find incorporating the deep output layer helps the model to converge faster and gets better performance. To make the model more robust, we adopt the Maxout <ref type="bibr" target="#b9">[10]</ref> scheme to calculate s t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>We utilize two standard video captioning benchmarks to validate the performance of our proposed method in the experiments: the widely used Microsoft Video Description Corpus (MSVD) <ref type="bibr" target="#b4">[5]</ref> and one recently proposed dataset the Montreal Video Annotation Dataset (M-VAD) <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Datasets</head><p>The Microsoft Video Description Corpus (MSVD): The Microsoft Video Description Corpus (MSVD) <ref type="bibr" target="#b4">[5]</ref> contains 1,970 videos with multiple descriptions labeled by the Amazon Mechanical Turkers. Annotators are requested to provide a single sentence description to a picked up short clips. The total number of clip-description pairs is about 80,000. The original dataset consists of multi-lingual descriptions while we only focus on the English description as the previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>. We utilize the standard splits provided in <ref type="bibr" target="#b37">[38]</ref> for fair comparisons with state-ofthe-art video captioning systems <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>, which separate the original dataset into training, validation and testing with 1,200 clips, 100 clips, and the remaining clips, respectively.</p><p>The Montreal Video Annotation Dataset (M-VAD): The Montreal Video Annotation Dataset (M-VAD) is a newly collected large-scale video description dataset from the DVD descriptive video service (DVS) narrations. There are 92 DVD movies in the M-VAD dataset, which is further divided into 49,000 video clips. Each clip in the video has one corresponding narration as the groundtruth of the clip description. Since the narrations are generated in a semiautomatically transcribed way, the grammar used in the description is much more complicated than the one in MSVD. Same as previous works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37]</ref>, we utilize the standard splits provided in <ref type="bibr" target="#b33">[34]</ref>, which consists of 39,000 clips in the training set, 5,000 clips in the validation set, and 5,000 clips in the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Preprocessing</head><p>Visual Features: We use GoogLeNet <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12]</ref> to extract the frame-level features in our experiment. All the videos' lengths are kept to 200 frames. For a video with more than 200 frames, we drop the extra frames. For a video without enough frames, we pad zero frames. These are common approaches to ensure all the videos have the same length <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43]</ref>. Instead of directly inputting the features into HRNE, we learn a linear embedding of the features as the input of our model.</p><p>Description preprocessing: We convert all descriptions to lower case, and use the PTBTokenizer in Stanford CoreNLP tools 1 <ref type="bibr" target="#b18">[19]</ref> to tokenize sentences and remove punctuation. This yields a vocabulary of 12,976 in size for the MSVD dataset and a vocabulary of 15,567 in size for the M-VAD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>Several standard metrics such as BLEU <ref type="bibr" target="#b21">[22]</ref>, ME-TEOR <ref type="bibr" target="#b7">[8]</ref>, ROUGE-L <ref type="bibr" target="#b17">[18]</ref> and CIDEr <ref type="bibr" target="#b35">[36]</ref> are used commonly for evaluating visual captioning tasks, mainly following the machine translation field. The authors of <ref type="bibr" target="#b35">[36]</ref> evaluated the above four metrics in terms of the consistency with human judgment, and found that METEOR is always better than BLEU and ROUGE. Thus, METEOR is used as the main metric in the evaluation. We utilize the Microsoft COCO evaluation server <ref type="bibr" target="#b5">[6]</ref> to obtain all the results reported in this paper, which makes our results directly comparable with the previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Compared Algorithms</head><p>• FGM <ref type="bibr" target="#b32">[33]</ref>: It first obtains confidences on subject, verb, object and scene elements. Then a factor graph model is used to infer the most likely (subject, verb, object) tuple in the video. Finally it generates sentence based on a template.</p><p>• Average pooling + LSTM decoder <ref type="bibr" target="#b37">[38]</ref> (denoted as Mean pool): It uses the average pooling frame-level feature to represent the whole video. Then LSTM is</p><p>• S2VT <ref type="bibr" target="#b36">[37]</ref>: It first introduces stacked LSTM as an encoder-decoder model to video captioning tasks. It consists of two phases. In the first phase, it serves as a video encoder and in the second phase, it stops accepting video sequence and begins generating video descriptions.</p><p>• Temporal Attention <ref type="bibr" target="#b41">[42]</ref> (SA): It applies attention mechanism on temporal locations and then utilizes the recurrent language model LSTM to generate the video description.</p><p>• LSTM embdding <ref type="bibr" target="#b20">[21]</ref> (LSTM-E): It uses embedding layers to project the visual feature and text feature into one space, with a modified loss between description and visual features.</p><p>• Paragraph RNN decoder <ref type="bibr" target="#b42">[43]</ref> (p-RNN): It introduces a hierarchical structure in decoder for language processing and introduce the paragraph description in addition to the standard sentence description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training Details</head><p>In the training phase, we add a begin-of-sentence tag &lt;BOS&gt; to start each sentence and an end-of-sentence tag &lt;EOS&gt; to end each sentence, so that our captioning model can deal with sentences of varying lengths. In the testing phase, we input &lt;BOS&gt; into video decoder to start generating video descriptions and during each step, we choose the word with the maximun probability after softmax until we reach &lt;EOS&gt;.</p><p>We adopt different parameter settings to train different datasets.When we are training on MSVD, we use the following settings: All the LSTM units are set to 1,024, the visual feature embedding size and the word embedding size are set as 512 empirically. When training on M-VAD, we find our HRNE is easier to overfit than in MSVD, so we set all the LSTM units to be 512 and still keep the visual feature embedding size and the word embedding size to be half of the number of LSTM units. As the videos in the two datasets are very short, a two-layer HRNE is sufficient to capture the temporal structure of videos. Nevertheless, one may use HRNE with more layers to deal with longer videos.</p><p>The length of the LSTM chain at the bottom layer is 8, and we set the stride to be 8 in all the experiments. We set the size of mini-batch as 128. We apply the first-order optimizer ADAM to minimize the negative log-likelihood loss for the training process and we set the learning rate η = 2 × 10 −4 , the decay parameters β 1 = 0.9, β 2 = 0.999 as defaulted in Kingman and Ba <ref type="bibr" target="#b15">[16]</ref>, which generally shows good performance and does not need heavily tuned. Since we observe serious overfitting problems when training our model on M-VAD dataset, we apply the simple yet effective neural model regularization method Dropout <ref type="bibr" target="#b29">[30]</ref> with rate of 0.5 on the input and the output of LSTMs but not on the recurrent transitions as suggested by Zaremba et al. <ref type="bibr" target="#b44">[45]</ref>. We find that the proposed model has better generalization ability in this way, empirically. More details of parameter choice can be found in the supplementary material.</p><p>We train the model for 400 epoches, or stop the training until the evaluation metric does not improve on the validation set. We utilize Theano <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> framework to conduct our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>We evaluate our HRNE model on video captioning on both MSVD and M-VAD. We report results on MSVD in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref>. We firstly report the results only using static frame-level features in <ref type="table">Table 1</ref>. We additionally compare our HRNE with GoogLeNet feature to other video captioning systems which combine multiple ConvNet features in <ref type="table" target="#tab_2">Table 2</ref>. We also report the result of our HRNE with fusion of GoogLeNet feature and C3D feature <ref type="bibr" target="#b34">[35]</ref>. Lastly, we conduct the experiment on the more challenging dataset M-VAD, and report the results in <ref type="table">Table 4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment results on the MSVD dataset</head><p>We report experiment results where only static framelevel features are used in <ref type="table">Table 1</ref> on the MSVD dataset. Our method achieves better result than S2VT, which indicates our hierarchical structure increases the learning capability and enables our model encode richer temporal information of multiple granularities. Both Mean pool and SA ignore temporal dependencies along video sequences. They adopt the weighted averages of frame-level features to represent videos. Our HRNE outperforms both Mean pool and SA, due to the exploration of temporal information of videos. Hierarchical description decoder is adopted in p-RNN to generate complex descriptions, while our HRNE has better performance than p-RNN, which indicates exploring temporal information of videos is more important for video captioning. To further improve our HRNE, we add attention mechanism, which again improves its performance.</p><p>We additionally compare our HRNE to other video captioning systems with fusion in <ref type="table" target="#tab_2">Table 2</ref>. We first compare our HRNE with only GoogLeNet feature to systems which combine multiple ConvNet features. Our HRNE achieves the best result in METEOR. It means although adding more features helps improve video captioning systems' performance, our method still achieves the best performance. This result confirms the effectiveness of our HRNE. We notice that p-RNN outperforms our HRNE in terms of BLEU. However, our method outperforms p-RNN in almost all other cases (see <ref type="table" target="#tab_2">Table 1 and Table 2</ref>) and, more importantly,  <ref type="bibr" target="#b41">[42]</ref> 28.7 ---38.7 S2VT-(V)-(A) <ref type="bibr" target="#b36">[37]</ref> 29.8 ----SA-(G)-(C) <ref type="bibr" target="#b41">[42]</ref> 29.6 ---41.9 LSTM-E-(A) <ref type="bibr" target="#b20">[21]</ref> 28.  as demonstrated in <ref type="bibr" target="#b35">[36]</ref>, METEOR is more reliable than BLEU. Our HRNE with fusion of GoogLeNet faeture and C3D feature indicates adding more features can improve the performance of our method, which is consistent with <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43]</ref>. Compared with stacked LSTM, our HRNE can significantly reduce the computation operations. We provide experiment analysis in the supplementary material.</p><p>In <ref type="table">Table 3</ref>, we show a few examples of the descriptions generated by our method. We notice that our HRNE can generate an accurate description of the video even in some difficult cases. In addition, the results with the attention mechanism is generally better than those without the attention mechanism, which is consistent with the results reported in <ref type="table" target="#tab_2">Table 1 and Table 2</ref>. <ref type="table">Table 4</ref> reports the results on M-VAD. Compared with MSVD, M-VAD is a more challenging dataset, because it contains more visual concepts and complex sentence structures. Since the result on BLEU metric is close to 0 2 , we do not consider BLEU metric in this experiment. Our HRNE achieves 5.8% in METEOR, which outperforms both S2VT <ref type="table">Table 3</ref>: Example results on the MSVD Youtube video dataset. We present the video descriptions generated by our HRNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiment results on the M-VAD dataset</head><p>Model METEOR SA-GoogLeNet+3D-CNN <ref type="bibr" target="#b41">[42]</ref> 5.7 SA-GoogLeNet+3D-CNN <ref type="bibr" target="#b41">[42]</ref>  <ref type="bibr" target="#b3">4</ref> 4.1 S2VT-RGB(VGG) <ref type="bibr" target="#b36">[37]</ref> 6.7 HRNE 5.8 HRNE (with attention) 6.8 <ref type="table">Table 4</ref>: Experiment results on the M-VAD dataset. and SA 3 . After adding the attention mechanism, our performance (in METEOR) is further improved from 5.8% to 6.8%. Such performance even outperforms S2VT which combines M-VAD and MPII-MD <ref type="bibr" target="#b23">[24]</ref> for training. Because combining two datasets introduces much more training data than just one dataset as the standard setting we used for training, this result again validates the effectiveness of our HRNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In this paper, we proposed a new method, namely Hierarchical Recurrent Neural Encoder (HRNE), to generate video representation with emphasis on temporal modeling. Compared to existing approaches, the proposed HRNE is more capable of video modeling because 1) HRNE reduces the length of input information flow and exploits tempo-ral structure in longer range at a higher level; 2) more non-linearity and flexibility are added in HRNE; and 3) HRNE exploits temporal transitions with multiple granularities. Extensive experiments in video captioning demonstrate the efficacy of HRNE.</p><p>Last but not least, the proposed video representation is generic which can be applied to a wide range of video analysis applications. We will explore the application of the encoder on video classification in the future work, which plugs with a softmax classifier upon the encoder and video labels instead of the LSTM language decoder in this work to validate the generalization capability of this framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Analogical illustration of temporal operations of HRNE to spatial convolutional operations of ConvNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>A comparison between stacked LSTM and the proposed Hierarchical Recurrent Neural Encoder. This figure takes a two layer hierarchy as an example to showcase. The red line in each subfigure shows one of the paths from the visual appearance input at t = 1 to the output video vector representation. There are 10 time steps in stacked LSTM and only 6 time steps in our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1: Experiment results on the MSVD dataset. We compare our method with the baselines using static frame-level features only in this table.</figDesc><table>Model 
METEOR 
B@1 
B@2 
B@3 B@4 
FGM [33] 
23.9 
-
-
-
-
Mean pool [42] 
28.7 
-
-
-
38.7 
SA [42] 
29.0 
-
-
-
40.3 
S2VT [37] 
29.2 
-
-
-
-
LSTM-E [21] 
29.5 
74.9 
60.9 
50.6 
40.2 
p-RNN [43] 
31.1 
77.3 
64.5 
54.6 
44.3 

HRNE 
32.1 
78.4 
66.1 
55.1 
43.6 
HRNE with attention 
33.1 
79.2 
66.3 
55.1 
43.8 

Model 
METEOR 
B@1 
B@2 
B@3 
B@4 
Mean pool-(G) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Experiment results on the MSVD dataset with fusion. (A) denotes AlexNet, (V) denotes VGGNet, (C) denotes C3D and (G) denotes GoogLeNet in the model's name.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">version 3.4.1utilized as a recurrent language model to produce the description given the visual feature.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">SA<ref type="bibr" target="#b41">[42]</ref> achieves only 0.7% BLEU-4 on this dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Only S2VT and SA have reported result on this challenging dataset.<ref type="bibr" target="#b3">4</ref> <ref type="bibr" target="#b36">[37]</ref> notes that<ref type="bibr" target="#b41">[42]</ref> achieves 4.1% METEOR with the same evaluation script as<ref type="bibr" target="#b36">[37]</ref>, while the 5.7% METEOR reported in<ref type="bibr" target="#b41">[42]</ref> is caused by different tokenization.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This work was in part supported by the 973 Program(No.2012CB316400), the NSFC (No. U1509206),the China Knowledge Centre for Engineering Sciences and Technology (CKCEST); in part supported by the Data to Decisions Cooperative Research Centre www.d2dcrc.com; in part supported by the ARC DECRA and DP.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>HRNE A man is playing a guitar HRNE: A woman is adding noodles into a pot HRNE with attention: A dog is swimming.</p><p>HRNE with attention: A man is playing a guitar. HRNE with attention: A woman is cooking. Ground truth: A dog is swimming in a pool.</p><p>Ground truth: A boy is playing a guitar.</p><p>Ground truth: A woman dips a shrimp in batter. Ground truth: A biker rides along the beach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HRNE:</head><p>Ground truth: A basketball player is doing a hook shot.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. NIPS Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult. Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SciPy)</title>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Maxout networks. In ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6026</idno>
		<title level="m">How to construct deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="222" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A hierarchical recurrent encoder-decoder for generative context-aware query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Using descriptive video services to create a large data source for video annotation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01070</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">C3D: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sequence to sequence -video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A discriminative CNN video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
