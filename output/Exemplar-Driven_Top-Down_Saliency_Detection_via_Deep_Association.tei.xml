<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exemplar-Driven Top-Down Saliency Detection via Deep Association</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exemplar-Driven Top-Down Saliency Detection via Deep Association</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Top-down saliency detection is a knowledge-driven search task. While some previous methods aim to learn this "knowledge" from category-specific data, others transfer existing annotations in a large dataset through appearance matching. In contrast, we propose in this paper a locateby-exemplar strategy. This approach is challenging, as we only use a few exemplars (up to 4) and the appearances among the query object and the exemplars can be very different. To address it, we design a two-stage deep model to learn the intra-class association between the exemplars and query objects. The first stage is for learning object-to-object association, and the second stage is to learn background discrimination. Extensive experimental evaluations show that the proposed method outperforms different baselines and the category-specific models. In addition, we explore the influence of exemplar properties, in terms of exemplar number and quality. Furthermore, we show that the learned model is a universal model and offers great generalization to unseen objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The human visual system has an outstanding ability to rapidly locate salient regions in complex scenes <ref type="bibr" target="#b19">[20]</ref>. Our attention is mainly drawn by factors relevant to either bottom-up or top-down saliency detection. Bottomup visual saliency is stimulus-driven, and thus sensitive to the most interesting and conspicuous regions in the scene. Top-down visual saliency, on the other hand, is knowledgedriven and involves high-level visual tasks, such as intentionally looking for a specific object.</p><p>In computer vision, bottom-up saliency detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref> receives much research attention, due to its task-free nature. For the same reason, it can only capture the most salient object(s) in the scene. On the other hand, top-down saliency <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7]</ref> aims to locate all the intended objects in the scene, which can help * Corresponding author. reduce the search space for object detection. Existing methods typically learn the "knowledge" that guides top-down saliency detection, from a set of categorized training data. Thus, they are confined to the pre-defined categories and restricted from training a universal model.</p><p>However, human knowledge does not only come from memory (i.e., locating salient objects in the scene using knowledge from training data), but also from object association (i.e., locating objects in the scene using known or unknown exemplars) <ref type="bibr" target="#b1">[2]</ref>. For example, we can easily identify and locate a similar object in an image when given an unseen example object in another. As suggested in cognitive studies <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref>, instead of recognizing an object according to an explicit category representation, human brain categorizes objects by associating an input unseen object to a set of instances. This motivates us to learn the intra-class association between an input query object and some exemplars. This is a challenging task as such association should be universal and is built from a few exemplars (only 2 -4 exemplars in our experiments) rather than the entire dataset. In addition, objects from the same category may appear in different colors, scales, and viewpoints, which makes the task even more challenging.</p><p>In this paper, we propose a multi-exemplar convolutional neural network (CNN) <ref type="bibr" target="#b24">[25]</ref> for detecting top-down saliency by establishing visual association between a query image and multiple object exemplars, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The intra-class association is learned in a unified manner by jointly extracting features from the exemplars and query object in a two-stage scheme. These two stages correspond to association and background discrimination learning. The main contributions of our work are summarized as follows:</p><p>1. We design a two-stage deep model ( <ref type="figure">Figure 2</ref> left) to detect top-down saliency by associating multiple exemplars with the query object, and explore the performance of different network structures. 2. We delve into the relationship between exemplars and the learned associations. In particular, we explore how different numbers of exemplars as well as the exemplar quality affect saliency detection performance. 3. We explore the proposed deep model in different tasks, including same-class identification, object location predication, and top-down saliency detection <ref type="table">(Figure 2 right)</ref>. Experiments on the Pascal VOC 2012 dataset show that the proposed model outperforms different baselines and the state-of-the-art categoryspecific methods. 4. We investigate the generalization capability of the learned intra-class association by applying it to unseen objects. The proposed networks offer surprisingly good generalization. To the best of our knowledge, our work is the first to design and explore a multi-exemplar deep model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we first discuss relevant top-down saliency detection methods. We then describe object localization methods, as they share a similar objective to topdown saliency detection.</p><p>Top-down saliency includes two main processes, dictionary learning for each category (i.e., learning category "knowledge") and saliency computation (i.e., knowledgedriven searching). An early work by Torralba et al. <ref type="bibr" target="#b32">[33]</ref> propose to use contextual information in a Bayesian framework to detect top-down saliency. Gao et al. <ref type="bibr" target="#b11">[12]</ref> propose to characterize discriminant features using the statistical differences of presense/absense of the target class. Judd et al. <ref type="bibr" target="#b20">[21]</ref> and Borji <ref type="bibr" target="#b2">[3]</ref> combine bottom-up and top-down models by introducing high-level information to detect saliency, as objects like human persons, faces, and cars typically attract human attention. Ehing et al. <ref type="bibr" target="#b9">[10]</ref> incorporate target appearance, location and scene context to model saliency. Kanan et al. <ref type="bibr" target="#b21">[22]</ref> use independent component analysis (ICA) to learn target appearance, and then a trained SVM to detect top-down saliency. In <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23]</ref>, top-down saliency is modeled by jointly learning category-specific dictionaries and CRF parameters. Similar to top-down saliency, Oquab et al. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> propose to generate a confidence map for each category location using CNN. In <ref type="bibr" target="#b27">[28]</ref>, large-scale knowledge in ImageNet is transferred to locate objects, while in <ref type="bibr" target="#b28">[29]</ref>, a weakly-supervised CNN is used to predict object locations. However, adapting limited amount of training data to unlimited test data is always desirable. Existing methods requires category-specific learning, and are thus restricted from training a universal model.</p><p>Object Localization aims to produce bounding boxes on the target objects, and can be roughly categorized into two classes, generic object localization and accurate object localization. Generic object localization (or object proposal detection) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref> aims to cover all objects in the scene with fewer and better bounding boxes than sliding windows, and to reduce the computational overhead of object detection. However, they are too general to obtain high accuracy with few proposals. Accurate object localization is mainly to produce one bounding box for each target object category in the image. It is much challenging and typically requires additional information. Dai et al. <ref type="bibr" target="#b7">[8]</ref> assume that a given detected bounding box is not accurate, and propose to re-localize the object by propagating the target appearance information. In <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>, annotations in the database are used to label target object class. Song et al. <ref type="bibr" target="#b31">[32]</ref> combine a discriminative submodular cover problem with a smoothed latent SVM formulation to locate objects with minimal supervision. While these works are promising, they attempt to find the best bounding box with visually consistent appearance to the training data. On the contrary, the proposed method is able to locate objects using just a few exemplars, which may contain large appearance variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Intra-class Association Network</head><p>Given a few exemplar objects from the same category and a query image, our goal is to determine whether the query image belongs to the same category of the exemplars. This process should not rely on any semantic information, and it should be as universal as possible and able to apply to unseen objects. Our approach is to train the proposed multi-exemplar deep model in two stages. As objects from the same class shares similar properties and features, the first stage is to learn to extract intra-class features, which determines objects being in the same category. To remove  <ref type="figure">Figure 2</ref>: The pipeline of the proposed method. We treat the exemplars and query images as a unified multi-channel input and learn the intra-class association in a two-stage process. The first stage is fed with object patches to learn the intra-class association. The second stage is fed with sliding patches to learn background discrimination. The trained network is powerful and can be applied to different tasks.</p><p>background distraction, the second stage is to learn how to discriminate the background. The trained network can then be applied to top-down saliency detection, same-class identification, and object location prediction. The pipeline of the proposed method is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Initialization and Network Architecture</head><p>As demonstrated in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>, a network pre-trained on a large scale dataset shows great generalization for different types of tasks and input data. Starting with a pre-trained network can significant improve the task performance, even if the task is very different from the one in pre-training. Similarly, we initialize our network using the fast VGG network (VGG-f) <ref type="bibr" target="#b4">[5]</ref> pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>, which comprises 5 convolutional layers and 3 fully connected layers. (Other networks can also be used here.) As the number of inputs is different from the pre-training task, we need to alter the network architecture to adapt to our problem. There are two possible models for our purpose.</p><p>Siamese network is a multi-tower architecture <ref type="bibr" target="#b3">[4]</ref>. Each tower has the same layer structure and shares the same set of parameters. Fully connected layers are placed on top of these towers. Each input image is assigned to one tower to extract features of the image. The output feature maps are concatenated and passed to the fully connected layers. To adapt our problem to this network, we initialize each tower as a VGG-f network without the fully connected layers, which are added back to cover all the towers after the initialization. The sizes of the fully connected layers are expanded accordingly to measure the similarity among the images. The number of outputs for the last fully connected layer is set to 2, as we are solving a binary classification problem.</p><p>This type of network is mainly used to compare the sim-ilarity of two images <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b14">15]</ref>, and it shares a similar idea as the traditional descriptor-based approach. Each tower takes one image as input. This process can be viewed as descriptor computation. The fully connected layers at the top measure all these feature maps and thus can be viewed as a similarity function. However, extracting features from individual inputs is not a proper way to learn object association, especially with multiple exemplars. This is because the network only learns to describe image locally, i.e., the learned features are independent of the other inputs. Based on these mutually independent features, learning a similarity function is not enough to identify the large intra-class appearance differences.</p><p>Unified network learns to describe all input images jointly. In contrast to the Siamese network, all input images here are treated as a single multi-channel input. For example, four exemplars and one query image are combined to form a 5 × 3 channels image volume, with 3 being the 3 color channels. Due to the change in input dimension, the first convolutional layer needs to expand its channels accordingly. In our implementation, we have tried setting the parameters of the first convolutional layer in two ways: with random values and making multiple copies (equal to the number of exemplars) of the parameter values from the pretrained model. As expected, the latter approach performs much better than randomly initialized parameters. Other layers of the VGG-f network remain the same, except that the number of outputs for the last fully connected layer is set to 2 for binary classification.</p><p>Compared with the Siamese network, our unified network has a greater flexibility to learn features of multiple objects, as all of them are considered jointly and the extracted features are intra-class features. In addition, the unified network is faster in both training and testing, especially with a large number of exemplars, as the computational complexity of the Siamese network is proportional to the number of inputs. The performance of the Siamese network and our unified network will be examined in Section 4.</p><p>Once the network architecture is determined and the parameters are initialized, we may then train the network for object association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stage 1: Object Association Training</head><p>As the initial network is pre-trained for image classification with centered objects, we further train the network on the Pascal VOC 2012 dataset <ref type="bibr" target="#b10">[11]</ref> with object-level supervision. In order to learn the association among objects, the training process should be supervised in an object-to-object manner. As a result, we crop all the objects in the training data into patches for training. These patches are resized to the same size as the input size of the first convolutional layer (224 × 224 for the VGG-f network). The object-based training data is augmented by introducing jitter for robust association learning and combating overfitting. All training patches are randomly flipped, shifted, rotated, and enlarged/shrinked by a small amount.</p><p>There are two types of inputs for our network, exemplars and query images. Different construction methods for the input image volume lead to intrinsically different supervision approaches. If we train the network by randomly sampling objects from the training set, it is equivalent to identifying if a set of images belong to the same category, which is not our purpose. The proposed model is exemplardriven, which means that all given exemplars should come from the same category. This construction method reduces the learning ambiguity, allowing the network to focus on delving into the relationship between the known-positives and unknown query (i.e., multiple-to-one connection), and the exemplars are able to provide guidance for both training and testing. For each training query object, its label is randomly defined by selecting exemplars from the same class of the query object or from other classes. (At least 30% of the data is positive to balance data distribution.) Note that the selected exemplars belong to the same category for both positive and negative training samples.</p><p>The network is trained using stochastic gradient descent (SGD) with a batch size of 50. Cross-entropy error is minimized in our network. The learning rate for this stage is set to 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stage 2: Background Discrimination Training</head><p>In stage 1, we learn the association between the exemplars and query object. However, in order to effectively detect top-down saliency, we also needs to differentiate cluttered background to prevent background distraction. We fine-tune our network using the sliding window strategy to obtain diverse patches for training. The patches are extracted following the sliding window setting in <ref type="bibr" target="#b27">[28]</ref>. All the patches are square with width s = min(w, h)/λ, where w and h are the image width and height, and λ ∈ {1, 1.3, 1.6, 2, 2.4, 2.8, 3.2, 3.6, 4}. They are sampled on a regular grid with at least 50% overlap with its neighbors. In total, there are around 500 patches for a 500 × 400 image. Similar to stage 1, these patches are resized to 224 × 224 before feeding to the network. Compared to the training with object proposals <ref type="bibr" target="#b12">[13]</ref>, the bounding boxes obtained by sliding windows are more diverse and thus can train the network with less patches.</p><p>For each target category in the image, we randomly select exemplars from the same category, and the positive query patches are defined loosely with certain extent of background. The label of patch P is positive if all the following conditions are satisfied: (i) the intersection ratio between P and the ground truth bounding box G c of class c is larger than 0.2|P |; (ii) large portion of object G c is included in P such that |P ∩ G c | ≥ 0.6|G c |; (iii) P includes only one object. The training setting is the same as in stage 1 except for the learning rate, which is set to a smaller value of 0.0001 for fine-tuning the parameters of stage 1.</p><p>The training process in stage 2 has a different objective to the one in stage 1. Stage 1 trains on objects only, and its goal is to learn to identify what makes objects being in the same category. Stage 2 fine-tunes the network with arbitrary data using a smaller learning rate, and its goal is to learn to differentiate the background as well as the negative patches that partially overlap with the object. While stage 2 may be more important to top-down saliency performance (reducing background errors), stage 1 is the key for learning association and a universal model. As such, we intentionally bias the training process to stage 1. We will show results of different stages in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Prediction</head><p>Once the network is properly trained, we are ready to apply it to different tasks. For all these tasks, we only use the final network trained by the two-stage approach.</p><p>Same-class identification: The most intuitive task of the propose network is to classify if a query object belongs to the same class of the exemplars. This task is the most straightforward way to show the learned association. It is also fundamental to the other tasks.</p><p>Top-down saliency detection: To detect saliency, we apply same-class identification to the entire image. Like the training process in stage 2, we first extract a set of patches from the image, but unlike it, the patch sampling strategy is not limited to sliding windows. In practice, we have found that the learned network has great generalization capability and can process patches with arbitrary sizes and scales. We have tested two bounding box sampling strategies: sliding windows in Section 3.3 and EdgeBoxes <ref type="bibr" target="#b41">[42]</ref>. The first one  <ref type="table">Table 1</ref>: Same-class identification results on the Pascal VOC 2012 validation set. The proposed method is evaluated with different network architectures, stages, and numbers of exemplars. The category-specific network is trained using the same training strategy and architecture as ours. The proposed method consistently outperforms the category-specific model.</p><p>produces diverse bounding boxes, while the latter locates objects tightly. However, object proposal detection requires a larger number of windows (around 1000) in order to cover most of the objects. As such, we use sliding windows in all our experiments, as a trade-off between accuracy and efficiency. Each patch is then fed to the network, and the saliency value of pixel Q for class c is computed as:</p><formula xml:id="formula_0">sal(Q, c) = M i=1 y(P i , 1) · δ(P i , Q, y),<label>(1)</label></formula><p>where M is the total number of patches. y(P i , 1) is the confidence score of P i belonging to the same class as the exemplars. δ is an indicator function:</p><p>δ(P i , Q, y) = 1, if Q ∈ P i and y(P i , 1) &gt; y(P i , 0) 0, otherwise.</p><p>(2) The final saliency map is normalized to [0, 1].</p><p>Object location prediction: The proposed method is able to output a precise location of a specific object. Based on the top-down saliency map of a target category, the object location can be easily obtained by applying a global max-pooling operation on the entire map. An example of the saliency map and its corresponding predicted location are shown in <ref type="figure">Figure 2</ref> right. Note that this approach is able to obtain one location per category, but it is sufficient to evaluate the accuracy of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the performance of the proposed method, explore the use of different numbers of exemplars, and investigate the generalization capability of it to unseen objects. The proposed method is implemented using MatConvnet <ref type="bibr" target="#b34">[35]</ref> and tested on a PC with an i7 3.4GHz CPU, a Nvidia 980M GPU, and 32GB RAM. It takes 2-3s to process a 500 × 400 image. In our experiments, all the compared networks are trained with the same amount of data (i.e., the same number of epoches). Stage 1 takes 4 -5 days for the training to converge, and stage 2 takes 3 -4 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Same-class Identification</head><p>We first evaluate the learned association on same-class identification. The network is trained on the Pascal VOC 2012 training set and tested on the validation set. The input exemplars and query images are cropped according to the ground truth bounding boxes. In total, there are 13,609 object images in the training set, and 13,841 in the validation set. For both training and validation sets, there are at least 280 object images per category. During testing, we use the objects from the training set as exemplars and those from the validation set as query objects. As the use of different numbers of exemplars may affect identification performance, we randomly generate 5 exemplar sets for each number of input exemplars (1 -4 are tested in our evaluations). All the other evaluations use the same 5 sets of inputs. <ref type="table">Table 1</ref> shows the average performance for the 5 sets of exemplars on each class and the average per-class standard deviation for evaluating the influence of different numbers of exemplars.</p><p>Compared with the Siamese network. We first compare the proposed network with the Siamese network, which has a multi-tower structure. The Siamese network is trained using 4 exemplars and follows the same training strategy as ours. As shown in <ref type="table">Table 1</ref>, the Siamese network (row 1) performs much worse than ours (row 7) using 4 exemplars. This is because the Siamese network extracts features from the input exemplars individually, while the proposed network jointly considers all the inputs and thus has higher flexibility to learn the association.</p><p>Results of different stages. We then evaluate if training the network with only one of the two stages can achieve good results. Two networks are trained with the same amount of data individually with 4 exemplar inputs. The learning rate is set to 0.001 for both stages. To make the comparison fairer, we randomly skipped 20% background training samples while training stage 2 to make the positive and negative samples balance. Due to the intrinsically different objectives and training processes of the two stages, the performances are different on identification. As shown in rows 2 and 3 of <ref type="table">Table 1</ref>  <ref type="formula">(2 expls</ref>  <ref type="table">Table 2</ref>: Top-down saliency precision rates (%) at EER on the Pascal VOC 2012 validation set. All the compared methods (rows 1 -3) are category-specific approaches. tion performance than stage 2. This is because stage 1 focuses on object-to-object association, while stage 2 biases to object-to-background learning. The trained network of stage 2 is difficult to classify objects across classes.</p><p>Relationship between exemplars and query object. We further explore if more exemplars help the identification performance. As shown in rows 4 -7 of <ref type="table">Table 1</ref>, more exemplars indeed improve identification performance. There are two possible reasons. First, the chance of selecting good exemplars is higher with more inputs. Second, intra-class association is learned more robustly with more exemplars. Regarding the first conjecture, it is also related to whether exemplar quality affects identification performance. We report the average per-class standard deviation in the second last column. We can see that the variances of training with more exemplars are smaller than those with fewer inputs. This indicates that the association trained with fewer exem-plars relies more on input quality and vice versa. However, in general, the small standard deviations show that the exemplar quality will not significantly influence the identification performance. We will further demonstrate this observation in Section 4.2 qualitatively.</p><p>Compared with a category-specific network. In this experiment, a category-specific VGG-f network pre-trained on ImageNet and fine-tuned on Pascal VOC 2012 (using the same learning rate of 0.001) is used as a baseline. Its performance is shown in row 8 of <ref type="table">Table 1</ref>. Surprisingly, this category-specific network performs only similarly to ours when using 2 exemplars. It is even worse than ours when using 3 or 4 exemplars. We attribute this to our restricted multi-exemplar model. Exemplars are selected from the same class, which reduces the learning ambiguity. This also suggests that once the network has learnt the association, exemplars provide powerful guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Top-down Saliency Detection</head><p>We then examine the performance of top-down saliency detection and delve into the learnt association. We compare our method with two latest top-down saliency detection methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23]</ref>, and one latest object localization method <ref type="bibr" target="#b28">[29]</ref>. All these methods compared are categoryspecific. We use the sliding window sampling strategy in Section 3.3 to extract patches for saliency detection. Similarly, we randomly generate 5 sets of exemplars per test image for comparison. We evaluate the top-down saliency maps using the segmentation annotation of the Pascal VOC 2012 validation set, which consists of 1,449 images. Unlike the evaluation setting in <ref type="bibr" target="#b37">[38]</ref>, we evaluate the saliency map in pixel-level rather than patch-level for higher accuracy. We first binarize the saliency map for every threshold in the range of [0, 255] to generate the precision-recall curves (P-R curves), and the performance of each category is summarized by the precision rates at equal error rates (EER, where precision is equal to recall). The performances of different methods are shown in <ref type="table">Table 2</ref>. The two state-of-theart top-down saliency detection methods (rows 1 -2 in Table 2) encode object information using dictionary learning, but the large appearance differences among the objects of the same class are difficult to capture using their approach. The CNN-based approach (row 3 in <ref type="table">Table 2</ref>) performs not as good as ours, due to the learning guidance provided by our two-stage training process.</p><p>Relationship between exemplars and query object. As shown in <ref type="table">Table 2</ref>, the performance of our method increases with the number of exemplars, and the per-class variations are also small. This is similar to the last experiment. Here, we mainly explore how exemplar quality influences detection performance. <ref type="figure" target="#fig_2">Figure 3</ref> shows some topdown saliency detection examples. The saliency maps are produced using the same sets but differnt numbers of examplars. It demonstrates how each additional exemplar may affect the result. We can also see that a bad exemplar harms the detection. In the first example, the second exemplar is a bad one and it distracts the detection to the wooden wall (due to a similar color). In the second example, the second exemplar of the human feet causes the second saliency map to focus on the human face. In the third example with 3 exemplars, the proposed method wrongly renders the bus as salient, since it shares a similar appearance to the added exemplar. All these cases suggest that color similarity is one of the main influential factors. However, bad exemplar only produces false positive and will not significantly affect the true positive results. In addition, the association learnt using more exemplars is more robust to outliers. As we can see from all three examples with 4 exemplars, the 4-exemplar network is more capable of tolerating bad exemplars and can properly predict salient regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Location Prediction</head><p>We further evaluate the accuracy of the predicted object locations. As mentioned above, a simple max-pooling operator applied on the saliency map is able to predict an object location for a target category. Here, we compare to the three methods used in Section 4.2. In addition, we add the stage-of-the-art object detector RCNN <ref type="bibr" target="#b12">[13]</ref> as a baseline, which outputs a bunch of bounding boxes along with the confidence values in order to cover all the objects in the image. We select the bounding box with the highest confidence value for each target category, and pick the center pixel as the object location. The localization performances of all these methods are examined by simply labeling the predicted location as correct if it falls into the ground truth bounding box of the target category, and negative otherwise. Unlike <ref type="bibr" target="#b28">[29]</ref>, which sets a 18-pixel tolerance to the predicted location, we restrict the correct predicted location to be within the ground truth bounding box for a more accurate evaluation. The confidence values of the predicted locations are used to generate the P-R curves, and the final performance of each category is summarized by Average Precision (AP). We note that his metric can be challenging for cluttered scenarios. The location prediction experiment is conducted on the Pascal VOC 2012 validation set.</p><p>The location prediction results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Our method with 2, 3 or 4 exemplars outperforms all three existing methods and the state-of-the-art object detector overall. Our method with 4 exemplars achieves the best perfor-   mance in most of the classes. Note that the bounding box sampling strategy affects prediction performance. Since most of the top-down saliency detection methods (including Yang et al. <ref type="bibr" target="#b37">[38]</ref>, Oquab et al. <ref type="bibr" target="#b28">[29]</ref>, and ours) detect objects in a sliding window fashion, they may not be able to precisely locate small scale objects, e.g., bottles. On the contrary, RCNN uses a large number of object proposals and can thus capture objects in different sizes and scales. However, the large number of object proposals increases the error rates due to false positive. As a result, its overall performance is not as good as ours. Some examples of object location predictions from our method are shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Unseen Category Evaluation</head><p>The high accuracy of the proposed method on the Pascal VOC 2012 dataset does not guarantee that the learnt association can be generalized to unseen categories. To evaluate the generalization capability of the proposed method, we apply it to the much larger MS COCO dataset <ref type="bibr" target="#b25">[26]</ref>, which consists of 80 classes. Due to the significant increase in the number of categories from Pascal VOC 2012 (which has 20 categories), we fine-tune the proposed network by randomly selected additional 16 categories for training, leaving us 44 unseen categories for evaluation.</p><p>We test the unseen categories on same-class identification using 4 exemplars. Again, a category-specific VGG-f network trained on the MS COCO training set is used as the baseline. <ref type="figure" target="#fig_4">Figure 5</ref> shows results on the unseen cat-egories. We can see that the proposed method performs slightly worse than, but still comparable to, the categoryspecific network on unseen objects. This suggests that the proposed network has good generalization capability to unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a novel locate-byexemplar top-down saliency detection framework. With this approach, object association is captured by a multiexemplar network and learnt in a two-stage training process. We have shown that the network learnt with more exemplars achieves more robust association quantitatively and qualitatively. We have also shown that the proposed network outperforms the state-of-the-art category-specific methods in different tasks. Even for unseen objects, the proposed network can infer the association from learnt knowledge.</p><p>The proposed same-class identification is a fundamental task for a lot of vision applications. As a future work, we aim to extend it to same-object identification, which would be useful for visual object tracking to identify objects under different circumstances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed method can locate top-down saliency using a few exemplars (shown under each saliency map), even though there are significant differences among them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Saliency maps generated by the proposed method using different numbers of exemplars. The target objects (top to bottom) are cow, person, and car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples of object location prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Same-class identification results for unseen categories on the MS COCO validation set. The category-specific network is trained on the entire training set. The proposed method is trained on a subset of categories and tested on the unseen ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>❚✇♦✲❙t❛❣❡ ■♥tr❛✲❝•❛ss ❆ss♦❝✐❛t✐♦♥ ❚r❛✐♥✐♥❣</figDesc><table>❚❛s❦s 

❊①❡♠♣•❛rs 

◗✉❡r② ❖❜•❡❝t 

❈♦♥✈✶ 
P♦♦•✶ 
❈♦♥✈✺ 
P♦♦•✺ 
❋❈✶ 
❋❈✷ 
❋❈✸ 

❙t❛❣❡ ✶✿ 
❆ss♦❝✐❛t✐♦♥ ❚r❛✐♥✐♥❣ 

❊①❡♠♣•❛rs 

◗✉❡r② P❛t❝❤❡s 

❈♦♥✈✶ 
P♦♦•✶ 
❈♦♥✈✺ 
P♦♦•✺ 
❋❈✶ 
❋❈✷ 
❋❈✸ 

❙t❛❣❡ ✷✿ 
❉✐s❝r✐♠✐♥❛t✐♦♥ ❚r❛✐♥✐♥❣ 

(❡s 
◆♦ 

❙❛♠❡✲❝•❛ss 
■❞❡♥t✐✜❝❛t✐♦♥ 

❖❜•❡❝t 
▲♦❝❛t✐♦♥ 
Pr❡❞✐❝t✐♦♥ 

❚♦♣✲❉♦✇♥ 
❙❛•✐❡♥❝② 
❉❡t❡❝t✐♦♥ 

❚r❛♥s❢❡r P❛r❛♠❡t❡rs 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>, stage 1 has better identifica-</figDesc><table># 

Method 
plane bike bird boat btl bus car cat chair cow table dog horse moto pers plant sheep sofa train tv mStd Mean 
1 
Yang [38] 
14.7 28.1 9.8 6.1 2.2 24.1 30.2 17.3 6.2 7.6 10.3 11.5 12.5 24.1 36.7 2.2 20.4 12.3 26.1 10.2 
-
15.6 
2 Kocak [23] 
46.5 45.0 33.1 60.2 25.8 48.4 31.4 64.4 19.8 32.2 44.7 30.1 41.8 72.1 33.0 40.5 38.6 12.2 64.6 23.6 
-
40.4 
3 Oquab [29] 48.9 42.9 37.9 47.1 31.4 68.4 39.9 66.2 27.2 54.0 38.3 48.5 56.5 70.1 43.2 42.6 52.2 34.8 68.1 43.4 
-
48.1 
4 Ours (1 expl) 38.5 32.4 48.7 31.4 27.5 80.4 52.7 68.2 25.9 62.6 30.0 68.6 62.5 65.1 45.5 37.6 61.2 39.5 66.8 52.4 1.85 50.5 
5 Ours </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Method plane bike bird boat btl bus car cat chair cow table dog horse moto pers plant sheep sofa train tv mStd mAP Oquab [29] 83.2 68.2 71.9 69.2 33.7 79.0 57.8 73.8 42.0 75.8 50.1 72.7 75.7 75.7 77.6 37.1 76.7 44.2 81.1 60.6 -65.3 4 RCNN [13] 86.5 72.1 74.2 66.7 43.1 78.3 68.8 80.8 44.9 62.3 51.1 74.4 73.6 83.0 83.0 49.2 78.4 40.6 74.1 69.2 -67.7 5 Ours (1 expl) 77.4 81.9 67.6 40.6 26.4 85.0 52.2 85.4 38.1 87.3 33.8 80.5 84.0 87.5 79.6 51.4 85.5 49.6 79.7 53.8 2.03 66.4 6 Ours (2 expls) 84.1 80.3 69.8 40.6 26.8 87.5 55.1 92.7 38.7 92.7 37.7 84.9 87.8 90.9 86.7 51.9 89.7 55.2 80.0 54.5 1.78 69.4 7 Ours (3 expls) 87.1 85.5 71.3 43.6 30.8 87.3 58.0 93.9 45.3 93.6 40.5 84.3 88.7 91.8 85.8 57.8 90.9 55.7 83.9 59.2 1.59 71.7 8 Ours (4 expls) 86.8 87.2 72.7 46.8 31.7 91.0 58.6 95.2 44.5 94.8 41.5 87.0 91.4 94.3 89.2 57.7 93.5 59.2 84.7 60.5 1.53 73.4</figDesc><table># 

1 
Yang [38] 
57.2 49.6 47.6 45.0 10.3 58.5 41.0 54.6 14.5 40.1 21.4 49.7 57.6 50.1 58.3 22.7 54.4 17.2 51.6 36.3 
-
41.9 
2 Kocak [23] 
70.7 55.4 60.9 53.4 27.3 68.4 52.3 75.4 31.8 60.1 36.1 64.9 70.5 69.6 71.8 33.3 68.2 29.2 70.8 52.5 
-
56.1 
3 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Object location prediction results on the Pascal VOC 2012 validation set.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marqués</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The proactive brain: using analogies and associations to generate predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="280" to="289" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boosting bottom-up and top-down visual features for saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Top-down saliency with locality-constrained contextual sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to localize detected objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="3322" to="3329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling search for people in 900 scenes: A combined source model of eye guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hidalgo-Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual cognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="945" to="978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="1005" />
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale knowledge transfer for object localization in imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3202" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patch-based matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Saliency detection with flash and no-flash image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="110" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SuperCNN: A superpixelwise convolutional neural network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Oriented object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Computational modelling of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="2001-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sun: Topdown saliency using natural statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="979" to="1003" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Top down saliency estimation via superpixel-based discriminative dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cizmeciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erkut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond sliding windows: Object localization by efficient subwindow search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention, similarity and the identificationcategorization relationship</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="39" to="57" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1990" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting an object location using a global image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1729" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contextual guidance of eye movements and attention in realworld scenes: the role of global features in object search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="766" to="786" />
			<date type="published" when="2006-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MatConvNet -convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2798" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Top-down visual saliency via joint crf and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2296" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1592" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Minimum barrier salient object detection at 80 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mȇch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
