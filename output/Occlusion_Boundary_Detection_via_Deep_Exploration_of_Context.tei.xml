<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Occlusion Boundary Detection via Deep Exploration of Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
							<email>huan.fu@student.uts.edu.auchaohui.wang@u-pem.frdacheng.tao@uts.edu.aublack@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">QCIS and FEIT</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 8049</orgName>
								<orgName type="institution" key="instit1">Université Paris-Est</orgName>
								<orgName type="institution" key="instit2">LIGM -CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">QCIS and FEIT</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Occlusion Boundary Detection via Deep Exploration of Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Occlusion boundaries contain rich perceptual information about the underlying scene structure. They also provide important cues in many visual perception tasks such as scene understanding, object recognition, and segmentation. In this paper, we improve occlusion boundary detection via enhanced exploration of contextual information (e.g., local structural boundary patterns, observations from surrounding regions, and temporal context), and in doing so develop a novel approach based on convolutional neural networks (CNNs) and conditional random fields (CRFs). Experimental results demonstrate that our detector significantly outperforms the state-of-the-art (e.g., improving the F-measure from 0.62 to 0.71 on the commonly used CMU benchmark). Last but not least, we empirically assess the roles of several important components of the proposed detector, so as to validate the rationale behind this approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Occlusions are ubiquitous in 2D images of natural scenes ( <ref type="figure">Fig. 1</ref>). They are introduced in the 3D-to-2D projection process during the image formation, due to the overlapping of the 2D extents (in the image plane) of 3D components/surfaces. In this paper, we focus on the problem of detecting occlusion boundaries, each of which separates two 2D regions projected from two parts of scene surfaces that overlap locally in either of those regions.</p><p>Occlusion boundary detection is of interest in computer vision, image analysis, and other related fields (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>). Indeed, occlusions constitute an obstacle to designing rigorous models and efficient algorithms in computer vision and image analysis. Besides the lack of information on invisible scene components, another main reason is that occlusions invalidate the assumption that two neighboring pixels in a 2D image correspond to two adjacent points lying on a common part of a 3D surface. Nevertheless, this invalid assumption is often made, either ex- <ref type="figure">Figure 1</ref>. Illustration of ubiquitous occlusions and local occlusion patterns (source image from <ref type="bibr" target="#b37">[38]</ref>). plicitly or implicitly, in existing methods (e.g., the use of smoothness priors for aggregating spatial information in the 2D image). The localization of occlusion boundaries would, therefore, be very useful for overcoming this limitation and improving the solution in these tasks. Furthermore, since occlusion boundaries separate visible scene components from locally occluded components and usually correspond to an abrupt change in depth (along the line of view), these boundaries contain rich perceptual information about the underlying 3D scene structure, the exploitation of which would be beneficial in various visual perception applications. For example, occlusion boundaries can serve as important cues for object discovery and segmentation (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>), since an object is generally delimited from its environment by the isolation of its 3D surface. Indeed, psychologists have long studied their importance in human visual perception (e.g., <ref type="bibr">Gibson, Biederman)</ref>  <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Despite the considerable number of studies on occlusion boundary detection (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>), the stateof-the-art is still unsatisfactory. We believe that one main reason for this is that contextual information has not been sufficiently explored in an efficient way. In fact, numerous previous studies developed their approaches based on structural modeling tools, e.g., Markov/Conditional Ran-dom Fields (MRFs/CRFs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b47">48]</ref>, to exploiting contextual information and demonstrated the importance of this information in solving various computer vision and image analysis problems (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>). This has motivated us to better explore contextual information in detecting occlusion boundaries.</p><p>In this work, we are interested in exploring three main types of contextual information that are useful for occlusion boundary detection: (i) local contextual correlations in pixel labeling 1 (e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>); (ii) contextual correlations between the labeling of pixels (e.g., patch) and the observations from the surrounding area of the region (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>); and (iii) temporal contextual information contained in video sequences (e.g., <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>). Moreover, we aim to jointly model these three types of information so as to better explore them and boost occlusion boundary detection performance. To this end, we finally propose a novel approach for occlusion boundary detection based on convolutional neural networks (CNNs) <ref type="bibr" target="#b3">[4]</ref> and CRFs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b47">48]</ref>, two of the most powerful and successful modeling tools in computer vision and related fields.</p><p>More specifically, in order to better explore type (i) and (ii) contextual information, our CNN model considers a relatively big image patch "L" as input, performs reasoning on it, and outputs the state of a relatively small patch "S" with the same center with "L" (referred to as L2S). It provides not only a probabilistic labeling map on "S", but also deep features that aggregate the high-level contextual information on "L". These are then fed into our CRF model to globally infer the occlusion boundaries in the whole image. For exploring type (iii) contextual information, we consider the scenario in which a video sequence is the input data (similar to many existing works e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>) 2 , and two simple optical-flow-based motion features are exploited to efficiently capture temporal contextual information.</p><p>Experimental results demonstrate that the proposed detector significantly outperforms the state-of-the-art, e.g., by improving the F-measure from 0.62 to 0.71 on the commonly used CMU benchmark <ref type="bibr" target="#b39">[40]</ref> (see <ref type="figure">Fig. 2</ref> for the precision-recall curves and Tab. 1 for the F-measures). Last but not least, we empirically demonstrate the importance of spatial and temporal contextual information in occlusion boundary detection, and compare several CNN-based alternative methods to illustrate that L2S provides more robust and discriminative deep occlusion features than those variants. These validate the underlying rationale of our approach, which would also be helpful for addressing other visual perception tasks. <ref type="bibr" target="#b0">1</ref> Like most existing methods, we formulate the problem by endowing each pixel with a binary variable denoting whether the pixel is on occlusion boundaries. <ref type="bibr" target="#b1">2</ref> It should be noted that our method can also be applied directly to the scenario where an individual 2D image is the input (see <ref type="table">Tab.</ref> 3). <ref type="figure">Figure 2</ref>. Precision-recall curves. Based on the commonly used CMU benchmark <ref type="bibr" target="#b39">[40]</ref>, we compare our results with those of Sundberg et al. <ref type="bibr" target="#b43">[44]</ref>, Sargin et al. <ref type="bibr" target="#b35">[36]</ref>, Stein et al. <ref type="bibr" target="#b39">[40]</ref> and He et al. <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Occlusion boundary estimation has attracted extensive attention in computer vision over the past few years. Several methods have been proposed to detect occlusion boundaries in a single image. For example, Saxena et al. <ref type="bibr" target="#b36">[37]</ref> learn an MRF to capture 3D scene structure and depth information from single images. Hoiem et al. <ref type="bibr" target="#b20">[21]</ref> demonstrate the importance of 2D perceptual cues and 3D surface and depth cues for occlusion boundary detection, and compute these geometric contexts to reason about occlusions within their CRF model.</p><p>Due to the fact that occlusion boundary detection from a single 2D image is ambiguous, many applications consider videos or image sequences as inputs and extend occlusion boundary detection to the temporal dimension. Apostoloff and Fitzgibbon <ref type="bibr" target="#b0">[1]</ref> observe that the T-junction is a particularly strong occlusion indicator, and thus learn a relevance vector machine (RVM) T-junction classifier on spatiotemporal patches and fuse Canny edges and T-junctions to detect occlusion edges in the spatial domain. Feldman and Weinshall <ref type="bibr" target="#b13">[14]</ref> define the average of the second moment matrix around a pixel as a gradient structure tensor by regarding the video sequence as a spatiotemporal intensity function, and demonstrate that the smallest eigenvalue of this tensor is the occlusion indicator. Stein and Hebert <ref type="bibr" target="#b39">[40]</ref> exploit subtle relative motion cues present at occlusion boundaries during a sequence of frames and develop a global boundary model that combines these motion cues and standard appearance cues based on an initial edge detector <ref type="bibr" target="#b29">[30]</ref>. Black and Fleet <ref type="bibr" target="#b6">[7]</ref> represent occlusion boundaries via a generative model that explicitly encodes the orientation of the boundary, the velocities on either side, the motion of the occluding edge over time, and the appear-ance/disappearance of pixels at the boundary. Based on this model, the motion of occlusion boundaries is predicted and thus information over time is integrated. Besides, both motion boundaries and image boundaries are combined within an MRF framework in <ref type="bibr" target="#b5">[6]</ref> to better reason about the occlusion structure in the scene over time.</p><p>Although some aforementioned methods attempt to develop discriminative occlusion features on a spatiotemporal volume, recent works have shown that directly using flow-based occlusion features as the temporal information is more convenient and efficient. To name a few, Sargin et al. <ref type="bibr" target="#b35">[36]</ref> introduce a probabilistic cost function to generate a spatiotemporal lattice across multiple frames to produce a factor graph. Boundary feature channels are then learned using this factor graph, by taking some independent flow-based occlusion feature channels into account. He and Yuille <ref type="bibr" target="#b17">[18]</ref> argue that image depth discontinuities often occur at occlusion boundaries and estimate the pseudo-depth using the singular value decomposition (SVD) technique from motion flow as a cue for their occlusion detector. Sunberg et al. <ref type="bibr" target="#b43">[44]</ref> recompute occlusion motion flows on each edge fragment at region boundaries from the initial optical flow <ref type="bibr" target="#b9">[10]</ref>, based on the observation that an occlusion boundary can be handled by comparing the difference in optical flow in regions on either side. Reporting that local patch features are unable to handle highly variable appearances or intra-object local motion, Raza et al. <ref type="bibr" target="#b34">[35]</ref> estimate temporally consistent occlusion boundaries via an MRF model whose potentials are learned by random forests using global occlusion motion features and a high-level geometric layout on segmentation boundaries.</p><p>Regarding contextual information, it has already proved to play an important role in many computer vision tasks such as object detection, localization, and recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47]</ref>. Recently, context modeling has also been introduced to boundary detection. Dollár and Zitnick <ref type="bibr" target="#b11">[12]</ref> adopt random decision forests <ref type="bibr" target="#b24">[25]</ref> to capture structural information of local patch edges. Weinzaepfel et al. <ref type="bibr" target="#b50">[51]</ref> extend <ref type="bibr" target="#b11">[12]</ref> to video datasets and exploit temporal information and static image cues to learn correlations between motion edges within local patches (edges between motion objects).</p><p>Previous studies have suggested that the brain encodes contextual information and biologically inspired deep CNNs have been shown to be powerful for feature extraction and description <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>, which have motivated us to learn the internal correlation of an occlusion boundary in local patches using the CNN framework. Patch-level CNNs have been widely used in a variety of computer vision tasks, with excellent progress made over recent years. For example, Fan et al. <ref type="bibr" target="#b12">[13]</ref> combine local image patches and a holistic view in a CNN framework to learn contextual information for human pose estimation. Wang et al. <ref type="bibr" target="#b48">[49]</ref> exploit physical constraints in local patches using a CNN-based model or surface normal estimation. Sun et al. <ref type="bibr" target="#b42">[43]</ref> and Li et al. <ref type="bibr" target="#b27">[28]</ref> learn convolutional features from multiple local regions for facial trait recognition. Besides, Sun et al. <ref type="bibr" target="#b41">[42]</ref> formulate an MRF model to remove non-uniform motion blur using the patch-level probabilistic motion blur distribution by CNNs. Motivated by nearest neighbor relationships within a local patch, Ganin and Lempitsky <ref type="bibr" target="#b15">[16]</ref> detect edges by learning a 4 × 4 label feature vector for each patch and matching against a sample CNN output dictionary corresponding to training patches with known annotation. Shen et al. <ref type="bibr" target="#b38">[39]</ref> make use of the structural information of object contours in contour detection, by classifying image patches into different boundary types and accordingly defining a special loss function for training a CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Proposed Occlusion Boundary Detector</head><p>Firstly, in order to better characterize local contextual correlations in pixel labeling and contextual correlations between regional pixel labeling and surrounding observations, we: (i) consider each individual pixel patch as the unit of interest, and (ii) adopt a CNN to learn and predict a patch's occlusion boundary map based on the observation of a larger patch of pixels with the same center. Secondly, we efficiently explore and encode temporal contextual information within the whole framework by adopting effective motion features in the CNN. Finally, we use a CRF model to integrate patch-based occlusion boundary maps and soft contextual correlations between neighboring pixels to achieve occlusion boundary estimation for the entire image. Each part is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Patch-based Labeling using CNNs</head><p>We are interested in modeling and predicting labeling of a patch of pixels based on the observation of a larger patch with the same center via a structured learning/prediction process. Mathematically, given the K-channel observed data on an N × N patch centered at pixel c, denoted x c ∈ R K * N 2 , we aim to obtain the weighted occlusion boundary map y c ∈ R M 2 on an M × M (M &lt; N ) patch that is also centered at pixel c, which is achieved via our structured CNN convolutional neural network illustrated in <ref type="figure" target="#fig_0">Fig. 3</ref>. Below we first briefly describe the architecture of our structured CNN and then discuss the initial input features/cues used for occlusion boundary detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">CNN Architecture</head><p>We train a CNN using a cross entropy loss function to predict the probability distribution in a small 7 × 7 patch from a large 27 × 27 image patch (i.e., M = 7 and N = 27 in our experiments). The overall CNN architecture is shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. The input of our CNN consists of 3 static color channels and 2 temporal channels  <ref type="bibr" target="#b48">(49)</ref>, which corresponds to a probabilistic labeling map of size 7×7. Here, conv, maxp, LRN, f c, and dropout denote the convolutional layer, max pooling layer, local response normalization layer, fully-connected layer, and dropout layer, respectively. The LRN scheme implements a form of lateral inhibition, encouraging competition for large activations in the neuronal output <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>. The dropout layer is used to prevent units from co-adapting too much when training a large neural network <ref type="bibr" target="#b19">[20]</ref>.</p><formula xml:id="formula_0">→ maxp1 → LRN 1 → conv2 (64@11*11) → maxp2 → LRN 2 → conv3 (96@6*6) → maxp3 → f c1 (2048) → dropout1 → f c2 (2048) → dropout2 → f c3</formula><p>In our CNN architecture, the rectified linear units (ReLU s) non-linear active function, f (x) = max(0, x), is followed by all conv and f c layers except f c3. A sigmoid function is applied to f c3 to obtain a probabilistic labeling map; and accordingly the cross entropy loss function is adopted for the training process. Furthermore, the output of M axp3 provides learned deep features that aggregate the high-level contextual information (referred to as deep contextual features), which are then used in the CRF model (see Sec. 2.2) to globally reason about occlusion boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Initial Features for Occlusion Reasoning</head><p>Many previous occlusion boundary detection methods have attempted to extract various specific features that character-ize occlusions in raw images such as T-junctions, relative depths, and other useful 3D scene properties <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>. However, accurate automatic extraction of such features is also challenging. To avoid these difficulties, we aim to perform occlusion reasoning by using simple but effective initial features/cues. To this end, we first convert an RGB image to Lab space and consider the gradient magnitude of the Lab maps as three feature maps for the CNN model. In addition, we include two optical-flow-based motion features to efficiently encode temporal contextual information in video sequences and further improve detection performance. Finally, the input of our CNN model consists of 5 (3 static + 2 motion) feature maps. The two motion features are detailed below.</p><p>• Motion Feature 1 The first occlusion motion feature OM F 1 aims to capture optical flow discontinuity which suggests occlusion boundaries. We use f t,t+t0 (t 0 ∈ N * ) to denote the optical flow map from frame t to frame t + t 0 (t 0 = 5 in the experiments). To capture the discontinuity of f t,t+t0 , we compute the unoriented gradient magnitude GF t,t+t0 :</p><formula xml:id="formula_1">GF t,t+t0 = |∇f t,t+t0 |<label>(1)</label></formula><p>Since both forward flow f t,t+t0 and backward flow f t,t−t0 provide motion information from frame t, in order to achieve robustness, we compute GF t,t−t0 similarly together with GF t,t+t0 and consider their geometric mean as one occlusion motion feature OM F 1 : • Motion Feature 2 The second occlusion motion feature OM F 2 models the fact that the consistency of the flow f t,t+t0 and reverse flow f t+t0,t is not satisfied when occlusion occurs <ref type="bibr" target="#b21">[22]</ref>. We measure these inconsistencies using both location and angle, illustrated in <ref type="figure" target="#fig_1">Fig. 4</ref>. Let f l and f ′ l denote the flow values at location l in the forward and reverse flow maps f t,t+t0 and f t+t0,t , respectively. If a point located at l in frame t is visible at t and t + t 0 , its correspondence in frame t+t 0 should be located at l+f l and should return to its start position l after being transported to frame t by the reverse flow f ′ l+f l . And with respect to angle, flow f l and reverse flow f ′ l+f l should be π apart if consistent. Hence, we measure these inconsistencies as follow:</p><formula xml:id="formula_2">OM F 1 = GF t,t+t0 * GF t,t−t0<label>(2)</label></formula><formula xml:id="formula_3">Γ l = |f l + f ′ l+f l | (3) Λ l =    0 P |Q arccos{ −f l ·f ′ l+f l |f l ||f ′ l+f l | } others<label>(4)</label></formula><p>where P and Q represent |f l | &lt; δ and |f ′ l+f l | &lt; δ, respectively, and are used to filter out the likely static pixels and prevent the denominator of the formulation above from being 0 (δ = 0.01 * max l (|f l |) for each frame in the experiments). Since both Γ and Λ describe inconsistent properties when occlusions occur, we combine them to obtain our inconsistency descriptor IC t,t+t0 , via a Gaussian smoothness process:</p><formula xml:id="formula_4">IC t,t+t0 (l) = l * σ(d − |l * − l|)e − |l * −l| 2 2 Γ l * Λ l * (5)</formula><p>where σ(x) = 1 when x ≥ 0 and 0 otherwise, and d = 2 in the experiments. Similar to OM F 1 , OM F 2 also takes the backward flow into consideration and is defined as:</p><formula xml:id="formula_5">OM F 2 = IC t,t+t0 * IC t,t−t0 (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image-level Reasoning via CRFs</head><p>We then adopt CRFs to efficiently integrate patch-based occlusion boundary maps and soft contextual correlations between neighboring pixels, so as to achieve global occlusion boundary estimation for the entire image. Here, we consider the most common pairwise CRF with 4neighborhood system used in computer vision and image analysis <ref type="bibr" target="#b2">3</ref> . In the CRF model, the nodes correspond to the pixel lattice and the edges to pairs of neighboring nodes. Let V and E denote the node set and the edge set, respectively. The CRF energy is defined as:</p><formula xml:id="formula_6">E(x) = i∈V θ i (x i ) + {i,j}∈E θ ij (x ij )<label>(7)</label></formula><p>Unary potentials (θ i (·)) i∈V are used to encode the data likelihood on individual pixels based on the patch-based probabilistic labeling maps provided by the CNN presented in Sec. 2.1, by defining θ i (·) as the negative logarithm of the average probabilityp i (·) over all output patches that cover the pixel i:</p><formula xml:id="formula_7">θ i (x i ) = − logp i (x i )<label>(8)</label></formula><p>Let R i denote the deep contextual features of the local patch centered at pixel i provided by maxp3 layer of our CNN, and the l 2 norm between R i and R j is measured to capture the dissimilarity between neighboring pixels i and j. To penalize different labels between neighboring pixels, the Pairwise potentials (θ ij (·)) {i,j}∈E between pairs of nodes are defined as:</p><formula xml:id="formula_8">θ ij (x i , x j ) = 0 if x i = x j w · exp{− R i − R j } otherwise (9)</formula><p>where w is a weight coefficient balancing the importance of the unary and pairwise terms (w = 2.1 in the experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Implementation Details</head><p>We adopt the region detector provided in <ref type="bibr" target="#b1">[2]</ref>, which produces a large number of small regions, so as to preserve nearly all types of boundaries, including occlusion boundaries. The occlusion boundary detection boils down to a binary classification problem which determines whether the boundary between regions is or is not an occlusion boundary, which is the same setting as many previous work (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>). In order to address the ground truth labeling bias (e.g., the original set of boundaries created by <ref type="bibr" target="#b1">[2]</ref> are often 1 or 2 pixels away from the corresponding ground truth boundaries drawn by hand <ref type="bibr" target="#b39">[40]</ref>), we consider all pixels within 2 pixels of the boundaries obtained by <ref type="bibr" target="#b1">[2]</ref> to produce image patches 4 . To balance the number of positive patches (patches containing an occlusion boundary curve) and negative patches during training, we randomly sample 100,000 training patches in a 1:1 ratio.</p><p>The 3 image and 2 motion cue channels are the CNN input to learn internal correlations around occlusion boundaries and predict probabilistic labeling maps and extract deep contextual features. See Sec. 2.1 for the motion cues computation and structured CNN framework. Our structured CNN model is built based on Caffe <ref type="bibr" target="#b23">[24]</ref>, developed by the Berkeley Vision And Learning Center (BVLC) and community contributors. The CRF model is then constructed to globally estimate occlusion boundaries for each image using the probabilistic labeling maps and the deep contextual features provided by the learned CNN model. Regarding the CRF inference, many powerful off-the-shelf algorithms can be directly applied to solve the CRF model <ref type="bibr" target="#b47">[48]</ref>. We simply used sum-product loopy belief propagation <ref type="bibr" target="#b51">[52]</ref> to estimate approximate-marginal probabilities of all nodes/pixels via message passing over the graph, so as to get a probabilistic boundary labeling map on the entire image and directly compare with previous methods using the same quality metric, i.e., F-measure.</p><p>In the final step, we apply the method in Arbelaez et al. <ref type="bibr" target="#b1">[2]</ref> to remove isolated pixels and connect disconnected short lines that might belong to a long boundary in our probabilistic occlusion boundary map η. This produces contour boundary map Ω. We then combine η and Ω by learning a weight factor α using SVM to get obtain our final occlusion boundary detector ξ:</p><formula xml:id="formula_9">ξ = α * η + (1 − α) * Ω<label>(10)</label></formula><p>In this paper, the final α is 0.65.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results</head><p>Following most previous work on occlusion boundary detection, such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>, we evaluate the performance of the proposed detector on the CMU benchmark <ref type="bibr" target="#b39">[40]</ref>, and perform quantitative comparison with previous work using the precision (P re) vs. recall (Rec) curve and F-measure (F-measure = 2 * P re * Rec P re+Rec ) as quality measures. In addition, we also perform quantitative evaluation on the datasets provided by <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b34">[35]</ref> and compare with their methods. In the following, we will first exhibit the obtained qualitative and quantitative results, and then empirically analyze the importance of several major components via the comparison between the corresponding variants of the proposed detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>F-measure Stein et al. <ref type="bibr" target="#b39">[40]</ref> 0.48 Sargin et al. <ref type="bibr" target="#b35">[36]</ref> 0.57 He et al. <ref type="bibr" target="#b17">[18]</ref> 0.47 Sundberg et al. <ref type="bibr" target="#b43">[44]</ref> 0.62 Leordeanu et al. <ref type="bibr" target="#b26">[27]</ref> 0.62 Our detector (probability) 0.71 Our detector (hard) 0.63 <ref type="table">Table 1</ref>. F-measure values (the average of maximal F-measure values over the whole benchmark) obtained by the considered methods on the CMU benchmark.</p><p>Algorithm F-measure Dataset <ref type="bibr" target="#b43">[44]</ref> Dataset <ref type="bibr" target="#b34">[35]</ref> Sundberg et al. <ref type="bibr" target="#b43">[44]</ref> 0.56 N/A Raza et al. <ref type="bibr" target="#b34">[35]</ref> N/A 0.60 Our detector 0.59 0.63 Qualitative and Quantitative Results A representative set of qualitative results on the CMU benchmark are exhibited in <ref type="figure">Fig. 5</ref>. The quantitative comparison with several important previous methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> is shown in Tab. 1 and <ref type="figure">Fig. 2</ref>, where the quantitative results of the previous methods are taken from their papers. Specifically, the P recision vs. Recall curves of different methods are shown in <ref type="figure">Fig. 2</ref>, which indicates that our detector significantly outperforms previous methods, especially withn a recall interval of [0.6, 0.9].</p><p>The evaluation based on F-measure is shown in Tab. 1, which also demonstrates a significant improvement over previous methods. Following previous work, we range the threshold on the probabilistic occlusion boundary map obtained for each image and compute the average of the maximal F-measure (AMF) across the whole dataset, and report the obtained results in Tab. 1. Moreover, considering that: (i) the optimal threshold that leads to the maximal Fmeasure of the occlusion boundary map generally varies between input test images, (ii) hard boundary labeling results (i.e., each pixel is labeled either 0 or 1) is often desirable for certain research problems and applications, we also report in Tab. 1 the average F-measure on the hard occlusion labeling maps 5 obtained by our method with the same parameter setting for all testing data.</p><p>Finally, quantitative comparison with <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b34">[35]</ref> on their datasets is shown in Tab. 2 and also demonstrates the <ref type="bibr">Figure 5</ref>. Representative occlusion boundary detection results. Each row corresponds to one testing sequence in the CMU benchmark <ref type="bibr" target="#b39">[40]</ref> and consists of (from left to right): a reference frame, the occlusion boundary ground truth, the occlusion boundary map obtained by Stein et al. <ref type="bibr" target="#b39">[40]</ref> (F-measure = 0.48), global probability boundary (gPb) map obtained by Arbelaez et al. <ref type="bibr" target="#b1">[2]</ref> (F-measure = 0.53), and the occlusion boundary map obtained by our method (F-measure = 0.71).</p><p>Features F-measure Image 0.60 Image + OMF1 0.65 Image + OMF2 0.64 Image + OMF1 + OMF2 0.71 <ref type="table">Table 3</ref>. Temporal contextual exploration. The contribution of the motion cues in the whole boundary detector is demonstrated.</p><p>superiority of our detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of Temporal Cues</head><p>Based on Classic-NL <ref type="bibr" target="#b40">[41]</ref>, we performed experiments to estimate the contribution of each type of occlusion motion context to our algorithm. We can see from Tab. 3 that: (i) motion contexts are important cues that have a large impact on the performance of the occlusion boundary detection, and (ii) the two motion features used in our method both significantly contribute to the performance of the method, and jointly using them achieves the highest accuracy. Furthermore, in order to evaluate how the method's performance depends on the accuracy of optical flow computation, in Tab. 4, we report the F-measures using three typical optical flow algorithms proposed by: Broxs et al. <ref type="bibr" target="#b9">[10]</ref>, Sun et al. <ref type="bibr" target="#b40">[41]</ref> and Weinzaepfel et al. <ref type="bibr" target="#b49">[50]</ref>. These similar F-measures demonstrate that our method is quite robust with respect to the choice of optical flow algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optical Flow</head><p>F-measure LDOF <ref type="bibr" target="#b9">[10]</ref> 0.68 Classic-NL <ref type="bibr" target="#b40">[41]</ref> 0.71 Deepflow <ref type="bibr" target="#b49">[50]</ref> 0.69 <ref type="table">Table 4</ref>. F-measures using different optical flow algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different CNN Frameworks</head><p>The way contextual information is explored via local patches is a key factor of the method, since the edge and node potentials in the final CRF framework are related to the contextual information aggregated by CNNs. In order to validate our choice (i.e., L2S: we learn the mapping from a large patch "L" to a small patch "S" with the same center with "L" ), we compare it with four variants, which are: (i) L2P: we learn the mapping from "L" to the pixel at the center of "L"; (ii) L2SP: we learn the mapping from "L" to "S" by independently learning the mapping to each individual pixel located within "S"; (iii) L2L: we learn the mapping from "L" to "L"; and (iv) S2S: we learn the mapping from "S" to "S". The results are shown in Tab. 5, from which we observe that: (i) the F-measure by S2S is obviously lower than that of other methods since the small input image patches contain much less contextual information and the contextual information of the surrounding area is ignored; (ii) compared to L2S, the CNN is slightly less effective at extracting discriminative spatial contextual information when learning the mapping from "L" to its center pixel, each individual pixel located within "S", and "L" itself (L2P, L2SP, and L2L). We explain these differences as follows: (i) L2P and L2SP: the CNN concentrates more on learning the differences between input samples to binary classify large patches and ignores correlations around occlusion edges within local image patches; (ii) L2L: on the one hand, the fixed training set becomes over sparse when the size of labeling map is too large, in which case the CNN can only learn superficial structural features; on the other hand, contextual correlation between the labeling of pixels and the observations from the surrounding area is not considered; and (iii) L2S: it properly handles the aforementioned issues exhibited in the variants so as to achieve better discriminative structured features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we aim to exploit contextual information, including local structural boundary patterns, observations from surrounding regions, temporal context, and soft contextual correlations between neighboring pixels to improve performance of occlusion boundary detection. Computed occlusion motion cues and color cues are fed into a CNN framework to obtain a probabilistic occlusion boundary map on a small patch from a large patch with the same center and also to aggregate deep contextual features. Based on these, a CRF model is then adopted to achieve global occlusion boundary estimation. Our detector significantly outperforms the current state-of-the-art (e.g., F-measure increases from 0.62 <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44]</ref> to 0.71 on the CMU benchmark). Last but not least, we empirically demonstrate the importance of the temporal contextual cues and the advantage of our approach to exploring contextual information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the CNN architecture and the output of several layers. For a 27 × 27 patch of the given input sequence, we first extract 5 (3 static + 2 motion) initial feature maps, which is the input of the CNN. The output of M axp3 layer corresponds to deep features that aggregate the high-level contextual information (referred to as deep contextual features). f c3 layer outputs a probabilistic labeling map on a 7 × 7 patch. of size 27 × 27 (detailed in Sec. 2.1.2). The CNN structure can be described by the size of the feature map at each layer as follows: conv1 (32@25*25)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of flow inconsistencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Comparison with<ref type="bibr" target="#b43">[44]</ref> and<ref type="bibr" target="#b34">[35]</ref> on their datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 5. F-measures using different mapping methods.</figDesc><table>Mapping methods F-measure 
L2P 
0.66 
L2SP 
0.65 
L2L 
0.66 
S2S 
0.63 
L2S 
0.71 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The whole method is modular with respect to the choice of CRFs. A main reason to adopt the pairwise CRF with 4-neighborhood system in the experiments, instead of more sophisticated CRFs, is to demonstrate more clearly the effectiveness of the whole method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This operation also prevents CNN from paying too much attention to the center of the label patch and assigning a high probability to it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In the experiments, the hard occlusion labeling maps is computed from the obtained probabilistic occlusion boundary map with a threshold of 0.4.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is partly supported by Australian Research Council Projects (DP-140102164, FT-130101457, LE140100061) and CNRS INS2I-JCJC-INVISANA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal t-junctions for occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Apostoloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detachable object detection: Segmentation and depth ordering from short-baseline video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ayvaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1942" to="1951" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep learning. Book in preparation for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the semantics of a glance at a scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining intensity and motion for incremental segmentation and tracking over long image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic detection and tracking of motion boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="245" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Markov random fields for vision and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning mid-level features for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A statistical model for general contextual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carbonetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Motion segmentation and depth ordering using an occlusion detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1171" to="1185" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nˆ4-fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The perception of surface layout: A classification. Unpublished &quot;Purple Perils&quot; essay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Occlusion boundary detection using pseudo-depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recovering occlusion boundaries from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to find occlusion regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video repairing under variable illumination using cyclic motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="832" to="839" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structured class-labels in random forests for semantic image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buló</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient closed-form solution to generalized boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shape driven kernel adaptation in convolutional neural network for robust facial trait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond categories: The visual memex model for reasoning about object relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning object relationships via graph-based context model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TICS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Camouflaging an object from many viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Flint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Finding temporally consistent occlusion boundaries in videos using geometric context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probabilistic occlusion boundary detection on spatiotemporal lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Sargin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nešić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Occlusion boundaries from motion: Low-level detection and mid-level reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="325" to="357" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="137" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Occlusion boundary detection and figure/ground assignment from optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context-based vision system for place and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Auto-context and its application to highlevel vision tasks and 3d brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Markov random field modeling, inference &amp; learning in computer vision &amp; image understanding: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="1610" to="1627" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to detect motion boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Comparing the mean field method and belief propagation for approximate inference in mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Mean Field Methods -Theory and Practice</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Model selection for unsupervised learning of visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="201" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
