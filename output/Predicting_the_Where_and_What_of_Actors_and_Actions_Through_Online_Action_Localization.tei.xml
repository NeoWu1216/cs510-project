<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting the Where and What of actors and actions through Online Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
							<email>ksoomro@cs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="institution">University of Central Florida (UCF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
							<email>haroon@cs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="institution">University of Central Florida (UCF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@cs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="institution">University of Central Florida (UCF)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting the Where and What of actors and actions through Online Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel approach to tackle the challenging problem of 'online action localization' which entails predicting actions and their locations as they happen in a video. Typically, action localization or recognition is performed in an offline manner where all the frames in the video are processed together and action labels are not predicted for the future. This disallows timely localization of actions -an important consideration for surveillance tasks.</p><p>In our approach, given a batch of frames from the immediate past in a video, we estimate pose and oversegment the current frame into superpixels. Next, we discriminatively train an actor foreground model on the superpixels using the pose bounding boxes. A Conditional Random Field with superpixels as nodes, and edges connecting spatio-temporal neighbors is used to obtain action segments. The action confidence is predicted using dynamic programming on SVM scores obtained on short segments of the video, thereby capturing sequential information of the actions. The issue of visual drift is handled by updating the appearance model and pose refinement in an online manner. Lastly, we introduce a new measure to quantify the performance of action prediction (i.e. online action localization), which analyzes how the prediction accuracy varies as a function of observed portion of the video. Our experiments suggest that despite using only a few frames to localize actions at each time instant, we are able to predict the action and obtain competitive results to state-of-the-art offline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Predicting what and where an action will occur is an important and challenging computer vision problem for automatic video analysis <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4]</ref>. In many ap-Online&amp;Action&amp;Localization&amp;=&amp;Action&amp;Detection&amp;+&amp;Prediction Off line&amp;Action&amp;Localization&amp;=&amp;Action&amp;Detection&amp;+&amp;Recognition <ref type="bibr">Figure 1</ref>. This figure illustrates the problem we address in this paper. The top row shows the case when we have an entire video to detect and recognize actions, i.e., offline action localization. The bottom row is an example of online action localization, which involves predicting the action class (e.g. Kicking) as well as the location of the actor in every frame, as the video is streamed.</p><p>plications associated with monitoring and security, it is crucial to detect and localize actions in a timely fashion. A particular example is detection and localization of undesirable or malicious actions. There have been recent efforts to predict activities by early recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14]</ref>. These methods only attempt to predict the label of the action, what of an action, without any localization. Thus, the important question about where an action is being performed cannot be answered easily.</p><p>Existing action localization methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref> classify and localize actions after completely observing an entire video sequence (top row in <ref type="figure">Fig. 1</ref>). The goal is to localize an action by finding the volume that encompasses an entire action. Some approaches are based on sliding-windows <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22]</ref>, while others segment the video into supervoxels which are merged into action proposals <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. The action proposals from either methods are then labeled using a classifier. Essentially, an action segment is classified after it has been localized. Since offline methods have whole video at their disposal, they can take advantage of observing entire motion of action instances. In this paper, we address the problem of Online Action Localization, which aims at localizing an action and predicting its class label in a streaming video (see bottom row in <ref type="figure">Fig. 1</ref>). Online action localization involves the use of limited motion information in partially observed videos for frame-byframe action localization and label prediction.</p><p>Low-level motion features, both hand-crafted <ref type="bibr" target="#b33">[34]</ref> and deep learned <ref type="bibr" target="#b35">[36]</ref> have imparted significant gains to the performance of action recognition and localization algorithms. They have been extensively employed in various action recognition methods. However, human actions inherently consists of articulation which low-level features cannot model explicitly. On the other hand, the compact and low-dimensional nature of highlevel representations such as human poses (locations of different joints), makes them sensitive and unstable for action recognition. An incorrect estimation of pose translates to large variation in descriptors that aim to capture the configuration of joints both in space and time. This drawback can hamper the performance of any action localization and recognition algorithm. Nonetheless, a few methods (e.g. <ref type="bibr" target="#b34">[35]</ref>) have successfully employed pose features for offline action localization.</p><p>In this paper, we propose to use the high level structural information using pose in conjunction with a superpixel based discriminative actor foreground model that distinguishes the foreground action and the background. This superpixel-based model incorporates visual appearance using color features, as well as structural cues through joint locations. Using the superpixelbased actor foreground model we generate a confidence map, that is later used to predict and locate the action segments by inferring on a Conditional Random Field. Since the appearance of an actor changes due to articulation and camera motion, we retrain foreground model as well as impose spatio-temporal constraints on poses in an online manner to maintain representation that is both robust and adaptive.</p><p>In summary, 1) we address the problem of Online Action Localization in a streaming video, 2) by using highlevel pose estimation to learn a mid-level superpixelbased foreground model at each time instant. 3) The label and confidences for action segments are predicted using dynamic programming on SVM scores trained on partial action clips. Finally, 4) we also introduce an evaluation measure to quantify performance of action prediction and online localization. The rest of the paper is organized as follows. In Sec. 2 we review literature relevant to our approach. Sec. 3 covers the technical details of our approach. We report results in Sec. 4 and conclude with suggestions for future work in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Online Action Prediction aims to predict actions from partially observed videos without any localization. These methods typically focus on maximum use of temporal, sequential and past information to predict labels. Li and Fu <ref type="bibr" target="#b17">[18]</ref> predict human activities by mining sequence patterns, and modeling causal relationships between them. Zhao et al. <ref type="bibr" target="#b44">[45]</ref> represent the structure of streaming skeletons (poses) by a combination of humanbody-part movements and use it to recognize actions in RGB-D. Hoai and De la Torre <ref type="bibr" target="#b7">[8]</ref> simulate the sequential arrival of data while training, and train detectors to recognize incomplete events. Similarly, Lan et al. <ref type="bibr" target="#b15">[16]</ref> propose hierarchical 'movemes' to describe human movements and develop a max-margin learning framework for future action prediction.</p><p>Ryoo <ref type="bibr" target="#b25">[26]</ref> proposed integral and dynamic bag-ofwords for activity prediction. They divide the training and testing videos into small segments and match the segments sequentially using dynamic programming. Kong et al. <ref type="bibr" target="#b13">[14]</ref> proposed to model temporal dynamics of human actions by explicitly considering all the history of observed features as well as features in smaller temporal segments. Yu et al. <ref type="bibr" target="#b42">[43]</ref> predict actions using Spatial-Temporal Implicit Shape Model (STISM), which characterizes the space-time structure of the sparse local features extracted from a video. Cao et al. <ref type="bibr" target="#b1">[2]</ref> perform action prediction by applying sparse coding to derive the activity likelihood at small temporal segments, and later combine the likelihoods for all the segments. In contrast, we perform both action prediction as well as localization in an online manner.</p><p>Offline Action Localization has received significant attention in the past few years <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b10">11]</ref>. The first category of approaches uses either rectangular tubes or cuboid-based representations. Lan et al. <ref type="bibr" target="#b16">[17]</ref> treated the human position as a latent variable, which is inferred simultaneously while localizing an action. Yuan et al. <ref type="bibr" target="#b43">[44]</ref> used branch-and-bound with dynamic programming, while Zhou et al. <ref type="bibr" target="#b45">[46]</ref> used a split-and-merge algorithm to obtain action segments that are then classified with LatentSVM <ref type="bibr" target="#b5">[6]</ref>. Oneata et al. <ref type="bibr" target="#b21">[22]</ref> presented an approximation to Fisher Vectors for tractable action localization. Tran et al. <ref type="bibr" target="#b29">[30]</ref> used Structured SVM to localize actions with inference performed using Max-Path search method. <ref type="bibr">Ma et al. [19]</ref> automatically discovered spatio-temporal root and part filters, whereas Tian  <ref type="figure">Figure 2</ref>. This figure shows the framework of the approach proposed in this paper. (a) Given an input video, (b) we over-segment each frame into superpixels and detect poses using an off-the-shelf method <ref type="bibr" target="#b39">[40]</ref>. (c) An appearance model is learned using all the superpixels inside a pose bounding box as positive, and those outside as negative samples. (d) In a new frame, the appearance model is applied on each superpixel of the frame to obtain a foreground likelihood. (e) To handle the issue of visual drift, poses are refined using spatio-temporal smoothness constraints on motion and appearance. (f) Finally, a CRF is used to obtain local action proposals, which are then utilized to predict the action through dynamic programming on SVM scores. et al. <ref type="bibr" target="#b28">[29]</ref> developed Spatio-temporal Deformable Parts Model <ref type="bibr" target="#b5">[6]</ref> to detect actions in videos and can handle deformities in parts, both in space and time. Recently, Yu and Yuan <ref type="bibr" target="#b41">[42]</ref> proposed a method for generating action proposals obtained by detecting tubes with high actionness scores after non-maximal suppression.</p><p>The second category uses either superpixels or supervoxels as the base representations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref>. Jain et al. <ref type="bibr" target="#b9">[10]</ref> recently proposed a method that extends selective search approach <ref type="bibr" target="#b30">[31]</ref> to videos. They merge supervoxels using appearance and motion costs and produce multiple layers of segmentation for each video. Gkioxari and Malik <ref type="bibr" target="#b6">[7]</ref> use selective search <ref type="bibr" target="#b30">[31]</ref> to generate candidate proposals for video frames, whose spatial and motion Convolutional Neural Network (CNN) features are evaluated using SVMs. The per-frame action detections are then linked temporally for localization. There have been few similar recent methods for quantifying actionness <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref>, which yield fewer regions of interest in videos. Similar to these methods, our approach can delineate contours of an action, but with the goal of performing prediction and localization in a streaming fashion.</p><p>Pose for Action Recognition was used by Maji et al. <ref type="bibr" target="#b19">[20]</ref>, who implicitly captured poses through 'poselet activation vector' and employed those for action recognition in static images. However, such a representation is not useful for detecting an action foreground. Xu et al. <ref type="bibr" target="#b38">[39]</ref> detect poses through <ref type="bibr" target="#b39">[40]</ref> and couple them with independently computed local motion features around the joints for action recognition. Wang et al. <ref type="bibr" target="#b32">[33]</ref> also extended <ref type="bibr" target="#b39">[40]</ref> to videos and represented videos in terms of spatio-temporal configurations of joints to perform action recognition. Raptis and Sigal <ref type="bibr" target="#b23">[24]</ref> recognize and detect interactions from videos by modeling poselets as latent variables in a structured SVM formulation. Joint recognition of action and pose estimation in videos was recently proposed by Nie et al. <ref type="bibr" target="#b36">[37]</ref>. They divide the action into poses, spatio-temporal parts and then parts, and model their inter-relationships through And-Or graphs. Pirsiavash et al. <ref type="bibr" target="#b22">[23]</ref> predict quality of sports actions by training a regression model from spatio-temporal pose features, to scores from expert judges. Poses were recently used for offline action localization by Wang et al. <ref type="bibr" target="#b34">[35]</ref>, who detect actions using a unified approach that discovers action parts using dynamical poselets, and the relations between them. In contrast to these methods, we use pose in conjunction with low-level iDTF features <ref type="bibr" target="#b33">[34]</ref> and mid-level superpixels. Moreover, we predict and localize actions in an online manner in partially observed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Localizing and Predicting Actions</head><p>The proposed approach <ref type="figure">(Fig. 2</ref>) begins by segmenting the testing video frames into superpixels and detecting pose hypotheses within each frame. The features computed for each superpixel are used to learn a superpixel-based appearance model, which distinguishes the foreground from the background. Simultaneously, the conditional probability of pose hypotheses at current time-step (frame) is computed using pose confidences and consistency with poses in previous frames. The superpixel and pose-based foreground probability is used to infer the action location at each frame through Conditional Random Field. The action label is predicted within the localized action bounding box through dynamic programming using scores from Support Vector Machines (SVMs) on short video clips. These SVMs were trained on temporal segments of the training videos. After localizing action at each time-step (frame), we refine poses in a batch of few frames by imposing spatio-temporal consistency. Similarly, the appearance model is updated to avoid visual drift. This process is repeated for every frame in an online manner (see <ref type="figure">Fig.  2</ref>) and gives action localization and prediction at every frame.</p><p>Let s t represent superpixels by its centroid in frame t and p t represent poses in frame t. Since our goal is to localize the action in each frame, we use X t to represent, a sequence of bounding boxes (tube) in a small window of δ frames. Each bounding box is represented by its centroid, width and height. Similarly, let S t and P t respectively represent all the superpixels and poses within that time window. Given the pose and superpixel-based observations till time t, S 1:t and P 1:t , the state estimate X t at time t is obtained using the following equation through Bayes Rule:</p><formula xml:id="formula_0">p(X t |S 1:t , P 1:t ) = Z −1 p(S t |X t ).p(P t |X t ). p(X t |X t−1 ).p(X t−1 |S 1:t−1 , P 1:t−1 )dX t−1 , (1)</formula><p>where Z is the normalization factor, and the state transition model is assumed to be Gaussian distributed, i.e., p(X t |X t−1 ) = N (X t ; X t−1 , Σ). Eq. 1 accumulates the evidence over time on the superpixels and poses in batch-streaming mode. The state which maximizes the posterior (MAP) estimate in Eq. 1 is selected as the new state. Next, we define the pose and superpixel based foreground likelihoods used for estimating Eq. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Superpixel-based Foreground Likelihood</head><p>Learning an appearance model helps in distinguishing the foreground actions from the background. Given foreground and background superpixels in the previous frames t−δ : t−1, we group them into k = 1 . . . K clusters. Furthermore, let ξ k define the ratio of foreground to background superpixels for the kth cluster. Then, the appearance-based foreground score is given by:</p><formula xml:id="formula_1">H fg (s t ) = exp φ color (s t ) − c k ) r k · ξ k + exp φ flow s t ) − µ k σ k ,<label>(2)</label></formula><p>where c k is the center, r k is the radius, µ k is the mean optical flow and σ k is the flow variance for kth cluster. In Eq. 2, the clusters are updated incrementally at each time-step (frame) to recover from the visual drift using a temporal window of past δ frames. Note that, background pixels within a foreground bounding box are inevitably considered as foreground, and introduce noise during model update. The ξ k helps to compensate for this issue by quantifying the foreground/background ratio for each cluster. Finally, the superpixel-based foreground likelihood in Eq. 1 is given as: p(S t |X t ) = α fg · H fg (s t ), where α fg is the normalization factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose-based Foreground Likelihood</head><p>We use a pre-trained pose detector to obtain pose hypotheses in each frame. Each pose p t is graphically represented with a tree, given by T = (Π, Λ). The body joints π ∈ Π are based on appearance, that are connected by λ ∈ Λ edges capturing deformations. The joint j with its location in pose p t is represented by π j t , consisting of its x and y locations. Then, the cost for a particular pose p t is the sum of appearance and deformation costs:</p><formula xml:id="formula_2">H raw (p t ) = j∈Πt Ψ π j t + (j,j ′ )∈Λt Φ π j t , π j ′ t ,<label>(3)</label></formula><p>where Ψ and Φ are linear functions of appearance features of pose joints, and their relative joint displacements (deformations) w.r.t each other. Poses are obtained using <ref type="bibr" target="#b39">[40]</ref>, which optimizes over latent variables that capture different joint locations and pose configurations. Since the pose estimation in individual frames is inherently noisy, and does not take into account the temporal information available in videos, we impose the following smoothness constraints in the previous δ frames to reevaluate poses in Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appearance Smoothness of Joints:</head><p>Since the appearance of a joint is not expected to change drastically in a short window of time, we impose the appearance consistency between superpixels at joint locations:</p><formula xml:id="formula_3">J app (p t ) = |Πt| j=1 H fg (ŝ j t ) − H fg (ŝ j t−1 ) ,<label>(4)</label></formula><p>whereŝ j t is the enclosing superpixel of the joint π j t . Location Smoothness of Joints: We ensure that joint locations are smooth over time. This is achieved by fitting a spline to each joint on the past δ frames, γ j t . Then the location smoothness cost is given by:</p><formula xml:id="formula_4">J loc (p t ) = |Πt| j=1 γ j t − π j t .<label>(5)</label></formula><p>Scale Smoothness of Joints: Let j min , j max respectively denote minimum and maximum for γ, i.e. the vertical dimension of the bounding box circumscribing all the splines fitted on joints. And j ′ min , j ′ max denote minimum and maximum for joints in actual poses π t ∈ Π t . Then, the scale smoothness cost essentially computes the overlap between vertical dimensions of the two:</p><formula xml:id="formula_5">J sc (p t ) = (j max − j min ) − (j ′ max − j ′ min ) . (6)</formula><p>The cost of a particular pose is defined as its raw cost plus the smoothness costs across time, i.e., H pose (p t ) = H raw (p t )+J app (p t )+J loc (p t )+J sc (p t ). Similar to Sec. 3.1, we use a temporal window of past δ frames to refine the pose locations. We propose an iterative approach to select poses in the past t − δ : t frames. Given an initial set of poses, we fit a spline to each joint π j t . Then, our goal is to select a set of poses from t − δ to t frames, such that the following cost function is minimized:</p><formula xml:id="formula_6">( * p t−δ , . . . , * p t ) = arg min p t−δ ,...,pt t τ =t−δ H pose (p τ ) . (7)</formula><p>This function optimizes over pose detection, and the appearance, location and scale smoothness costs of joints (see <ref type="figure">Fig.2</ref> (e)) by greedily selecting the minimum cost pose in every frame through multiple iterations. Finally, the pose-based foreground likelihood in Eq. 1 is given by p(P t |X t ) = exp(α pose · H pose (p t )), where α pose is the normalization factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Action Localization using CRF</head><p>Once we have the superpixel and pose-based foreground likelihoods, we infer the action segments using a history of δ frames. Although the action location is computed online for every frame, using past δ frames adds robustness to segmentation. We form a graph G(V, E) with superpixels as nodes connected through spatial and temporal edges. Let variable a denote the foreground/background label of a superpixel. Then, the objective function of CRF becomes:</p><p>− log p(a t−δ , . . . , a t |s t−δ , . . . , s t , p t−δ , . . . , p t ) = t τ =t−δ Θ a τ |s τ , p τ unary potential</p><formula xml:id="formula_7">+ Υ a τ , a ′ τ |s τ , s ′ τ spatial smoothness + t−1 τ =t−δ Γ a τ , a ′ τ +1 |s τ , s ′ τ +1 temporal smoothness ,<label>(8)</label></formula><p>where the unary potential, with the associated weights symbolized with α, is given by:</p><p>Θ a τ |s τ , p τ = α fg H fg (s τ ) + α pose H pose (p τ ), <ref type="bibr" target="#b8">(9)</ref> and the spatial and temporal binary potentials, with weights β and distance functions d, are given by:</p><formula xml:id="formula_8">Υ a τ , a ′ τ |s τ , s ′ τ = β col d col (s τ , s ′ τ ) + β hof d hof (s τ , s ′ τ ) + β µ d µ (s τ , s ′ τ ) + β mb d mb (s τ , s ′ τ ) + β edge d edge (s τ , s ′ τ ),<label>(10)</label></formula><p>and</p><formula xml:id="formula_9">Γ a τ , a ′ τ −1 |s τ , s ′ τ −1 = β col d col (s τ , s ′ τ −1 ) + β hof d hof (s τ , s ′ τ −1 ) + β µ d µ (s τ , s ′ τ −1 ),<label>(11)</label></formula><p>respectively. In Eqs. 10, and 11, β col d col (.) is the cost of color features in HSI color space, β hof d hof (.) and β µ d µ (.) compute compatibility between histogram of optical flow, and mean of optical flow magnitude, of two superpixels. Similarly, β mb d mb (.) and β edge d edge (.) quantify incompatibility between superpixels with prominent boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Action Prediction</head><p>Since localization requires predicting the class of an action in every frame of the streaming video, we make online prediction of actions in the tubes localized by our approach. Our aim is to capture the sequential information present in a video. For training, we divide the videos into 1 second temporal segments and train an SVM on each segment 0 → 1 sec, 1 → 2 sec, . . . of all the videos for that action. Given testing video segments, we apply SVM classification for all the 1 second training segments to each segment of the test video, and then use dynamic programming to accumulate matching confidences of the most similar sequence. At each step of the dynamic programming, the system effectively searches for the best matching segment that maximizes the SVM confidences from past segments. This method is applied independently for each action, and gives the confidence for each action. This shares resemblance to Dynamic Bag-Of-Words approach <ref type="bibr" target="#b25">[26]</ref> who used RBF function to compute distance between training and testing segments. Note that the performance of classification for action localization depends on the quality of localized tubes / cuboids, as the classifiers are only evaluated on such video segments. This is in contrast to other action prediction methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b7">8]</ref> which do not spatially localize the actions of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our online action localization approach on two challenging datasets: 1) JHMDB and 2) UCF Sports. We provide details for the experimental setup followed by the performance evaluation and analysis of the proposed algorithm. Features: For each frame of the testing video we extract superpixels using SLIC <ref type="bibr" target="#b0">[1]</ref>. This is followed by extraction of color features (HSI) for each superpixel in the frame, as well as improved Dense Trajectory features (iDTF: HOG, HOF, MBH, Traj) <ref type="bibr" target="#b33">[34]</ref> within the streamed volumes of the video. Each superpixel descriptor has a length of 512 and we set K = 20. The pose detections are obtained using <ref type="bibr" target="#b39">[40]</ref> and pose features using <ref type="bibr" target="#b11">[12]</ref>. We build a vocabulary of 20 words for each pose feature, and represent a pose with 180d vector. Parameters and Distance Functions: We use Euclidean distance for d µ , chi-squared distance for d hof and d col , and geodesic distance for d mb and d edge . We normalize the scores used in CRF, therefore, we set absolute values of all the parameters α and β to 1. Evaluation Metrics: Since the online localization algorithm generates tubes or cuboids with associated confidences, the Receiver Operating Characteristic (ROC) curves are computed at fixed overlap thresholds. Following experimental setup of Lan et al. <ref type="bibr" target="#b16">[17]</ref>, we show ROC @ 20% overlap. Furthermore, Area Under the Curve (AUC) of ROC at various thresholds gives an overall measure of performance.</p><p>Inspired from early action recognition and prediction works <ref type="bibr" target="#b25">[26]</ref>, we also quantify the performance as a function of Video Observation Percentage. For this method, the localization and classification for testing videos are sampled at different percentages of observed video (0, 0.1, 0.2, . . . , 1). The ROC curve is computed at multiple overlap thresholds, and AUC is computed under ROC curves at different thresholds. Baseline for Online Action Localization: We compare with offline methods which use entire videos to localize actions, and also compute results for a competitive online localization baseline for comparison. For the proposed baseline, we exhaustively generate bounding boxes with overlaps at multiple scales in each frame. These boxes are connected with appearance similarity costs. Over time, the boxes begin to merge into tubes. For temporal window of δ = 5 frames (same as our method), we evaluate each tube with classifiers for all the actions using iDT features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>JHMDB Dataset: The JHMDB <ref type="bibr" target="#b11">[12]</ref> dataset is a subset of the larger HMDB51 <ref type="bibr" target="#b14">[15]</ref> dataset collected from digitized movies and YouTube videos. It contains 928 videos consisting of 21 action classes. The dataset has annotations for all the body joints and has recently been used for offline action localization <ref type="bibr" target="#b6">[7]</ref>. We use a codebook size of 4000 to train SVMs using iDTF features. UCF Sports Dataset: The UCF Sports <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref> dataset consists of 150 videos with 10 action classes. We evaluated our approach using the methodology proposed by Lan et al. <ref type="bibr" target="#b16">[17]</ref>, with a train-test split and intersectionover-union criterion at an overlap of 20%. To train SVMs, we use a codebook size of 1000 on iDTFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Analysis</head><p>Action Prediction with Time: The prediction accuracy is evaluated with respect to the percentage of video observed. <ref type="figure" target="#fig_1">Fig. 3(a)</ref> shows the accuracy against time for JHMDB and UCF Sports datasets. It is evident that predicting the class of an action based on partial observation is very challenging, and the accuracy of correctly predicting the action increases as more information becomes available. An analysis of prediction accuracy per action class is shown in <ref type="figure" target="#fig_2">Fig. 4</ref> for (a) JHMDB and (b) UCF Sports datasets. This figure shows that certain actions are more challenging to predict compared to others. For example, the actions Jump and Swing Side are the most challenging actions when compared to Golf and Kicking. This is due to the difficulty in correctly estimating pose under high human body articulation.</p><p>Since each action has its own predictability, we analyze how early we can predict each action. We arbitrarily set the prediction accuracy to 30% and show the percentage of video observation required for each action of JHMDB and UCF Sports datasets in <ref type="table">Table 1</ref>    <ref type="table">Table 1</ref>. This figure shows the the percentage of video observation required to achieve a prediction accuracy of 30%. Results in the first two rows are from JHMDB, and the last row is from UCF Sports dataset. Actions with missing values indicate that they did not reach a prediction accuracy of 30% until video completion.</p><p>not reach such prediction accuracy even until the completion of the video. This shows the challenging nature of online action prediction and localization.</p><p>Action Localization with Time: To evaluate online performance, we analyze how the localization performance varies across time by computing accuracy as a function of observed video percentage. <ref type="figure" target="#fig_1">Fig. 3</ref>(b-c) shows the AUC against the percentage of observed video for different overlap thresholds (10% − 60%) for (b) JH-MDB and (c) UCF Sports. We compute the AUC with time in a cumulative manner such that the accuracy at 50% means localizing an action from start till one-half of the video has been observed. This gives an insight into how the overall localization performance varies as a function of time or observed percentage in testing videos. These graphs show that it is challenging to localize an action at the beginning of the video, since there is not enough discriminative motion observed by the algorithm to distinguish different actions. Furthermore, our approach first learns an appearance model from pose bounding boxes, which are improved and refined as time progresses. This improves the superpixel-based appearance confidence, which then improves the localization, and stabilizes the AUC. The curves also show that the AUC is inversely proportional to the overlap threshold.</p><p>There are two interesting observations that can be made from these graphs. First, for the JHMDB dataset in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>, the results improve initially, but then deteriorate in the middle, i.e. when the observation percentage is around 60%. The reason is that most of the articulation and motion happens in the middle of the video. Thus, the segments in the middle are the most difficult to localize, resulting in drop of performance. Second, the curves for UCF Sports in <ref type="figure" target="#fig_1">Fig. 3</ref>(c) depict a rather unexpected behavior in the beginning, where localization improves and then suddenly worsens at around 15% observation percentage. On closer inspection, we found that this is due to rapid motion in some of the actions, such as diving and swinging (side view). For these actions, the initial localization is correct when the actor is stationary, but both actions have very rapid motion in the beginning, which violates the continuity constraints applicable to many other actions. This results in a drop in performance, and since this effect accumulates as the percentage of video observation increases, the online algorithm never attains the peak again for many overlap thresholds despite observing the entire video.</p><p>Action Localization with Offline Methods: We also evaluate the performance of our method against existing offline state-of-the-art action localization methods. The curves for the proposed method is shown in red, while other offline localization methods including Lan et al. <ref type="bibr" target="#b16">[17]</ref>, Tian et al. <ref type="bibr" target="#b28">[29]</ref>, Wang et al. <ref type="bibr" target="#b34">[35]</ref>, Gemert et al. <ref type="bibr" target="#b31">[32]</ref>, Jain et al. <ref type="bibr" target="#b9">[10]</ref>, Jain et al. <ref type="bibr" target="#b10">[11]</ref>, and Gkioxari et al. <ref type="bibr" target="#b6">[7]</ref> are shown with different colors, with baseline for online localization in gray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5(a)</head><p>shows the results of the proposed method, on JH-MDB dataset, in red, and that of <ref type="bibr" target="#b6">[7]</ref> in blue. The difference in performance is attributed to the online vs. offline nature of the methods, as well as the use of CNN features by <ref type="bibr" target="#b6">[7]</ref>. Furthermore, we outperform a competitive online localization baseline shown in gray. A quantitative comparison on UCF Sports using AUC and ROC @ 20% is shown in <ref type="figure" target="#fig_3">Fig. 5(b)</ref> and (c) respectively.</p><p>Pose Refinement: Pose-based foreground likelihood refines poses in an iterative manner using spatio-temporal smoothness constraints. Our qualitative results in <ref type="figure" target="#fig_4">Fig. 6</ref> show the improvement in pose joint locations. Action Segments: Since we use superpixel segmentation to represent the foreground actor, our approach outputs action segments. Our qualitative results in <ref type="figure" target="#fig_6">Fig. 7</ref> show the fine contour of each actor (yellow) along with the ground truth (green). Using superpixels and CRF, we are able to capture the shape deformation of the actors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced a new prediction problem of online action localization where the goal is to simultaneously localize and predict actions in an online manner. We presented an approach which uses representations at different granularities -from high-level poses for initialization, mid-level features for generating action tubes, and low-level features such as iDTF for action prediction. We also refine pose estimation in a small batch of frames using spatio-temporal constraints. The localized tubes are obtained using CRF, and classification confidences come from dynamic programming on SVM scores. The intermediate results and ablation study indicate that such an approach is capable of addressing this difficult problem, and performing on par with some of the recent offline action localization methods. For future research, we plan to leverage training data to perform localization and prediction simultaneously by learning costs for superpixel merging for different actions.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>This figure shows action prediction and localization performance as a function of observed video percentage. (a) shows prediction accuracy for JHMDB and UCF Sports datasets; (b) and (c) show localization accuracy for JHMDB and UCF Sports, respectively. Different curves show evaluations at different overlap thresholds: 10% (red), 30% (green) and 60% (pink).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>This figure shows per-action prediction accuracy as a function of observed video percentage for (a) JHMDB and (b) UCF Sports datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>This figure shows localization results of proposed method along with existing methods on JHMDB and UCF Sports datasets. (a) shows AUC curves for JHMDB, while (b) and (c) show AUC and ROC @ 20%, respectively, for UCF Sports dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>This figure shows qualitative results for pose refinement. Results show a comparison of raw poses (top row) and refined poses (bottom row) for (a) Kicking and (b) Walking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>This figure shows qualitative results of the proposed approach, where each action segment is shown with yellow contour and ground truth with green bounding box. Results in the top three rows are from JHMDB, and the bottom three rows are from UCF Sports datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. Although we set a reasonable prediction target, certain actions do</figDesc><table>0.2 

0.4 
0.6 
0.8 
1 
0 

0.2 

0.4 

0.6 

0.8 

1 

Video Observation Percentage 

Accuracy 

Brush Hair 
Catch 
Clap 
Climb Stairs 
Golf 
Jump 
Kick Ball 
Pick 
Pour 
Pullup 
Push 

Run 
Shoot Ball 
Shoot Bow 
Shoot Gun 
Sit 
Stand 
Swing Baseball 
Throw 
Walk 
Wave 
Mean Accuracy 

0.2 
0.4 
0.6 
0.8 
1 
0 

0.2 

0.4 

0.6 

0.8 

1 

Video Observation Percentage 

Accuracy 

Diving 
Golf Swing 
Kicking 
Lifting 
Riding Horse 
Run 
SkateBoarding 
Swing Bench 
Swing Side 
Walking 
Mean Accuracy 

(a) 
(b) 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-theart superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<idno>2012. 5</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognize human activities from partially observed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Actionness ranking with lattice conditional ordinal random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving semantic concept detection through the dictionary of visuallydistinct elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Detecting actions, poses, and objects with relational phraselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno>ECCV. 2012. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. 2015. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action detection in complex scenes with spatial and temporal ambiguities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Event detection in crowded videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A discriminative model with multiple temporal scales for action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. 2014. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hierarchical representation for future action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative figurecentric models for joint action localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prediction of human activity by discovering temporal sequence patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action recognition and localization by hierarchical spacetime segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majiwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatiotemporal object detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient action localization with approximately normalized fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Assessing the quality of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>ECCV. 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Poselet key-framing: A model for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action mach: a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action localization in videos through context walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition in realistic sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision in Sports</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="181" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable part models for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Max-margin structured output regression for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Apt: Action localization psroposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video action detection with relational dynamic-poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04868</idno>
		<title level="m">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A unified framework for locating and recognizing human actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Combining skeletal pose with local motion for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Krovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Articulated Motion and Deformable Objects</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast action detection via discriminative random forest voting and top-k subvolume search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Goussies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Predicting human activities using spatio-temporal structure of interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Discriminative video pattern search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Online human gesture recognition from motion data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Z</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning spatial and temporal extents of human actions for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
