<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Icsi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berkeley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">T</forename><surname>Austin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umass</forename><surname>Lowell</surname></persName>
						</author>
						<title level="a" type="main">Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired imagesentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts. Current deep caption models can only describe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-sentence data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past year, several deep recurrent neural network models have demonstrated promising results on the task of generating descriptions for images and videos <ref type="bibr" target="#b49">[36,</ref><ref type="bibr" target="#b18">5,</ref><ref type="bibr" target="#b29">16,</ref><ref type="bibr" target="#b28">15,</ref><ref type="bibr" target="#b34">21]</ref>. Large corpora of paired images and descriptions, such as MSCOCO <ref type="bibr" target="#b33">[20]</ref> and Flickr30k <ref type="bibr" target="#b24">[11]</ref> have been an important factor contributing to the success of these methods. However, these datasets describe a relatively small variety of objects in comparison to the number of labeled objects in object recognition datasets, such as ImageNet <ref type="bibr" target="#b16">[3]</ref>. Consequently, though modern object recognition systems </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unpaired Text Data</head><p>A otter that is sitting in the water.</p><p>A dog sitting on a boat in the water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unpaired Image Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Existing Methods</head><p>Otter Alpaca</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pizza Bus</head><p>Yummy pizza sitting on the table.</p><p>Otters live in a variety of aquatic environments.</p><p>Pepperoni is a popular pizza topping. <ref type="figure">Figure 1</ref>: Existing deep caption methods are unable to generate sentences about objects unseen in caption corpora (like otter). In contrast, our model effectively incorporates information from independent image datasets and text corpora to compose descriptions about novel objects without any paired image-sentence data.</p><p>have the capacity to recognize thousands of object classes, existing state-of-the-art caption models lack the ability to form compositional structures which integrate new objects with known concepts without explicit examples of imagesentence pairs. To address this limitation, we propose the Deep Compositional Captioner (DCC) which can combine visual groundings of lexical units to generate descriptions about objects which are not present in caption corpora (paired image-sentence data), but are present in object recognition datasets (unpaired image data) and text corpora (unpaired text data). DCC builds on recent deep captioning models <ref type="bibr" target="#b49">[36,</ref><ref type="bibr" target="#b18">5,</ref><ref type="bibr" target="#b29">16,</ref><ref type="bibr" target="#b28">15,</ref><ref type="bibr" target="#b34">21]</ref> which combine convolutional and recurrent networks for visual description. However unlike previous models which can only describe objects that are present in paired image-sentence data, DCC is compositional in the sense that it can seamlessly construct sentences about new objects by combining them with already seen linguistic expressions in paired training data. To illustrate, consider the image of the otter in <ref type="figure">Figure 1</ref>. To describe the image ac-1 1 curately, any captioning model needs to identify the constituent visual elements such as "otter", "water" and "sitting" and combine them to generate a coherent sentence. While previous deep caption models learn to combine visual elements into a cohesive description exclusively from image and caption pairs, DCC can compose a caption to describe a new visual element such as the "otter" by understanding that "otters" are similar to "animals" and can thus be composed in the same way with other lexical expressions.</p><p>To effectively describe new objects, our model incorporates two key design elements. First, DCC consists of a separate lexical classifier and language model, which can each be trained independently on unpaired image data and unpaired text data. Additionally, the lexical classifier and language model can be combined into a deep caption model which is trained jointly on paired image-sentence data. Second, and crucial for generating compositional captions, is the multimodal layer where knowledge from known objects in paired image-sentence datasets can be transferred to new objects only seen in unpaired datasets. In this work, we leverage external text corpora to relate novel objects to concepts seen in paired data and propose two mechanisms to transfer knowledge from known objects to novel objects.</p><p>We demonstrate the ability of DCC to generate captions about new objects by empirically studying results on a training split of the MSCOCO dataset which excludes certain objects. Qualitatively, we show that our model can be used to describe a variety of objects in the Imagenet 7k dataset which are not present in caption datasets. Furthermore, we demonstrate that the efficacy of DCC is not limited to images, but can also be used to describe new objects in videos by presenting results on a collection of Youtube video clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Captioning. In the last year, a variety of models <ref type="bibr" target="#b18">[5,</ref><ref type="bibr" target="#b49">36,</ref><ref type="bibr" target="#b28">15,</ref><ref type="bibr" target="#b29">16,</ref><ref type="bibr" target="#b19">6,</ref><ref type="bibr" target="#b34">21]</ref> have achieved promising results on the image captioning task. Some <ref type="bibr" target="#b18">[5,</ref><ref type="bibr" target="#b49">36,</ref><ref type="bibr" target="#b28">15]</ref> follow a CNN-RNN framework: first high-level features are extracted from a CNN trained on the image classification task, and then a recurrent model learns to predict subsequent words of a caption conditioned on image features and previously predicted words. Others <ref type="bibr" target="#b29">[16,</ref><ref type="bibr" target="#b34">21]</ref> adopt a multimodal framework in which recurrent language features and image features are embedded in a multimodal space. The multimodal embedding is then used to predict the caption word by word. Retrieval methods <ref type="bibr" target="#b17">[4]</ref> based on comparing the knearest neighbors of training and test images in a deep image feature space, have also achieved competitive results on the captioning task. However, retrieval methods are limited to words and descriptions which appear in a training set of paired image-sentence data. As opposed to using high level image features extracted from a CNN, another approach <ref type="bibr" target="#b19">[6,</ref><ref type="bibr" target="#b50">37]</ref> is to train classifiers on visual concepts such as objects, attributes and scenes. A language model, such as an LSTM <ref type="bibr" target="#b50">[37]</ref> or maximum entropy model <ref type="bibr" target="#b19">[6]</ref>, then generates a visual description conditioned on the presence of classified visual elements. Our model most closely resembles the framework suggested in <ref type="bibr" target="#b34">[21]</ref> which uses a multimodal space to combine features from image and language, however our approach modifies this framework considerably to describe concepts that are never seen in paired image-sentence data. Zero-Shot Learning. Zero-shot learning has received substantial attention in computer vision <ref type="bibr" target="#b40">[27,</ref><ref type="bibr" target="#b37">24,</ref><ref type="bibr" target="#b32">19,</ref><ref type="bibr" target="#b43">30,</ref><ref type="bibr" target="#b20">7]</ref> since it becomes difficult to obtain sufficient labeled images as the number of object categories grows. In particular, our method draws on previous zero-shot learning work that mines object relationships from external text data <ref type="bibr" target="#b40">[27,</ref><ref type="bibr" target="#b43">30,</ref><ref type="bibr" target="#b20">7]</ref>. <ref type="bibr" target="#b40">[27]</ref> uses text corpora to determine how objects are related to each other, then classifies unknown objects based on their relationship to known objects. In <ref type="bibr" target="#b43">[30,</ref><ref type="bibr" target="#b20">7]</ref>, images are mapped to semantic word vectors corresponding to their classes, and the resulting image embeddings are used to detect and distinguish between unseen and seen classes. We also exploit transfer learning via an intermediate-level semantic word vector representation, however, the above approaches focus specifically on assigning a category label, while our method generates full sentence descriptions. In <ref type="bibr" target="#b25">[12]</ref>, zero-shot object detectors are learned by transferring information about how network weights trained on the classification task differ from weights trained on the detection task. We explore a similar transfer method to transfer information from weights which are trained on image-sentence data to weights which are only trained on text data. Describing New Objects in Context. Many early caption models <ref type="bibr" target="#b44">[31,</ref><ref type="bibr" target="#b30">17,</ref><ref type="bibr" target="#b21">8,</ref><ref type="bibr" target="#b31">18,</ref><ref type="bibr" target="#b22">9]</ref> rely on first discerning visual elements from an image, such as subjects, objects, scenes, and actions, then filling in a sentence template to create a coherent visual description. These models are capable of describing objects without being provided with paired imagesentence examples containing the objects, but are restricted to generating descriptions using a fixed, predetermined template. More recently, <ref type="bibr" target="#b34">[21]</ref> explore describing new objects with a deep caption model with only a few paired imagesentence examples during training. However, <ref type="bibr" target="#b34">[21]</ref> do not consider how to describe objects when no paired imagesentence data is available. Our model provides a mechanism to include information from existing vision datasets as well as unpaired text data, whereas <ref type="bibr" target="#b34">[21]</ref> relies on additional image-sentence annotations to describe novel concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Compositional Captioner</head><p>DCC composes novel sentences about objects unseen in paired image-sentence data. Although it is common to pretrain deep caption models on unpaired image data, unlike existing models, we are able to describe objects present in unpaired image data but not present in paired imagesentence data. Additionally, to enhance the language structure, we train our model on independent text corpora. Further, we explore methods to transfer knowledge between semantically related words to compose descriptions of new objects. Our method consists of three stages: 1) training a deep lexical classifier and deep language model with unpaired data, then, 2) combining the lexical classifier and language model into a caption model which is trained on paired image-sentence data, and, finally, 3) transferring knowledge from words which appear in paired image-sentence data to words which do not appear in paired image-sentence data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep Lexical Classifier</head><p>The lexical classifier <ref type="figure">(Fig 2, left)</ref> is a CNN which maps images to semantic concepts. In order to train the lexical classifier, we first mine concepts which are common in paired image-text data by extracting the part-of-speech of each word <ref type="bibr" target="#b46">[33]</ref> and then select the most common adjectives, verbs, and nouns. We do not refine the mined concepts, which means some of the concepts, such as "use", are not strictly visual. In addition to concepts common in paired image-sentence data, the classifier is also trained on objects that we wish to describe outside of the caption datasets.</p><p>The lexical classifier is trained by fine-tuning a CNN which is pre-trained on the training split of the ILSVRC-2012 <ref type="bibr" target="#b41">[28]</ref> dataset. When describing images, multiple visual concepts from the image influence the description. For example, the sentence "An alpaca stands in the green grass." includes the visual concepts "alpaca", "stands", "green", and "grass". In order to apply multiple labels to each image, we use a sigmoid cross-entropy loss. We denote the image feature output by the lexical classifier as f I , where each index of f I corresponds to the probability that a particular concept is present in the image. Our idea of learning visual classifiers from text descriptions for captioning is similar to <ref type="bibr" target="#b39">[26]</ref> who learn classifiers for objects, verbs, and locations and <ref type="bibr" target="#b19">[6]</ref> who learn visual concepts using multiple instance learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Language Model</head><p>The language model <ref type="figure">(Fig 2, right)</ref> learns sentence structure using only unpaired text data and includes an embedding layer which maps a one-hot-vector word representation to a lower dimensional space, an LSTM <ref type="bibr" target="#b23">[10]</ref>, and a word prediction layer. The language model is trained to predict a word given previous words in a sentence. At each time step, the previous word is input into the embedding layer. The embedded word is input into an LSTM, which learns the recurrent structure inherent in language. The embedded word and LSTM output are concatenated to form the language features, f L . f L is input to an inner product layer which outputs the next word in a generated sequence. At training <ref type="figure">Figure 2</ref>: DCC consists of a lexical classifier, which maps pixels to semantic concepts and is trained only on unpaired image data, and a language model, which learns the structure of natural language and is trained on unpaired text data. The multimodal unit of DCC integrates the lexical classifier and language model and is trained on paired image-sentence data. time, the ground truth word is always used as an input to the language model, but at test time we input the previous word predicted by our model. We also find that results improve by enforcing a constraint that the model cannot predict the same word twice in a row. We explore a variety of sources for unpaired text corpora as described in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Caption Model</head><p>The caption model integrates the lexical classifier and the language model to learn a joint model for image description. As shown in <ref type="figure">Fig 2 (</ref>center) the multimodal unit in the caption model combines the image features, f I and the language features, f L . The multimodal unit we use is an affine transformation of the image and language features:</p><formula xml:id="formula_0">p w = softmax(f I W I + f L W L + b)<label>(1)</label></formula><p>where W I , W L , and b are learned weight matrices and p w is a probability distribution over the predicted word. Intuitively, the weights in W I learn to predict a set of words which are likely to occur in a caption given the visual elements discerned by the lexical classifier. In contrast, W L learns the sequential structure of natural language by learning to predict the next word in a sequence given the previous words. By summing f I W I and f L W L , the multimodal unit combines the visual information learned by the lexical classifier with the knowledge of language structure learned by the language model to form a coherent description of an image. Both the language model and caption model are trained to predict a sequence of words, whereas the lexical classifier is trained to predict a fixed set of candidate visual elements for a given image. Consequently, the weights W L , which map language features to a predicted word are learned when training the language model, but the weights W I are not. Weights in W L are pretrained using unpaired text data before fine-tuning with paired image-sentence data, W I are trained purely with image-sentence data. Though we use a linear multimodal unit, our results are comparable to results achieved by other methods which include a nonlinear layer for word prediction. For example, on the MSCOCO validation set <ref type="bibr" target="#b18">[5]</ref> achieves a METEOR score of 23.7, and DCC achieves a METEOR of 23.2.</p><p>The caption model is designed to enable easy transfer of learned weights from words which appear in the paired image-sentence data to words which do not appear in the image-sentence data. First, by using a lexical classifier to extract image features, image features have explicit semantic meaning. Consequently, it is trivial to expand the image feature to include new objects and to adjust weights in the multimodal unit which correspond to specific objects. Second, by learning language features using unpaired text data, we ensure that the model learns a good embedding for words which are not present in paired image-sentence data. Finally, by using a single-layer, linear multimodal unit, the dependence between image and language features and predicted words is straightforward to understand and easy to exploit for semantic transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Transferring Information Between Objects</head><p>Direct Transfer. The first method we explore to transfer weights between objects directly transfers learned weights in W I , W L and b from words that appear in the paired image-sentence dataset to words which do not appear in a paired image-sentence dataset <ref type="figure" target="#fig_1">(Fig 3)</ref>. Intuitively, the direct transfer model requires that a new word is described in the same way that semantically similar words are described. To illustrate, consider the new word "alpaca" which is semantically close to the known word "sheep". Let v a and v s indicate the index of the words alpaca and sheep in the vocabulary. Given image and language features, f I and f L respectively, the probability of predicting the word "sheep" is proportional to:</p><formula xml:id="formula_1">f I W I [:, v s ] + f L W L [:, v s ] + b[v s ]<label>(2)</label></formula><p>In order to construct sentences with "alpaca" in the same way sentences are constructed with the word "sheep", we  <ref type="figure" target="#fig_1">Fig 3)</ref>. Additionally, we expect the prediction of the word "sheep" to be highly dependent on the likelihood that a "sheep" is present in the image. In other words, we expect W I [:, c s ] to strongly weight the output of the lexical classifier which corresponds to the word "sheep". However, W I [:, c a ] should strongly weight the lex- Delta Transfer. Instead of directly transferring weights, we can also transfer how weights change when trained on paired image-text data. Again, consider transferring the word "sheep" to the word "alpaca". We determine ∆ L for a given word as:</p><formula xml:id="formula_2">∆ L = W L−caption [:, v s ] − W L−language [:, v s ]<label>(3)</label></formula><p>where W L−caption are weights learned when training with both images and sentences and W L−language are weights learned when training only with language. The weights for the new word "alpaca" are updated as: To ensure that excluded objects are at least similar to some included ones, we cluster the 80 objects annotated in the MSCOCO segmentation challenge using the vectors from the word2vec embedding described in Section 3.4 and exclude one object from each cluster. The following words are chosen: "bottle", "bus", "couch", "microwave", "pizza", "racket", "suitcase", and "zebra". We randomly select 50% of the MSCOCO validation set for validation, and set aside the other 50% for testing. We use the validation set to determine all model hyperparameters, and present all results on the test set. We label the visual concepts in each image based on the five ground truth caption annotations provided in the MSCOCO dataset. If any of the ground truth captions mention an object, the corresponding image is considered a positive example for that object.</p><formula xml:id="formula_3">W L−caption [:, v a ] = W L−language [:, v a ] + ∆ L<label>(</label></formula><p>In addition to empirically evaluating our model, we also qualitatively examine the performance of DCC at a large scale by describing objects outside of the paired imagesentence corpora. Specifically, we select 642 objects from the full ImageNet object recognition dataset <ref type="bibr" target="#b16">[3]</ref> which do not occur in MSCOCO and are also present in the WebCorpus text dataset (see section 4.3) vocabulary. We do no manual concept pruning; consequently some selected concepts refer to a broad variety of objects (e.g., the class "fauna" contains all animals) and other classes only contain a small number of images (e.g., there are three "discus" images). We use 75% of images from each class to train the lexical classifier, and evaluate on the rest. We stress that we do not have any descriptions for these categories. Video Description. For empirical evaluation on video description, we use a collection of Youtube clips from the Microsoft Video description (MSVD) corpus <ref type="bibr" target="#b15">[2]</ref>, which contains 1,970 short annotated clips. Our basic experimental setting follows previous video description works <ref type="bibr" target="#b48">[35,</ref><ref type="bibr" target="#b47">34]</ref>. However, we hold out paired video-sentence data for some objects during training. Because there is significant variation in the number of video clips containing each object in the MSVD dataset, we hold out objects in the MSVD dataset which appear in five or fewer training videos and at least one test video and also appear in the ILSVRC2015 video object detection challenge set. 1 Our MSVD heldout set excludes paired video-sentence training data which include "zebra", "hamster", "broccoli", and "turtle".</p><p>We also qualitatively evaluate our method on the ILSVRC object detection challenge videos (initial release) 1 http://image-net.org/challenges/LSVRC/2015/ which consists of 1,952 video snippets of the 30 objects from the ILSVRC2015 object detection in video. Objects which we describe in the detection challenge videos include "whale", "fox", "hamster", "lion", "zebra", and "turtle".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training the Lexical Classifier</head><p>Image description. We consider both MSCOCO and Im-ageNet as sources of labeled image data to train the lexical classifier. For all objects in paired image-sentence data, we use COCO images which are labeled with 471 visual concepts to train the lexical classifier. For the eight objects which do not appear in the paired image-sentence data, we explore training the lexical classifier using MSCOCO images (in-domain) and ImageNet images (out-of-domain). For qualitative experiments on ImageNet objects, we use Imagenet images to train the lexical classifier on new visual concepts. The lexical classifier is trained by fine-tuning a deep convolutional model (VGG-16 layer <ref type="bibr" target="#b42">[29]</ref>) trained on the ILSVRC-2012 <ref type="bibr" target="#b41">[28]</ref> object recognition training subset of the ImageNet dataset. Video Description. Unlike images, videos consist of a sequence of frames which need to be mapped to a set of semantic concepts by the lexical classifier. To build a lexical classifier for videos, we mean-pool f c 7 features across all frames in a video clip before classification. We use both MSVD and ImageNet videos to train the lexical classifier. We use the VGG-16 layer model to extract f c 7 layer features from video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training the Language Model</head><p>Image Description. We consider three different sources for unpaired text data to train the language model: (1) MSCOCO consists of all captions from the MSCOCO train set (2) Text from Image Description Corpora (Caption-Txt) consists of text data from other paired image and video description datasets: Flickr1M <ref type="bibr" target="#b26">[13]</ref>, Flickr30k <ref type="bibr" target="#b24">[11]</ref>, Pascal-1k <ref type="bibr" target="#b38">[25]</ref> and ImageCLEF-2012 <ref type="bibr" target="#b45">[32]</ref> and sentence descriptions of Youtube clips from the MSVD training corpus. This corpus does not include sentences from MSCOCO.</p><p>(3) External text (WebCorpus) consists of 60 million sentences from the British National Corpus (BNC), UkWaC, and Wikipedia. Video Description. We consider two sources of text to train the video description language model. The first is the WebCorpus text described above. We also consider a slight variant on the CaptionTxt described above which includes descriptions from MSCOCO, Flickr-30k <ref type="bibr" target="#b24">[11]</ref>, Pascal-1k <ref type="bibr" target="#b38">[25]</ref> and the MSVD sentence descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training the Caption Model</head><p>After training the lexical classifier and language model, the weights in the multimodal layer of the caption model are trained with paired image-sentence data. For the direct </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Metrics</head><p>To evaluate our transfer methods, we must choose a metric that indicates whether or not a generated sentence includes a new object. Common caption metrics such as BLEU <ref type="bibr" target="#b36">[23]</ref> and METEOR [1] measure overall sentence meaning and fluency. However, for many objects, it is possible to achieve good BLEU and METEOR scores without mentioning the new object (e.g., consider sentences describing the boy playing tennis in <ref type="figure" target="#fig_2">Figure 4</ref>). To definitively report our model's ability to integrate new vocabulary, we also report the F1-score. The F1-score considers "false positives" (when a word appears in a sentence it should not appear in), "false negatives" (when a word does not appear in a sentence it should appear in), and "true positives" (when a word appears in a sentence it should appear in). We consider generated sentences "positive" if they contain at least one mention of a held out word and ground truth sentences "positive" if a word is mentioned in any ground truth annotation that describes an image.</p><p>We train our models using Caffe <ref type="bibr" target="#b27">[14]</ref>. <ref type="bibr" target="#b15">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Description</head><p>As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, DCC is capable of integrating new vocabulary into image descriptions in a cohesive manner. Direct Transfer Versus Delta Transfer. <ref type="table">Table 1</ref> compares the average F1-score across the eight held-out training classes. As shown by the F1-scores reported in <ref type="table">Table 1</ref>, both the delta transfer and direct transfer methods are capable of integrating new words into their vocabulary. We also report the BLEU-1 score, which measures the overlap between generated words and words in reference sentences. By measuring the METEOR score, we ensure that our model maintains sentence fluency when inserting new objects. DCC consistently increases METEOR scores indicating that overall sentence quality improves with DCC. The direct transfer method improves F1-scores, BLEU, and METEOR scores by a larger amount than the delta transfer method and is thus used for the remainder of our experiments.</p><p>Importantly, BLEU and METEOR scores do not decrease for objects which are present in the held-out training data set. When trained with all image-sentence training examples, our model achieves an average BLEU-1 of 69.36 and METEOR of 23.98 on held-out classes.</p><p>To illustrate which words our model works best on, we report the F1-score for individual objects in <ref type="table" target="#tab_4">Table 2</ref>. We compare to a model which is trained with image-sentence pairs for the eight held-out objects. For all objects, DCC is able to compose sentences which include the object. Analysis of Transfer Words.</p><p>In general, determining word similarity with a word2vec embedding works well. Words such as "zebra"/"giraffe" and "mi-   crowave"/"refrigerator" are close in embedding space and are also used in similar ways in natural language, suggesting they will work well for transfer. Some transfer pairs ("racket"/"tennis" and "bus"/"stop") are used together frequently but play different structural roles in sentences. Consequently, the word "racket" is frequently used like the word "tennis" leading to grammatical errors. However, similar errors do not occur when transferring "stop" to "bus". Pre-Training with Out-of-Domain Data. In the above experiments the lexical classifier and language model are pre-trained using MSCOCO images and text. In a real world scenario, it is unlikely that available unpaired image and text data will be from the same domain as paired imagesentence data. However, it is essential that the model learns good image and language features. Naturally, if the lexical classifier is unable to recognize certain objects, DCC will not be able to describe the objects. Perhaps more subtly, if the language model is not trained with unpaired text which includes an object, it will not learn a proper embedding for the new word and will not produce cohesive descriptions about new objects. <ref type="table" target="#tab_5">Table 3</ref> demonstrates the impact of using outside image and text corpora to train the lexical classifier and language model. Our model performs best when provided with indomain image and text for all training stages, but performance is comparable when using ImageNet images to train the lexical classifier and CaptionTxt or WebCorpus text data to train the language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Describing ImageNet Objects</head><p>We qualitatively assess our model by describing a variety of ImageNet objects which are not included in the MSCOCO data set <ref type="figure" target="#fig_4">(Fig 5)</ref>. DCC accurately describes 335 Dress → Tutu, Dress → Chiffon No transfer: A woman in a dress shirt is holding a tennis racket. DCC: A woman in a chiffon tutu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Plane → Spaceship</head><p>No transfer: A blue and white airplane is flying in the air. DCC: A spaceship is flying through the air.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree → Baobab</head><p>No transfer: A large giraffe standing in a tree. DCC: A large baobab in a field with trees in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kite → Trapeze</head><p>No transfer: A woman is holding a skateboard in the air. DCC: A woman is holding a trapeze in the air.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bird → Toad</head><p>No transfer: A green and white bird sitting on top of a green field. DCC: A toad is sitting on the ground.  : Image Description: DCC is able to describe Imagenet objects (bolded) which are not mentioned in any of the paired image-sentence data, and therefore cannot be described by existing deep caption models. X → Y indicates that the known word X is transferred to the new word Y. new words including entry-level words like "toad" as well as fine-grained categories like "baobab". Though most Imagenet words we transfer are nouns, we are able to successfully transfer some adjectives such as "chiffon". DCC achieves more than simple noun replacement. For example, the sentence "A large giraffe standing in a tree" changes significantly to "A large baobab in a field with trees in the background" after transfer. Importantly, our model is able to compose sentences by placing objects in the correct context. For example, comparing <ref type="figure" target="#fig_4">Fig 5 (top)</ref> to the image in <ref type="figure">Fig 1,</ref> the object "otter" is correctly described as either "sitting in the water" or "standing on top of a lush green field" depending on visual context. <ref type="figure" target="#fig_5">Figure 6</ref> examines a few common error types: New Object Not Mentioned. <ref type="figure" target="#fig_5">(Figure 6</ref>, top) For some images, DCC produces relevant sentences, but fails to mention the new object. Grammatically Incorrect. <ref type="figure" target="#fig_5">(Figure 6</ref>, second row) Some  sentences incorporate new words, but are grammatically incorrect. For example, though DCC describes sentences with the word "gymnastics", the resulting sentences are frequently grammatically incorrect (e.g., "A woman playing gymnastics on a gymnastics court"). This is likely because the word "tennis" is transferred to "gymnastics". Though both of these words are sports, one does not "play" gymnastics and gymnastics is not performed on a "court." Object Hallucination. <ref type="figure" target="#fig_5">(Figure 6</ref>, third row) DCC frequently hallucinates objects which commonly occur in a specific visual context. For example, in a beach image, the model commonly includes the word "surfboard". Irrelevant Description. <ref type="figure" target="#fig_5">(Figure 6</ref>, bottom) Some captions do not mention any salient objects correctly. Such errors can be caused by poor image recognition or because the language model is unable to construct a reasonable sentence from constituent visual elements.</p><p>More examples are in our supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Video description</head><p>We believe DCC can be especially beneficial in domains, such as video description, where the amount of paired training data is small. <ref type="table">Table 4</ref> presents empirical results of direct transfer DCC on videos in the MSVD corpus (Section 4.1). We report the average F1 score on all held-out classes, and METEOR scores on the complete test dataset. As seen by the F1 score, transferring weights allows us to describe new objects in video. Additionally, the METEOR score improves with transfer demonstrating that DCC improves overall sentence quality. Similar to the trend seen for image captioning, training on in-domain text corpora achieves slightly better performance than training on external text. When adding ImageNet videos, both F1 and METEOR increase suggesting that including outside image data is beneficial. Including ImageNet videos to learn better lexical classifiers especially improves the F1 score, which increases from 6.0 to 22.2. <ref type="figure" target="#fig_6">Figure 7</ref> presents qualitative re-Model (Video) METEOR F1 Baseline <ref type="table">(No Transfer)</ref> 28.8 0.0 + DT 28.9 6.0 + ILSVRC Videos <ref type="table">(No Transfer)</ref> 29.0 0.0 + ILSVRC Videos + DT 29.1 22.2 <ref type="table">Table 4</ref>: Video Description: METEOR scores across the test dataset and average F1 scores for the four held-out categories (All values in %) using direct transfer (DT). The DCC models were trained on videos with 4 objects removed and the language model was trained on in-domain sentences.</p><p>A hamster is eating food in a bowl.</p><p>A turtles are running.</p><p>A zebra is eating some grass. sults of our best model on snippets with the held out objects in MSVD corpus and the ILSVRC validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present the Deep Compositional Captioner (DCC) which can be used to describe new objects which are not present in current caption copora. Our quantitative and qualitative results demonstrate our model's ability to integrate new vocabulary into generated image and video descriptions by effectively using existing vision datasets and unpaired text data. By integrating data from disparate sources and transferring knowledge between semantically related concepts, DCC improves upon current deep caption models by providing rich descriptions which are not limited by the availability of paired image-sentence corpora.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>first directly transfer the weights W I [:, v s ], W L [:, v s ], and b[v s ] (indicated in red in Fig 3) to W I [:, v a ], W L [:, v a ], and b[v a ] (indicated in green in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Method for transferring knowledge from words trained with paired image-sentence data to words trained without imagesentence data. See Section 3.4 for details.ical classifier which corresponds to the word "alpaca". To enforce this, we set W I [r a , c a ] = W I [r s , c s ] where r a and r s indicate the index in the image features which correspond to the alpaca and sheep classifiers respectively. Finally, we do not expect the output of the word "alpaca" to depend on the presence of a sheep in the image and vice versa. Consequently, we set W I [r s , c a ] = W I [r a , c s ] = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Image Description: Comparison of captions generated by a model without transfer, DCC with in-domain training (MSCOCO), with out-of-domain training (ImageNet and WebCorpus), and a model trained with paired image-sentence supervision for all MSCOCO objects. DCC is capable of integrating new words and generates sentences similar to those generated when paired image-sentences for all objects are present during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Giraffe → Impala No transfer: A close up of a bird on a field. DCC: A impala is standing in the grass.Cake → Scone No transfer: A close up of a pizza on a plate. DCC: A close up of a scone on a plate. Bird → Otter No transfer: A couple of birds standing on top of a lush green field. DCC: A otter standing on top of a lush green field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure 5: Image Description: DCC is able to describe Imagenet objects (bolded) which are not mentioned in any of the paired image-sentence data, and therefore cannot be described by existing deep caption models. X → Y indicates that the known word X is transferred to the new word Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Image Description: We highlight four common error types generated by the DCC. See Section 5.2 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Video Description: Captions generated by DCC on videos of novel objects unseen in paired training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Image Description: Comparison of F1 scores for direct transfer DCC model (DT) and a model trained with image-sentence 
training examples for all objects. (Values in %) 

Lexical 
Language 
B-1 
METEOR 
F1 
classifier 
model 
MSCOCO MSCOCO 64.40 
21.00 
39.78 
Imagenet 
MSCOCO 64.00 
20.71 
33.60 
Imagenet 
CaptionTxt 64.79 
20.66 
35.53 
Imagenet WebCorpus 64.85 
20.66 
34.94 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Image Description: We compare the effect of pre-training 
the lexical classifier and language model with different unpaired 
image and text data sets. As expected, we see the best result when 
using in domain MSCOCO data to train the lexical classifier and 
language model, though training with out of domain corpora is 
comparable. (Values in %) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Error: New object (lifejacket) not mentioned DCC: A group of people sitting on a bench.Error: Grammatically Incorrect DCC: A woman is playing gymnastics on a gymnastics court.Error: Object Hallucination DCC: A woman in a snorkel is holding a surfboard.Error: Irrelevant description DCC: A dog is sitting on a white bench.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code can be found at http://www.eecs.berkeley.edu/ lisa_anne/dcc_project_page.html.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Lisa Anne Hendricks is supported by the NDSEG Fellowship. Marcus Rohrbach was supported by a fellowship within the FITweltweit-Program of the German Academic Exchange Service (DAAD). Trevor Darrell was supported in part by DARPA; AFRL; DoD MURI award N000141110688; NSF awards IIS-1212798, IIS-1427425, and IIS-1536003, and the Berkeley Vision and Learning Center. Raymond Mooney and Kate Saenko were supported in part by DARPA under AFRL grant FA8750-13-2-0026 and a Google Grant. Mooney was also supported by ONR ATL Grant N00014-11-1-010.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pair Supervision: A pizza with a lot of toppings on it. No Transfer: A plate of food with a glass of wine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">A pizza sitting on a wooden table with a glass of wine behind it. DCC (out): A pizza sitting on top of a wooden table. Pair Supervision: A dog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">A dog laying on a couch with a large window in the background. DCC (out): A dog laying on a couch in a room</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Pair Supervision: A white microwave oven sitting on top of a counter. No Transfer: A white and black cat is sitting on a toilet. DCC (in): A white microwave sitting on a brick wall</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A white microwave sitting next to a white oven</title>
	</analytic>
	<monogr>
		<title level="j">DCC</title>
		<imprint/>
	</monogr>
	<note>Pair Supervision: A boy is holding a tennis racket on a court. No Transfer: A boy is playing tennis on a court</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">A boy is playing with a racket on a court</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">A young boy is playing racket on a racket</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pair Supervision: A car with a suitcase on the seat in the back seat of a car. No Transfer: A car with a bag of bananas in the back</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A car with a suitcase and a plastic suitcase behind it. DCC (out): A car with a suitcase inside of it &apos; s back. Pair Supervision: A zebra is grazing in a grassy area</title>
	</analytic>
	<monogr>
		<title level="j">DCC</title>
		<imprint/>
	</monogr>
	<note>No Transfer: Two giraffes are eating grass in the field</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DCC (in): Two zebra grazing in a green grass field</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two zebra standing in a field with grass in the background. Pair Supervision: A group of three different colored vases with different designs. No Transfer: A table with many different types of wine</title>
	</analytic>
	<monogr>
		<title level="j">DCC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">A table with many bottle of bottle of bottle</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A counter with a lot of bottle and bottle of bottle</title>
	</analytic>
	<monogr>
		<title level="j">DCC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pair Supervision: A bus is driving down the street in front of a bus stop. No Transfer: A green and white street sign on a city street</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
	<note>A green and white bus parked on the side of the street</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">What does classifying more than 10,000 image categories tell us? In ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Language models for image captioning: The quirks and what works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guptal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LSDA: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The mir flickr retrieval evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIR &apos;08: Proceedings of the 2008 ACM International Conference on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhuditnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">TACL</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating natural-language video descriptions using text-mined knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning like a child: Fast novel visual concept learning from sentence descriptions of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collecting image annotations using amazon&apos;s mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>GCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What helps Where -and Why? Semantic Relatedness for Knowledge Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Overview of the imageclef 2012 flickr photo annotation and retrieval task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Sequence to sequence -video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Image captioning with an intermediate attributes layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01144</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
