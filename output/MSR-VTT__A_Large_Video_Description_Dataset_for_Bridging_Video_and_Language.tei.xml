<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSR-VTT: A Large Video Description Dataset for Bridging Video and Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<email>tiyao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
							<email>yongrui@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MSR-VTT: A Large Video Description Dataset for Bridging Video and Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on specific fine-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content.</p><p>In this paper we present MSR-VTT (standing for "MSR-Video to Text") which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text. This is achieved by collecting 257 popular queries from a commercial video search engine, with 118 videos for each query. In its current version, MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers. We present a detailed analysis of MSR-VTT in comparison to a complete set of existing datasets, together with a summarization of different state-of-the-art video-to-text approaches. We also provide an extensive evaluation of these approaches on this dataset, showing that the hybrid Recurrent Neural Networkbased approach, which combines single-frame and motion representations with soft-attention pooling strategy, yields the best generalization capability on MSR-VTT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It has been a fundamental yet emerging challenge for computer vision to automatically describe visual content with natural language. Especially, thanks to the recent development of Recurrent Neural Networks (RNNs), there has been tremendous interest in the task of image caption-ing, where each image is described with a single natural sentence <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b36">36]</ref>. Along with this trend, researchers have provided several benchmark datasets to boost research on image captioning (e.g., Microsoft CO-CO <ref type="bibr">[21]</ref> and Flickr 30K <ref type="bibr" target="#b41">[41]</ref>), where tens or hundreds of thousands of images are annotated with natural sentences.</p><p>While there has been increasing interest in the task of video to language, existing approaches only achieve severely limited success in terms of the variability and complexity of video contents and their associated language that they can recognize <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b34">34]</ref>. This is in part due to the simplicity of current benchmarks, which mostly focus on specific fine-grained domains with limited data scale and simple descriptions (e.g., cooking <ref type="bibr" target="#b5">[5]</ref>, YouTube <ref type="bibr" target="#b16">[16]</ref>, and movie <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b32">32]</ref>). There are currently no large-scale video description benchmarks that match the scale and variety of existing image datasets because videos are significantly more difficult and expensive to collect, annotate and organize. Furthermore, compared with image captioning, the automatic generation of video descriptions carries additional challenges, such as modeling the spatiotemporal information in video data and pooling strategies. Motivated by the above observations, we present in this paper the MSR-VTT dataset (standing for MSR-Video to Text), which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text. This is achieved by collecting 257 popular queries from a commercial video search engine, with 118 videos for each query. In its current version, MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering a comprehensive list of 20 categories and a wide variety of video content. Each clip was annotated with about 20 natural sentences.</p><p>From a practical standpoint, compared with existing datasets for video to text, such as MSVD <ref type="bibr" target="#b3">[3]</ref>, YouCook <ref type="bibr" target="#b5">[5]</ref>, M-VAD <ref type="bibr" target="#b32">[32]</ref>, TACoS <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b28">28]</ref>, and MPII-MD <ref type="bibr" target="#b27">[27]</ref>, our MSR-VTT benchmark is characterized by the following major unique properties. First, our dataset has the largest number of clip-sentence pairs, where each video clip is annotated with multiple sentences. This can lead to a better training of RNNs and in consequence the generation of <ref type="figure">Figure 1</ref>. Examples of the clips and labeled sentences in our MSR-VTT dataset. We give six samples, with each containing four frames to represent the video clip and five human-labeled sentences. more natural and diverse sentences. Second, our dataset contains the most comprehensive yet representative video content, collected by 257 popular video queries in 20 representative categories (including cooking and movie) from a real video search engine. This will benefit the validation of the generalization capability of any approach for video to language. Third, the video content in our dataset is more complex than any existing dataset as those videos are collected from the Web. This plays as a ground challenge for this particular research area. Last, in addition to video content, we keep audio channel for each clip, which leaves a door opened for related areas. <ref type="figure">Fig. 1</ref> shows some examples of the videos and their annotated sentences. We will make this dataset publically available to the research community to support future work in this area.</p><p>From a methodology perspective, we are interested in answering the following questions regarding the best performing RNN-based approaches for video to text. What is the best video representation for this specific task, either single-frame-based representation learned from the Deep Convolutional Neural Networks (DCNN) or temporal features learned from 3D CNN? What is the best pooling strategy over frames? What is the best network structure for learning spatial representation? What is the best combination of different components if considering performance and computational costs? We examine these questions empirically by evaluating multiple RNN architectures that each takes a different approach to combining learned representation across the spatiotemporal domain.</p><p>In summary, we make the following contributions in this work: 1) we build to-date the largest dataset called MSR-VTT for the task of translating video to text, which contains diverse video content corresponding to various categories and diverse textual descriptions, and 2) we summarize existing approaches for translating video to text into a single framework and comprehensively investigate several stateof-the-art approaches on different datasets.</p><p>The remaining of this paper are organized as follows. Section 2 reviews related work on vision to text. Section 3 describes the details of MSR-VTT dataset. Section 4 introduces the approaches to video to text. Section 5 presents evaluations, followed by the conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The research on vision to language has proceeded along two dimensions. One is based on language model which first detects words from visual content by object recognition and then generates a sentence with language constraints, while the other is leveraging sequence learning models (e.g., RNNs) to directly learn an embedding between visual content and sentence. We first review the state-of-the-art research along these two dimensions. We then briefly introduce a collection of datasets for videos.</p><p>Image captioning has been taken as an emerging ground challenge for computer vision. In the language model-based approaches, objects are first detected and recognized from the images, and then the sentences can be generated with syntactic and semantic constraints <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b42">42]</ref>. For example, Farhadi et al. perform object detection to infer a triplet of S-V-O and convert it into a sentence by predefined language templates <ref type="bibr" target="#b9">[9]</ref>. Li et al. move one step further to consider the relationships among the detected objects for composing phrases <ref type="bibr" target="#b20">[20]</ref>. Recently, researchers have explored to leverage sequence learning to generate sentences <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>. For example, Kiros et al.</p><p>propose to use a log-bilinear model with bias features derived from the image to model the embedding between text and image <ref type="bibr" target="#b13">[13]</ref>. Hao et al. propose a three-step approach to image captioning including word detection by multiple instance learning, sentence generation by language models, and sentence reranking by deep embedding <ref type="bibr" target="#b8">[8]</ref>. Similar works have started to adopt RNNs for generating image descriptions by conditioning the output from RNN on the image representation learned from the Convolutional Neural Network (CNN) <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b36">36]</ref>. In <ref type="bibr" target="#b19">[19]</ref>, Lebret et al. propose to use a phrase-based model rather than single word to generate sentences, while Xu et al. leverage visual attention mechanism to mimic human ability to compress salient visual information into descriptive language <ref type="bibr" target="#b37">[37]</ref>.</p><p>In the video domain, similar approaches have been proposed for video description generation. The first research dimension applies video representation to template-based or statistical machine translations <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b28">28]</ref>. These approaches generate sentences by mapping semantic sentence representation, modeled with a Conditional Random Field (CRF), to high-level concepts such as the actors, actions and objects. On the other hand, sequence learning can be applied to video description as video is naturally a sequence of objects and actions <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref>. Donahue et al. leverage CNN to learn the single frame representation as the input to the long-term recurrent convolutional networks to output sentences <ref type="bibr" target="#b7">[7]</ref>. In <ref type="bibr" target="#b35">[35]</ref>, Venugopalan et al. design an encoder-decoder neural network to generate descriptions. By mean pooling, the features over all frames can be represented by one single vector, which is the input of the RNN. Compared to mean-pooling, Li et al. propose to utilize the temporal attention mechanism to exploit temporal structure as well as a spatiotemporal convolutional neural network <ref type="bibr" target="#b10">[10]</ref> to obtain local action features <ref type="bibr" target="#b39">[39]</ref>. Besides, Venugopalan et al. <ref type="bibr" target="#b34">[34]</ref> propose a end-to-end sequence-tosequence model to generate captions for videos.</p><p>There are several existing datasets for video to text. The YouTube cooking video dataset, named YouCook <ref type="bibr" target="#b5">[5]</ref>, con- tains the videos about the scenes where people are cooking various recipes. Each video has a number of humanannotated descriptions about actions and objects. Similarly, TACos <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b28">28]</ref> and TACos Multi-Level <ref type="bibr" target="#b26">[26]</ref> include a set of video descriptions and temporal alignment. MSVD is a collection of 1,970 videos from Youtube with multiple categories <ref type="bibr" target="#b3">[3]</ref>. The sentences in this dataset are annotated by AMT workers. On the other hand, M-VAD <ref type="bibr" target="#b32">[32]</ref> and MPII-MD <ref type="bibr" target="#b27">[27]</ref> provide movie clips with aligned Audio Description (AD) from the Descriptive Video Service (DVS) and scripts rather than human-labeled sentences. We will provide comparison and statistics for all these datasets later.</p><p>In this work, we build to-date the largest video description dataset with comprehensive video context and well-defined categories. Besides, we summarize the existing approaches for translating video to text into one single framework and conduct a comprehensive evaluation of different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The MSR-VTT Datataset</head><p>The MSR-VTT dataset is characterized by the unique properties including the large scale clip-sentence pairs, comprehensive video categories, diverse video content and descriptions, as well as multimodal audio and video streams. We next describe how we collect representative videos, select appropriate clips, annotate sentences, and split the dataset. Finally, we would like to summarize the existing video description datasets together and make a comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Collection of Representative Videos</head><p>Current datasets for video to text mostly focus on specific fine-grained domains. For example, YouCook <ref type="bibr" target="#b5">[5]</ref>, TACoS <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b28">28]</ref> and TACoS Multi-level <ref type="bibr" target="#b26">[26]</ref>  video search results for each query. We remove duplicate and short videos, as well as the videos with bad visual quality, to maintain the data quality. As a result, we have 30,404 representative videos. All the videos were downloaded with high quality and audio channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Clip Selection and Sentence Annotation</head><p>Since our goal is to collect short video clips that each can be described with one single sentence in our current version of MSR-VTT, we adopt color histogram-based approach to segmenting each video (Section 3.1) into shots <ref type="bibr" target="#b23">[23]</ref>. As a result, there are 3,590,688 shots detected. As one video clip could have multiple consecutive shots, we asked 15 subjects to watch the videos and select appropriate consecutive shots to form video clips. For each video, at most three clips are selected to ensure the diversity of the dataset. In total there are 30K clips selected, among which we randomly selected 10K clips (originated from 7,180 videos) in the current version of MSR-VTT and left the remaining clips in our second version. The median number of shots for single video clip is 2. The duration of each clip is between 10 and 30 seconds, while the total duration is 41.2 hours.</p><p>Although one can leverage Audio Descriptions (AD) to annotate movies <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b32">32]</ref>, it is difficult to obtain quality sentence annotation for web videos. Therefore, we rely on Amazon Mechanical Turk (AMT) workers(1317) to annotate these clips. Each video clip is annotated by multiple workers after being watched. In the post processing, duplicated sentences and too short sentences are removed. As a result, each clip is annotated with 20 sentences by different workers. There are 200K clip-sentence pairs (corresponding to 1.8M words and 29,316 unique words) which represents the dataset with the largest number of sentences and vocabulary. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the category distribution of these 10K clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dataset Split</head><p>To split the dataset to training, validation and testing sets, we separate the video clips according to the correspondhow-to, travel, science (technology), animal, kids (family), documentary, food, cooking, beauty (fashion), advertisement.</p><p>ing searched queries. The clips from the same video or the same queries will not appear solely in the training or testing set to avoid overfitting. We split the data according to 65%:30%:5%, corresponding to 6,513, 2,990 and 497 clips in the training, testing and validation sets, respectively. <ref type="table">Table 1</ref> lists the statistics and comparison among different datasets. We will release more data in the future. In this work, we denote our dataset MSR-VTT-10K as it contains 10, 000 video clips. Our MSR-VTT is the largest dataset in terms of clip-sentence pairs (200K) and word vocabulary <ref type="bibr" target="#b29">(29,</ref><ref type="bibr">316)</ref>. A major limitation for existing datasets is limited domain and annotated sentences <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b28">28]</ref>. Although MPII-MD and M-VAD contain a number of clips, both of them are originated from one single domain (i.e., movie). The MSR-VTT is derived from a wide variety of video categories (7,180 videos from 20 general domains/categories), this can benefit the generalization capability of model learning. In addition, compared with the scripts and DVS sentences in the MPII-MD and M-VAD, since MSR-VTT has the largest vocabulary with each clip annotated with 20 different sentences, it can lead to a better training of RNNs and in consequence the generation of more natural and diverse sentences. MSVD <ref type="bibr" target="#b3">[3]</ref>, which is the most similar dataset to ours, has a small number of clips and sentences. In summary, the MSR-VTT represents the most comprehensive, diverse, and complex dataset for video to language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Data Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approaches to Video Descriptions</head><p>We briefly describe different video-to-text approaches that we benchmark on our proposed MSR-VTT dataset. Most of state-of-the-art methods for video to text are based on the Long-Short Term Memory (LSTM), which is a variant of RNN and can capture long-term temporal information by mapping sequences to sequences. As this dimension of research achieves better performance than language model-based approaches, we summarize all the RNN-based approaches in one single framework, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Specifically, given an input video, 2-D CNN is utilized to extract the visual representation for single frames. Moreover, optical flow or 3-D CNN is exploited to represent the motion information in the video. Then the video representation, which is comprised of 2-D CNN and/or 3-D CNN outputs, is feed into RNN. There are multiple ways that can be used to combine all the frame-level or clip-level representation to generate the video-level visual representation. The first one is mean pooling strategy which can easily create a fixed length vector as video representation <ref type="bibr" target="#b35">[35]</ref>. The second one uses soft attention mechanism to selectively focus on only a small subset of frames instead of the simple mean pooling over the frame-level representations <ref type="bibr" target="#b39">[39]</ref>. RNN model is trained to predict each word of the sentence after it has seen the entire video as well as all the preceding words. Please note that the video-level representation can be only input once or at each time step.</p><p>Among all the RNN-based state-of-the-art methods for translating video to text, we mainly investigate and evaluate in terms of two directions on our MSR-VTT dataset: the mean pooling model proposed in <ref type="bibr" target="#b35">[35]</ref> and the soft attention method proposed in <ref type="bibr" target="#b39">[39]</ref>. For mean pooling method, we design 7 runs, i.e.  <ref type="figure">VGG-19</ref>). The first 5 runs employ mean pooling over the frame/clip-level features from AlexNet <ref type="bibr" target="#b17">[17]</ref>, GoogleNet <ref type="bibr" target="#b31">[31]</ref>, VGG-16 <ref type="bibr" target="#b29">[29]</ref>, VGG-19 <ref type="bibr" target="#b29">[29]</ref> and C3D <ref type="bibr" target="#b33">[33]</ref> networks, respectively. The last 2 runs feed the concatenations of C3D and VGG- <ref type="bibr" target="#b16">16</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluations</head><p>We conducted all the experiments on our newly created MSR-VTT-10K dataset 2 and empirically verify the RNN-based video sentence generation from three aspects: 1) when different video representation is used, 2) when different pooling strategy is exploited, and 3) how the performance is affected when using different size of hidden layer of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Settings</head><p>To describe the visual appearances of frames in video, we adopt the output of 4096-dimensional fc6 layer from AlexNet, VGG-16 and VGG-19 and pool5/7x7 s1 layer of GoogleNet which are all pre-trained on ImageNet dataset <ref type="bibr" target="#b6">[6]</ref>. C3D architecture, which is pre-trained on Sports-1M video dataset <ref type="bibr" target="#b12">[12]</ref> and has been proved to be powerful in action recognition tasks, is utilized to model the temporal information across frames in video. Specifically, each continuous 16 frames are treated as one short video clip and taken as the inputs of C3D. The 4,096-dimensional outputs of fc6 layer in C3D are regarded as the representations of each video clip. Each sentence is represented as a vector of words, and each word is encoded by one-hot vector. In our experiments, we use about 20,000 most frequent words as the word vocabulary. In our experiments, with an initial learning rate 0.01 and mini-batch size set 1, 024, the objective value can decrease to 20% of the initial loss and reach a reasonable result after 5, 000 iterations (about 100 epochs). All videos are resized to resolution 320 Ã— 240 and 30 fps.</p><p>To evaluate the generated sentences, we use the BLEU@N <ref type="bibr" target="#b24">[24]</ref> and METEOR <ref type="bibr" target="#b0">[1]</ref> metrics against all ground truth sentences. Both metrics are widely used in machine translation literature and already shown to be well correlated with human judgement. Specifically, BLEU@N measures the fraction of N -gram (up to 4-gram) that are in common between a hypothesis and a reference or set of references, while METEOR computes unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance Comparison between Different</head><p>Video Representations <ref type="table">Table 2</ref> show the results using different video feature with mean pooling method. Since its weak performance, we will show RNN based method in the below parts.</p><p>The first experiment was conducted to examine how different video representations work on sentence generation. <ref type="table">Table 3</ref> shows the performances of five runs averaged over all the test videos in our dataset. It is worth noting that the performances in <ref type="table">Table 3</ref> are all with mean pooling. The performance trend is similar with that using soft attention.</p><p>Overall, the results across BLEU@4 and METEOR consistently indicate that video representations learnt from a temporal clip using C3D leads to a performance boost against frame-based representations. There is a slightly performance difference between VGG-19 and VGG-16. Though both runs utilize VGG network, VGG-19 is deeper than VGG-16 and thus learns a more powerful frame representations. Similar observations are also found when comparing to AlexNet and GoogleNet. The results indicate that improvement can be generally expected when learning frame-based representations by a deeper CNN. <ref type="figure">Figure 4</ref> (a) details the performance comparison by using VGG-19 and C3D across different categories in terms of METEOR. The two kinds of video representations show different characteristics in different types of categories. For instance, the videos in category "sports/actions" are diverse in appearance, resulting in poor performance by VGG-19.  <ref type="table">Table 3</ref>. BLEU@4 and METEOR for comparing the quality of sentence generation on different video representations. The experiments are all based on mean-pooling strategy, and the size of hidden layer in LSTM is set to 512. All values are reported as percentage (%). Instead, temporal representations by C3D is found to be more helpful for this category. In the case of category "documentary," where temporal information is relatively few, frame-based representations by VGG-19 show better performance. Moreover, the complementarity between framebased visual representations and clip-based temporal representations is generally expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance Comparison between Different Pooling Strategies</head><p>We second investigated how the performance is affected with different pooling strategies. Two pooling approaches, i.e., mean pooling and soft attention model, are compared. <ref type="table">Table 4</ref> lists the performances of seven video representations with mean pooling and soft attention method, respectively. Soft attention model consistently outperforms mean pooling across different video representations. In particular, the METEOR of soft attention model (SA-LSTM (C3D + VGG-19)) can achieve 29.9%, making the improvement over mean pooling (MP-LSTM (C3D + VGG-19)) by 1.4%, which is generally considered as a good progress on sentence generation task. Similar to the observations in Section 5.2, clip-based temporal representation by C3D exhibits better performance than frame-based visual representations and VGG-19 achieves the best performance across all the frame-based representations. The performances could be further boosted when concatenating the video representations learnt by C3D and VGG-19 with both mean pooling and soft attention models. <ref type="figure">Figure 4</ref> (b) further details the METEOR performances of mean pooling and soft attention model for all the 20 categories. Note that all the performances are given on video representations of C3D+VGG-19. Basically, different categories respond differently to the two pooling strategies. For instance, videos in the category "news" are better presented with soft attention model as there are always multiple scenes in the videos of this category. On the other hand, videos in the category "cooking" are often in a single scene and thus mean pooling shows much better results.</p><p>We also present a few sentence examples generated by different methods and ground truth in <ref type="figure">Figure 5</ref>. From these exemplar results, it is easy to see that SA-LSTM (C3D+VGG-19) can generate more accurate sentences. For instance, compared to the sentence "Kids are playing toys" by MP-LSTM (AlexNet) and "People are playing in the room" by SA-LSTM (GoogleNet), the generated sentence "Children are painting in room" by SA-LSTM (C3D + VGG-19) encapsulates the first video more clearly. For the last video, the sentences generated by all the methods are not accurate as the video contains multiple diverse scenes. Therefore, there is still much space for researchers in this area to design new algorithms to boost performance on this dataset, especially dealing with complex visual content.</p><p>Besides, in <ref type="table">Table 5</ref>  <ref type="table">Table 6</ref>. Performance comparison of different size of hidden layer in LSTM. The video representation here is the clip-based temporal representations by C3D and the pooling strategy is mean pooling. by pooling method can achieve a better performance than single frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">The Size of hidden Layer of LSTM</head><p>In order to show the relationship between the performance and hidden layer size of LSTM, we compare the results of the hidden layer size in the range of 128, 256, and 512. The results shown in <ref type="table">Table 6</ref> indicate increasing the hidden layer size can lead to the improvement of the performance with respect to both BLEU@4 and METEOR. Therefore, in our experiments, the hidden layer size is empirically set to 512 which achieves the best performance. <ref type="figure">Figure 5</ref>. Examples of sentence generation results from different approaches and ground truth. Each video clip is represented by four frames. (1) refers to the sentence generated by MP-LSTM (AlexNet) <ref type="bibr" target="#b35">[35]</ref>. <ref type="bibr" target="#b2">(2)</ref> refers to the sentence generated by SA-LSTM (GoogleNet) <ref type="bibr" target="#b39">[39]</ref>. <ref type="bibr" target="#b3">(3)</ref> refers to the sentence generated by SA-LSTM (C3D+VGG-19). GT is a random human generated sentence in the ground truth. Sentences in bold highlight the most accurate sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Correctness Grammar Relevance  <ref type="table">Table 7</ref>. Human evaluation of different methods on MSR-VTT. Each method is evaluated by 5 persons (scale 1-10, lower is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Human Evaluations</head><p>We have extracted the SVO parts from all the sentences and calculated the overlapped percentages of SVO on the 20 annotated sentences to show the human consistency for our dataset. The mean overlapping percentage is 62.7%, which proves the good human consistency for all annotated sentences. Besides, we have conducted human evaluations on different approaches in our dataset in <ref type="table">Table 7</ref> in terms of correctness, grammar, and relevance, which shows similar results compared with the above metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We introduced a new dataset for describing video with natural language. Utilizing over 3,400 worker hours, a vast collection of video-sentence pairs was collected, annotated and organized to drive the advancement of the algorithms for video to text. This dataset contains the most representative videos covering a wide variety of categories and to-date the largest amount of sentences. We comprehensively evaluated RNN-based approaches with variant components on related and our dataset. We found that the temporal representation learned from convolutional 3D networks plays strong complement to the spatial representation, and the soft-attention pooling strategy shows powerful capability to model complex and long video data.</p><p>There are several promising directions for future study on our dataset. There remains space to boost the performance on certain categories (corresponding to complex video content) that the approaches introduced in this paper cannot work well. The audio information has not been exploited for video description generation. Using audio and its AD information may further improve existing performance. The dataset can also be utilized for video summarization if one can build the embedding between video frames and the words. Furthermore, emotion and action recognition could be integrated into existing framework to make the generated language more diverse and natural.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The distribution of video categories in our MSR-VTT dataset. This distribution well aligns with the real data statistics in a commercial video site.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Summarization of RNN-based approaches for video to text. The left side reflects how to learn the representation of whole video as an input. The right side in the figure tells about the procedure of how LSTM works and the how the sentence is generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, MP-LSTM (AlexNet), MP-LSTM (GoogleNet), MP-LSTM (VGG-16), MP-LSTM (VGG-19), MP-LSTM (C3D), MP-LSTM (C3D + VGG-16) and MP-LSTM (C3D +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, C3D and VGG-19 into the LSTM model. Similarly, for soft attention strategy, we also compare 7 runs with different input frame/clip-level features, named SA-LSTM (Alex), SA-LSTM (Google), SA-LSTM (VGG-16), SA-LSTM (VGG-19), SA-LSTM (C3D), SA-LSTM (C3D + VGG-16) and SA-LSTM (C3D + VGG-19), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>are mainly designed for cooking behavior. MSR-VTT focuses on general videos in our life, while MPII-MD<ref type="bibr" target="#b27">[27]</ref> and M-VAD<ref type="bibr" target="#b32">[32]</ref> on movie domain. Although MSVD<ref type="bibr" target="#b3">[3]</ref> contains general web videos which may cover different categories, the very limited size (1,970) is far from representativeness. To collect representative videos, we obtain the top 257 representative queries from a commercial video search engine, corresponding to 20 categories 1 . We then crawl the top 150Table 1. Comparison of video description datasets. Please note that TACos M-L means TACos Multi-Level dataset. Although MSVD dataset has multiple video categories, the category information is not provided. In our MSR-VTT-10K dataset, we provide the category information for each clip. Among all the above datasets, MPII-MD, M-VAD and MSR-VTT contain audio information.</figDesc><table>Dataset 

Context 
Sentence Source #Video 
#Clip 
#Sentence 
#Word 
Vocabulary Duration (hrs) 

YouCook [5] 
cooking 
labeled 
88 
-
2,668 
42,457 
2,711 
2.3 
TACos [25, 28] 
cooking 
AMT workers 
123 
7,206 
18,227 
-
-
-
TACos M-L [26] 
cooking 
AMT workers 
185 
14,105 
52,593 
-
-
-
M-VAD [32] 
movie 
DVS 
92 
48,986 
55,905 
519,933 
18,269 
84.6 
MPII-MD [27] 
movie 
DVS+Script 
94 
68,337 
68,375 
653,467 
24,549 
73.6 
MSVD [3] 
multi-category 
AMT workers 
-
1,970 
70,028 
607,339 
13,010 
5.3 
MSR-VTT-10K 
20 categories 
AMT workers 
7,180 
10,000 
200,000 
1,856,523 
29,316 
41.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The performance of KNN baselines with different video representations and mean-pooling strategy.</figDesc><table>Feature 

BLEU@4 METEOR 

AlexNet 
6.3 
14.1 
GoogleNet 
8.1 
15.2 
VGG-16 
8.7 
15.5 
VGG-19 
7.3 
14.5 
C3D 
7.5 
14.5 
Table 2. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>. Performance comparison on our MSR-VTT dataset of seven video representations with mean pooling and soft attention method, respectively. The number of the hidden layer of LSTM is set to 512 in all the experiments.Table 5. Performance comparison among different pooling methods (with VGG-19 feature and 512 hidden layers in LSTM).</figDesc><table>33.4 

34.3 

54.4 

35.5 
31.4 
34.3 
37.8 

29.5 
28.4 

44.2 

31.4 
29.2 
30.8 
30.0 
30.2 
34.8 
36.2 
34.7 
31.4 
28.3 
31.4 
34.2 

51.1 

28.9 
30 
33.1 
33.2 
29.8 
30.2 

46.9 

33.7 
28.6 
29.3 
29.1 
27.9 

36.9 
33.8 
32.8 
30.7 
28.2 

0 

10 

20 

30 

40 

50 

60 
C3D 
VGG-19 

(a) Comparison between VGG-19 and C3D with mean pooling. 

34.2 
35.9 

55.2 

36.2 
32.6 
35.7 
38.4 

29.4 
29.6 

45.6 

31.2 
30.1 
31.2 
31.2 
28.7 
35.4 
37.4 
36.2 
32.5 
28.5 
34.7 
35.8 

50.4 

41.3 
36.7 
38.2 

31.6 
31.9 
28.7 

46.4 

31.6 
30.6 
36.2 
31.7 
31.5 

43.2 
37.8 

29.6 
35.3 
32.4 

0 

10 

20 

30 

40 

50 

60 
MP-LSTM 
SA-LSTM 

(b) Comparison between mean pooling and soft attention. 

Figure 4. Per-category METEOR scores across all the 20 categories. 

Model 
BLEU@1 BLEU@2 BLEU@3 BLEU@4 METEOR 

MP-LSTM (AlexNet) [35] 
75.9 
60.6 
46.5 
35.4 
26.3 
MP-LSTM (GoogleNet) 
76.8 
61.3 
47.2 
36.7 
27.5 
MP-LSTM (VGG-16) 
78.0 
62.0 
48.7 
37.2 
28.6 
MP-LSTM (VGG-19) 
78.2 
62.2 
48.9 
37.3 
28.7 
MP-LSTM (C3D) 
79.8 
64.7 
51.7 
39.9 
29.3 
MP-LSTM (C3D+VGG-16) 
79.8 
64.7 
52.0 
40.1 
29.4 
MP-LSTM (C3D+VGG-19) 
79.9 
64.9 
52.1 
40.1 
29.5 
SA-LSTM (AlexNet) 
76.9 
61.1 
46.8 
35.8 
27.0 
SA-LSTM (GoogleNet) [39] 
77.8 
62.2 
48.1 
37.1 
28.4 
SA-LSTM (VGG-16) 
78.8 
63.2 
49.0 
37.5 
28.8 
SA-LSTM (VGG-19) 
79.1 
63.3 
49.3 
37.6 
28.9 
SA-LSTM (C3D) 
80.2 
64.6 
51.9 
40.1 
29.4 
SA-LSTM (C3D+VGG-16) 
81.2 
65.1 
52.3 
40.3 
29.7 
SA-LSTM (C3D+VGG-19) 
81.5 
65.0 
52.5 
40.5 
29.9 
Table 4Feature 
BLEU@4 METEOR 

Single frame 
32.4 
22.6 
Mean pooling 
37.3 
28.7 
Soft-Attention 
37.6 
28.9 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>, we have compared the performance of single frame (middle frame) with mean-pooling and softattention on the VGG-19 feature. It can prove approaches Hidden layer size BLEU@4 METEOR Parameters</figDesc><table>128 
32.5 
26.6 
3.7M 
256 
38.0 
29.0 
7.6M 
512 
39.9 
29.3 
16.3M 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">These 20 categories include music, people, gaming, sports (actions), news (events/politics), education, TV shows, movie, animation, vehicles,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In addition to MSR-VTT-10K, we will release more data in the future.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A woman is laying in bed.</head><p>GT: A girl sitting in bed knocks on the wall and then begins texting someone on her phone.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A black and white horse runs around</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop</title>
		<meeting>ACL Workshop</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video in sentences out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Burchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coroian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mussman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mind&apos;s Eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unifying visualsemantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural language description of human activities from video images based on concept hierarchy of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating natural-language video descriptions using text-mined knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phrase-based image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Composing simple image descriptions using web-scale N-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Natural Language Learning</title>
		<meeting>International Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="220" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimedia search reranking: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grounding action descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic concept discovery from parallel text and visual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2596" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Using descriptive video services to create a large data source for video annotation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01070</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">C3D: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Show and Tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>DaumÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intl Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Intl Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="444" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Annotation for free: Video tagging by mining user search behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Building a comprehensive ontology to refine video concept detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international workshop on Workshop on multimedia information retrieval</title>
		<meeting>the international workshop on Workshop on multimedia information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
