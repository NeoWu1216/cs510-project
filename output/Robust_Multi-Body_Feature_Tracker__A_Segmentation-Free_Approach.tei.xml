<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Multi-body Feature Tracker: A Segmentation-free Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ANU</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ANU</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
							<email>mathieu.salzmann@epfl.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
							<email>yiran.zhong@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ANU</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Multi-body Feature Tracker: A Segmentation-free Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature tracking is a fundamental problem in computer vision, with applications in many computer vision tasks, such as visual SLAM and action recognition. This paper introduces a novel multi-body feature tracker that exploits a multi-body rigidity assumption to improve tracking robustness under a general perspective camera model. A conventional approach to addressing this problem would consist of alternating between solving two subtasks: motion segmentation and feature tracking under rigidity constraints for each segment. This approach, however, requires knowing the number of motions, as well as assigning points to motion groups, which is typically sensitive to the motion estimates. By contrast, here, we introduce a segmentationfree solution to multi-body feature tracking that bypasses the motion assignment step and reduces to solving a series of subproblems with closed-form solutions. Our experiments demonstrate the benefits of our approach in terms of tracking accuracy and robustness to noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Feature tracking is a prerequisite for many computer vision tasks, such as visual SLAM and action recognition. Among all the feature tracking methods, the Kanade-Lucas-Tomasi (KLT) tracker <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref>, although developed 30 years ago, still remains one of the most widely used techniques. One of the reasons for this popularity is its computational efficiency; the KLT tracker is local, in the sense that it treats each local region independently of the others, which makes it highly parallelizable. This locality, however, comes at a cost in tracking robustness: the tracking of each feature cannot benefit from intrinsic scene constraints, and thus often suffers from drift.</p><p>Real-world scenes, however, are often strongly constrained. For example, in autonomous driving, most of the moving objects (cars, vehicles, pedestrian) are rigid, or quasi-rigid if seen from afar. Several methods have therefore been proposed to exploit this scene rigidity to improve feature tracking <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref>. Unfortunately, these methods all assume an affine camera model and are thus ill-suited to handle strong perspective effects. More importantly, they work either as a post-processing step on an entire sequence <ref type="bibr" target="#b22">[23]</ref>, which is sensitive to initial tracking results and does not apply to online feature tracking, or within a temporal sliding window <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>, which is sensitive to initialization in the first few frames.</p><p>By contrast, in this paper, we introduce a novel feature tracker that takes advantage of multi-body scene rigidity to improve tracking robustness under a general perspective camera model. A conventional approach to addressing this problem would consist of alternating between two subtasks: motion segmentation and feature tracking under rigidity constraints for each segment. This, however, suffers from the following drawbacks: First, it requires knowing the number of observed motions; and, second, it relies on assigning points to individual motions, which is very sensitive to the initial motion estimates.</p><p>Here, we introduce a segmentation-free multi-body feature tracker that overcomes these drawbacks. Specifically, our approach bypasses the motion assignment step by making use of subspace constraints derived directly from the epipolar constraints of multiple motions. As a result, our algorithm does not require prior knowledge of the number of motions. Furthermore, this allows us to formulate tracking as an optimization problem whose subproblems all have closed-form solutions.</p><p>We demonstrate the effectiveness of our method on both feature point tracking and frame-by-frame motion segmentation on real world sequences. Our experiments show that, by incorporating multi-motion constraints, our tracker yields better accuracies and is more robust to noise than the standard KLT tracker and the state-of-the-art tracking algorithm of <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The KLT tracker <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref> was derived from the Lucas-Kanade algorithm for image alignment <ref type="bibr" target="#b16">[17]</ref>. Feature tracking was achieved by optimizing the sum of squared differences between a template patch and an image patch with the Gauss-Newton method. It was later extended to handle relatively large displacements via the use of image pyramids <ref type="bibr" target="#b0">[1]</ref>.</p><p>Global rigidity constraints have been incorporated in feature point tracking to improve robustness. For instance, Torresani and Bregler <ref type="bibr" target="#b22">[23]</ref> proposed to regularize tracking with a global low-rank constraint on the trajectory matrix of the whole sequence. They relied on the original KLT tracker to get a set of reliable tracks, and explicitly factorized the reliable trajectory matrix into two low-rank matrices with the rank given a priori. One of the low-rank matrices, called the motion parameter matrix, was then used to rectify the unreliable tracks. In short, this method can be viewed as a post-processing step on the results of the KLT tracker, and is therefore not suitable for online frame-to-frame tracking.</p><p>Instead of using the whole sequence, low-rank constraints <ref type="bibr" target="#b2">[3]</ref> and similar subspace priors <ref type="bibr" target="#b18">[19]</ref> were applied within a temporal sliding window. Specifically, Buchanan and Fitzgibbon <ref type="bibr" target="#b2">[3]</ref> exploited the low-rank constraints within a Bayesian tracking framework, making predictions of the new location of a particular point using a low rank approximation obtained from the previous frames. Recently, Poling et al. <ref type="bibr" target="#b18">[19]</ref> proposed a better feature tracker by adding soft subspace constraints to the original KLT tracker and jointly solving for the displacement vectors of all feature points. These methods, however, assume an affine camera model within a temporal window, and are therefore ill-suited to handle strong perspective effects. Moreover, since the lowrank constraints are enforced in a temporal sliding window, these methods are sensitive to initialization in the first few frames.</p><p>By contrast, <ref type="bibr" target="#b17">[18]</ref> exploits perspective projection by making use of epipolar constraints to track edgels in two consecutive frames. This method, however, was specifically designed to model a single motion, and thus does not easily extend to the multi-body case.</p><p>In the closely related optical flow literature, several methods have been devoted to improving robustness via rigidity constraints. For instance, Valgaerts et al. <ref type="bibr" target="#b24">[25]</ref> introduced a variational model to jointly recover the fundamental matrix and the optical flow; Wedel et al. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref> leveraged the fundamental matrix prior as an additional weak prior within a variational framework. These methods, however, assume that the scene is mostly stationary (and thus a single fundamental matrix is estimated), and treat the dynamic parts as outliers <ref type="bibr" target="#b27">[28]</ref>. Garg et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> proposed to make use of subspace constraints to regularize the multi-frame optical flow within a variational approach. This approach, however, assumes an affine camera model and works over entire sequences.</p><p>While, to the best of our knowledge, explicitly modeling multi-body motion has not been investigated in the context of feature tracking and optical flow estimation, a large body of work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref> has been devoted to multi-body motion segmentation given good point trajectories in relatively long sequences. Typically, these tracks are first obtained with the KLT tracker, and then manually cleaned up, e.g., the Hopkins155 dataset <ref type="bibr" target="#b23">[24]</ref>. In a sense, the lack of better tracking algorithms that can incorporate the intrinsic constraints of dynamic scenes prevents the practical use of these motion segmentation algorithms.</p><p>In this paper, we seek to track feature points in dynamic scenes where multiple motions are present. In this scenario, a single fundamental matrix is not sufficient to express the epipolar constraints any more. While one could think of alternating between estimating multiple fundamental matrices, motion assignments and displacement vectors, the resulting algorithm would typically be very sensitive to initialization, since the motion assignments strongly depend on the motion estimates. By contrast, we introduce a segmentation-free approach that bypasses the motion assignment problem by exploiting subspace constraints derived from epipolar geometry. This yields a robust multi-body tracking algorithm that, as demonstrated by our experiments, opens up the possibility to perform motion segmentation in realistic scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-body Feature Tracker</head><p>We now introduce our approach to multi-body feature tracking. Formally, let I(x) denote the current image, T (x) the previous image (or template image), and x ij = [x ij , y ij ] T the j th image point in the i th patch Ω i of the template image. Our goal is to estimate the displacement vector u = [u T i , · · · , u T N ] T ∈ R 2×N for all N tracked feature points. To this end, we rely on the standard brightness constancy assumption <ref type="bibr" target="#b20">[21]</ref>, which lets us derive the data term</p><formula xml:id="formula_0">D(u) = N i=1 xij ∈Ωi ψ I(x ij + u i ) − T (x ij ) ,<label>(1)</label></formula><p>where, typically, ψ(x) = x 2 or ψ(x) = |x|. In particular, we use the ℓ 1 norm, which provides robustness to outliers.</p><p>Estimating the displacements from this data term only is typically sensitive to noise and may be subject to drift. A general approach to making the process more robust consists of introducing a regularizer R(u) to form an energy function of the form</p><formula xml:id="formula_1">F(u) = γD(u) + R(u) .<label>(2)</label></formula><p>As mentioned above, several attempts at designing such a regularizer have been proposed. For example, under an affine camera model, R(u) can encode a low-rank prior <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref>; with a general projective camera model, R(u) can represent epipolar constraints (i.e., a fundamental matrix prior) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18]</ref>. In the latter case, the fundamental matrix can be either pre-computed via an existing feature matching method <ref type="bibr" target="#b17">[18]</ref>, or re-computed iteratively. When multiple motions are present, however, a single epipolar constraint is not sufficient. Instead, multiple fundamental matrices should be estimated so as to respect the assignments of the tracked points to individual motions. A straightforward way to addressing this problem consists of adding a motion segmentation step in the tracking algorithm, so that the fundamental matrices can be iteratively re-estimated. This leads to the simple segmentation-based approach to multi-body feature tracking described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A First Attempt: Segmentation-based Tracking</head><p>To derive a segmentation-based approach, we rely on epipolar constraints. Recall that, in epipolar geometry <ref type="bibr" target="#b8">[9]</ref>, the homogeneous coordinatesx</p><formula xml:id="formula_2">′ i = (x ′ i , y ′ i , 1) T andx i = (x i , y i , 1)</formula><p>T of two corresponding image points in two frames are related by a fundamental matrix F, such that</p><formula xml:id="formula_3">x ′T i Fx i = 0 .<label>(3)</label></formula><p>It is therefore natural to exploit these constraints to regularize tracking according to the motion assignments of the different points. More specifically, in the segmentation-based approach, three types of variables must be estimated: the displacement vector u, the fundamental matrices {F k } k=1,··· ,K (where K is the number of motions), and the motion label of each tracked point. Let us denote byx k i the homogeneous coordinate of the i th feature point (i.e., the center of the patch Ω i ) assigned to motion k. We can define a multi-body regularization term as</p><formula xml:id="formula_4">R 1 (u, F k ) = k i (x k i +ū i ) T F kxk i 2 ,<label>(4)</label></formula><formula xml:id="formula_5">whereū i = [u T i , 0] T .</formula><p>The energy function can then be approximately minimized by iterating over the following three steps:</p><p>1. Update u by first-order gradient descent <ref type="bibr" target="#b18">[19]</ref>; 2. Estimate F k for each motion given the current point assignments; 3. Re-assign the motion labels of the feature points to the nearest F k .</p><p>This segmentation-based approach suffers from several drawbacks. First, the number of motions needs to be known a priori, which is typically hard for general-purpose tracking. Second, and more importantly, the quality of the solution obtained with this approach will strongly depend on the initialization of F k and of the motion labels. This, in a sense, is a chicken-and-egg problem, since good initialization for these variables could be obtained from good motion estimates. To overcome this, in the remainder of this section, we introduce a new segmentation-free approach that bypasses the need to explicitly compute the fundamental matrices and the motion assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Our Segmentation-free Approach</head><p>In this section, we introduce our segmentation-free multi-body feature tracker, which is the key contribution of this paper. We first show how the epipolar constraints can be converted to subspace constraints, and incorporated into our tracking formalism. We then derive the solution to the resulting optimization problem by decomposing it into several convex subproblems all with closed-form solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Epipolar Subspace Constraints</head><p>As in the segmentation-based approach, we seek to rely on epipolar geometry. To this end, we make use of the constraint expressed in Eq. 3. We first note that this constraint can be re-writen as</p><formula xml:id="formula_6">f T vec(x ′ ix T i ) = 0 ,<label>(5)</label></formula><p>where f ∈ R 9 is the vectorized fundamental matrix F, and</p><formula xml:id="formula_7">vec(x ′ ix T i ) = (x i x ′ i , x i y ′ i , x i , y i x ′ i , y i y ′ i , y i , x ′ i , y ′ i , 1) T . (6) Let us define w i = vec(x ′ ix T i ).</formula><p>Then, w i lies in the orthogonal complement of f T , which is a subspace of dimension up to eight 1 , and which we call the epipolar subspace. Since image points undergoing the same motion share the same fundamental matrix, all w i s corresponding to points belonging to the same rigid motion lie on the same subspace <ref type="bibr" target="#b14">[15]</ref>.</p><p>Therefore, in our multi-body feature tracking scenario, if the feature points are correctly tracked, the data vectors defined as</p><formula xml:id="formula_8">w i = vec (x i +ū i )x T i , ∀ 1 ≤ i ≤ N ,<label>(7)</label></formula><p>should lie in a union of linear subspaces. This subspace constraint can be characterized by the self-expressiveness property <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>, i.e., a data point drawn from one subspace in a union of subspaces can be represented as a linear combination of the points lying in the same subspace. In our case, this self-expressiveness property can be expressed as</p><formula xml:id="formula_9">W (u) = W (u) C ,<label>(8)</label></formula><p>where W (u) = [w 1 · · · w N ] 2 , and C is the coefficient matrix encoding the linear combinations. On its own, this term has a trivial solution for C (i.e., the identity matrix).</p><p>To avoid this solution, C needs to be regularized. In the subspace clustering literature, C is encouraged to be either sparse <ref type="bibr" target="#b4">[5]</ref> by minimizing C 1 , low rank <ref type="bibr" target="#b15">[16]</ref> by minimizing C * , or dense block diagonal <ref type="bibr" target="#b10">[11]</ref> by minimizing C 2 F . Here, we choose the Frobenius norm, which has proven effective and is easy to optimize. Furthermore, we explicitly model noise and outliers, which are inevitable in real-world sequences.</p><p>More specifically, we write our regularization term for multi-body tracking as</p><formula xml:id="formula_10">R 2 (u, C) = 1 2 C 2 F +λ E 1 , s.t. W (u) = W (u) C+E ,<label>(9)</label></formula><p>where E accounts for noise and outliers, and is thus encouraged to be sparse. Note that, for a given displacement u, and ignoring noise, the optimal value of this regularizer depends on the intrinsic dimension of the motion <ref type="bibr" target="#b10">[11]</ref>. Since here we optimize u, this regularizer therefore tends to favor degenerate rigid motions over purely arbitrary rigid motions. This actually reflects reality, since, in real scenes, cars, people and other objects typically move in a wellconstrained manner.</p><p>Importantly, this regularization term requires explicitly computing neither the fundamental matrices, nor the motion assignments. As such, it therefore yields a segmentationfree approach.</p><p>Altogether, the energy function of our multi-body tracking framework can be written as</p><formula xml:id="formula_11">F(u, C) = γD(u) + R 2 (u, C) .<label>(10)</label></formula><p>Our goal is to minimize F(u, C) w.r.t. u and C. We next show how to solve this optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Approximation and Problem Reformulation</head><p>To optimize Eq. 10, we first approximate the data term in the same manner as the original KLT. In other words, given an initial displacement u 0 i for patch i, we approximate the intensity values I(x ij + u i ) with their first-order Taylor expansion at x ij + u 0 i . This can be written as</p><formula xml:id="formula_12">I(x ij +u i ) ≈ I(x ij +u 0 i )+▽I(x ij +u 0 i )(u i −u 0 i ) . (11) For notational convenience, let ▽I ij = ▽I(x ij + u 0 i ), and τ ij = ▽I ij u 0 i + T (x ij ) − I(x ij + u 0 i ).</formula><p>Then, the data term can be expressed as</p><formula xml:id="formula_13">D(u) = i,j |▽I ij u i − τ ij | .<label>(12)</label></formula><p>By combining this data term with our regularizer, we get the optimization problem</p><formula xml:id="formula_14">min u,C,E γ A (u) 1 + 1 2 C 2 F + λ E 1 s.t. W (u) = W (u) C + E ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_15">A ij = ▽I ij u i − τ ij .</formula><p>For convenience of optimization, we introduce an auxiliary variable Z = A (u) . Then, <ref type="bibr" target="#b12">(13)</ref> can be equivalently written as</p><formula xml:id="formula_16">min u,C,E,Z γ Z 1 + 1 2 C 2 F + λ E 1 s.t. Z = A (u) , W (u) = W (u) C + E .<label>(14)</label></formula><p>The main hurdle in optimizing <ref type="bibr" target="#b13">(14)</ref> now lies in the term with W (u) due to its seemingly complicated dependency on u. However, we show below that this term can be simplified by a few matrix derivations. First, note that, by definition, we have </p><formula xml:id="formula_17">vec(W (u) ) =   x 1 ⊗ I 3×3 . . .x N ⊗ I 3×3    P (x +ū) ,<label>(15)</label></formula><formula xml:id="formula_18">wherex = [x T 1 · · ·x T N ] T ,ū = [ū T 1 · · ·ū T N ] T , I 3×3</formula><formula xml:id="formula_19">γ Z 1 + 1 2 C 2 F + λ E 1 s.t. Z = A (u) , W (m) = W (m) C + E , m = Pu ,<label>(16)</label></formula><p>where now vec(W (m) ) = b + m. The above optimization problem involves a large number of variables. We propose to solve it via the Alternating Direction Method of Multipliers (ADMM) <ref type="bibr" target="#b1">[2]</ref>, which decomposes a big optimization problem into several small subproblems. Below, we show how this can be achieved for our problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">ADMM Solution</head><p>To apply the ADMM, we first need to derive the augmented Lagrangian of (16), which can be expressed as</p><formula xml:id="formula_20">Lρ = γ Z 1 + 1 2 C 2 F + λ E 1 + y T (m − Pu) + (17) Y1, W − WC − E + Y2, Z − A (u) + ρ 2 W − WC − E 2 F + Z − A (u) 2 F + m − Pu 2 2 ,</formula><p>where ·, · denotes the matrix inner product, Y 1 ,Y 2 , y are Lagrange multipliers, and ρ is the penalty parameter. The ADMM then works by alternatively minimizing L ρ w.r.t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Solving (16) via the ADMM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Image I and template T , positions of the feature points x in T , initial displacement vector u 0 , parameters γ, λ, ρ 0 , ρ m , η, ǫ</p><formula xml:id="formula_21">Initialize: C = 0, Y 1 = 0, Y 2 = 0, y = 0, A (u 0 ) , W (u 0 )</formula><p>while not converged do As shown in the supplementary material, the five subproblems derived from the augmented Lagrangian are all convex problems that can be solved efficiently in closedform. These closed-form solutions can be written as</p><formula xml:id="formula_22">Z = T γ ρ [A (u) − Y2/ρ] ,<label>(18)</label></formula><formula xml:id="formula_23">E = T λ ρ [W − WC + Y1/ρ] ,<label>(19)</label></formula><formula xml:id="formula_24">C = (I + ρW T W) −1 [ρW T (W − E + Y1/ρ)] ,<label>(20)</label></formula><p>u = (ρP T P + ρH) −1 (g + P T y + ρP T m) ,</p><formula xml:id="formula_25">M = −(ρG + BQ + T)(λQ + ρI) −1 ,<label>(21)</label></formula><p>where m = vec(M) is the vectorized form of M, T α [x] = sign(x) · max(|x| − α, 0) is the soft-thresholding operator, and the definitions of g, H, Q, T are given in the supplementary material. Finally, the Lagrange multipliers and penalty parameter can be updated as</p><formula xml:id="formula_27">Y1 = Y1 + ρ(W − WC − E) ,<label>(23)</label></formula><formula xml:id="formula_28">Y2 = Y2 + ρ(Z − A (u) ) ,<label>(24)</label></formula><formula xml:id="formula_29">y = y + ρ(m − Pu) (25) ρ = min(ηρ, ρm) ,<label>(26)</label></formula><p>where η &gt; 1, and ρ m is the predefined maximum of ρ. Our approach to solving (16) is outlined in Algorithm 1. Note that the problem we are trying to solve is non-convex in that (i) the intensity function I(x; u) is non-convex w.r.t. u; (ii) the optimization problem (16) involves a bilinear term in an equality constraint. While the ADMM does not guarantee convergence, it has proven effective in practice <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Our Multi-body Feature Tracker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Image I and template T , positions of the feature points x in T , initial displacement vector u 0 , number of pyramid levels L, parameters γ, λ, ρ, ρ m , max i , ǫ for ℓ = L − 1 : 0 do Update u 0 ← u 0 /2 ℓ , x ← x 0 /2 ℓ and compute ▽I at current image pyramid level; for i = 1 : max i do 1. Approximate the image intensities with Eq. 11, and compute τ , P, H according to their definitions; 2. Update u with Algorithm 1; 3. Check the convergence condition u − u 0 &lt; ǫ; 4. If not converged, update u 0 = u. end for Update u ← 2 ℓ u, u 0 ← 2 ℓ u 0 , and x ← 2 ℓ x. end for Output: Displacement vector u, coefficient matrix C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Our Complete Multi-body Feature Tracker</head><p>In the same spirit as <ref type="bibr" target="#b0">[1]</ref>, we make use of an image pyramid to handle large displacements and avoid local optima. The results obtained at a coarser level ℓ of the pyramid are used as initialization for the next (finer) level ℓ − 1. Within each pyramid level, the initial displacement u 0 , where the firstorder Taylor approximation is performed, is updated with the displacement vector of the previous iteration. We iterate over successive Taylor approximations until the displacement vector does not change significantly. Our complete segmentation-free multi-body feature tracker is outlined in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To show the benefits of our multi-body feature tracker, we performed extensive experiments on different sequences. In the remainder of this section, we present both qualitative and quantitative results.</p><p>In our experiments, we compare our approach with the following baselines: the original KLT tracker (KLT), the L1-norm KLT tracker (L1-KLT), and the more recent Better Feature Tracker (BFT) through Subspace Constraints <ref type="bibr" target="#b18">[19]</ref>. For the original KLT, we used the Matlab built-in vision toolbox vision.PointTracker; we implemented the L1-norm KLT tracker using the same framework as our method by just disabling the regularization term; and for BFT, we used the code released by the authors.</p><p>Due to the lack of benchmark datasets for feature tracking, we make use of motion segmentation datasets where both the ground-truth tracks and the original videos are KLT L1-KLT BFT Our Method <ref type="figure" target="#fig_1">Figure 1</ref>: Performance of different trackers on the 1RT2TC checkerboard sequence: The red points denote the current positions of the feature points, and the green lines the motion since the previous frame. Best viewed zoomed-in on screen.</p><p>available. Since those videos are typically only provided for illustration purpose, they are generally highly compressed and not ideal for reliable feature tracking. This, however, is not really a problem when one seeks to evaluate feature tracking methods, since (i) it essentially represents a challenging scenario; and (ii) all algorithms are evaluated on the same data. In particular, here, we employed 10 checkerboard (indoor) sequences and 12 cars-and-people (outdoor) sequences from the well-known Hopkins155 dataset <ref type="bibr" target="#b23">[24]</ref>. Moreover, we used another 8 outdoor sequences from the more recent MTPV dataset <ref type="bibr" target="#b14">[15]</ref>. To test the robustness of the different methods, we added different levels of Gaussian noise (with variance σ 2 = 0.01, 0.02, 0.03, or 0.04) <ref type="bibr" target="#b3">4</ref> to the images. Altogether, this results in 150 evaluation sequences. The values of the parameters (γ = 1.8 × 10 4 and λ = 1.0 × 10 4 ) of our method were tuned on a separate validation set and kept unchanged for all our experiments.</p><p>To compare the algorithms, we measure the number of tracking errors, i.e., the number of points that drift from the ground-truth by more than a certain error tolerance ε. Note that, in the sequences that we use, the ground-truth was obtained by the standard KLT tracker and then manually cleaned up, so the ground-truth itself contains some noise whose level depends on the scene itself. In particular, we observed that the ground-truth of the indoor checkerboard sequences generally has more noise than that of the outdoor sequences. Therefore, we set a larger error tolerance for the checkerboard sequences (ε = 10) than for the outdoor ones (ε = 5). For every sequence, we compute the average number of incorrectly tracked feature points over all the frames, and then average this number over the sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hopkins Checkerboard Sequences</head><p>We first evaluated our method and the baselines on the Hopkins checkerboard sequences, which depict controlled indoor scenes with multiple rigidly moving objects. The average number of tracks in this dataset is 202.9. Generally, the repetitive texture in these sequences makes feature tracking more ambiguous and thus harder. However, in this experiment, we show that our multi-body feature tracker is more robust to this ambiguity. To provide a fair comparison, we used the same patch size (7 × 7) and the same number of image pyramid levels (4) for all the methods. Furthermore, we initialized all the tracking methods with the ground-truth locations of the feature points in the first frame. From <ref type="table" target="#tab_0">Table 1</ref>, we can see that the L1-KLT tracker consistently achieves better results than the original KLT tracker and than BFT. Our algorithm, however, consistently outperforms L1-KLT, which clearly evidences the benefits of incorporating our multi-body prior. We observed that BFT generally fails to track moving objects, as illustrated in <ref type="figure" target="#fig_1">Fig.1</ref>. This is mainly because BFT heavily relies on a good estimate of the global motion, obtained by registering the entire current image to the previous one. For scenes with multiple motions, however, global motion estimation becomes unreliable, thus causing BFT to fail to track the moving objects. Note that the performance of all the trackers remain relatively unaffected as the noise level increases. This is mainly due to the fact that the corners in the checkerboard, while resembling each other, are very strong features that are robust to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hopkins Car-and-People Sequences</head><p>We then evaluated the algorithms on the Hopkins Carand-People sequences, depicting real-world outdoor scenes with multiple rigid motions. The number of tracks provided by the ground-truth ranges from 147 to 548 with an average of 369. Here, for all the methods, we used the same patch size and image pyramid levels as in the previous experi-  <ref type="figure">Figure 2</ref>: Tracking error as a function of the frame number: In these two typical sequences, our method consistently outperforms the baselines, and is less prone to tracking drift over time.</p><p>ment, and initialized the feature points with their groundtruth locations in the first frame. The average number of tracking errors for the different methods under different image noise levels is reported in <ref type="table" target="#tab_1">Table 2</ref>. Again, our multibody feature tracker achieves the lowest tracking error compared to the baselines, which confirms the robustness of our method.</p><p>To give a better idea of the behavior of the methods over time, in <ref type="figure">Fig. 2</ref>, we show the tracking error as a function of the frame number for two typical sequences (cars1 and cars2 with σ 2 =0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MTPV Sequences</head><p>We further tested our method on the MTPV sequences, which provide images of higher quality and resolution 5 than the Hopkins dataset and include sequences with strong perspective effects. In contrast to Hopkins, however, this dataset contains some outliers and missing data. For evaluation purpose, i.e., to create a complete and accurate groundtruth, we discarded the outliers and missing data. Since the image resolution is higher in this dataset, we used a larger patch size of 13 × 13 for all the methods. The results of all the algorithms are provided in <ref type="table" target="#tab_2">Table 3</ref>. Note that we still outperform all the baselines for most noise levels, with the exception of BFT for σ 2 = 0.04. We believe that the slightly less impressive gap between our approach and the baselines, in particular BFT, is due to the fact that the feature points in this dataset are often dominated by the background. See <ref type="figure" target="#fig_2">Fig. 3</ref> for typical examples of this dataset.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">KITTI Sequences</head><p>To evaluate the algorithms on realistic, high-quality images, we employed four sequences 6 from KITTI <ref type="bibr" target="#b7">[8]</ref>, depicting street/traffic scenes with multiple motions. Since no ground-truth trajectories are provided with this data, to obtain quantitative results, we took 10 consecutive frames from each sequence, applied the KLT tracker to them, and manually cleaned up the results to get ground-truth trajectories with an average of 177 points per sequence. The results of this experiment for different levels of noise added to the input are reported in <ref type="table" target="#tab_3">Table 4</ref>, and <ref type="figure">Fig. 4</ref> shows a qualitative comparison of the algorithms. Note that our method also outperforms the baselines on this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Frame-by-Frame Motion Segmentation</head><p>In our formulation, we optimize our energy function w.r.t. two variables: the displacement vector u and the self-</p><formula xml:id="formula_30">KLT L1-KLT BFT</formula><p>Our Method <ref type="figure">Figure 4</ref>: Performance of different trackers on a KITTI sequence: The red points denote the current positions of the feature points, and the green lines the motion since the previous frame. As evidenced by the regions highlighted with a blue rectangle, L1-KLT and BFT make more tracking errors than our approach. Best viewed zoomed-in on screen. expressiveness coefficients C. While the vector u provides the tracking results, the matrix C, as in the subspace clustering literature, can be used to build an affinity matrix for spectral clustering, and thus, if we assume that the number of motions is known a priori, lets us perform motion segmentation. In other words, our method can also be interpreted as simultaneous feature tracking and frame-by-frame motion segmentation. In this experiment, we therefore aim to evaluate the frame-by-frame motion segmentation accuracy of our method. Since, to the best of our knowledge, no existing motion segmentation methods perform feature tracking and frame-by-frame motion segmentation jointly, we compare our results with the following twosteps baselines: first, we find the tracks by KLT or L1-KLT and form the epipolar subspaces as in Eq. 7; second, we apply a subspace clustering method, i.e., Sparse Subspace Clustering (SSC) or Efficient Dense Subspace Clustering (EDSC), to perform motion segmentation. This results in four baselines denoted by KLT+SSC <ref type="bibr" target="#b14">[15]</ref>, KLT+EDSC, L1+SSC and L1+EDSC. The results of motion segmentation on the 22 Hopkins sequences used previously are shown in <ref type="table" target="#tab_4">Table 5</ref>. These results clearly evidence that our method outperforms the baselines significantly in terms of motion segmentation accuracy.</p><p>Runtimes: Typical runtimes (e.g., car8 of Hopkins155 which contains 192 trajectories) are: KLT -0.002 sec, L1-KLT -1.1 sec, BFT -0.6 sec, Ours -0.9 sec. This indicates that, while much slower than the original KLT, L1-KLT, BFT and our method are on par in terms of runtimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we have introduced a novel feature tracker that incorporates a multi-body rigidity prior into feature tracking. To this end, we have derived epipolar subspace constraints that prevent us from having to compute fundamental matrices and motion assignments explicitly. Our formulation only involves a series of convex subproblems, all of which have closed-from solutions. We have demonstrated the effectiveness of our method via extensive experiments on indoor and outdoor sequences.</p><p>While adding global rigidity constraints (be it the lowrank or the epipolar subspace constraints) to the local KLT tracker improves robustness, it comes with some computational overhead. In the future, we will therefore study how to speed up our approach, for instance by exploiting the GPU. Furthermore, our current model assumes that each patch undergoes only translation between consecutive frames. We therefore plan to investigate the use of more accurate models, such as affine transformations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is the 3 × 3 identity matrix and ⊗ denotes the Kronecker product. Let us define b =Px (or equivalently b i = vec(x ix T i ) ) and introduce another auxiliary variable m = Pu (where P is obtained by removing every 3i th column ofP)<ref type="bibr" target="#b2">3</ref> . Our optimization problem then becomes min u,C,E,Z,m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Update Z, E, C, u and m in closed-form via Eqs. 18-22, respectively; 2. Update A (u) and W (m) with updated u and m; 3. Update the Lagrange multipliers and penalty parameter via Eqs. 23-26; 4. Check the convergence conditions m − Pu ∞ ≤ ǫ, W (m) −W (m) C−E ∞ ≤ ǫ, and Z−A (u) ∞ ≤ ǫ; end while Output: Displacement vector u, coefficient matrix C one of the five variables u, C, E, Z, m while keeping the remaining four fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The MAN and MONK sequences of the MTPV dataset: The feature points are marked in red. Note that the number of points on the walking man and monk is much smaller than that on the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Average number of tracking errors (ε = 10) on the Hopkins checkerboard sequences with noise of different variances σ 2 . The lower, the better.</figDesc><table>Methods 
KLT 
L1-KLT 
BFT 
Ours 

σ 2 = 0.00 
47.63 
34.69 
39.68 
27.77 
σ 2 = 0.01 
46.92 
30.86 
39.30 
27.32 
σ 2 = 0.02 
45.95 
29.69 
38.84 
27.13 
σ 2 = 0.03 
46.59 
30.16 
39.16 
28.18 
σ 2 = 0.04 
47.19 
31.16 
39.35 
27.21 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Average number of tracking errors (ε = 5) on the Hopkins Car-and-People sequences with noise of different variances σ 2 . The lower, the better.</figDesc><table>Methods 
KLT 
L1-KLT 
BFT 
Ours 

σ 2 = 0.00 
21.71 
24.28 
49.13 
16.14 
σ 2 = 0.01 
34.59 
29.31 
51.69 
18.82 
σ 2 = 0.02 
54.95 
36.32 
54.63 
26.56 
σ 2 = 0.03 
76.02 
46.49 
57.57 
33.80 
σ 2 = 0.04 
95.17 
56.92 
58.36 
42.43 

Frame number 

2 
4 
6 
8 
10 

Number of tracking errors-cars1 

10 

20 

30 

40 

KLT 
L1-KLT 
BFT 
Ours 

Frame number 

2 
4 
6 
8 
10 

Number of tracking errors-cars2 

10 

20 

30 

40 

KLT 
L1-KLT 
BFT 
Ours 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Average number of tracking errors (ε = 5) on the MTPV sequences with noise of different variances σ 2 . The lower, the better.</figDesc><table>Methods 
KLT 
L1-KLT 
BFT 
Ours 

σ 2 = 0.00 
3.07 
13.34 
6.83 
2.34 
σ 2 = 0.01 
17.76 
22.12 
8.84 
3.87 
σ 2 = 0.02 
28.39 
27.26 
11.17 
6.94 
σ 2 = 0.03 
40.61 
35.53 
11.26 
9.92 
σ 2 = 0.04 
47.69 
38.93 
12.34 
13.22 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Average number of tracking errors (ε = 5) on the KITTI sequences with different noise variances σ 2 . The lower, the better.</figDesc><table>Methods 
KLT 
L1-KLT 
BFT 
Ours 

σ 2 = 0.01 
21.43 
22.05 
27.48 
14.18 
σ 2 = 0.02 
24.35 
22.85 
27.80 
16.70 
σ 2 = 0.03 
31.15 
26.88 
27.85 
17.70 
σ 2 = 0.04 
34.43 
29.23 
27.75 
20.33 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Average error rate (in %) of two-frame motion segmentation on the 22 Hopkins sequences with noise of different variances σ 2 . The lower, the better.</figDesc><table>Methods 
KLT+SSC 
KLT+EDSC 
L1+SSC 
L1+EDSC 
Ours 

σ 2 = 0.00 
19.76 
20.57 
18.71 
19.11 
8.97 
σ 2 = 0.01 
19.76 
20.61 
19.61 
20.41 
9.35 
σ 2 = 0.02 
19.21 
20.99 
21.02 
21.92 
9.33 
σ 2 = 0.03 
20.63 
20.69 
22.21 
20.48 
9.89 
σ 2 = 0.04 
20.38 
19.82 
21.35 
20.80 
11.26 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that, in practice, this dimension is typically smaller than 8, since, in real scenes, the motion of objects, such as cars or people, is not arbitrary, and thus corresponds to degenerate (i.e., low-rank) motion<ref type="bibr" target="#b14">[15]</ref>.<ref type="bibr" target="#b1">2</ref> In the following, we make use of subscript (u), i.e., W (u) , to indicate that W depends on the variable u. For compactness, and without causing confusion, we drop this explicit dependency in Section 3.2.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that Pu =Pū, sinceū i = [u T i , 0] T .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that the intensity of the images is normalized to [0, 1]. Therefore, a Gaussian noise with σ 2 = 0.04 already represents a much stronger noise than what typically occurs in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note, however, that they are still highly compressed and not wellsuited for tracking, as pointed out in the readme file of the dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">2011 09 26 drive 0018, 2011 09 26 drive 0051, 2011 09 26 drive 0056, and 2011 09 28 drive 0016.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the anonymous reviewers for their helpful comments. HL thanks the support of ARC grants DP120103896 and LP100100588, the ARC Centre of Excellence on Robotic Vision (CE140100016) and NICTA (Data61).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pyramidal implementation of the affine lucas kanade feature tracker description of the algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Bouguet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>Intel Microprocessor Research Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining local and global motion models for feature point tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buchanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multibody factorization method for independently moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="179" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dense multi-frame optic flow for non-rigid objects using subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A variational approach to video registration with subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="314" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">521540518</biblScope>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Robust motion segmentation with unknown correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<idno>ECCV. 2014. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient dense subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shape interaction matrix revisited and robustified: Efficient subspace clustering with corrupted and incomplete data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Null space clustering with applications to motion segmentation and face clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Two-view motion segmentation from linear programming relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perspective motion segmentation via collaborative clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Good edgels to track: Beating the aperture problem with epipolar geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Piccini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nordberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Better feature tracking through subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Computer vision: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Detection and tracking of point features. School of Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon Univ. Pittsburgh</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Space-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3-d motion segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A variational model for the joint recovery of the fundamental matrix and the optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generalized principal component analysis (GPCA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1945" to="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Segmentation of dynamic scenes from the multibody fundamental matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structureand motion-adaptive regularization for high accuracy optic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Duality TV-L1 flow with fundamental matrix prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IVCNZ</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
