<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Image Camera Calibration with Lenticular Arrays for Augmented Reality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Schillebeeckx</surname></persName>
							<email>ischillebeeckx@wustl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Washington University in St. Louis</orgName>
								<address>
									<addrLine>1 Brookings Dr</addrLine>
									<postCode>63130</postCode>
									<settlement>St. Louis</settlement>
									<region>MO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
							<email>pless@wustl.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Washington University in St. Louis</orgName>
								<address>
									<addrLine>1 Brookings Dr</addrLine>
									<postCode>63130</postCode>
									<settlement>St. Louis</settlement>
									<region>MO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single Image Camera Calibration with Lenticular Arrays for Augmented Reality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of camera pose estimation for a scenario where the camera may have continuous and unknown changes in its focal length. Understanding frame by frame changes in camera focal length is vital to accurately estimating camera pose and vital to accurately rendering virtual objects in a scene with the correct perspective. However, most approaches to camera calibration require geometric constraints from many frames or the observation of a 3D calibration object -both of which may not be feasible in augmented reality settings. This paper introduces a calibration object based on a flat lenticular array that creates a color coded light-field whose observed color changes depending on the angle from which it is viewed. We derive an approach to estimate the focal length of the camera and the relative pose of an object from a single image. We characterize the performance of camera calibration across various focal lengths and camera models, and we demonstrate the advantages of the focal length estimation in rendering a virtual object in a video with constant zooming.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Camera calibration is often the first step for Computer Vision and Augmented Reality (AR) applications because it defines how the scene is projected onto an image by the camera. Calibration characterizes the photometric and geometric properties of the camera, that define, respectively, how the pixels of the camera report color and intensity of the scene, and where scene elements appear on the image.</p><p>Typically, camera calibration techniques are based on images of a recognizable object at different views. Constraints on the camera geometry are created by finding 3D points on the object and corresponding them to 2D image points. When it is necessary to calibrate the intrinsic camera parameters such as the camera focal length, it is often necessary to user either multiple images of a planar object, or an object with substantial 3D extent. This is because the image of a single flat object may look similar in an image <ref type="bibr">Figure 1</ref>. A calibration object made from 3 lenticular arrays. Each lenticular array has an observed color that changes depending on its viewing angle. (Left) When viewed from reasonably far away, the arrays have relatively consistent colors because they are being viewed from approximately the same angle. (Right) A wide angle view from much closer has significant color variation because the direction from the camera to different parts of the object varies substantially. This paper uses this color variation to derive strong geometric constraints for simultaneous, single-image pose estimation and camera calibration. taken from a wide angle camera near the object or a more zoomed in camera far away.</p><p>This creates a challenge for augmented reality applications that are required to work with dynamic scenes and changing camera parameters such as a movie shot that requires camera zooming. This paper offers a new approach to geometric calibration of a camera that requires a single image of a calibration object that may lie within a larger scene. The calibration object that we propose is shown in <ref type="figure">Figure 1</ref>.</p><p>This calibration object is based on several lenticular arrays. A lenticular array is a sheet of plastic which is comprised of many tiny parallel cylindrical lenses. Interleaving different hues behind each lens makes a lenticular pattern whose apparent hue depends on the viewing angle.At the scale of the whole lenticular array, this means that the perspective of a camera with a wider field of view will see many different colors (Bottom Left), while a camera with a narrow field of view will only see a few hues (Bottom Right). picts how these lenses focus parallel rays of light onto an interleaved pattern on the back of the lenticular array. As a result, for different viewpoints, the lenticular array has different appearances. In the case of children's toys, different frames of an animation are interleaved, and thus create the appearance of an animation as the lenticular array is rotated <ref type="bibr" target="#b13">[14]</ref>. Some modern TVs use lenticular arrays to show different images to different viewer directions in order to create a 3D effect without the need for extra equipment like special glasses <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>We construct a lenticular pattern inspired by <ref type="bibr" target="#b16">[17]</ref>, based on a pattern of interleaved colors. This creates an apparent hue dependent on the relative incident angle of light rays viewing the lenticular array. For a perspective camera viewing a planar surface, pixels may have differing viewing angle and therefore will measure a different hue. Therefore as seen in the bottom row of <ref type="figure">Figure 2</ref>, a camera with a wide field of view would see many different hues, while a camera with a narrow field of view would see fewer hues. This fundamental relationship between a color-coded lenticular array and a camera provides a novel geometric constraint to calibrate a camera.</p><p>The contributions of this paper are:</p><p>• an extension of the constraints derived in <ref type="bibr" target="#b16">[17]</ref> to allow lenticular patterns to constrain the camera focal length, • an approach to correct manufacturing problems of alignment and stretching that make relationship be-tween color and angle vary across a lenticular array, and • experimentation with a physical instantiation of a prototype calibration object, showing calibration accuracy in different settings and a complete end-to-end augmented reality demonstration with a variable zoom video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Geometric camera calibration is a well studied problem, and geometric constraints relating correspondences, camera motion and camera calibration are well understood <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref>, including the development of popular toolboxes <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2]</ref>. Perhaps the most common approach to camera calibration is Zhang's method based on taking many pictures of a grid of known size <ref type="bibr" target="#b24">[25]</ref>. The method simultaneously solves for the pose of the grid in each frame and the camera parameters that are consistent across many frames.</p><p>Calibrating a camera with a single image is possible by imaging calibration objects with known 3D geometry, for example with 2 orthogonal planes <ref type="bibr" target="#b17">[18]</ref>. Other methods use known shapes such as 1D lines <ref type="bibr" target="#b12">[13]</ref> and 2 co-planar circles <ref type="bibr" target="#b3">[4]</ref> or images of man-made objects such as buildings in a city and exploit vanishing lines <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref> or low-rank textures <ref type="bibr" target="#b25">[26]</ref> as a source of calibration constraints. With many images from a variety of viewpoints, more standard structure from motion approaches can be used, and these have been specialized for the case of unknown focal lengths in various ways, including understanding the minimal constraint sets <ref type="bibr" target="#b2">[3]</ref>, extensions to uses features such as line segments and right angles to provide stronger geometric constraints <ref type="bibr" target="#b9">[10]</ref>, and a linearization of the EPnP problem <ref type="bibr" target="#b10">[11]</ref> to speed up the estimation of pose for the uncalibrated camera case <ref type="bibr" target="#b14">[15]</ref>.</p><p>In Augmented Reality (AR), one seeks to render digital content on top of a video feed of a scene to digitally enhance the physical world. In order to properly project digital elements into the video feed, the relative pose of the digital element and the camera must be known, and the camera calibration must be known.</p><p>Calibration methods which require multiple images of an object in different poses or calibration objects with substantial variation in all 3 dimensions may not be appropriate for all applications. To our knowledge, the only papers to study this problem in the context of an augmented reality application are the recent work of Taketomi, et al. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. They use a KLT tracker to find many points in the scene and use a Structure from Motion formulation to solve for 3D scene points, and the camera position, rotation and focal length in every frame, regularizing all camera parameters to vary smoothly throughout the video. Then, a fiducial marker pattern is located in each frame, and an object is rendered in a position defined by that fiducial marker, with a perspective that depends on the estimated camera focal length.</p><p>In contrast to these existing approaches, our approaches calibrates every image independently using a flat calibration object. This object is based on a lenticular pattern. Lenticular arrays and their 2D counter-part, microlens arrays, give geometric constraints on the incident angle of light rays viewing the arrays. Previous research has used color coded microlens arrays as "light field probes" for Schlieren photography <ref type="bibr" target="#b21">[22]</ref>, imaged reflections of these arrays to support the reconstruction of surface geometry of transparent objects <ref type="bibr" target="#b22">[23]</ref>, and for the reconstruction of the refractive index of gases <ref type="bibr" target="#b8">[9]</ref>. Large lenticular arrays have been used to estimate object rotation <ref type="bibr" target="#b16">[17]</ref> with correspondences, while very small lenticular arrays were used as fiducial markers to estimate the pose of an object <ref type="bibr" target="#b15">[16]</ref>.</p><p>Our work addresses the problem of joint intrinsic and extrinsic camera calibration needed in AR applications where cameras may change their zoom to keep track of an object. Such an approach is necessary to integrate AR with new commercial systems such as Soloshot (http: //soloshot.com) that sell Pan-Tilt-Zoom cameras that automatically track a radio tag, but which do not have encoders on their zoom lenses that tag imagery with the zoom level.</p><p>The next section introduces our calibration object that is suitable for such applications, We then derive the geometric constraints this object offers and evaluate the performance both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AR Calibration Object</head><p>Our calibration object is inspired by the lenticular array used to estimate object rotation <ref type="bibr" target="#b16">[17]</ref>; we adopt their notation in our discussion here. Three lenticular arrays are mounted perpendicular to each other on a plane, where the 2 flanking arrays have the same orientation, but orthogonal to the middle array. These arrays are orthogonal so that any rotation of the calibration object creates a change; when the object is oriented as shown on the left of <ref type="figure">Figure 1</ref>, rotation around the horizontal axis causes the two edge arrays to change color, while rotating around the vertical axis causes the central part to change color. Small black strips are added to make it easier to distinguish the 3 arrays when they are oriented so that their colors are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Calibrating a Color Coded Lenticular Array</head><p>The relationship of the apparent viewing angle to the observed color depends on the relative angle of the lenticular array, in particular the rotation around the axis of the lenticular lenses. The relationship between this rotation and observed color is captured in the Hue Response Function (HRF), which is a 1-to-1 relationship for incident angles of up to ≈ 40 degrees (after which the colors repeat). For a lenticular array created as described in <ref type="bibr" target="#b16">[17]</ref>, we found that the hue response function varies across the array. We demonstrate this in <ref type="figure" target="#fig_1">Figure 3</ref>, which shows a picture of the calibration object taken from far away with a very long focal length length lens, giving a field of view in this picture of 1 degree. Therefore the color difference observed at the two highlighted circles is substantial. When measuring this color difference as the calibration array rotates, we see a consistent shift. We believe this is due to the challenges of manufacturing and printing the color coded pattern that sits behind each lenticular lens. For this lenticular array that has 2 lenticular lenses per millimeter, if the backplane is stretched 0.1mm extra over the course of this array, this would cause the observed color shift.</p><p>To address this challenge, while still employing standard manufacturing processes for lenticular arrays, we explicitly calibrate the HRF, the function that relates hue to orientation, at regular intervals in the local reference frame of the arrays. We use the corners of the rectangular lenticular arrays as anchor points, and for each image compute a homography mapping the observed lenticular patterns to a canonical coordinate system. The process is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. This calibration object is placed on a controlled rotation mount and rotated through 1 degree increments. For each calibration point, we record the angle at which that calibration point is viewed (which may vary across the calibration grid because of perspective effects), and the measured hue for that angle. The result of this is a curve like those shown in <ref type="figure" target="#fig_1">Figure 3</ref> for each of the calibration points. This process is repeated separately for the center and each of the two side lenticular arrays that make up the calibration object.</p><p>The next section derives how images of this calibration object can exploit the measured colors for additional geometric cues to the pose of the calibration object and the focal length of the camera. When converting the observa- tions of the object into geometric constraints, we again find the corners of the array to compute the homography, and sample the colors at these same grid locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Calibration Constraints</head><p>In this section we derive the calibration constraints that arise from observing the lenticular calibration object. We follow the presentation of <ref type="bibr" target="#b16">[17]</ref> which solved for rotation given known calibration. They derive the geometric constraint that says that a ray observing a particular color must lie in a plane, and they represent that plane by it's surface normal n hue . The overall constraints for the imaging geometry start with a pixel p that observes the lenticular array. In the coordinate system of the camera, a pixel p captures light traveling along a ray r that depends on the calibration matrix K as:</p><formula xml:id="formula_0">r = K −1 p<label>(1)</label></formula><p>In the coordinate system of the lenticular pattern, this ray has direction RK −1 p, and it must satisfy the constraint (RK −1 p) · n hue = 0.</p><p>This can be written as (RK −1 p) ⊤ n hue = 0, which is equivalent to: p ⊤ K −1 ⊤ R ⊤ n hue = 0. Collecting terms, we write: p ⊤ (K −1 ⊤ R ⊤ ) n hue = 0, and re-write as: p ⊤ (RK −1 ) ⊤ n hue = 0.</p><p>Given a pixel location p and a n hue , this linear constraint on K and R. Previous work <ref type="bibr" target="#b16">[17]</ref> uses this constraint and a nonlinear optimization to solve for R given a known K matrix. In this paper we use a similar optimization to get an initial estimate for R and K by parameterizing K by it's focal length f :</p><formula xml:id="formula_1">K(f ) =   f 0 x 0 0 f y 0 0 0 1  <label>(2)</label></formula><p>In K(f ), pixels are assumed to be square and x 0 and y 0 are assumed to be the center of the image.</p><p>Algorithm: For each frame, our algorithm follows these steps to get an initial estimate of the calibration object pose and camera focal length:</p><p>1. Find the four corners of the lenticular calibration object.</p><p>2. Solve for the homography to map image coordinates onto object coordinates.</p><p>3. Measure the hue at each grid point on the homography, and use these hue measurements to solve for an initial estimate of the rotation and the focal length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Given that rotation and focal length, use the lenticular marker based constraints introduced in [16] to get an estimate of the object translation.</head><p>The initial estimate is refined by minimizing the following cost function:</p><formula xml:id="formula_2">argmin ρ,T,f i h(R(ρ), T, f, i)−hue i 2 +λ g(R(ρ), T, f )−p i 2 2</formula><p>(3) where the first term penalizes the difference between hue i which is the measured hue at grid-point i (of all lenticular arrays), and h(R(ρ), T, f, i), the hue predicted for grid point i when it is projected onto the image based on camera intrinsic and extrinsic parameters R, T, f , using the HRF function calibrated for grid-point i. Here, R is parameterized via rodgrigues parameters ρ. The second term measures the spatial reprojection error between the location p i and the predicted location for that grid point g(R, T, F, i) based on R, T and f . A relative weighting function λ was found emprically to balance hue and position error which are measured in very different coordinate systems. In all experiments we show, λ was set to 1/4000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section we quantify the ability of our algorithm and this calibration object to estimate the object pose and camera focal length. We also explore the sensitivity of parts of the algorithm to various intermediate processing steps.</p><p>The first stage of the algorithm is tracking the corners of the calibration object. This is a vital step in most AR pose estimation algorithms, but it has additional importance in our algorithm because we are modeling the fact that the HRF that maps color to angle may vary across the calibration object. Thus, in Section 5.1 we evaluate the sensitivity <ref type="figure">Figure 5</ref>. To create location specific HRF functions that map measured hues to angular constraints, we sample hues of a lenticular array at a local grid of points. The location of these points depends on localizing the anchor points at the corners of the lenticular array. We show the small prediction errors for 8 px permutations of these anchor points per HRF (top) and per image (bottom).</p><p>of the algorithm to errors in corner tracking. Second, Section 5.2 explores the accuracy of our approach for estimating the focal length of the camera and rotation and translation of the calibration object. We characterize the error using our physical prototype with different cameras and with the calibration object oriented in different directions. We conclude with Section 5.3 with results showing an object added to a video taken with varying focal length, and compare the realism of the added AR object when there isn't the ability to dynamically estimate the focal length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Sensitivity to point tracking</head><p>Because the lenticular array may not have the same mapping from angle to color everywhere, we need to know where on the calibration pattern we are measuring the color in order to look up the correct location-specific HRF. Therefore, this approach may be especially sensitive to estimating the position of the corners of the calibration object. We evaluate this by rotating the lenticular array around the vertical axis in one degree increments from −35 to 35 degrees. For each image, we follow these steps:</p><p>1. determine 4 anchor points of the lenticular array, 2. project the lenticular array into the local reference frame via a homography 3. sample the hue from the grid-points of the local reference frame image.</p><p>For each grid point we compute the angle at which the point was viewed to the angle predicted by the measured hue. To estimate the effect of noise in estimating the lenticular array corners, we perturb the anchor points by 8 pixels in random directions 20 times per image and assess the difference in angle predicted by the HRFs. We show the scale of one such perturbation in the supplementary material <ref type="figure">(Figure S1</ref>). <ref type="figure">Figure 5</ref> shows results. The top shows a box and whisker plot showing the distributions of errors in estimating the angle for each of the 100 grid points where the HRF was calculated. The box in each columns shows the 25th and 75th percentiles of the distribution. This experiment shows that modeling the HRF at each location of the lenticular array leads to nearly all angular measurements being within 0.5 degrees of the true incident angle.</p><p>We also evaluate if the errors in estimating angle from hue depend on the angle at which the calibration object is observed. <ref type="figure">Figure 5</ref> computes the distribution of errors across the entire array for each image angle. Again the error is consistently small, even though these statistics are computed using anchor points that are substantially perturbed.</p><p>We believe that an error of 0.25 • is near the limit of a simple geometric constraint based on hue measured at one pixel. The lenticular array shows colors across the hue spectrum over a range of about 40 • , so 0.25 • is less than 1% of the range of angles that are viewed. Reliably measuring the hue of pixels in 8-bit RGB images to better than 1% precision is also challenging. In Section B of the supplementary material, we explore the color precision of an 8-bit RGB camera and subsequently the angular precision when viewing a lenticular array.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Pose and Focal Length Estimation</head><p>In a laboratory setting, we assess the performance of rotation, translation, and focal length estimation across different viewpoints. On a motorized stage we rotate the calibration object in increments of 5 degrees from −25 to 25 degrees around the vertical axis and take images at each increment. We calibrate the ground truth camera focal length with the MATLAB 2014a implementation of Zhang's method <ref type="bibr" target="#b24">[25]</ref>. <ref type="figure">Figure 6</ref> shows the rotation estimation performance per image in the left column as well as in summary in the right column. We show rotation error for each local axis as the angular difference of our estimate to the true rotation. The estimates from our initialization algorithm is shown in the top row and show errors at the scale of a few degrees. The <ref type="figure">Figure 6</ref>. We compare the rotation estimations our method gets initially and after refinement. In the left column, we see the rotation error per local axes for each image as the calibration object is rotated, while in the right column we see the summary statistics. Although we start with good rotation estimates, the refinement process still gives improvement. <ref type="figure">Figure 7</ref>. We report the focal length estimations for different orientations of the calibration object per image (on the left) and in summary (on the right). The initial estimations (top) start with considerable error in focal length estimation. After reprojection refinement, however, the results are improved significantly achieving a median of less than 5% error. bottom of this plot shows results after minimizing the reprojection error as defined in Equation 3, when we get rotation estimates with a median error of 1 degree. <ref type="figure">Figure 7</ref> quantifies error in the focal length estimation, and <ref type="figure">Figure 8</ref> quantifies error in the translation estimation. Both the initialization and refinement results shown strong correlations between the focal length error and the translation error. The refinement step reduces the error of both to a median error of about 4%. The correlation in error between the focal length and the translation arises from the ambiguity that an object can appear bigger either by moving closer to the camera or by the camera changing its focal length. In the AR demo shown later, we see that a 4% error does not appear to lead to a perceptually noticeable error in rendering <ref type="bibr">Figure 8</ref>. We present the per image (left column) and summary performance statistics (right column) for initial translation estimation (top) and refined translation estimation (bottom). In these plots, we show the distance error for each axis. The overwhelming majority of error is in the Z-axis, which is the depth of the camera. The z-axis translation errors reflect errors in estimating focal length.</p><p>the correct perspective of the object. <ref type="figure">Figure 9</ref> shows quantitative results for focal length estimation from single images of the calibration object taken at different orientations and different focal lengths. For each image, we show the results that visualize rotation by rendering the local coordinate system on top of the original image. The image title shows the ground truth focal length, our estimated focal length, and the percent error. We include images from cell phone cameras, as well as a DSLR camera. The first two images are from an iPhone 5 and a Galaxy S6 with focal lengths of 5 and 5.8 mm. The images following those are from a Nikon D90 at focal lengths of 18, 49, 90, 115, and 185 mm.</p><p>Focal length estimates are relatively accurate for shorter focal lengths. Very long focal lengths correspond to imaging geometries with a smaller field of view. For small fields of view, small errors in estimating angular constraints may lead to larger errors in estimating focal length. To ground this, we show the impact of mis-estimating the field of view by 0.25 • degrees on the estimate of the focal length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Augmented Reality Application</head><p>In a desktop scene, we record video of the calibration object while moving the camera in a freehand trajectory. When the camera is moved farther away from the scene and the calibration object, we digitally zoom to keep the calibration object as large as possible in the image. For each frame we estimate the camera focal length, rotation, and transla- <ref type="figure">Figure 9</ref>. Eight examples of single frame estimation of the focal length and object rotation estimates. The first two images are taken from an iPhone and Galaxy S6, while the remaining images are taken by a Nikon DSLR camera at different zoom settings. <ref type="figure">Figure 10</ref>. The focal length error that arises from mis-estimating the field of view by 0.25 • changes as the field of view gets smaller (and, correspondingly, the focal length gets longer). tion using the calibration object as detailed in Section 4. In <ref type="figure">Figure 11</ref>, we compare our estimated focal length with the ground truth focal length (which we know because this is a digital zoom) per frame. We can see that the focal length estimations follow the zooming trajectory well. We emphasize that our algorithm does not have access to this digital zoom information.</p><p>As a comparison, we consider an AR algorithm that doesn't have access to the digital zoom and does not have the ability to estimate it from image data. When such an algorithm uses a pre-calibrated focal length which becomes wrong in part of the video sequence, virtual objects are rendered with incorrect perspective. <ref type="figure">Figure 12</ref> shows 3 frames from the video in each column. We render a virtual wireframe box to highlight perspective effects. The top row shows the original images, the center row shows the box rendering given the estimates made with a dynamic focal length, and the bottom row shows the box rendering given the estimates made with a static focal length. The digital <ref type="figure">Figure 11</ref>. We show the focal length estimates used to render a box into a video in our AR demonstration. We show how our estimates follow the focal length changes from zooming. box has a base the size of the calibration object and is 45mm deep.</p><p>Our scene contains graph paper that is aligned to show a cartesian coordinate to help the viewer assess perspective effects. The wire-frame box should appear aligned just short (10mm or 2 boxes) of the end of the paper grid. In comparing our method of estimating a dynamic focal length against estimating a static focal length, we see that the rendered box looks unnaturally too large and with too much perspective in the case of a static focal length. This holds true in general for all frames, and we include the entire video in the supplementary materials.</p><p>In the supplementary material, we show an additional AR video. In this video, we render a 3D model into a video with free-hand camera zoom. <ref type="figure">Figure 12</ref>. We show focal length estimation results in 3 frames of a video where we render a box over the calibration object. The original image (top row) is digitally zoomed to maximize the size of the calibration object in the image. By estimating the focal length dynamically in each image (middle row) versus estimating a single static focal length (bottom row), we achieve a much more natural rendering that is the correct relative size and has the right amount of perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We present an end to end system and physical calibration object for simultaneous camera focal length estimation and pose-estimation. This calibration object uses lenticular arrays that offer geometric constraints based on the observed color, allowing for the estimation of camera focal length from a single image. In contrast to <ref type="bibr" target="#b2">[3]</ref> which suffers from formal ambiguity and thus high error for frontoparallel views when using coplanar points <ref type="bibr" target="#b17">[18]</ref>, our calibration object based on lenticular arrays is robust for all viewing angles. The ability to estimate focal length from a single image is vital for augmented reality applications from cameras that are zooming as they record a scene.</p><p>Our geometric constraints are based on measuring image colors. Color is notoriously difficult to measure accurately. Apparent colors may change based on lighting or camera white balance settings. In this paper we did not address this issue, but recent work suggests that these effects can be mitigated by explicitly including a color correction term in the geometric optimization <ref type="bibr" target="#b15">[16]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 Figure 2 .</head><label>22</label><figDesc>(Top Left) The side view of a lenticular array shows how parallel light rays are focused onto the back focal plane. (Top Right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(Left) an image of the calibration object taken with a very long lens, observing all parts of the calibration array with a nearly orthographic imaging geometry. The observed color differences indicate the the hue for a given viewing direction is not consistent. (Right) The observed hue measured at the two yellow circles as the calibration object is rotated, showing a consistent bias.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>To take hue measurements for calibration and estimation, we employ the following strategy: With the original image (left), we identify anchor points, shown as blue points, at the corners of the lenticular array. These points are used to learn a homography (center) where we then take local measurements evenly across the array, shown as green crosses. (Right) a simplified image of the hue recorded at each calibration point.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using vanishing points for camera calibration</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="139" />
			<date type="published" when="1990-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Camera calibration toolbox for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Bouget</surname></persName>
		</author>
		<idno>2013-10- 09. 2</idno>
		<ptr target="http://vision.caltech.edu/bouguetj/calib_doc/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A general solution to the p4p problem for camera with unknown focal length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bujnak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kukelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Full camera calibration from a single view of planar scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Advances in Visual Computing</title>
		<meeting>the International Symposium on Advances in Visual Computing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic camera calibration from a single manhattan image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deutscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="175" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Analytical photogrammetry applied to a single terrestrial photograph mensuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gracie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A compressive light field projection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reconstructing gas flows using light-path approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pose estimation with unknown focal length using points, directions and lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Astrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Epnp: An accurate o (n) solution to the pnp problem. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="155" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d tv: a scalable system for real-time acquisition, transmission, and autostereoscopic display of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="814" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple camera claibration from a single image using five points on two orthogonal 1-d objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Miyagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Three-dimensional imaging techniques. Elsevier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okoshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exhaustive linearization for robust camera pose and focal length estimation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Penate-Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrade-Cetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2387" to="2400" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The geometry of colorful, lenticular fiducial markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schillebeeckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured light field design for correspondence free rotation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schillebeeckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On plane-based camera calibration: A general algorithm, singularities, applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Computer Vision: Algorithms and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">geometric registration for zoomable camera using epipolar constraint and pre-calibrated intrinsic camera parameter change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Taketomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ieee International Symposium On</title>
		<imprint>
			<publisher>ieee</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="295" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Camera poes estimation under dynamic intrinsic parameter change for augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Taketomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hand-Held Schlieren Photography with Light Field Probes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Refractive shape from light field distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roodnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flexible camera calibration by viewing a plane from unknown orientations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="666" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Camera calibration with lens distortion from low-rank textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2321" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
