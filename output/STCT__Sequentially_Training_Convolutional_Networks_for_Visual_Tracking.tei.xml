<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STCT: Sequentially Training Convolutional Networks for Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STCT: Sequentially Training Convolutional Networks for Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the limited amount of training samples, finetuning pre-trained deep models online is prone to overfitting. In this paper, we propose a sequential training method for convolutional neural networks (CNNs) to effectively transfer pre-trained deep features for online applications. We regard a CNN as an ensemble with each channel of the output feature map as an individual base learner. Each base learner is trained using different loss criterions to reduce correlation and avoid over-training. To achieve the best ensemble online, all the base learners are sequentially sampled into the ensemble via important sampling. To further improve the robustness of each base learner, we propose to train the convolutional layers with random binary masks, which serves as a regularization to enforce each base learner to focus on different input features.</p><p>The proposed online training method is applied to visual tracking problem by transferring deep features trained on massive annotated visual data and is shown to significantly improve tracking performance. Extensive experiments are conducted on two challenging benchmark data set and demonstrate that our tracking algorithm can outperform state-of-the-art methods with a considerable margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual tracking is a fundamental problem in computer vision that has been receiving a rapidly growing attention. It has a variety of subfields ranging from single-target to multi-target tracking. The focus here is single-target, model-free online tracking <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref>, where a category agnostic target is indicated by a bounding box in the first frame, and the tracker aims at locating the target in the following each frame. Due to significant target appearance changes caused by abrupt motion, deformation, occlusion and illumination variation, visual tracking is still a very challenging problem.</p><p>Prior approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> rely on hand-crafted features to describe the target and have addressed the above challenging factors to a certain extend. However, these STCT OTCT <ref type="figure">Figure 1</ref>. Due to limited number of training samples, online training CNNs for tracking (denoted as OTCT) can easily lead to overfitting and cause tracking failure. We propose a sequential learning method (denoted as STCT) for CNNs to address this issue.</p><p>hand-crafted features are designed for certain scenarios. Thus, they can not generalize well and are incapable to capture the semantic information of the target, which can easily lead to tracking failure in challenging conditions.</p><p>Recently, deep Convolutional Neural Networks (CNNs) trained on large scale image classification data sets (e.g. <ref type="bibr" target="#b4">[5]</ref>) have demonstrated great success in many vision tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25]</ref>. These semantic representations discovered by the learning process are shown to be very effective at distinguishing objects of various categories. However, supervised training of deep CNNs with millions of parameters entails a large number of annotated training samples. To apply deep CNNs for tasks with a limited amount of training samples, previous approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref> adopt a transfer learning method by first pre-training a deep CNN on a source task with a large scale training data set and then fine-tuning the learned feature on the target task. Due to the good generalization capability of CNN features across different data sets, this transfer learning approach is effective and has shown state-of-the-art performance.</p><p>However, for online visual tracking, the lack of training samples becomes even more severe, since the only training sample with ground truth label is provided in the first frame, and the tracking results used for updating the tracker are also obtained in a sequential manner. Thus, directly online fine-tuning a pre-trained deep CNN is prone to over-fitting, which will degrade the tracker and gradually leads to tracking drift (See <ref type="figure">Figure 1</ref> as an example). In order to address the above issue, we propose a sequential training method for CNNs to effectively transfer pre-trained deep features for online visual tracking. Specifically, a CNN is regarded as an ensemble, while each channel of the convolutional feature map is treated as a base learner and is updated using a different loss criterion, such that they are not highly correlated with each other. Online fine-tuning of the CNN is then formulated as a sequential ensemble learning problem. To build the best ensemble, we sequentially select the base learners via important sampling and add them into the ensemble. Online tracking is conducted as foreground/background separation by the sequentially learned ensemble. To further reduce over-fitting, we propose to train the convolutional layers with random binary masks, which can effectively enforce different convolutional kernels to focus on different target parts.</p><p>The contribution of this paper can be summarized into three-folds: i) we propose a sequential training method for CNNs, which can effectively transfer pre-trained deep features for online application and reduce over-fitting; ii) we develop an effective visual tracking algorithm based on the proposed sequential learning method, where deep features trained on the Imagenet image classification task <ref type="bibr" target="#b4">[5]</ref> are utilized to predict the position and scale of the target simultaneously; iii) extensive experiments are conducted on two popular benchmark data sets and demonstrate that the proposed tracking algorithm performs favorably against stateof-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A typical tracking method mainly contains two important components: an appearance model to estimate the likelihood of target candidates, and a search strategy to find the most likely target location. In this paper, we mainly focus on the design of a robust appearance model. In some prior methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31]</ref>, the target appearance is represented by generative models and the candidate with the maximum likelihood is predicted as the target. The "EigenTracking" algorithm <ref type="bibr" target="#b1">[2]</ref> utilizes pre-trained eigen basis to describe the target appearance. Later on, Ross et al. <ref type="bibr" target="#b26">[27]</ref> propose to incrementally update both the eigenbasis and mean to adapt to target appearance changes. Sparse representation has also been applied to tracking <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33]</ref>, where the target is reconstructed by a sparse combination of target templates.</p><p>Meanwhile, some methods cast visual tracking as a foreground and background separation problem using discrimi-native models. Online learning algorithms based on boosting <ref type="bibr" target="#b9">[10]</ref>, structured SVM <ref type="bibr" target="#b10">[11]</ref>, multiple instance learning <ref type="bibr" target="#b0">[1]</ref>, and correlation filters <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> are applied in tracking and achieve good performance. Among others, Danelljan et al. <ref type="bibr" target="#b3">[4]</ref> propose to estimate the scale changes of the target using correlation filters learned from HOG features. Our method bears a similar spirit with <ref type="bibr" target="#b3">[4]</ref> in predicting target scale changes. However, ours differs from <ref type="bibr" target="#b3">[4]</ref> in that we exploit a scale prediction network trained on deep features which are more robust to significant appearance changes.</p><p>Deep convolutional neural networks have improved state-of-the-art performance in many computer vision applications <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26]</ref> in recent years. Existing methods have also explored the usage of CNNs in online tracking. In <ref type="bibr" target="#b20">[21]</ref>, a three-layer CNN is trained on-line. Without pre-training and with limited training samples obtained online, CNN fails to capture object semantics and is not robust to deformation. In <ref type="bibr" target="#b35">[36]</ref>, a deep autoencoder is first pre-trained offline and then finetuned for binary classification in online tracking. Since the pre-training is performed in an unsupervised way by reconstructing gray images with very low resolutions, the learned deep feature has limited discriminative power for tracking. Both <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b35">[36]</ref> train deep networks online with limited training samples, and inevitably suffer from over-fitting. Consequently, they only achieve comparable or even inferior performance against state-of-the-arts. More recent methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22]</ref> adopt deep convolution networks trained on a large scale image classification task <ref type="bibr" target="#b4">[5]</ref> to improve tracking performance. <ref type="bibr" target="#b13">[14]</ref> predicts saliency maps using deep features. <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b21">[22]</ref> propose to estimate foreground heat maps by training either CNNs or correlation filters using feature maps of multiple convolution layers. In contrast, we provide a new paradigm to transfer rich features of pre-trained deep CNNs for online tacking. Instead of directly finetuning deep features like <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, we cast online training CNN as learning ensembles to effectively remove feature correlation and avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN Training as Ensemble Learning</head><p>Before elaborating the proposed sequential training method for CNNs, we first introduce some background of ensemble learning to put our method in a proper context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sequential Sampling for Ensemble Learning</head><p>Given a data point x, the goal of supervised learning is to predict the likely valueŷ of the ground truth value y associated with x. The prediction rule is often defined as a functionŷ = F (x) that can be learned by minimizing the expected loss over all the training data</p><formula xml:id="formula_0">{z i = (x i , y i )} N 1 F * (x) = arg min F (x) 1 N N i=1 L(y i , F (x i )),<label>(1)</label></formula><p>where L(y,ŷ) indicates the loss of predicting a valueŷ when the true value is y. The prediction function can have various forms depending on the particular problem to be handled. In <ref type="bibr" target="#b6">[7]</ref>, Friedman and Popescu formulate the prediction function as an integral</p><formula xml:id="formula_1">F (x) = π 0 + Γ π(γ)f (x; γ)dγ,<label>(2)</label></formula><p>where f (x; γ) denotes a base learner parameterized by γ ∈ Γ, and π(γ) is the coefficient function. Numerical quadrature is utilized to approximate (2) by a linear combination of base learners at M evaluation points γ m ∈ Γ as </p><formula xml:id="formula_2">F (x) ≃ a 0 + M m=1 a m f (x; γ m ).</formula><formula xml:id="formula_3">Q(γ) = min α0,α 1 N N i=1 L(y i , α 0 + αf (x; γ)).<label>(3)</label></formula><p>The optimal single point can then be obtained by minimizing the irrelevance as γ * = arg min Q(γ). For an ensemble of base learners {f (x; γ m )} M 1 , the characteristic scale can be employed to measure its quality</p><formula xml:id="formula_4">σ = 1 M M m=1 [Q(γ m ) − Q(γ * )] .<label>(4)</label></formula><p>A small value of σ implies that many base learners in the ensemble are very similar to the optimal base learner f (x; γ * ). Since they are highly correlated with each other, the ensemble of base learners fail to provide additional information beyond the single optimal base learner f (x; γ * ).</p><p>On the contrary, if σ is too large, most of the base learners are irrelevant to the problem which will ultimately degrade the performance of the ensemble. Based on the above observation, Friedman and Popescu <ref type="bibr" target="#b6">[7]</ref> propose a sequential sampling method to generate an ensemble of evaluation points, in which the irrelevance measure of each successive point γ m depends on the previously sampled points {γ l } m−1 <ref type="bibr" target="#b4">(5)</ref> where the parameter η controls the impact of previously sampled points on the current relevance measure. Then each sequentially selected parameter point γ m is determined by</p><formula xml:id="formula_5">1 Qm(γ|{γ l } m−1 1 ) = min α 0 ,αm 1 N N i=1 L yi, α0 + αmf (xi; γ) + η m−1 l=1 α l f (xi; γ l ) ,</formula><formula xml:id="formula_6">γ m = arg min γ∈Γ Q m (γ|{γ l } m−1 1 ).<label>(6)</label></formula><p>As the sampling proceeds, the irrelevance defined by (5) will increasingly differ from that for the single parameter point <ref type="bibr" target="#b2">(3)</ref>. Consequently, the sampled parameter points will not be highly correlated with each other. We refer the readers to <ref type="bibr" target="#b6">[7]</ref> for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Online Training CNNs as Sequential Ensemble Learning</head><p>For online applications, one simple approach to transfer offline pre-trained CNN features is to add one or more randomly initialized CNN layers, named as adaptation layers, on top of the pre-trained CNN model. Then keep the parameters, i.e. convolutional kernels and bias, of the pre-trained CNN fixed and only learn the parameters of the adaptation layers online to fit the current task. However, we empirically observe that this transfer learning method suffers from severe over-fitting. The online learned parameters mainly focus on recent training samples and are less likely to well generalize to both historical and future samples. This phenomenon can be fatal to online visual tracking where the target often undergoes significant appearance changes or heavy occlusion.</p><p>To tackle the above issue, we propose to train a CNN model online as sequentially learning ensembles to better transfer pre-trained deep features. Denote the pre-trained CNN as CNN-E, which takes the RGB image as input and outputs a convolutional feature map X. An online adapted CNN, named as CNN-A is randomly initialized and consists of two convolutional layers interleaved with an ReLU layer as the nonlinear activation unit. It takes the feature map X as input and generates the final feature map {F c 2 (X)|c = 1, 2, . . . , C 2 }, where F c 2 (X) ∈ R m×n indicates the c-th channel of the feature map generated by the second layer with spatial size of m × n.</p><p>The feature map in the second layer is obtained by convolving the kernel with the feature map in the first layer as</p><formula xml:id="formula_7">F c 2 (X) = C1 k=1 w c k * F k 1 (X) + b c ,<label>(7)</label></formula><p>where C 1 denotes the number of channels of the feature map output by the first layer; w c k represents the convolution kernel connecting the k-th channel of the first layer feature map with the c-th channel of the second layer feature map; b c is the bias and * denotes convolution operation; the summation is conducted element-wisely. In order to introduce randomness into the parameter learning process, we regard the output feature map as an ensemble of base learn-</p><formula xml:id="formula_8">ers F c 2 (X) = C1 k=1 f (X; γ c k ),</formula><p>where each base leaner is defined as f (X; γ c k ) = w c k * F k 1 (X)+b k c , and the parameter γ c k indicates the corresponding kernel weights and bias in both the first layer (i.e., weights and bias of F k 1 (X))and the second layer (i.e., of the CNN-A network is then equivalent to online updating each base learner and sequentially sample an optimal set of base learners into the ensemble. Since our proposed online training method is conducted independently in each channel of the output feature map, in the following discussion, we will take one output channel as an example to describe the training method. For notational simplicity, we omit the superscript channel number and use {γ k |k = 1, 2, . . . , C 1 } to denote the parameters of the base learners for any one output feature map channel. At the beginning of the online training process, the parameter γ k of each base learner is randomly initialized and independently trained using the first training sample by stochastic gradient descent (SGD). The parameter γ * with the smallest training error is selected as the single optimal parameter and added to the ensemble set E, while the rest C 1 − 1 parameters constitute the candidate set C.</p><p>In the following training process, the parameters in the candidate set is sequentially added to the ensemble set. All the parameters in the ensemble set are used to form an ensemble with output F (X; E) = 1 |E| γi∈E f (X; γ i ) for online testing.</p><p>At the t-th time step, a new training sample X t with target output Y t is obtained. The parameters in the ensemble set E is jointly updated by SGD using the loss function L E = L(Y t , F (X t ; E)). Meanwhile, each parameter γ j ∈ C is updated independently by SGD using the following loss function</p><formula xml:id="formula_9">L C (Y t , f (X t ; γ j )) = L(Y t , f (X t ; γ j )+ηF (X t ; E)) (8)</formula><p>where F (X t ; E) is fixed and the parameter η is used to balance the impact of the ensemble on the candidate base learners, such that the update of the base learner parameter γ j ∈ C considers both the target output Y t and the output of the ensemble F (X t ; E). If the training error L E is higher than a predefined threshold and the candidate set C is not empty, a new base learner parameter is sampled from the candidate set C according to the following sampling probability density</p><formula xml:id="formula_10">p(γ) = q(L C (Y t , f (X t ; γ))), γ ∈ C<label>(9)</label></formula><p>where q(·) is a monotonically decreasing function of its argument. And the sampled parameter is removed from the candidate set C and added into the ensemble set E. The above online training approach is conducted sequentially in each time step as illustrated in <ref type="figure" target="#fig_1">Figure 2 (b)</ref>. When all the base learner parameters are sampled from the candidate set to the ensemble set, the ensemble F (X; E) evolves into a complete CNN model <ref type="figure" target="#fig_1">(Figure 2 (b)</ref>, t = T ). The parameters of this CNN model are trained using different loss criterions and thus demonstrate a moderate diversity, which is empirically shown in our experiments to improve performance and reduce over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convolutional with Mask Layer</head><p>Dropout <ref type="bibr" target="#b12">[13]</ref> is commonly used to regularize the deep neural networks by randomly setting a subset of activations in a fully connected layer into zero. However, this regularization is not suitable for convolutional layers. As an alternative, SpatialDropout is proposed in <ref type="bibr" target="#b29">[30]</ref> to improve generalization performance for convolutional layers, which sets all the values across the randomly selected channels of the feature map into zeros. This regularization is effective for offline training. However, we find in our initial experiments that randomly "dropping-out" all the activations in a subset of feature map channels sometimes leads to divergence when training the CNN online with limited amount of training samples.</p><p>Instead, we propose a convolutional with mask layer, which aims at further reducing the correlation between the learned features and preventing over-training. Specifically, each channel of the output feature map is associated with an individual binary mask which has the same spatial size with the input feature map. All the masks are initialized in a Algorithm 1 Online tracking algorithm Input: Initial target location p 1 , pre-trained CNN-E and random initialized CNN-A. Output: Predicted target location p t . <ref type="bibr" target="#b0">1</ref> Crop region I t at last location and extract feature map X t . <ref type="bibr" target="#b5">6</ref>:</p><formula xml:id="formula_11">Predict heat mapM t = 1 |E| γi∈E f (X t ; γ i ) with confidence conf t . 7:</formula><p>Crop regionÎ t at predicted location and extract feature mapX t . <ref type="bibr">8:</ref> Predict target scale asŝ = arg max s l ∈S F S (T (X t , s l )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>if conf t &gt; θ then <ref type="bibr">10:</ref> Update base learner and SPN via <ref type="formula" target="#formula_0">(13)</ref> and <ref type="bibr" target="#b11">(12)</ref>. <ref type="bibr">11:</ref> if L E &gt; ε and C = ∅ then <ref type="bibr">12:</ref> Sampleγ * from C via <ref type="bibr" target="#b13">(14)</ref>. <ref type="bibr" target="#b12">13</ref>: random manner and then fixed throughout the online training process. The forward propagation of the convolutional layer at the training stage is then conducted as</p><formula xml:id="formula_12">E ← E {γ * }, C ← C/{γ * }.</formula><formula xml:id="formula_13">F c = K k=1 w c k * (M c ⊙ X k ) + b c ,<label>(10)</label></formula><p>where X k indicates the k-th channel of the input feature map; M c denotes the binary mask associated with the cth channel of the output feature map F c ; and ⊙ is the Hadamard product. Accordingly, the backward propagation is also conducted by considering the binary masks. Trained in this way, the learned convolution kernels are enforced to focus on different part of the input feature maps through the binary masks. For inference, the convolution is conducted in a conventional way without mask, such that the learned kernels can search for certain input pattern throughout the whole input feature map. The initialization manner of the binary masks can be customized for the particular problem at hand. In our method, we divide each mask into a grid of 2 × 2 blocks. All the values within each block are initialized by one random variable which is drawn from a Bernoulli distribution. The case where all the four blocks of the mask are set to zeros are deliberately avoided by re-initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Tracking Algorithm</head><p>Overview. The overall tracking procedure is presented in Algorithm 1. The feature extraction network CNN-E consists of the first ten convolutional layers of the 16layer VGG network <ref type="bibr" target="#b28">[29]</ref> trained on Imagenet Classification task <ref type="bibr" target="#b4">[5]</ref>, which takes an RGB image as input and outputs a feature map X of 512 channels. The first layer of the adaptation network CNN-A employs the proposed convolution with mask layer with convolution kernels of 5 × 5 spatial size and generates a feature map of 100 channels (corresponding to 100 base learners). The second layer of CNN-A is a convolution layer with kernel size 3 × 3 and produces a feature map of one channel for target localization. To handle scale variation, a scale prediction network SPN is further built on top of the pre-trained CNN-E. The SPN network takes the output feature map X of CNN-E as input and first applies a set of predefined scale transformations S = {s l |l = 1, 2, . . . , n s } to obtain the corresponding scale-transformed feature maps {T (X, s l )|} ns 1 , where s l denotes the parameter (scale factor) for the l-th scale transformation. Then all the transformed feature maps are passed through a fully connected layer which predicts an optimal scale s * for the current target.</p><p>Initialization. Given the ground truth target location in the first frame, we crop a rectangle image region I 1 centered at the target location with twice the size of the target bounding box. The corresponding feature map X 1 is extracted by CNN-E. As described in Section 3.2, the base learners {f (X 1 ; γ k )} 100 1 are initialized independently to predict the target score map M 1 using the Euclidean loss</p><formula xml:id="formula_14">L(M 1 , f (X 1 ; γ k )) = M 1 − f (X 1 ; γ k ) 2 2 ,<label>(11)</label></formula><p>where M 1 is a Gaussian distribution centered at the ground truth target location with a small scale. The optimal parameter γ * ∈ {γ k } 100 1 for the base learner with the smallest training error is used to initialize the ensemble set E, whereas the rest constitute the candidate set C. Meanwhile, the SPN network is trained to predict the current scale of the target using a hinge loss L S = max 0, 1 + max s l =s * ,s l ∈S F S (T (X, s l )) − F S (T (X, s * )) +R S <ref type="bibr" target="#b11">(12)</ref> where s * indicates the ground truth scale of the target; F S is the score predicted by SPN and R S denotes weight decay.</p><p>Online Tracking. In the t-th frame, a rectangle image region I t centered at the last location is cropped from the input image and passed through the CNN-E network to obtain X t . The ensemble of base learners take the corresponding feature map X t as input and predict a heat map asM t = 1 |E| γi∈E f (X t ; γ i ). The center location of the target is then determined by the location on the heat map with the maximum value. The maximum heat map value then serves as the confidence conf t of this prediction. To predict the current scale, we crop another image regionÎ t which is centered at the predicted target location and has   Online Update. To avoid updating using contaminated training samples, online update is conducted only if the confidence of the location prediction is higher than a predefined threshold θ. The base learners in the ensemble set E and candidate set C are updated respectively with the following loss function using SGD</p><formula xml:id="formula_15">LE (Mt, E) = Mt − 1 |E| γ i ∈E f (Xt; γi) 2 2 , LC(Mt, γj ∈ C) = Mt − f (Xt; γj) − η 1 |E| γ i ∈E f (Xt; γi) 2 2 ,<label>(13)</label></formula><p>where γ i ∈ E is fixed when updating γ j ∈ C; M t denotes the heat map of Gaussian distribution centered at the estimated location. If the current training error L E of the ensemble is higher than a threshold ε and the candidate set C is not empty, we sample without replacement one base learner parameter from the candidate set C and add it to the ensemble set E. In our experiment, we adopt the following sampling probability density</p><formula xml:id="formula_16">p(γ j ) = δ(γ * − γ j ), γ j ∈ C,<label>(14)</label></formula><p>where δ(·) is the Dirac delta function;γ * ∈ C denotes the optimal parameter with the smallest training error L C . Meanwhile, the SPN network is also updated via (12) using sampleX t and the predicted scaleŝ as ground truth, where the training sampleX t is resized into a fixed spatial size (256 × 256 in our experiments) to fit the input size of SPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Implementation Details The proposed tracker is implemented in MATLAB with Caffe framework <ref type="bibr" target="#b15">[16]</ref>, and runs at 2.5 fps on a PC with a 3.4GHz CPU and a TITAN GPU. The source code is publicly available <ref type="bibr" target="#b0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation on OTB Data Set</head><p>Data Set and Evaluation Settings. The OTB data set <ref type="bibr" target="#b36">[37]</ref> includes 50 sequences tagged with 11 attributes which covers various challenging factors. We evaluate the proposed STCT tracker against 29 state-of-the-art trackers 2 and three recently proposed trackers: MEEM <ref type="bibr" target="#b39">[40]</ref>, TGPR <ref type="bibr" target="#b7">[8]</ref> and KCF <ref type="bibr" target="#b11">[12]</ref>. Three different experiments are conducted as in <ref type="bibr" target="#b36">[37]</ref>, including one pass evaluation (OPE), temporal robustness evaluation (TRE) and spatial robustness evaluation (SRE). Among others, TRE randomizes the starting frame of the evaluation, and SRE randomizes the initial bounding boxes by perturbation. As additional evaluations to OPE, TRE and SRE can better demonstrate the robustness of the We use the precision plot and the success plot to evaluate all the trackers. The precision plot demonstrates the percentage of frames where the distance between the predicted target location and the ground truth location is within a given threshold. Whereas the success plot illustrates the percentage of frames where the overlap ratio between the predicted bounding box and the ground truth bounding box is higher than a threshold τ ∈ [0, 1]. The area under curve (AUC) is used to rank the tracking algorithms in each plot.</p><p>Evaluation Results. <ref type="figure" target="#fig_4">Figure 3</ref> demonstrates the average precision plots and success plots on all the 50 sequences of the top nine trackers, including MEEM <ref type="bibr" target="#b39">[40]</ref>, TGPR <ref type="bibr" target="#b7">[8]</ref>,  KCF <ref type="bibr" target="#b11">[12]</ref>, SCM <ref type="bibr" target="#b41">[42]</ref>, Struck <ref type="bibr" target="#b10">[11]</ref>, TLD <ref type="bibr" target="#b16">[17]</ref>, ASLA <ref type="bibr" target="#b14">[15]</ref>, CXT <ref type="bibr" target="#b5">[6]</ref>, and the proposed STCT tracker. Our method achieves the highest performance in terms of both evaluation metrics and outperforms the second best tracker (MEEM) with a considerable margin. Note that, among all three experiments, SRE is most challenging since the trackers are initialized with inaccurate target locations. Though all the evaluated trackers achieve lower performance in SRE, our method can still compare favorably against the other trackers, which demonstrates the robustness of our method. Qualitative tracking results on some challenging sequences are shown in <ref type="figure">Figure 4</ref>.</p><p>To facilitate more detailed analysis, we further report the performance (success scores) of the top 3 trackers on different attributes in <ref type="figure">Figure 5</ref>. Our method can well handle various challenging factors and consistently outperform the other two trackers in almost all the attributes.</p><p>To gain more insights on the effectiveness of the proposed sequential CNN training method, we also compare the proposed STCT tracker with two baseline methods: STCT-um and OTCT, where STCT-um denotes a variant of the proposed method that does not exploit the proposed con- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation on VOT2014 Data Set</head><p>Data Set and Evaluation Settings. The VOT2014 data set <ref type="bibr" target="#b17">[18]</ref> contains 25 video sequences with various real-life visual phenomena. According to the evaluation protocol, the evaluated tracker is re-initialized whenever a tracking failure (overlap between estimated and ground truth target bounding box equals zero) is detected. Following <ref type="bibr" target="#b17">[18]</ref>, we conduct two experiments: a baseline evaluation, where trackers are initialized with ground truth target location; a noise evaluation, where the initial target location is perturbed with random noises. Two metrics are used to rank all the trackers: accuracy and robustness, which measure the overlap ratio with ground truth bounding box and the probability of tracking failure, respectively. We evaluate the proposed STCT tracker against all the trackers submitted to VOT2014 challenge <ref type="bibr" target="#b17">[18]</ref>. Readers are referred to <ref type="bibr" target="#b17">[18]</ref> for more details about the compared trackers.</p><p>Evaluation Results. Due to limited space, we only present the average accuracy and robustness rank of top ten compared trackers in <ref type="table" target="#tab_2">Table 1</ref>. In both the baseline and region noise experiments, the proposed STCT tracker achieves the highest accuracy score with a relatively small number of tracking failure. Note that the DSST tracker <ref type="bibr" target="#b3">[4]</ref> also explicitly estimates the scale of the target using correlation filters learned from HOG features. In contrast, our method proposes to transfer pre-trained deep features for online tracking, which enables more accurate and robust tracking. trackers in the VOT2014 data set. Our method located at the upper-right corner achieves more favorable performance in terms of both accuracy and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we present a sequential training method for CNNs to effectively transfer pre-trained deep features for online applications. Different from prior approaches, our method regard the online training process for CNNs as sequentially learning an optimal ensemble of base learners, such that the learned features are not highly correlated with each other. A convolution with mask layer is proposed to further reduce over-fitting. Applied to visual tracking, the proposed training method significantly improves tracking performance and compares favorably against stat-of-the-art methods in two challenging tracking benchmark data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The learning problem in (1) then amounts to choosing a good ensemble of evaluation points {γ m } M 1 and their corresponding coefficients {a m } M 0 . For a base learner f (x; γ), its irrelevance to the current problem is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>w c k and b k c ) of CNN-A. The online training Illustration of the proposed sequential training method for CNNs. (a) Conventional method for training a two-layer CNN online to transfer pre-trained deep features. (b) The proposed method trains the CNN model via sequentially sampling optimal base learners into an ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>16:  until end of video sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Average success plots and precision plots of nine leading methods in OBT data set for OPE, TRE and SRE evaluations. Trackers are ranked according to the Area Under Curve (AUC) scores. twice the size of the target bounding box in the last frame. The current scale of the target is predicted by the SPN network asŝ = arg max s l ∈S F S (T (X t , s l )), whereX t denotes the feature map extracted by CNN-E from regionÎ t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. Both the CNN-A and the SPN networks are trained online using SGD with learning rates of 5e−7 and 1e−10, respectively. It takes 50 iterations to initialize in the first frame and 2 iterations for the following each updating step. We use n s = 11 scale parameters for scale transformation with a coverage of [0.82, 1.21] of the original scale. The threshold θ and ε for online update are set to 0.2 and 0.4, respectively. The parameter η in(13)is set to 0.2. A Bernoulli distribution B(0.5) is employed to randomly generate binary masks (Section 3.3). We fix all the parameters throughout the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Qualitative results of the proposed STCT tracker on a subset of challenging sequences: Singer1, Skating1, Car4, CarScale, Couple, Dog1, Doll, Freeman3, Freeman1, Soccer, Jogging-2, ,Matrix, MotorRolling, Walking2, Suv and Liquor. Average AUC scores of the success plots of the four leading trackers under different attributes of test sequences in OPE, including: illumination variation (IV), out-of-plane rotation (OPR), scale variation (SV), occlusion (OCC), deformation (DEF), motion blur (MB), fast motion (FM), in-plane rotation (IPR), out-ofview (OV), background cluttered (BC) and low resolution (LR). evaluated trackers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Average success plot and precision plot of the proposed tracker STCT against baseline methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7 shows the accuracy-robustness plots of the top 16 The robustness-accuracy ranking plots of 16 leading tracking methods under baseline and region noise experiments in VOT2014 data set. The better trackers are located at the upperright corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc>The average ranks of accuracy and robustness under baseline and region noise experiments in VOT2014. The first, second and third best methods are highlighted in red, blue and green colors, respectively Acc. Rank Rob. Rank Average Acc. Rank Rob. Rank Average volution with mask layer, and OTCT represents the tracking method that trains the CNN-A (Section 4) network using conventional training method (Training the whole network jointly by SGD). The success plot and precision plot inFigure 6demonstrate that the proposed sequential CNN training method can significantly improve the tracking performance and that the proposed convolution with mask layer can further reduce over-training.</figDesc><table>Trackers 
baseline 
region noise 
Overall Rank 
STCT 
6.26 
6.34 
6.30 
6.22 
5.87 
6.05 
6.17 
PLT 14 
7.50 
5.38 
6.44 
7.64 
4.81 
6.23 
6.33 
DGT 
7.02 
6.42 
6.72 
6.42 
6.90 
6.66 
6.69 
DSST 
6.86 
8.28 
7.57 
6.72 
8.29 
7.51 
7.54 
SAMF 
6.58 
7.67 
8.76 
6.82 
8.43 
7.63 
7.65 
KCF 
6.46 
8.98 
7.72 
7.22 
8.88 
8.05 
7.89 
eASMS 
8.34 
7.98 
8.16 
7.86 
7.83 
7.85 
8.00 
MCT 
8.64 
8.36 
8.50 
9.00 
8.42 
8.71 
8.61 
MatFlow 
10.20 
7.12 
8.66 
9.98 
9.15 
9.57 
9.11 
VTDMG 
9.42 
9.52 
8.47 
9.22 
8.70 
8.96 
9.21 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://ice.dlut.edu.cn/lu/index.html.<ref type="bibr" target="#b1">2</ref> We use the results of the 29 trackers reported in<ref type="bibr" target="#b36">[37]</ref> for fair comparison.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is supported by the Natural Sci- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust object tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1619" to="1632" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eigentracking: Robust matching and tracking of articulated objects using a view-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="84" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task recurrent neural network for immediacy prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context tracker: Exploring supporters and distracters in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Importance sampled learning ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page">94305</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transfer learning based visual tracking with gaussian processes regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised on-line boosting for robust tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive structural local sparse appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tracking-learningdetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2014 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojíř</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukežič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dimitriev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tracking by sampling trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust online visual tracking with a single convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust visual tracking using l1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning mutual visibility relationship for pedestrian detection with a deep model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual tracking via probability continuous outlier model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust visual tracking via least soft-threshold squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online object tracking with sparse prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="314" to="325" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust superpixel tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1639" to="1651" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meem: Robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sparse hashing tracking</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust object tracking via sparse collaborative appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2356" to="2368" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
