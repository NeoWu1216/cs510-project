<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Image Networks for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">QUVA Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">The Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Image Networks for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the concept of dynamic image, a novel compact representation of videos useful for video analysis especially when convolutional neural networks (CNNs) are used. The dynamic image is based on the rank pooling concept and is obtained through the parameters of a ranking machine that encodes the temporal evolution of the frames of the video. Dynamic images are obtained by directly applying rank pooling on the raw image pixels of a video producing a single RGB image per video. This idea is simple but powerful as it enables the use of existing CNN models directly on video data with fine-tuning. We present an efficient and effective approximate rank pooling operator, speeding it up orders of magnitude compared to rank pooling. Our new approximate rank pooling CNN layer allows us to generalize dynamic images to dynamic feature maps and we demonstrate the power of our new representations on standard benchmarks in action recognition achieving state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Videos comprise a large majority of the visual data in existence, surpassing by a wide margin still images. Therefore understanding the content of videos accurately and on a large scale is of paramount importance. The advent of modern learnable representations such as deep convolutional neural networks (CNNs) has improved dramatically the performance in many image understanding tasks. Since videos are composed of a sequence of still images, some of these improvements have been shown to transfer to videos directly. However, it remains unclear how videos could be optimally represented. For example, one can look at a video as a sequence of still images, perhaps enjoying some form of temporal smoothness, or as a subspace of images or image features, or as the output of a neural network encoder. Which one among these and other possibilities results in the best representation of videos is not well understood. * Equal contribution <ref type="bibr" target="#b0">1</ref> From left to right and top to bottom: "blowing hair dry", "band marching", "balancing on beam", "golf swing", "fencing", "playing the cello", In this paper, we explore a new powerful and yet simple representation of videos in the context of deep learning. As a representative goal we consider the standard problem of recognizing human actions in short video sequences. Recent works such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref> pointed out that long term dynamics and temporal patterns are a very important cues for the recognition of actions. However, representing complex long term dynamics is difficult, particularly if one seeks compact representations that can be processed efficiently. Several efficient representations of long term dynamics have been obtained by temporal pooling of image features in a video. Temporal pooling has been done using temporal templates <ref type="bibr" target="#b0">[1]</ref> or using ranking functions for video frames <ref type="bibr" target="#b5">[6]</ref> or subvideos <ref type="bibr" target="#b8">[9]</ref> or by more traditional pooling operators <ref type="bibr" target="#b22">[23]</ref>.</p><p>In this paper we propose a new long-term pooling operator which is simple, efficient, compact, and very powerful in a neural network context. Since a CNN provides a whole hierarchy of image representations, one for each intermediate layer, the first question is where temporal pooling should take place. For example, one could use a method such as "horse racing", "doing push-ups", "drumming". rank pooling <ref type="bibr" target="#b5">[6]</ref> to pool the output of the fully-connected layers of a standard CNN architecture pre-trained on still images and applied to individual frames. A downside of this solution is that the CNN itself is unaware of the lower level dynamics of the video. Alternatively, one can model the dynamics of the response of some intermediate network layer. In this case, the lower layers are still computed from individual frames, but the upper layers can reason about the overall dynamics in the video. An extreme version of this idea is to capture the video dynamics directly at the level of the image pixels, considering those as the first layer of the architecture.</p><p>Here we build on the latter intuition and build dynamics directly at the level of the input image. To this end, our first contribution (section 2) is to introduce the notion of a dynamic image, i.e. a RGB still image that summarizes, in a compressed format, the gist of a whole video (sub)sequence ( <ref type="figure" target="#fig_0">fig. 1</ref>). The dynamic image is obtained as a ranking classifier that, similarly to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, sorts video frames temporally; the difference is that we compute this classifier directly at the level of the image pixels instead of using an intermediate feature representation.</p><p>There are three keys advantages to this idea. First, the new RGB image can be processed by a CNN architecture which is structurally nearly identical to architectures used for still images, while still capturing the long-term dynamics in a video that relate to the long term dynamics therein. It is then possible to use a standard CNN architecture to learn suitable "dynamic" features from the videos. The second advantage of this method is its remarkable efficiency: the extraction of the dynamic image is extremely simple and efficient and it allows to reduce video classification to classification of a single image by a standard CNN architecture. The third advantage is the compression factor, as a whole video is summarized by an amount of data equivalent to a single frame.</p><p>Our second contribution is to provide a fast approximation of the ranker in the construction of the dynamic image. We replace learning the ranker by simple linear operations over the images, which is extremely efficient. We also show that, in this manner, it is possible to apply the concept of dynamic image to the intermediate layers of a CNN representation by constructing an efficient rank pooling layer. This layer can be incorporated into end-to-end training of a CNN for video data.</p><p>Our third contribution is to use these ideas to propose a novel static/dynamic neural network architecture (section 3) which can perform end-to-end training from videos combining both static appearance information from still frames, as well as short and long term dynamics from the whole video. We show that these technique result in efficient and accurate classification of actions in videos, outperforming the state-of-the-art in standard benchmarks in the area (sec-tion 4). Our findings are summarized in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Existing video representations can be roughly broken into two categories. The first category, which comprises the majority of the literature on video processing, action and event classification, be it with shallow <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref> or deep architectures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>, has viewed videos either as a stream of still images <ref type="bibr" target="#b20">[21]</ref> or as a short and smooth transition between similar frames <ref type="bibr" target="#b23">[24]</ref>. Although obviously suboptimal, considering the video as bag of static frames performs reasonably well <ref type="bibr" target="#b20">[21]</ref>, as the surroundings of an action strongly correlate with the action itself (e.g., "playing basketball" takes place usually in a basketball court). The second category extends CNNs to a third, temporal dimension <ref type="bibr" target="#b11">[12]</ref> replacing 2D filters with 3D ones. So far, this approach has produced little benefits, probably due to the lack of annotated video data. Increasing the amount of annotated videos would probably help as shown by recent 3D convolution methods <ref type="bibr" target="#b28">[29]</ref>, although what seems especially important is spatial consistency between frames. More specifically, a pixel to pixel registration <ref type="bibr" target="#b23">[24]</ref> based on the video's optical flow brings considerable improvements. Similarly, <ref type="bibr" target="#b7">[8]</ref> uses action tubes to to fit a double stream appearance and motion based neural network that captures the movement of the actor.</p><p>We can also distinguish two architectural choices in the construction of video CNNs. The first choice is to provide as input to the CNN a sub-video of fixed length, packing a short sequence of video frames into an array of images. The advantage of this technique is that they allow using simple modifications of standard CNN architectures (e.g. <ref type="bibr" target="#b14">[15]</ref>) by swapping 3D filters for the 2D ones. Examples of such techniques include <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b23">[24]</ref>.</p><p>Although the aforementioned methods successfully capture the local changes within a small time window, they cannot capture longer-term motion patterns associated with certain actions. An alternative solution is to consider a second family of architectures based on recurrent neural networks (RNNs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>. RNNs typically consider memory cells <ref type="bibr" target="#b10">[11]</ref>, which are sensitive to both short as well as longer term patterns. RNNs parse the video frames sequentially and encode the frame-level information in their memory. In <ref type="bibr" target="#b3">[4]</ref> LSTMs are used together with convolutional neural network activations to either output an action label or a video description. In <ref type="bibr" target="#b27">[28]</ref> an autoencoder-like LSTM architecture is proposed such that either the current frame or the next frame is accurately reconstructed. Finally, the authors of <ref type="bibr" target="#b32">[33]</ref> propose an LSTM with a temporal attention model for densely labelling video frames.</p><p>Many of the ideas in video CNNs originated in earlier architectures that used hand-crafted features. For example, the authors of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> have shown that local mo-tion patterns in short frame sequences can capture very well the short temporal structures in actions. The rank pooling idea, on which our dynamic images are based, was proposed in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> using hand-crafted representation of the frames, while in <ref type="bibr" target="#b4">[5]</ref> authors increase the capacity of rank pooling using a hierarchical approach.</p><p>Our static/dynamic CNN uses a multi-stream architecture. Multiple streams have been used in a variety of different contexts. Examples include Siamese architectures for learning metrics for face identification <ref type="bibr" target="#b1">[2]</ref> of for unsupervised training of CNNs <ref type="bibr" target="#b2">[3]</ref>. Simonyan et al. <ref type="bibr" target="#b23">[24]</ref> use two streams to encode respectively static frames and optical flow frames in action recognition. The authors of <ref type="bibr" target="#b18">[19]</ref> propose a dual loss neural network was proposed, where coarse and fine outputs are jointly optimized. A difference of our model compared to these is that we branch off two streams at arbitrary location in the network, either at the input, at the level of the convolutional layers, or at the level of the fully-connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dynamic Images</head><p>In this section we introduce the concept of dynamic image, which is a standard RGB image that summarizes the appearance and dynamics of a whole video sequence (section 2.1). Then, we show how dynamic images can be used to train dynamic-aware CNNs for action recognition in videos (section 2.2). Finally, we propose a fast approximation to accelerate the computation of dynamic images (section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Constructing dynamic images</head><p>While CNNs can learn automatically powerful data representations, they can only operate within the confines of a specific hand-crafted architecture. In designing a CNN for video data, in particular, it is necessary to think of how the video information should be presented to the CNN. As discussed in section 1.1, standard solutions include encoding sub-videos of a fixed duration as multi-dimensional arrays or using recurrent architectures. Here we propose an alternative and more efficient approach in which the video content is summarized by a single still image which can then be processed by a standard CNN architecture such as AlexNet <ref type="bibr" target="#b14">[15]</ref>.</p><p>Summarizing the video content in a single still image may seem difficult. In particular, it is not clear how image pixels, which already contain appearance information in the video frames, could be overloaded to reflect dynamic information as well, and in particular the long-term dynamics that are important in action recognition.</p><p>We show here that the construction of Fernando et al. <ref type="bibr" target="#b5">[6]</ref> can be used to obtain exactly such an image. The idea of their paper is to represent a video as a ranking function for its frames I 1 , . . . , I T . In more detail, let ψ(I t ) ∈ R d be a representation or feature vector extracted from each individual frame I t in the video.</p><formula xml:id="formula_0">Let V t = 1 d * = ρ(I 1 , . . . , I T ; ψ) = argmin d E(d), E(d) = λ 2 d 2 + (1) 2 T (T − 1) × q&gt;t max{0, 1 − S(q|d) + S(t|d)}.</formula><p>The first term in this objective function is the usual quadratic regularizer used in SVMs. The second term is a hinge-loss soft-counting how many pairs q &gt; t are incorrectly ranked by the scoring function. Note in particular that a pair is considered correctly ranked only if scores are separated by at least a unit margin, i.e. S(q|d) &gt; S(t|d) + 1.</p><p>Optimizing eq. (1) defines a function ρ(I 1 , . . . , I T ; ψ) that maps a sequence of T video frames to a single vector d * . Since this vector contains enough information to rank all the frames in the video, it aggregates information from all of them and can be used as a video descriptor. In the rest of the paper we refer to the process of constructing d * from a sequence of video frames as rank pooling.</p><p>In <ref type="bibr" target="#b5">[6]</ref> the map ψ(·) used in this construction is set to be the Fisher Vector coding of a number of local features (HOG, HOF, MBH, TRJ) extracted from individual video frames. Here, we propose to apply rank pooling directly to the RGB image pixels instead. While this idea is simple, in the next several sections we will show that it has remarkable advantages.</p><p>The ψ(I t ) is now an operator that stacks the RGB components of each pixel in image I t on a large vector. Alternatively, ψ(I t ) may incorporate a simple component-wise non-linearity, such as the square root function √ · (which corresponds to using the Hellinger's kernel in the SVM). In all cases, the descriptor d * is a real vector that has the same number of elements as a single video frame. Therefore, d * can be interpreted as standard RGB image. Furthermore, since this image is obtained by rank pooling the video frames, it summarizes information from the whole video sequence.</p><p>A few examples of dynamic images are shown in <ref type="figure" target="#fig_0">fig. 1</ref>. Several observations can be made. First, it is interesting to note that the dynamic images tend to focus mainly on the acting objects, such as humans or other animals such as horses in the "horse racing" action, or objects such as drums in the "drumming" action. On the contrary, background pixels and background motion patterns tend to be averaged away. Hence, the pixels in the dynamic image appear to focus on the identity and motion of the salient actors in videos, indicating that they may contain the information necessary to perform action recognition.</p><p>Second, we observe that dynamic images behave differently for actions of different speeds. For slow actions, like "blowing hair dry" in the first row of <ref type="figure" target="#fig_0">fig. 1</ref>, the motion seems to be dragged over many frames. For faster actions, such as "golf swing" in the second row of <ref type="figure" target="#fig_0">fig. 1</ref>, the dynamic image reflects key steps in the action such as preparing to swing and stopping after swinging. For longer term actions such as "horse riding" in the third row of <ref type="figure" target="#fig_0">fig. 1</ref>, the dynamic image reflects different parts of the video; for instance, the rails that appear as a secondary motion contributor are superimposed on top of the horses and the jockeys who are the main actors. Such observations were also made in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Last, it is interesting to note that dynamic images are reminiscent of some other imaging effects that convey motion and time, such as motion blur or panning, an analogy is illustrated in <ref type="figure" target="#fig_1">fig. 2</ref>. While motion blur captures the time and motion by integrating over subsequent pixel intensities defined by the camera shutter speed, dynamic images capture the time by integrating and reordering the pixel intensities over time within a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Using dynamic images</head><p>Given that the dynamic images are in the format of standard RGB images, they can be used to apply any method for still image analysis, and in particular any state-of-the-art CNN, to the analysis of video. In particular, we experiment with two usage scenarios.</p><p>Single Dynamic Image (SDI). In the first scenario, a dynamic image summarizes an entire video sequence. By training a CNN on top of such dynamic images, we implicitly capture the temporal patterns contained in the video. However, since the CNN is still applied to images, we can start from a CNN pre-trained for still image recognition, such as AlexNet pre-trained on the ImageNet ILSVRC data, and fine-tune it on a dataset of dynamic images. Fine-tuning allows the CNN to learn features that capture the video dynamics without the need to train the architecture from scratch. This is an important benefit of our method because training large CNNs require millions of data samples which may be difficult to obtain for videos.</p><p>Multiple Dynamic Images (MDI). While fine-tuning does not require as much annotated data as training a CNN from scratch, the domain gap between natural and dynamic images is sufficiently large that an adequately large fine-tuning dataset of dynamic images may be appropriate. However, as noted above, in most cases there are only a few videos available for training.</p><p>In order to address this potential limitation, in the second scenario we propose to generate multiple dynamic images from each video by breaking it into segments. In particular, for each video we extract multiple partially-overlapping segments of duration τ and with stride s. In this manner, we create multiple video segments per video, essentially multiplying the dataset size by a factor of approximately T /s, where T is the average number of frames per video. This can also be seen as a data augmentation step, where instead of mirroring, cropping, or shearing images we simply take a subset of the video frames. From each of the new video segments, we can then compute a dynamic image to train the CNN, using as ground truth class information of each subsequence the class of the original video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fast dynamic image computation</head><p>Computing a dynamic image entails solving the optimization problem of eq. (1). While this is not particularly slow with modern solvers, in this section we propose an approximation to rank pooling which is much faster and works as well in practice. Later, this technique, which we call approximate rank pooling, will be critical in incorporating rank pooling in intermediate layers of a deep CNN and to allow back-prop training through it.</p><p>The derivation of approximate rank pooling is based on the idea of considering the first step in a gradient-based optimization of eq. (1). Starting with d = 0, the first approximated solution obtained by gradient descent is d * = 0 − η∇E(d)| d= 0 ∝ −∇E(d)| d= 0 for any η &gt; 0, where (2) using time-averaged feature frames V t to the variant eq. (4) that ranks directly the feature frames ψ t as is.</p><formula xml:id="formula_1">∇E( 0) ∝ q&gt;t ∇ max{0, 1 − S(q|d) + S(t|d)}| d= 0 = q&gt;t ∇ d, V t − V q = q&gt;t V t − V q .</formula><p>We can further expand d * as follows</p><formula xml:id="formula_2">d * ∝ q&gt;t V q − V t = q&gt;t   1 q q i=1 ψ i − 1 t t j=1 ψ j   = T t=1 α t ψ t</formula><p>where the coefficients α t are given by</p><formula xml:id="formula_3">α t = 2(T − t + 1) − (T + 1)(H T − H t−1 ),<label>(2)</label></formula><p>where H t = t i=1 1/t is the t-th Harmonic number and H 0 = 0. Hence the rank pooling operator reduces tô ρ(I 1 , . . . , I T ; ψ) = T t=1 α t ψ(I t ).</p><p>(</p><p>which is a weighted combination of the data points. In particular, the dynamic image computation reduces to accumulating the video frames after pre-multiplying them by α t . The function α t , however, is non-trivial, as illustrated in <ref type="figure" target="#fig_2">fig. 3</ref>. An alternative construction of the rank pooler does not compute the intermediate average features V t = (1/t) T q=1 ψ(I q ), but uses directly individual video features ψ(I t ) in the definition of the ranking scores (1). In this case, the derivation above results in a weighting function of the type</p><formula xml:id="formula_5">α t = 2t − T − 1<label>(4)</label></formula><p>which is linear in t. The two scoring functions eq. (2) and eq. (4) are compared in <ref type="figure" target="#fig_2">fig. 3</ref> and in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dynamic Maps Networks</head><p>In the previous section we have introduced the concept of dynamic image as a method to pool the information contained in a number of video frames in a single RGB image. Here, we notice that every layer of a CNN produces as output a feature map which, having a spatial structure similar to an image, can be used in place of video frames in this construction. We call the result of applying rank pooling to such features a dynamic feature map, or dynamic map in short. In the rest of the section we explain how to incorporate this construction as a rank-pooling layer in a CNN (section 3.1) and how to accelerate it significantly and perform back-propagation by using approximate rank pooling (section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dynamic maps</head><p>The structure of a dynamic map network is illustrated in <ref type="figure" target="#fig_3">fig. 4</ref>. In the case seen so far (left in <ref type="figure" target="#fig_3">fig. 4</ref>), rank pooling is applied at the level of the input RGB video frames, which we could think of as layer zero in the architecture. We call the latter a dynamic image network. By contrast, a dynamic map network moves rank pooling higher in the hierarchy, by applying one or more layers of feature computations to the individual feature frames and applying the same construction to the resulting feature maps.</p><p>In particular, let a (l−1) 1 , . . . , a (l−1) T denote the feature maps computed at the l−1 layers of the architecture, one for each of the T video frames. Then, we use the rank pooling equation (1) to aggregate these maps into a single dynamic map,</p><formula xml:id="formula_6">a (l) = ρ(a (l−1) 1 , . . . , a (l−1) T ).<label>(5)</label></formula><p>Note that, compared to eq. (1), we dropped the term ψ; since networks are already learning feature maps, we set this term to the identity function. The dynamic image network is obtained by setting l = 1 in this construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank pooling layer (RankPool) &amp; backpropagation.</head><p>In order to train a CNN with rank pooling as an intermediate layer, it is necessary to compute the derivatives of eq. (5) for the backpropagation step. We can rewrite eq. (5) as a linear combination of the input data V 1 , . . . , V T , namely</p><formula xml:id="formula_7">a (l) = T t=1 β t (V 1 , . . . , V T )V t (6)</formula><p>In turn, V t is the temporal average of the input features and is therefore a linear function V t (a</p><formula xml:id="formula_8">(l−1) 1 , . . . , a (l−1) t )</formula><p>. Substituting, we can rewrite a (l) as</p><formula xml:id="formula_9">a (l) = T t=1 α t (a (l−1) 1 , . . . , a (l−1) T )a (l−1) t .<label>(7)</label></formula><p>Unfortunately, we observe that due to the non-linear nature of the optimization problem of equation <ref type="formula">(1)</ref>, the coefficients β t , α t depend on the data a (l−1) t themselves. Computing the gradient of a (l) with respect to the per frame data points a (l−1) t is a challenging derivation. Hence, using dynamic maps and rank pooling directly as a layer in a CNN is not straightforward.</p><p>We note that the rank pooling layer (RankPool) constitutes a new type of portable convolutional network layer, just like a max-pooling or a ReLU layer. It can be used whenever dynamic information must be pooled across time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Approximate dynamic maps.</head><p>Constructing the precise dynamic maps, or images, is in theory optimal, but not necessarily practical. On one hand computing the precise dynamic maps via an optimization is computationally inefficient. This is especially important in the context of CNNs, where efficient computations are extremely important for training on large datasets, and the optimization of eq. (5) would be slow compared to other components of the network. On the other hand, computing the gradients would be non trivial.</p><p>To this end we replace once again rank pooling with approximate rank pooling. With the approximate rank pooling we significantly accelerate the computations, even by a factor of 45 as we show later in the experiments. Secondly, and more importantly, the approximate rank pooling is also a linear combination of frames, where the per frame coefficients are given by eq. (2). These coefficients are independent of the frame features V t and ψ(I t ). Hence, the derivative of the approximate rank pooling is much simpler and can be easily computed as the vectorized coefficients of eq. (2), namely</p><formula xml:id="formula_10">∂ vec a (l) ∂(vec a (l−1) t ) ⊤ = α t I<label>(8)</label></formula><p>where I is the identity matrix. Interestingly, we would obtain the same expression for the derivative if α t in eq. (7) would be constant and did not depend on the video frames.</p><p>We conclude that using approximate rank pooling in the context of CNNs is not only practical, but also necessary for the optimization through backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We explore the proposed models on two state-of-theart datasets used for evaluating neural network based models for action recognition, namely UCF101 <ref type="bibr" target="#b26">[27]</ref> and HMDB51 <ref type="bibr" target="#b15">[16]</ref>.</p><p>UCF101. The UCF101 dataset <ref type="bibr" target="#b26">[27]</ref> comprises of 101 human action categories, like "Apply Eye Makeup" and "Rock Climbing" and spans over 13, 320 videos. The videos are realistic and relatively clean. They contain little background clutter and contain a single action. Also the videos are trimmed, thus almost all frames relate to the action in the video. The standard evaluation is average accuracy over three parts provided by the authors.</p><p>HMDB51. The HMDB51 dataset <ref type="bibr" target="#b15">[16]</ref> comprises of 51 human action categories , such as "backhand flip" and "swing baseball bat" and spans over 6, 766 videos. The videos are realistic, downloaded from Youtube contain a single action. The dataset is split in three parts and accuracy is averaged over all three parts, similar to UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>To maintain the same function domain and range we select for non-linear operations ψ(·) the square rooting kernel maps √ · and time varying mean vectors <ref type="bibr" target="#b5">[6]</ref>. We generate dynamic images for each color channel separately and then merge them so that they can be directly used directly as input to a CNN. As the initial dynamic images are not in the natural range of [0, 255] for RGB data, we apply minmax normalization. We use BVLC reference CaffeNet model <ref type="bibr" target="#b12">[13]</ref> trained on ImageNet images as a starting point to train our dynamic image networks. We fine-tune all the layers with the learning rate to be 10 − 3 and gradually decrease it per epoch. We use a maximum of 20 epoch during training. Sharing code, data, models. We share our code, models and data 2 . Furthermore, we have computed the dynamic images of the Sports1M dataset <ref type="bibr" target="#b13">[14]</ref>, and share the Alexnet and VGGnet dynamic image networks trained on the Sports1M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Mean, max and dynamic images</head><p>First, we compare "single image per video", namely the proposed Single Dynamic Image (SDI) with the per video sequence mean and max image. For all methods we first   compute the single images per video offline, and for SDI specifically we use SVR <ref type="bibr" target="#b25">[26]</ref>. Then we train and test on action recognition using CaffeNet network. Results are reported in <ref type="table" target="#tab_1">Table 1</ref>. From all representations we observe that SDI achieves the highest accuracy. We conclude that SDI model is a better single image model than the mean and max image models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Approximate Rank Pooling vs Rank Pooling</head><p>Next, we compare the approximate rank pooling and rank pooling in terms of speed (frames per second) and pairwise ranking accuracy, which is the common measure for evaluating learning-to-rank methods. We train on a subset of 10 videos that contain different actions and evaluate on a new set of 10 videos with the same type of actions respectively. We report results with the mean and the standard deviations in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>We observe that approximate rank pooling is 45× faster than rank pooling, while obtaining similar ranking performance. Further, in <ref type="figure">Figure 5</ref> we plot the score distributions for rank pooling and approximate rank pooling. We observe that their score profiles are also similar. We conclude that approximate rank pooling is a good approximation to rank pooling, while being two magnitudes faster as it involves no optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluating the effect of end-to-end training</head><p>Next, we evaluate in <ref type="table" target="#tab_4">Table 3</ref> rank pooling dynamic images with and without end-to-end training. We also evaluate rank pooling with dynamic maps. The first method generates multiple dynamic images on the RGB pixels as earlier. These dynamic images can be computed offline, then we train a network from end to end. The second method passes these dynamic images through the network, computes the fc6 activations using a pre-trained Alexnet and aggregates them with max pooling, then trains SVM classifiers per action class. The third method considers a RankPool layer after the conv1 to generate multiple dy-  <ref type="figure">Figure 5</ref>: Comparison between score profile of ranking functions for approximate rank pooling and rank pooling. Generally the approximate rank pooling follows the trend of rank pooling.  We observe that compared to a unified, end-to-end training is beneficial, bringing 2-3% accuracy improvements depending on the dataset. Furthermore, approximate dynamic maps computed after the convolutional layer 1 perform slighly below the dynamic images (dynamic maps computed after layer 0). We conclude that multiple dynamic images are better to be pooled on top of the static RGB frames. Furthermore, multiple dynamic images perform better when employed in end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Combining dynamic images with static images</head><p>Next, we evaluate how complementary dynamic image networks and static RGB frame networks are. For both networks we apply max pooling on the per video activations at pool5 layer. Results are reported in <ref type="table" target="#tab_6">Table 4</ref>.</p><p>As expected, we observe that static appearance information appears to be equally important to the dynamic appearance in the context of convolutional neural networks. A combination of the two, however, brings a noticeable 6% increase in accuracy. We conclude that the two representations are complementary to each other.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Further analysis</head><p>We, furthermore, perform a per analysis between static rgb networks and MDI based networks. We list in Table 5 the top 5 classes based on the relative performances for each method. MDI performs better for "PullUps" and "PushUps", where motion is dominant and discriminating between motion patterns is important. RGB static models seems to work better on classes such as "CricketShot" and "Drumming", where context is already quite revealing. We conclude that dynamic images are useful for actions where there exist characteristic motion patterns and dynamics.</p><p>Furthermore, we investigate whether dynamic images are complementary to state-of-the-art features, like improved trajectories <ref type="bibr" target="#b30">[31]</ref>, relying on late fusion. Results are reported in <ref type="table" target="#tab_8">Table 6</ref>. We obtain a significant improvement of 5.2% over trajectory features alone on HMDB51 dataset and 3.1% on UCF101 dataset.</p><p>Due to the lack of space we refer to the supplementary material for a more in depth analysis of dynamic images and dynamic maps and their learning behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">State-of-the-art comparisons</head><p>Last, we compare with the state-of-the-art techniques in UCF101 and HMDB51 in <ref type="table">Table 7</ref>, where we make a distinction between deep and shallow architectures. Note that similar to us, almost all methods, be it shallow or deep, ob-  <ref type="bibr" target="#b5">[6]</ref> 63.7 -Hoai et al. <ref type="bibr" target="#b9">[10]</ref> 60.8 -Lan et al. <ref type="bibr" target="#b16">[17]</ref> 65.4 89.1 Peng et al. <ref type="bibr" target="#b21">[22]</ref> 66.8 - <ref type="table">Table 7</ref>: Comparison with the state-of-the-art. Despite being a relatively simple representation, the proposed method is able to obtain results on par with the state state-of-the-art.</p><p>tain their accuracies after combining their methods with improved trajectories <ref type="bibr" target="#b30">[31]</ref> for optimal results. Considering deep learning methods, our method performs on par and is only outperformed from <ref type="bibr" target="#b33">[34]</ref>. <ref type="bibr" target="#b33">[34]</ref> makes use of the very deep VGGnet <ref type="bibr" target="#b24">[25]</ref>, which is a more competitive network than that the Alexnet architecture we rely on. Hence a direct comparison is not possible. Compared to the shallow methods the proposed method is also competitive. We anticipate that combining the proposed dynamic images with sophisticated encodings <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref> will benefit the accuracies further.</p><p>We conclude that while being in the context of CNNs a simple and efficient video representation, dynamic images allow for state-of-the-art accuracy in action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present dynamic images, a powerful and new, yet simple video representation in the context of deep learning that summarizes videos into single images. As such, dynamic images are directly compative to existing CNN architectures allowing for end-to-end action recognition learning. Extending dynamic images to the hierarchical CNN feature maps, we introduce a novel temporal pooling layer, Approximate-RankPool directly. Experiments on state-of-the-art action recognition datasets demonstrate the descriptive power of dynamic images, despite their conceptual simplicity. A visual inspection outlines the richness of dynamic images in describing complex motion patterns as simple 2d images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Dynamic images summarizing the actions and motions that happen in images in standard 2d image format. Can you guess what actions are visualized just from their dynamic image motion signature? 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left column: dynamic images. Right column: motion blur. Although fundamentally different both methodologically, as well as in terms of applications, they both seem to capture time in a similar manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The graph compares the approximated rank pooling weighting functions α t (for T = 11 samples) of eq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Dynamic image and dynamic map networks on the left and the right pictures respectively, after applying a rank pooling operation on top of the previous layer activations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparing several video representative image 
models using UCF101 

Method 
Speed 
Accuracy 
Appr. Rank Pooling 5920 fps 96.5 ± 0.9 
Rank Pooling 
131 fps 
99.5 ± 0.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Approximate rank pooling vs rank pooling.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Evaluating the effect of end-to-end training for multiple dynamic images and multiple dynamic maps after the convolutional layer 1.namic maps (MDM) based on approximate rank pooling. To generate the multiple dynamic images or maps we use a window size of 25 and a stride of 20 which allows for 80% overlap.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Evaluating complementarity of dynamic images with static images.</figDesc><table>Classes 
Diff. 
Classes 
Diff. 
SoccerJuggling +38.5 CricketShot 
-47.9 
CleanAndJerk 
+36.4 Drumming 
-25.6 
PullUps 
+32.1 PlayingPiano -22.0 
PushUps 
+26.7 PlayingFlute 
-21.4 
PizzaTossing 
+25.0 Fencing 
-18.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Class by class comparison between RGB and MDI networks, where the difference in scores using MDI and RGB are reported. A positive difference is better for MDI, a negative difference better for RGB</figDesc><table>Method 
HMDB51 UCF101 
Trajectories [31] 
60.0 
86.0 
MDI-end-to-end + static-rgb+trj 
65.2 
89.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Combining with trajectory features brings a noticeable increase in accuracy.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head></head><label></label><figDesc>Yue-Hei-Ng et al. [21] Fernando et al.</figDesc><table>Method 
HMDB51 UCF101 
This paper 
65.2 
89.1 

deep 
Zha et al. [34] 
-
89.6 
Simonyan et al. [24] 
59.4 
88.0 
-
88.6 

shallow 

Wu et al. [32] 
56.4 
84.2 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t t τ =1 ψ(I τ ) be time average of these features up to time t. The ranking function associates to each time t a score S(t|d) = d, V t , where d ∈ R d is a vector of parameters. The function parameters d are learned so that the scores reflect the rank of the frames in the video. Therefore, later times are associated with larger scores, i.e. q &gt; t =⇒ S(q|d) &gt; S(t|d). Learning d is posed as a convex optimization problem using the RankSVM<ref type="bibr" target="#b25">[26]</ref> formulation:</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/hbilen/dynamic-image-nets</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work acknowledges the support of the EPSRC grant EP/L024683/1, the ERC Starting Grant IDIU and the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The recognition of human movement using temporal templates. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative hierarchical rank pooling for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rank pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving human action recognition using score distribution and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asian Conference on Computer Vision</title>
		<meeting>Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving human action recognition using score distribution and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond gaussian pyramid: Multi-skip feature stacking for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conceptlets: Selective semantics for classifying video events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2199</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0767</idno>
		<title level="m">Learning spatiotemporal features with 3d convolutional networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards good practices for action video encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting Image-trained CNN Architectures for Unconstrained Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
