<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pairwise Decomposition of Image Sequences for Active Multi-View Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="laboratory">Dyson Robotics Laboratory at Imperial College</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="laboratory">Dyson Robotics Laboratory at Imperial College</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="laboratory">Dyson Robotics Laboratory at Imperial College</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pairwise Decomposition of Image Sequences for Active Multi-View Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A multi-view image sequence provides a much richer capacity for object recognition than from a single image. However, most existing solutions to multi-view recognition typically adopt hand-crafted, model-based geometric methods, which do not readily embrace recent trends in deep learning. We propose to bring Convolutional Neural Networks to generic multi-view recognition, by decomposing an image sequence into a set of image pairs, classifying each pair independently, and then learning an object classifier by weighting the contribution of each pair. This allows for recognition over arbitrary camera trajectories, without requiring explicit training over the potentially infinite number of camera paths and lengths. Building these pairwise relationships then naturally extends to the next-best-view problem in an active recognition framework. To achieve this, we train a second Convolutional Neural Network to map directly from an observed image to next viewpoint. Finally, we incorporate this into a trajectory optimisation task, whereby the best recognition confidence is sought for a given trajectory length. We present state-of-the-art results in both guided and unguided multi-view recognition on the ModelNet dataset, and show how our method can be used with depth images, greyscale images, or both.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider the scenario in <ref type="figure">Figure 1</ref>. What trajectory should the camera move around the object in order to achieve the highest recognition confidence in a given time? For practical tasks, recognition from a multi-view image sequence is a more realistic setting than the single-image recognition tasks typically addressed in computer vision, and controlling a camera actively for efficient recognition has great significance in real-world applications, where time or power constraints become realities. For example, a robot rotating an object before its eyes, or a mobile robot semantically mapping a room, benefit from efficient solutions.</p><p>Traditionally, multi-view object recognition has been achieved by building up compositions of hand-crafted features shared across viewpoints, and finding correspon-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Sequence to Image Pairs</head><p>Object Label Next View <ref type="figure">Figure 1</ref>: We propose a method for multi-view object recognition, by decomposing an image sequence into a set of image pairs. Training on these pairs then allows for recognition and trajectory planning, without the need to train directly over the infinite possible number of camera paths that may exist.</p><p>dences between a test image and the learned models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b27">28]</ref>. However, recent trends in Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> have seen attention in singleview object recognition move away from these explicit, hand-modelled, geometric solutions, and towards end-toend learning ideologies which inject fewer assumptions into the learned object models. Recently, the introduction of the ModelNet dataset of 3D CAD meshes <ref type="bibr" target="#b38">[39]</ref> provided data of sufficient magnitude for training deep networks with images covering the full sphere of viewpoints over an object, enabling view synthesis without the need for laborious manual labelling of each image <ref type="bibr" target="#b15">[16]</ref>. It was subsequently shown that rendering these meshes as synthetic greyscale images, and classifying objects in a view-based manner with a CNN architecture acting over a fixed trajectory, achieved stateof-the-art results for multi-view recognition <ref type="bibr" target="#b34">[35]</ref>. However, extending this to generalised recognition over trajectories of arbitrary paths and lengths is not readily adopted by traditional CNN architectures, due to the need for fixed-length input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">CNNs for Generalised Multi-View Recognition</head><p>One solution to multi-view recognition with CNNs would be to simply concatenate all observed images into a single input to a network. However, this would require intractable training due to the large size of each input, but more importantly, due to the need to train over every possible path of all possible lengths, which is of potentially infinite scale. We propose to address this by relaxing the joint model over images and decomposing an image sequence into a set of pairs, one for every pair of images across the sequence. Pairwise representations of full distributions have been popular in computer vision for learning distributions of local features <ref type="bibr" target="#b18">[19]</ref> and parts <ref type="bibr" target="#b9">[10]</ref>, and we migrate this idea from the image space domain to the temporal domain. Given this decomposition, a CNN is then trained on a fixedlength input consisting of the image pair, together with the relative pose between the associated viewpoints. To achieve classification of the full sequence, an ensemble framework is adopted, with weighting to increase the contribution of those image pairs which cover a more informative set of poses.</p><p>The problem then shifts to active recognition, with the aim of determining along which trajectory the camera should move, in order to achieve the best recognition accuracy in a given number of images. This is often presented as a Next-Best-View (NBV) prediction, where the mutual information is determined between the class probability distribution and each potential next view. However, this typically requires learning a generative model of the object and synthesising new views as an intermediate step. We propose to learn NBV prediction with a more powerful discriminative model, training a second CNN to map directly from an observed image to the rotation angle over which the camera should subsequently move.</p><p>Finally, we extend our NBV prediction to a full trajectory-optimisation framework, where we consider all possible images that can acquired along a trajectory as contributions, rather than simply following a sequence of NBV images as is often employed. To achieve this, we train a third CNN in a similar manner to the above NBV CNN, but training for regression to a recognition confidence score for all possible next viewpoints, rather then classification for the overall best viewpoint. As the image sequence evolves, all unvisited viewpoints accumulate scores based on the newly-observed images, and the optimum trajectory is chosen as the one which maximises the summation of these scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>In this paper, we present three key technical contributions all based on powerful CNN learning:</p><p>1. Multi-view object recognition over arbitrary camera trajectories by training only on image pairs, 2. Discriminatively-trained Next-Best-View prediction directly from an input image to the next viewpoint, 3. Trajectory optimisation by considering the impact of all observable images along the sequence.</p><p>All three contributions achieve state-of-the-art results in their respective benchmarks on the ModelNet dataset <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>View-Based Multi-View Recognition In its simplest form, the view-based approach aims to add viewpoint tolerance to a 2D image of an object, such as with viewpointinvariant local descriptors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> or deformation-tolerant global descriptors <ref type="bibr" target="#b5">[6]</ref>. Given training images across multiple viewpoints, a more stable set of features can be found by tracking those which are shared across multiple views and clustering images accordingly <ref type="bibr" target="#b22">[23]</ref>, or by learning their relative 2D displacements as the viewpoint changes, both with hard constraints for rigid bodies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and flexible constraints for deformable bodies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>. To add further fidelity to the true underlying object geometry, these 2D image elements can also be embedded within an implicit 3D model <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref>. If multiple views are available at testing, images can be combined and treated as a single, larger image <ref type="bibr" target="#b30">[31]</ref>, an approach which can also be addressed in two stages, by processing the individual images first to reduce the search space <ref type="bibr" target="#b4">[5]</ref>.</p><p>Recently, CNN architectures have been extended to allow for recognition from image sequences using a single network, by max pooling across all viewpoints <ref type="bibr" target="#b34">[35]</ref>, or by unwrapping an object shape into a panorama and max pooling across each row <ref type="bibr" target="#b32">[33]</ref>. However, both these methods assume that a fixed-length image sequence is provided during both training and testing, and hence are unsuitable for generalised multi-view recognition.</p><p>Shape-Based Multi-View Recognition Rather than modelling an object as a set of views with 2D features, an explicit 3D shape can be learned from reconstruction <ref type="bibr" target="#b36">[37]</ref> or provided by CAD models <ref type="bibr" target="#b38">[39]</ref>, and subsequently matched to from depth images <ref type="bibr" target="#b12">[13]</ref>, 3D reconstructions <ref type="bibr" target="#b0">[1]</ref>, or partial reconstructions with shape completion <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39]</ref>. Shape descriptors include distributions of local surface properties <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>, spherical harmonic functions over voxel grids <ref type="bibr" target="#b23">[24]</ref>, and 3D local invariant features <ref type="bibr" target="#b24">[25]</ref>. Recently, CNNs have been applied to 3D shapes by representing them as 3D occupancy grids, and building generative <ref type="bibr" target="#b38">[39]</ref> or discriminative <ref type="bibr" target="#b25">[26]</ref> networks.</p><p>As of now however, CNNs with 2D view-based methods <ref type="bibr" target="#b34">[35]</ref> have outperformed their counterpart 3D voxel-based methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b25">26]</ref>, and we therefore adopt the 2D approach in our work. However, it is not yet clear whether this greater performance arises from the superior abundance of 2D image data for pre-training deep networks, or the naturally more efficient representation of 2D than 3D in standard CNN architectures.</p><p>Active Recognition Methods for active recognition typically learn a generative model of the object, predict the object appearance from unvisited viewpoints, and select views based on a measure of entropy reduction. <ref type="bibr" target="#b37">[38]</ref> modelled objects as a 3D cloud of SIFT features, moving the camera to the view which would reveal the greatest number of features which have not yet been observed. A similar method was proposed in <ref type="bibr" target="#b1">[2]</ref> for guided mapping and robot navigation. The incorporation of active recognition into a Random Forests framework was presented in <ref type="bibr" target="#b7">[8]</ref>, whereby each decision tree encodes both object classification and viewpoint selection. Recently, the ShapeNets framework of <ref type="bibr" target="#b38">[39]</ref> proposed to model objects as a voxel grid, and learn a generative model based on Convolutional Deep Belief Networks to allow for view synthesis from unseen viewpoints.</p><p>However, these methods do not take into account the images acquired along a sequence towards the chosen next view. In <ref type="bibr" target="#b19">[20]</ref>, this was incorporated during active object reconstruction by visiting a sequence of actively-selected views, but reconstructing the object based on the entire image sequence that is observed between the views. For recognition, Partially Observable Markov Decision Processes (POMDPs) <ref type="bibr" target="#b8">[9]</ref> have seen success in optimising a trajectory for a particular task, although these require generative modelling rather than direct discriminative learning as we propose in our method. Finally, recurrent CNNs have recently been shown to be effective for active recognition from image sequences <ref type="bibr" target="#b14">[15]</ref>, and we believe that this approach has exciting future potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-View Object Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We train and test our proposed methods on the Model-Net dataset of 3D CAD meshes <ref type="bibr" target="#b38">[39]</ref>, which provides multiview training data of sufficient scale for training deep networks. As in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b34">35]</ref>, we discretise viewpoints into distinct steps, but whereas in these works rotations are constrained to being around the gravity vector, we relax this and allow rotations around the object's full viewing sphere to enable recognition from arbitrary camera trajectories. The camera pose is defined in spherical coordinates {r, θ, φ}, where r is fixed and θ and φ are divided into 30 • steps, and the camera is pointed towards the object's centroid. Camera paths then visit viewpoints along the viewing sphere with θ and φ either decreasing or increasing by one step, or remaining the same, between viewpoints. For every viewpoint, we render a greyscale image of the object object as with <ref type="bibr" target="#b34">[35]</ref>, together with a depth image for dual-modality imaging.</p><p>For comparisons with <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b34">35]</ref>, we also assume each object to be aligned in its canonical orientation as defined in the ModelNet dataset, although augmenting the training data by rotating models as necessary would allow for relaxation of this prior assumption. As with these works, we also assume the pose of the camera to be known with respect to the object's viewing sphere, whereas in practice this would be achieved by visual tracking or reconstruction, or by use of robot kinematics or other external sensors. Training and testing models are provided as CAD meshes and free from occlusion or clutter, although in practice a detection and segmentation task would precede our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pairwise Learning</head><p>Our proposed multi-view object recognition method requires computing the probability over class labels given a sequence of M views. To allow for flexibility of camera trajectories over all possible paths and lengths, we decompose a sequence into a set of N = M (M −1) 2 view pairs, denoted w 1 ...w N . Here, every new view acquired along a sequence forms a new view pair with all existing views in the sequence, and the task then becomes to compute a recognition score over all classes, f (y|w 1 ...w N ).</p><p>The data for each view pair w i is composed of three elements: the image x 1 i from the first view, the image x 2 i from the second view, and the relative camera pose ψ i between the two views, such that</p><formula xml:id="formula_0">w i = {x 1 i , x 2 i , ψ i }.</formula><p>For object recognition from a view sequence, we classify each view pair independently, and then weight the contribution from each, discussed further in Section 3.3. In this way, each view pair w i is processed with a weak classifier, with an associated weight λ i , and a strong classifier then computes the weighted average of these for a final distribution of scores over class labels:</p><formula xml:id="formula_1">f (y|w 1 ...w N ) = i=N i=1 λ i p(y|w i ) .<label>(1)</label></formula><p>To compute the class probability distribution p(y|w i ) for each view pair, we designed a CNN architecture, denoted CNN-1 (see <ref type="figure" target="#fig_0">Figure 2</ref>), to predict an object class based on the provided view pair. This architecture was inspired by the Siamese CNN <ref type="bibr" target="#b3">[4]</ref>, which consists of two CNN's running in parallel, each taking in one image from the pair, and with weights shared across both networks. Whilst this architecture is typically used to enforce similarity or dissimilarity between the outputs of the two networks, we use it to reduce the number of parameters to be learned, rather than concatenating the two images into a single input vector and training a larger network. Not only does this allow for efficient training, but also a fair comparison with the stateof-the-art <ref type="bibr" target="#b34">[35]</ref> because we are not adding additional capacity to the network, as this has been shown to dramatically improve classification performance <ref type="bibr" target="#b33">[34]</ref>.</p><p>For the convolutional layers of CNN-1, we follow <ref type="bibr" target="#b34">[35]</ref> and adopt the VGG-M network <ref type="bibr" target="#b2">[3]</ref> with five convolutional layers and three fully-connected layers. The final convolutional layers from the two images are concatenated, together with a vector using one-hot encoding to represent the relative camera pose between the two views. Depending on the available imaging modality, the framework can be used with greyscale or depth images, or both. When both are used, we also concatenate the outputs of the convolutional layers for both modalities. Finally, three fully-connected layers are added after this concatenation, with classification loss computed using softmax and cross-entropy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning the weights</head><p>Together with the two images, CNN-1 receives the relative pose of the view pair, and we use this to condition the classifier on how confident its output is likely to be. For example, a pair of images captured from two viewpoints at opposite ends of the viewing sphere may be more likely to be classified correctly than images from two adjacent viewpoints, because the former observes a greater coverage of the object and hence reveals more informative data upon which to make a classification decision.</p><p>We use cross entropy to measure the classification confi-dence for each relative pose, which computes the similarity between the ground truth distribution and the network's output distribution, a richer indicator than simply the classification error. For each relative pose ψ j in the discretised viewing sphere, the weight λ j is learned by averaging the cross entropy over all training image pairs whose viewpoints are separated by ψ j . Then, all viewpoints in the sequence are weighted accordingly in Equation 1 to give greater importance to those view pairs which are likely to be classified correctly. Although the output class distribution of each pair already implicitly exhibits a measure of confidence based on the entropy, this additional weighting acts as a regularisation by injecting prior knowledge of how easily different viewpoint pairings can be separated, independently of the image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Active Object Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Next-Best-View Prediction</head><p>Given one view of an object, predicting the next view to move the camera to enables an active approach to recognition, by maximising the classification accuracy over a given number of views. Previous works <ref type="bibr" target="#b38">[39]</ref> typically address this by building a generative model of the object, predicting the observable image content from all other viewpoints, and choosing the view which, if observed, would reduce the class distribution entropy the most. We propose to solve this in a discriminative end-to-end manner, by training a second CNN, denoted CNN-2 (see <ref type="figure">Figure 3</ref>), which directly outputs the best viewpoint to move to for any given input image. In this way, NBV prediction is trained in a direct and discriminative manner, with end-to-end learning which bypasses the intermediate step of generative prediction.</p><p>As with CNN-1, this network is based on the VGG-M network <ref type="bibr" target="#b2">[3]</ref>, with 5 convolutional and 3 fully-connected layers. However, rather than outputting a distribution over class labels as with CNN-1, the final layer consists of one node for every relative pose along the viewing sphere. To train CNN-2, every training image x k is paired with all other images from that same object, and the view pair is processed with CNN-1 to give a class distribution. Then, the view pair is chosen which yields the highest output for the ground truth class. The relative pose associated with this view pair, together with image x k , then forms a training pair for CNN-2. During testing, a single view is passed through CNN-2, and the output node with the highest value determines the relative pose for the camera's next movement. A series of NBV movements can then be created by iterating this procedure sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Trajectory Optimisation</head><p>Although this NBV prediction offers a simple solution to active recognition, it does not consider the images that could One output node per viewpoint <ref type="figure">Figure 3</ref>: Our CNN-2 architecture for classifying the nextbest-view given an input view, based on greyscale images, depth images, or both. This is also our CNN-3 architecture, where the output is a regression to predicted cross entropies over all viewpoints.</p><p>be acquired whilst the camera is moving towards the next view, and hence is not a globally-optimum solution. Rather than traversing a sequence of NBVs, recognition efficiency can be maximised by following a path which benefits from the contribution of all observable images over the trajectory. Our proposed heuristic is that the optimum trajectory is one whose summation of predicted cross entropies, over all view pairs in the sequence, is smaller than for all other possible trajectories. In this way, we aim to contribute a high classification confidence to Equation 1 for every view pair, rather than only for the view pair formed from the first and last image, as is the case with CNN-2. This is achieved as follows. For the current trajectory, let us denote the sequence of observed views as the set v ∈ V. We then maintain a distribution g(u) over all unobserved views u ∈ U on the viewing sphere, where g u indicates the cost of visiting view u in the current trajectory. g u is defined as the sum of predicted cross entropies based on CNN-1, for all view pairs formed between u and the set V. As each new view is observed and added to the sequence, g(u) is updated to reflect the scores based on the newly formed view pairs. The cost for each unobserved view is therefore:</p><formula xml:id="formula_2">g u = v∈V h(u, v) .<label>(2)</label></formula><p>Here, h(u, v) is the predicted cross entropy for classification of a view pair consisting of the unobserved view u, and the observed view v. To compute this value, we train a third CNN, denoted CNN-3 (see <ref type="figure">Figure 3</ref>), which is identical to CNN-2 except that it is trained for regression to a cross entropy value, rather than being trained for classification to the next-best-view. CNN-3 maps a single input image to a distribution of predicted cross entropies over all viewpoints on the viewing sphere. Training pairs for this are generated by taking each training image, forming view pairs with all other views of that same object, and then computing the classification cross entropy with CNN-1. CNN-3 is then trained to minimise the L2 distance between the ground truth cross entropies and the predicted cross entropies.</p><p>We now define U + as the set of 8 viewpoints adjacent to the camera's current viewpoint (∆θ = −30 • , 0, or +30 • , ∆φ = −30 • , 0, or +30 • ), representing all the positions which the camera can move to in its next step. For each viewpoint u ∈ U + , we compute the set of trajectories T u over which the camera could subsequently traverse, if it were to make its next move to u. These are found by a simple undirected graph search, for a given final trajectory length. We then assign a score s(t) to each trajectory t ∈ T u , by summing up the scores in the trajectory's set of unobserved views U t :</p><formula xml:id="formula_3">s(t) = u∈Ut g u (3)</formula><p>The optimum next view u * is then chosen as the one whose best trajectory has the highest score over all of the available next views:</p><formula xml:id="formula_4">u * = arg max u∈U + max t∈Tu s(t) .<label>(4)</label></formula><p>In this way, at every step along the trajectory, the best decision is taken for the next view given the available information. As the camera follows this guided trajectory, the scores assigned to these unobserved views will change, attracting the camera towards those viewpoints which, if visited, are likely to yield a high classification confidence when processed with CNN-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluated our method on the ModelNet dataset <ref type="bibr" target="#b38">[39]</ref>, which consists of 3D CAD meshes from everyday objects over a range of scales. For our experiments, two subsets were used as in <ref type="bibr" target="#b38">[39]</ref>: ModelNet10, containing 10 object categories with 4,905 unique objects, and ModelNet40, containing 40 object categories and 12,311 unique objects, both with a testing-training split. ModelNet is the only available dataset at this time with sufficient large-scale multi-view coverage of objects for training or testing our networks, and hence as with <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b34">35]</ref>, real-world experiments were not possible.</p><p>Training of the three networks was then carried out by rendering images of each model from all viewpoints on the discretised viewing sphere, and forming the full set of image pairs. Rendering was performed under perspective projection, with objects scaled uniformly to fit the viewing window, to yield images of 512-by-512 pixels for both greyscale and depth images. For rendering the greyscale images, Phong shading <ref type="bibr" target="#b29">[30]</ref> was used, and pre-training conducted with the ImageNet 1k dataset <ref type="bibr" target="#b6">[7]</ref> as with <ref type="bibr" target="#b34">[35]</ref>. Networks trained for the ModelNet10 dataset were pre-trained on ModelNet40. During testing, unless otherwise specified, every object was tested once per viewpoint, with the trajec-  <ref type="table">Table 1</ref>: Results for different implementations of our method over a sequence length of 6 views, for multi-view classification with Equation 1. The first column indicates whether all the view pairs or just the best pairs (based on λ weighting) were used for classification with CNN-1. The second column indicates whether or not the weighting λ was used, compared to an equal contribution from each view pair.</p><p>tory commencing at that viewpoint, and then the classification accuracy for that object was the average over all these trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pairwise Recognition</head><p>First, we explored four different implementations of our multi-view recognition method, with both greyscale and depth images as input. We evaluated the performance based on two parameters: the weighting system used in <ref type="figure">Equation  1</ref>, and the way in which view pairs are formed in a view sequence. <ref type="table">Table 1</ref> shows recognition results for random trajectories of 6 views, with and without learned weights for each view pair, such that without the weighting, all view pairs contribute equally to the final class distribution. Then, for selection of All pairs, M (M −1) 2 pairs were chosen for a sequence of length M , such that every possible pair was used during recognition. For selection of Best pairs, the top M pairs with the greatest weight λ were chosen, such that the number of pairwise classifications was linear rather than combinatorial with the sequence length. Results show the positive effect of the weighting and the inclusion of all pairs in the sequence, with the larger number of pairs offering slightly more benefit than the inclusion of weighting. For the remaining implementations of our method, weighting was used together with the full set of view pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multi-View Recognition</head><p>We then compared our method against two recent competing methods: ShapeNets <ref type="bibr" target="#b38">[39]</ref> and Multi-View CNN (MVCNN) <ref type="bibr" target="#b34">[35]</ref>, together with a baseline which we call View Voting. For ShapeNets, we used code provided by the authors, and we implemented our own version of MVCNN as per the publication details, achieving similar to their quoted results. For the View Voting method, we trained a CNN with 5 convolutional layers and 3 fully-connected layers, similar to CNN-1, to classify views independently based on the image alone. A voting system was then em-ployed to combine the classification outputs of each view. We explored three implementations of our method: using only greyscale images, using only depth images, and then using both image modalities. With the competing methods, ShapeNets was implemented with depth images and MVCNN was implemented greyscale images, as per their original descriptions, and View Voting was implemented with greyscale images. <ref type="table" target="#tab_2">Table 2</ref> shows recognition results for view sequences at an elevation of 30 • from the ground plane, constrained to rotations about the gravity vector, as was the experimental setting in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b34">35]</ref>. Our method outperforms all other methods, and the combination of both greyscale and depth images achieves a small boost in performance over single image modality. As was presented in <ref type="bibr" target="#b34">[35]</ref>, rendering 2D images in a view-based manner achieves much better recognition results than the generative volumetric approach of ShapeNets. However, the capacity of ShapeNets for shape completion provides a strength that view-based methods cannot. The MVCNN method achieves the second-best results for sequences covering 360 • , but for shorter sequences the performance degrades dramatically. This illustrates the unsuitability of their method for arbitrary sequence lengths, owing to testing and training requiring the same sequence length. We note that our method achieves comparable performance to MVCNN with only half the number of views. In our implementation of MVCNN, we trained on the full 360 • set of images regardless of the length of the testing sequence, although this could likely be improved by training on varying sequence lengths. However, once the constraint of moving only about the gravity vector is removed, this would become intractable. <ref type="table" target="#tab_3">Table 3</ref> then shows recognition results for arbitrary view sequences, where we exclude MVCNN due to its inability to generalise in this way. Our method here was provided with both greyscale and depth images. We implement each method with two variations of how the next view along the sequence is selected. Random chooses the next view randomly from the adjacent views, and Straight follows a straight path around the viewing sphere from the beginning to the end of the sequence, similar to the results in 2 except that the sequence direction is randomly selected rather than being constrained around the gravity vector. Again, our method achieves state-of-the-art recognition results, and we note that results are slightly lower than those in <ref type="table" target="#tab_2">Table 2</ref> due to the suitability of viewpoints at an elevation of 30 • for common household objects in their canonical orientation. Choosing a straight path rather than a random path increases recognition accuracy, due to the tendency of random walks to revisit old viewpoints, or remain within a local region and hence fail to explore the object sufficiently.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Active Recognition</head><p>We then evaluated the performance of our NBV and trajectory optimisation extensions to multi-view recognition, and we compared against the NBV method of ShapeNets <ref type="bibr" target="#b38">[39]</ref>. Our method here was provided with both greyscale and depth images. For both our method and ShapeNets, we implemented two strategies for view selection. NBV Global chooses the next-best-view from the entire viewing sphere, and NBV Adjacent chooses the next-best-view from the views adjacent to the current view. The Optimised implementation of our method is that which incorporates all images along the trajectory rather than just the start and end positions, as in Section 4.2. For all implementations, if the selected viewpoint was one which had already been visited, then the highest-scoring of all the unvisited viewpoints was selected. <ref type="table">Table 4</ref> shows that our method achieves the best recognition performance, and whilst the global NBV implementation sees best recognition accuracy, our Optimised implementation, which would be used in reality due to its greater practical efficiency, achieves a close second. <ref type="figure" target="#fig_3">Figure 4</ref> plots the recognition accuracy on Model-Net40 against view sequence length for our method and ShapeNets, with each under active and random trajectories and following adjacent viewpoints. Even without trajectory optimisation, our method significantly outperforms ShapeNets, and we see that our trajectory optimisation  maintains an advantage over random viewpoint selection, for a range of sequence lengths. Finally, <ref type="figure" target="#fig_4">Figure 5</ref> visualises some image sequences which our method observes under optimised trajectories. We note that the chosen trajectory often passes over the top, or beneath, the object, showing how the constraint of MVCNN to rotations around the gravity vector is sub-optimal.  <ref type="table">Table 4</ref>: Recognition results over different sequence lengths, for unconstrained trajectories, using trajectory optimisation methods. Numbers represent the percentage of correctly-classified objects from the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have presented a new method for multiview object recognition over unconstrained camera trajectories, using greyscale images, depth images, or both modalities combined. We have shown that decomposing an image sequence into a set of view pairs enables training in a tractable manner for any trajectory over the viewing sphere of an object. Experiments show that our method outperforms the voxel-based generative ShapeNets method, together with the Multi-View CNN method, and we achieve state-of-the-art recognition on the ModelNet dataset. We have also shown how our pairwise method can extend to next-best-view prediction by learning discriminatively in an end-to-end manner, and this can then be incorporated into a trajectory optimisation scheme to achieve the best camera path for recognition over a given sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>Research presented in this paper has been supported by Dyson Technology Ltd.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Our CNN-1 architecture for classification of a view pair, for use with greyscale images, depth images, or both.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Recognition accuracy for different view sequence lengths on ModelNet40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example greyscale images observed from our optimised trajectories with the ModelNet10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Recognition results over different sequence lengths, from trajectories constrained to rotations about the gravity vector, at an elevation of 30 • . Numbers represent the percentage of correctly-classified objects from the test set.</figDesc><table>ModelNet10 
ModelNet40 

Method 
View Selection 
3 views 6 views 12 views Average 
3 views 6 views 12 views Average 

View Voting 
Random 
85.5 
87.2 
87.6 
88.3 
84.0 
85.8 
87.0 
85.6 
Straight 
86.1 
88.8 
89.9 
88.3 
84.3 
87.0 
88.1 
86.5 

ShapeNets [39] 
Random 
76.0 
81.6 
82.2 
79.9 
70.0 
74.4 
77.2 
73.9 
Straight 
77.0 
81.9 
82.2 
80.4 
70.2 
74.5 
77.2 
74.0 

Ours 
Random 
86.8 
87.8 
91.0 
88.5 
86.1 
88.4 
90.1 
88.2 
Straight 
88.6 
91.2 
93.0 
90.9 
86.7 
89.3 
91.0 
89.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Recognition results over different sequence lengths, from unconstrained trajectories. Numbers represent the percentage of correctly-classified objects from the test set.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GIFT: A Real-time and Scalable 3D Shape Search Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Information-Theoretic Planning with Trajectory Optimization for Dense 3D Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Charrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems (RSS)</title>
		<meeting>Robotics: Science and Systems (RSS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a Similarity Metric Discriminatively, with Application to Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient multi-view object recognition and full pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active Random Forests: An Application to Autonomous Unfolding of Clothes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Active Perception and Scene Modeling by Planning with Probabilistic 6D Object Poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eigenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scharinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object Class Recognition by Unsupervised Scale-Invariant Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured Prediction of Unobserved Voxels From a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Rich Features from RGB-D Images for Object Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extended gaussian images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple Object Recognition with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Becoming the Expert -Interactive Multi-Class Machine Teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From Images to Scenes: Compressing an Image Cluster into a Single Scene Model for Place Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative Methods for Long-Term Place Recognition in Dynamic Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="314" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pairwise Probabilistic Voting: Fast Place Recognition Without RANSAC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autonomous Generation of Complete 3D Object Models Using Next Best View Manipulation Planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-View Object Class Detection with a 3D Geometric Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local Feature View Clustering for 3D Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rotation invariant spherical harmonic representation of 3D shape descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium of Geometry Processing</title>
		<meeting>the Symposium of Geometry Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hough transform and 3D SURF for robust three dimensional classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1115" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-View and 3D Deformable Part Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2232" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object Retrieval with Large Vocabularies and Fast Spatial Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Illumination for Computer Generated Pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Phong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using many cameras as one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shape distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C R</forename><surname>Osada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepPano: Deep Panoramic Representation for 3-D Shape Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2339" to="2343" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view Convolutional Neural Networks for 3D Shape Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards Multi-View Object Class Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reconstructing PASCAL VOC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Active Recognition and Pose Estimation of Household Objects in Clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dissanayake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
