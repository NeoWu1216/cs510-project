<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MDL-CW: A Multimodal Deep Learning Framework with Cross Weights</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mahdieh</roleName><forename type="first">Sarah</forename><surname>Rastegar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">AICT Innovation Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Engineering</orgName>
								<orgName type="institution">Sharif University of Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soleymani</forename><surname>Baghshah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">AICT Innovation Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Engineering</orgName>
								<orgName type="institution">Sharif University of Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Rabiee</surname></persName>
							<email>rabiee@sharif.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">AICT Innovation Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Engineering</orgName>
								<orgName type="institution">Sharif University of Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><forename type="middle">Mohsen</forename><surname>Shojaee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">AICT Innovation Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Engineering</orgName>
								<orgName type="institution">Sharif University of Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MDL-CW: A Multimodal Deep Learning Framework with Cross Weights</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has received much attention as of the most powerful approaches for multimodal representation learning in recent years. An ideal model for multimodal data can reason about missing modalities using the available ones, and usually provides more information when multiple modalities are being considered. All the previous deep models contain separate modality-specific networks and find a shared representation on top of those networks. Therefore, they only consider high level interactions between modalities to find a joint representation for them. In this paper, we propose a multimodal deep learning framework (MDL-CW) that exploits the cross weights between representation of modalities, and try to gradually learn interactions of the modalities in a deep network manner (from low to high level interactions). Moreover, we theoretically show that considering these interactions provide more intra-modality information, and introduce a multi-stage pre-training method that is based on the properties of multi-modal data. In the proposed framework, as opposed to the existing deep methods for multi-modal data, we try to reconstruct the representation of each modality at a given level, with representation of other modalities in the previous layer. Extensive experimental results show that the proposed model outperforms state-of-the-art information retrieval methods for both image and text queries on the PASCAL-sentence and SUN-Attribute databases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In real-world applications, we usually encounter data consisting of different modalities. Images annotated with tags and videos containing audio signal along with visual signal are examples of multimodal data. Although each modality has its own information and statistical properties, different modalities usually share high level concepts. The rationale for using different modalities to learn a shared representation is that those modalities may provide complementary information about a common concept. Moreover, if we could model and learn the cross-modal relations, we can reason about missing modalities using the available modalities or even improve the unimodal models. In recent years, there has been a growing interest in using deep networks for multi-modal learning. Ngiam et al. <ref type="bibr" target="#b12">[13]</ref> used deep autoencoder to extract high level features from speech and video signals, and then aggregated these features to find a shared representation. Srivastava et al. <ref type="bibr" target="#b19">[20]</ref> introduced the multimodal deep Boltzman machine that learns a deep generative model over joint space of image and text inputs. Recently, Sohn et al. <ref type="bibr" target="#b18">[19]</ref> proposed a training approach for multimodal deep learning that is based on minimizing variation of information instead of maximizing the likelihood. All of these methods for multimodal data have a common strategy in which they first learn layers of modality-specific representations and then learn a shared representation across multiple modalities at the top layer of the deep network. Besides deep multimodal learning approaches, some probabilistic methods have also been recently introduced for multimodal data. Xing et al. <ref type="bibr" target="#b23">[24]</ref> proposed dual-wing harmoniums to learn a joint representation of the image and text modalities. Zhen et al. <ref type="bibr" target="#b25">[26]</ref> introduced a probabilistic generative approach called multimodal latent binary embedding. Ozdemir et al. <ref type="bibr" target="#b13">[14]</ref> presented a model based on Bayesian nonparametric framework to learn the underlying semantically meaningful and abstract features of multimodal data. However, since different data modalities have different statistical properties, shallow models are not usually able to extract high-level concepts from multimodal data. One of the other recent challenges about multimodal datasets is their huge size. In order to achieve real time retrieval, some approaches such as multimodal hashing has been introduced which encodes the high-dimensional input vectors into compact binary strings while trying to increase similarity between different modalities that having the same concept in the resulted space. For example, Bronstein et al. <ref type="bibr" target="#b1">[2]</ref> applied a boosting procedure for cross-modality similarity learning. Zhen et al. <ref type="bibr" target="#b24">[25]</ref> proposed a co-regularized hashing based on a boosted co-regularization framework.</p><p>Rastegari et al. <ref type="bibr" target="#b15">[16]</ref> presented a predictable dual-view hashing. The main problem of those methods is that they are all discriminative and therefore unable to use large amount of unlabeled data as opposed to the deep learning methods that can use these unlabeled data to learn a better representation. As mentioned above, in all of the previous models, a joint representation for both modalities is considered. This high level joint representation only shows a common representation for both modalities which has information about the common concept behind modalities. The problem with this approach is that since the more powerful modality (e.g. text) has more information about a common concept, it always contributes more to this joint representation. However, in most of the multimodal applications, we look for a representation with more information about the weaker modality (e.g. image). As mentioned above, in all of the previous deep models, a joint layer is constructed on the top of the modality-specific deep networks to find the shared representation for multimodal data. However, in this paper, we show that considering interactions between modalities may lead to a better representation. The rationale for this approach is that high level concepts may not contain all the useful information about a modality. In the proposed method, we utilize the cross-weights (between modalities) that enable us to learn a better representation for each modality and to have a more powerful cross modality learning. Specially, the modality that contains the higher level information can help us to find a better representation for other modalities. For example, in the bi-modal data consisting of text and image modalities, the text modality can help to find better representation of the image modality. Moreover, in top layer of the proposed network, we consider a proportion for the dedicated hidden units to each of the modalities. Although our base model is not supervised, we can also consider supervision to achieve a higher performance. The experimental results show that the performance of the proposed supervised model outperforms the state-of-the-art retrieval methods on PASCAL-sentence and SUN-Attribute databases. In addition, the performance of our unsupervised model is comparable to that of the stateof-the-art supervised models. The rest of this paper is organized as follows. The related works are introduced in Section 2. The motivation of the proposed deep model for multi-modal data is presented in Section 3. The main ideas of the proposed deep architecture and the pre-training method are discussed in Section 4. In Section 5, we introduce the supervised and unsupervised fine-tuning methods of the proposed deep network. Experimental results are reported in Section 7, and finally we conclude the paper in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>We will use the upper case letters X and Z to show random variables corresponding to the modalities and the label random variable is shown as T. Moreover, h(X) denotes the differential entropy of the random variable X. In modeling unimodal data, we seek for a representation that has as high as possible information about data and also removes the noise-related information in the input data. For example, if X shows the input, we usually have its corrupted version, sayX, and would like to learn a representation that is a function ofX and has the highest mutual information with the clean input. Let f θ be the mapping on the corrupted input that results in the new representation. We need to solve the following optimization problem for a family of f functions:</p><formula xml:id="formula_0">f θ * = max θ I(f θ (X); X)<label>(1)</label></formula><p>In <ref type="bibr" target="#b0">[1]</ref>, it has been shown that using the stacked denoising auto-encoders leads to a good approximation of the mapping f θ that maximizes the mutual information between the obtained representation and the uncorrupted input while also keeping the dimensionality of the representation as low as possible. However, we cannot use this representation learning approach directly on multimodal data (by only considering concatenated data modalities as the input). Indeed, since the modalities have very distinct statistical properties and inter-modality correlations are much more strong, it is usually hard to learn intra-modality correlations by the standard stacked auto-encoders.</p><p>To this end, recent methods focus on finding a higher level representation for each modality and then find a joint representation from them. Methods like <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> learn a joint representation on top of two deep networks. However, for cross modality applications, we actually want to retrieve a modality from the other modality. Therefore, it is more rational to use their conditional probability instead of their joint probability. The authors in <ref type="bibr" target="#b18">[19]</ref> try to minimize the variation of information instead of maximizing the log likelihood. But, we usually desire to find a more informative representation using a weaker modality (e.g, image). Another approach is introduced in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8]</ref> which tries to match different building blocks in one modality to the other modality. However, the problem is that information in modalities are often complementary and not the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation</head><p>The previous methods try to find a joint representation for both modalities or to find an exact match between parts in different modalities. Sometimes, modalities have complementary information that help us to find a better representation. For example, we may have the following pictures; Black dog, Black cat, white cat, and white dog which may be categorized as Cat and Dog but their tags contain the word black or white. Exploiting these complementary information would help us to find a representation which focuses on differences between a Black dog and a Black cat. Thus, while in the previous methods, we may find a representation at the top of the image-specific network that may not consider a difference between a black and a white dog, in the proposed model, the word black will affect the next layer of the image network and leads to a more proper representation. In fact, an ideal mapping would preserve sufficient information about clean data while inferring the missing modality from the available modalities. To this end, in the proposed method, we use deep networks both for learning modality-specific representation and for learning representation of one modality from another one. We use the property of multi-modal data (stronger inter-modal correlations) and propose a new multistage pre-training and fine-tuning method for learning the weights in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pre-training Deep Multimodal Network with Cross Weights</head><p>In this section, we introduce a deep network for multimodal data and propose a suitable pre-training method for this network. Assume that we have found the optimal mappings in Eq. 1 for the modalities as f θ X (X) and f θ Z (Z). For simplicity we define two random variables Y 1 = f θ X (X) and Y 2 = f θ Z (Z). Consider the two generalized stacked denoising autoencoders <ref type="bibr" target="#b20">[21]</ref> that learn representation of the two input modalities as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The achieved representations (for the modalities) in the second layer can be good approximations for Y 1 and Y 2 . Then, we try to find g θ Z→Y 1 and g θ X→Y 2 such that the two random variables U 1 = g θ Z→X (Z) and U 2 = g θ X→Z (X) have the same marginal density as Y 1 and Y 2 , respectively. The easiest way to find these functions is to learn a mapping from the density of Z to that of Y 1 . In the proposed model, we define cross weights from Z to Y 1 and pre-train them to minimize the square error of constructing Y 1 from Z. In fact, we consider a single layer neural network with Z as the input and Y 1 as its output, and try to learn these cross weights as in <ref type="figure" target="#fig_1">Figure 2</ref> (cross weights from X to Y 2 are also found similarly). We may continue this method of pre-training cross weights in a deep manner (when we have the higher level representation of each modality) to find all the cross weights. The proposed network is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Consider different possibilities for the bi-modal input data consisting of text and image, i.e., missing text, missing image, and bi-modal input. For these cases, we intend to discuss the representation obtained in the second layer for the text modality (the resulted representation for the image modality can also be discussed, similarly): Image is missing: Because U 1 becomes zero, we only have Y 1 . Therefore, this situation is similar to the unimodal text-specific stacked auto-encoder before adding the cross weights. Thus, the obtained representation in the second layer of the text-way stacked auto-encoder in the multimodal network, which is shown with the blue color in <ref type="figure" target="#fig_2">Figure 3</ref>, is the same as the representation obtained in the second layer of the text-specific stacked auto-encoder, i.e., Y 1 .</p><p>Text is missing: Here, X contains zeros and we don't have Y 1 . However, Z is available and thus we can calculate U 1 which we have tried to make as close as possible to Y 1 . Thus, we can approximately reconstruct Y 1 by only using Z as input. Both are present: If both of the modalities are available, we would have Y 1 + U 1 in the second layer of the text stacked autoencoder. We can simply divide this amount into two representations that corresponds to approximation of Y 1 in the second layer.</p><p>Summarizing the above cases, we can simply divide the input of the second layer into the number of the available modalities and thus we approximately reach the same representation for unimodal and multimodal inputs. For the above proposed deep network containing crossweights, we use a multi-stage learning algorithm. We first pre-train the modality specific autoencoders. Then, the cross weights are pre-trained to minimize the squared error of constructing the modalities from each other. In the next section, we present the theory underlying the proposed network structure and the pre-training method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Improved multimodal representation using unimodal representation</head><p>In this section, we provide a theorem to show why the proposed method leads to a better representation than the previous deep methods. The graphical model in <ref type="figure" target="#fig_3">Figure 4</ref> shows the probabilistic model of our problem. As shown in this figure, only T causes the two modals X and Z to be dependent. Although this is a restricting condition, if T is a high level concept that is explanatory enough, this model would be the proper generative model for most problems (i.e., if the label random variable is explanatory enough, we can consider it as T).    <ref type="figure">Y )</ref> is a set-theoretic metric that is universal. Therefore, if any other distance measure distinguishes that X and Y are close to each other, then D will also judge them close <ref type="bibr" target="#b8">[9]</ref>. Theorem 4.1 For any two random variables X and Z, we define four random variables Y 1 , U 1 , Y 2 and U 2 as mentioned in section 4. If these random variables have the following properties:</p><p>1. Y 1 and U 1 have the same marginal density conditioned on X and Z respectively and this marginal density is log-concave.</p><p>2. Y 2 and U 2 have the same marginal density conditioned on Z and X respectively and this marginal density is log-concave.</p><formula xml:id="formula_1">3. Y 1 conditioned on X is independent of Y 2 conditioned on Z.</formula><p>then the following inequalities hold:</p><formula xml:id="formula_2">1. h((Y 1 + U 1 )/2, (Y 2 + U 2 )/2|X, Z) &lt; h(Y 1 , Y 2 |X, Z) 2. I((Y 1 + U 1 )/2; (Y 2 + U 2 )/2|X, Z) &gt; I(Y 1 ; Y 2 |X, Z) 3. d((Y 1 + U 1 )/2, (Y 2 + U 2 )/2|X, Z) &lt; d(Y 1 , Y 2 |X, Z) 4. D((Y 1 +U 1 )/2, (Y 2 +U 2 )/2|X, Z) &lt; D(Y 1 , Y 2 |X, Z) 5. I((Y 1 + U 1 )/2; X, Z) &gt; I(Y 1 ; X, Z)</formula><p>The proof is provided in the supplementary materials. Note that although density functions for Y 1 , U 1 , Y 2 , and U 2 may be complex distributions that are not log-concave, the conditional densities of these variables given the clean input modalities can be usually considered as Gaussian or other exponential family densities that are log-concave <ref type="bibr" target="#b0">1</ref> . The theorem ensures that if we find suitable U 1 and U 2 , i.e., if U 1 and U 2 have enough information about Z and X respectively, we can achieve better representation for each modality with more information about other modality and more information about the previous layer of representation compared to the unimodal representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Fine-tuning Deep Multimodal Network with Cross Weights</head><p>The proposed pre-training method in Section 4 is not the only way through which we can consider the same concept behind different modalities; we can go further and train the network to produce the same target for uni-modal and multimodal inputs with a common concept. According to the intended application, this target could be reaching the same representation or the same label for these inputs. For example, we intend to find the correct label in classification applications while we prefer to reach the same representation for uni-modal and multimodal inputs with the same concept in cross-modality retrieval applications. Based on the aforementioned applications, we propose both unsupervised and supervised fine-tuning methods for our deep multimodal network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Unsupervised Fine-tuning</head><p>To achieve the same representation for different modalities, after pre-training, the whole network is fine-tuned with the representation obtained for multimodal data at the last layer. Indeed, the output of the pre-trained network when both modalities are present in the input is considered which is shown inside a red rectangle in <ref type="figure" target="#fig_6">Figure 5</ref>. Now this representation is used as the target for both the unimodal and multimodal input cases. Thus, we intend to fine-tune weights such that the obtained representation is close to the representation of multimodal data even when only one of the modalities is present as the input. Therefore, for all the cases of missing text input, missing image input, and bi-modal input, we consider the representation obtained for multimodal data (at the output layer) as the target and use the corresponding unimodal and multimodal samples for the fine-tuning process. This procedure leads to a network which produces approximately the same representation for unimodal and multimodal inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Supervised Fine-tuning</head><p>Since labels can be a good approximation of high level concepts, we can use them to guide our model to learn a better representation. Specially for the text input, labels can improve the learned representation significantly. Our model has the flexablity of using different amount of supervision (i.e., it can be changed from an unsupervised model to a supervised one). Adding supervision usually improves obtained representations and also makes representations of modalities closer to each other. To incorporate supervisory information, we first learn two modality-specific stacked autoencoders as before. Then, these pre-trained networks are separately fine-tuned with the available labels. After that we learn the cross weights as before. Finally, the whole network is fine-tuned again with available labels (for unimodal and multimodal samples) as opposed to the above unsupervised learning that uses the multimodal representation as the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we first present results of an experiment conducted on a toy example. Then, we introduce the PASCAL-Sentence and SUN-Attribute datasets on which we run methods and evaluate results. After that, the experimental setup is presented and finally the results of our experiments are reported and discussed 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Toy Example on MNIST</head><p>We first evaluate our method on MNIST Dataset <ref type="bibr" target="#b10">[11]</ref> composed of hand written digits. As in <ref type="bibr" target="#b18">[19]</ref>, we halve every image to the left and the right parts and use these parts as the input data modalities. Then, we perform recognition task on the complete input data and halved input data. <ref type="table">Table  1</ref> shows recognition errors with different types of input for our method and methods which have been used for comparison in <ref type="bibr" target="#b18">[19]</ref>. Our network contains [392 1000 500 300] variables for the left pathway and [392 1000 500 300] for the right one and thus the whole network is composed of [784 2000 1000 600] neurons. Compared to the other methods, the proposed method has higher accuracy for both unimodal and multimodal queries. This in fact shows that our proposed method, uses the information between modalities to improve its knowledge about each of modalities and even leads to a better multimodal representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">PASCAL-Sentence</head><p>PASCAL-Sentece 2008 database <ref type="bibr" target="#b2">[3]</ref> is a collection of images from PASCAL 2008 images along with annotating senteces by Amazon Mechanical Turk workers. There are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input modalities at the test time Left</head><p>Right Left+Right ML (PCD) <ref type="bibr" target="#b18">[19]</ref> 14.98% 18.88% 1.57% Min VI (CD-Percloss) <ref type="bibr" target="#b18">[19]</ref> 9.42% 11.02% 1.71% Min VI (MP) <ref type="bibr" target="#b18">[19]</ref> 6.58% 7.27% 1.73% Our method (MDL-CW) 4.23% 5.99% 1.39% <ref type="table">Table 1</ref>. Test error on MNIST dataset for the methods reported in <ref type="bibr" target="#b18">[19]</ref> and our method. 50 images for each of the 20 categories in this dataset. Each image is annotated by five sentences. We used the same visual and textual features as in <ref type="bibr" target="#b2">[3]</ref>. Indeed, for images, several object detectors are run and the most confident one is found. For each detector, the coordination of detection along with the confidence value is considered. Moreover, the response of several SVMs trained for each category using GIST descriptor is computed as in <ref type="bibr" target="#b2">[3]</ref>. For textual features, a dictionary of discriminative and frequent words in the database sentences is found. For each image, a triplet of &lt;object, action, scene&gt; is extracted and the semantic similarity between each word in the triplet and all dictionary words is computed by Lin similarity measure <ref type="bibr" target="#b11">[12]</ref> on the WordNet hierarchy. Finally, the feature vector is computed as the sum of all the similarity vectors for the words in the triplet <ref type="bibr" target="#b2">[3]</ref>. This results in identical features for all of five sentences for an image, to make textual features distinct for every image, bag of words representation of each sentence was concatenated to features described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">SUN-Attribute</head><p>The SUN-Attribute dataset is a large-scale dataset with 14340 images and 717 categories. There are only 20 images for each category in this dataset and some categories are very close to each other. Each image is annotated with 102 binary attribute labels from three Amazon Mechanical Turk workers. For the final attribute vector, the mean of three annotated vectors is used. The precomputed image features include GIST, 2 × 2 histogram of oriented gradient, selfsimilarity measure, and geometric context color histograms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref> with dimension 19080. As in <ref type="bibr" target="#b13">[14]</ref>, we reduce the dimensionality from 19080 to 1000 by randomly selecting features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Experimental setup</head><p>For the PASCAL-Sentence dataset, we use a network composed of [1408 800 300 128] neurons in the text pathway and [260 400 200 128] neurons in the image pathway for the unsupervised model. Activation function in all encoder layers is set to rectified linear unit (ReLU). For supervised fine-tuning, a softmax layer with 20 nodes is added on top of each stacked auto-encoder. We also used four dropout <ref type="bibr" target="#b6">[7]</ref> modules in between layers to prevent overfitting. Dropout probability is set to 0.35. For the SUN-Attribute dataset, we use a network composed of [1000 500 200 128] neurons for the image pathway and [102 500 200 128] neurons for the attribute pathway. Rest of network design and learning procedure is similar to what was described for PASCAL-Sentence dataset. Activation used in all layers is ReLU and for supervised fine-tuning using category labels, a softmax layer with size 717 was added on top of each stacked auto-encoder. Also dropout modules with dropout probability of 0.35 was used in between the layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Experimental results</head><p>Our method is compared with several methods applied to multimodal data including Locality Sensitivity Hashing(LSH) <ref type="bibr" target="#b4">[5]</ref>, Spectral Hashing(SH) <ref type="bibr" target="#b21">[22]</ref>, multimodal Deep Boltzman Machines (mDBM) <ref type="bibr" target="#b19">[20]</ref>, iterative quantization (ITQ) <ref type="bibr" target="#b5">[6]</ref>, predictable dual-view hashing <ref type="bibr" target="#b15">[16]</ref>, and integrative Indian Buffet Process (iIBP) <ref type="bibr" target="#b13">[14]</ref>. We applied LSH and SH for each of the modalities separately to show that representation learning using multimodal data will lead to a better performance even on unimodal queries in both modalities. As in <ref type="bibr" target="#b13">[14]</ref>, we split the datasets to the same number of test and train images for each category. We used 256 bits for the final representation 3 for all the methods except to (ITQ and PDH) since they don't support this number of bits. For these two methods, we use the maximum number of bits  that they support. For each query, we find its representation using each of the above methods and according to the hamming distance between the representation of that query with those of the training images, we order the training images 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Supervised model</head><p>We consider supevision information for both datasets. However, we didn't fine-tune image related networks in the SUN-Attribute dataset as mentioned in Section 6.4. Mean precision curves for the both datasets are presented in Figure 6. As mentioned above, in the SUN-Attribute dataset, there would be only 10 samples for each class (from 7170 training samples) which leads to a poor performance. As it can be seen in this figure, our method strongly outperforms all the other methods. The qualititative results which is provided in the supplementary materials, show remarkable semantic similarity of both sets of results to the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Unsupervised model</head><p>In addition to the supervised model, we have also conducted experiments to evaluate results of our model without any supervision. In <ref type="figure" target="#fig_8">Figure 7</ref> , we compared our method with other methods on the PASCAL-Sentence dataset. The proposed method that uses unsupervised fine-tuning outperforms all the previous methods even the supervised ones. These results show that the proposed method can extract high level semantics between modalities even without use of any labels. Therefore, we can take advantage of any amount of su-pervision which makes our model more flexible than other methods. In addition to these sets of experiments, in <ref type="table" target="#tab_1">Tables 2 and 3</ref>, we compared our method with other state-of-the-art deep learning models. However, we used continuous representation (without thresholding) and cosine similarity in these experiments. The compared models include Deep fragment embeddings for bidirectional image sentence mapping <ref type="bibr" target="#b7">[8]</ref>, grounded compositional semantics for finding and describing images with sentences <ref type="bibr" target="#b16">[17]</ref>, parsing natural scenes and natural language with recursive neural networks <ref type="bibr" target="#b17">[18]</ref>, kernel and nonlinear canonical correlation analysis <ref type="bibr" target="#b9">[10]</ref>, and De-ViSE that is a deep visual-semantic embedding model <ref type="bibr" target="#b3">[4]</ref>. Settings for this sort of experiments are as in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we proposed a novel representation learning framework for multimodal data using deep networks, called MDL-CW. We tried to maximize the mutual information between representations of modalities in a deep manner while the information about individual modalities was also preserved. This leads to a representation that is better than representation obtained for each of the modalities, separately. In the proposed framework, a multi-stage learning method consisting of some pre-training and finetuning steps that are useful for multi-modal data was presented. Experimental results on challenging real world datasets demonstrated that MDL-CW outperforms the existing state-of-the-art multimodal methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Two modality-specific stacked autoencoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The network after training the first layer cross weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The network after training all of the cross weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Graphical model for the multimodal input problem. The shaded nodes have been observed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition 4. 1 (</head><label>1</label><figDesc>Variation of Information) A distance metric in information theory given byd(X, Y ) = h(X|Y ) + h(Y |X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Using last layer representation of the multimodal input as target for unsupervised fine-tuning of Deep Multimodal Network with Cross Weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>The result of the category retrieval for all query types (a) text-to-image and (b) image-to-image queries on PASCAL-Sentence dataset and (c) text-to-image and (d) image-to-image queries on SUN-Attribute dataset. Our method using supervised fine-tuning is compared with state-of-the-art supervised methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Results of category retrieval for all the query types (a) text-to-image and (b) image-to-image queries on PASCAL-Sentence dataset. Our method using unsupervised fine-tuning is compared with state-of-the-art supervised methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 2. Pascal1K ranking experiments in image annotation.</figDesc><table>Model 

Recall@1 Recall@5 Recall@10 Mean rank 
Random Ranking 
4.0 
9.0 
12.0 
71.0 
Socher [18] 
23.0 
45.0 
63.0 
16.9 
kCCA [10] 
21.0 
47.0 
61.0 
18.0 
DeViSE [4] 
17.0 
57.0 
68.0 
11.9 
SDT-RNN [17] 
25.0 
56.0 
70.0 
13.4 
Deep Fragment [8] 
39.0 
68.0 
79.0 
10.5 
Our method (MDL-CW) 
34.0 
70.0 
89.0 
9.2 

Model 
Recall@1 Recall@5 Recall@10 Mean rank 
Random Ranking 
1.6 
5.2 
10.6 
50.0 
Socher [18] 
16.4 
46.6 
65.6 
12.5 
kCCA [10] 
16.4 
41.4 
58.0 
15.9 
DeViSE [4] 
21.6 
54.6 
72.4 
9.5 
SDT-RNN [17] 
25.4 
65.2 
84.4 
7.0 
Deep Fragment [8] 
23.6 
65.2 
79.8 
7.6 
Our method (MDL-CW) 
35.2 
72.6 
90.6 
6.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Pascal1K ranking experiments in image search.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is the consequence of the chosen model in which we usually assume Gaussian noise and sigmoid activation function that lead to log-concave densities</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The codes are available at http://ml.dml.ir/mdl-cw.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This was done by threshholding the representation in the last layer for our method and mDBM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">iIBP uses its own method for finding similarity between binary representations.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized denoising auto-encoders as generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data fusion through cross-modality metric learning using similarity-sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical clustering using mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Andrzejak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europhysics Letters)</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">278</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>EPL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kernel and nonlinear canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fyfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A probabilistic framework for multimodal retrieval using integrative indian buffet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2751" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predictable dual-view hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fakhraei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved multimodal deep learning with variation of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR), 2010 IEEE conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mining associated text and images with dual-wing harmoniums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.1423</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Co-regularized hashing for multimodal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1376" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A probabilistic model for multimodal hash function learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="940" to="948" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
