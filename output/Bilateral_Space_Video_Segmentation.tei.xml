<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilateral Space Video Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Märki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Adobe Systems Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bilateral Space Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a novel approach to video segmentation that operates in bilateral space. We design a new energy on the vertices of a regularly sampled spatiotemporal bilateral grid, which can be solved efficiently using a standard graph cut label assignment. Using a bilateral formulation, the energy that we minimize implicitly approximates long-range, spatio-temporal connections between pixels while still containing only a small number of variables and only local graph edges. We compare to a number of recent methods, and show that our approach achieves state-of-the-art results on multiple benchmarks in a fraction of the runtime. Furthermore, our method scales linearly with image size, allowing for interactive feedback on real-world high resolution video.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video segmentation is a fundamental problem in computer vision, and a key component of numerous applications, including localized video editing, color grading, and compositing. Furthermore, semantically meaningful clustering of video input is a crucial step in higher-level vision problems such as scene understanding, learning object class models, and video summarization <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>There are many similar and related definitions of what specifically "video segmentation" refers to, but for the purpose of this work, we consider the problem of finding a binary partitioning of pixels into foreground and background classes.</p><p>In general, this problem must be addressed semiautomatically, as the task itself requires a high level understanding of both the scene and the implicit goals of the user. This is because defining what constitutes a "foreground object" is often application specific, and the same video could have multiple valid solutions. For this reason, we consider a semi-supervised approach that uses sparse user-given cues to produce a video segmentation. These cues can be either a mask specified on one or a few key-frames, in which case the problem can be described as mask propagation, or a sparse set of labels specified by user clicks or strokes, which <ref type="bibr">Figure 1</ref>. Example results of our bilateral space video segmentation which automatically propagates a user provided mask on the first frame (left column) through the complete video (remaining columns). Thanks to the efficiency of our method, errors in the later frames can be easily fixed in an interactive manner. are then interpolated through the video.</p><p>A crucial aspect of semi-automatic video segmentation methods is responsiveness. A user expects instant feedback, and any computation delays present significant challenges to the adoption of these technologies. This is one of the key reasons that segmentation related tasks, such as rotoscoping, form the bulk of manual labor, and therefore associated costs, of video effects. In this work, we present a highly efficient method for user-guided video segmentation that is able provide iterative feedback in a fraction of the time of previous approaches, while still generating high quality results in semi-supervised applications, as demonstrated on multiple benchmarks.</p><p>We accomplish this by performing the segmentation in "bilateral space", which is a high dimensional feature space, originally proposed for accelerated bilateral filtering <ref type="bibr" target="#b7">[8]</ref>, and recently extended to computing depth from stereo triangulation <ref type="bibr" target="#b3">[4]</ref>. We describe a novel energy on a "bilateral grid" <ref type="bibr" target="#b7">[8]</ref>, a regular lattice in bilateral space, and infer labels for these vertices by minimizing an energy using graph cuts. Processing on the bilateral grid has several advantages over other approaches. First, the regular and data-independent structure allows for a more efficient mapping from image to bilateral space (and vice versa) than super-pixels or kmeans clustering approaches. Second, it allows for flexible interpolation schemes that lead to soft assignments of pixels to multiple intermediate variables. And finally, a bilateral representation allows us to infer labels on a simple, locally connected graph, while still enforcing large spatio-temporal neighborhood regularization, which would be intractable to solve directly. We show that the combination of these advantages significantly improves segmentation quality, and importantly, allows us to segment video data, generating temporally consistent results with robustness to object and camera motion.</p><p>In summary, we present the first work to address video segmentation in bilateral space. Our approach contains several novel concepts, such as a fast "adjacent" interpolation scheme for high-dimensional grids, and a novel energy formulation that is justified by an analysis of locally connected labeling in bilateral space. Our method is highly efficient and scales linearly with image resolution, allowing us to process 1080p video with only minor increases in runtime. We compare our mask propagation to a number of existing approaches using multiple publicly available datasets, and demonstrate that using this simple to implement method we can achieve state-of-the-art results. While these comparisons are computed without user interaction, we note that the real strength of our approach is that it enables an interactive interface, due to the computational efficiency, which we show in a supplemental video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Graph Based Video Segmentation Images and videos naturally lend themselves to a regular graph structure where edges connect neighboring pixels in either a spatial or spatio-temporal configuration. Video segmentation can then be formulated as an optimization problem that tries to balance a coherent label assignment of neighboring vertices, while complying to a predetermined object model or user constraints. Graph-cuts techniques have long been used to efficiently solve this problem, both for image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">32]</ref> and video segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b35">35]</ref>.</p><p>Building on this general framework, subsequent methods have lowered the computational cost by reducing the number of nodes in the graph using clustering techniques such as a per-frame watershed algorithm <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b28">28]</ref>, mean-shift segmentation <ref type="bibr" target="#b35">[35]</ref>, or spatio-temporal superpixels <ref type="bibr" target="#b30">[30]</ref>. However, these methods still do not achieve interactive rates due to costly clustering steps, and allow only rough user control <ref type="bibr" target="#b21">[21]</ref>, or require expensive per-pixel refinement on each frame <ref type="bibr" target="#b35">[35]</ref>. Additionally, the above clustering methods can fail in regions with poorly defined image boundaries. Recently, efficient fully connected graphs have been exploited to improve robustness to long-range, possibly occluded connections <ref type="bibr" target="#b27">[27]</ref>.</p><p>Other Semi-Supervised Approaches Besides graphbased methods, other approaches have proposed solutions to the video segmentation problem using, for example, optical flow or nearest neighbor fields to propagate silhouettes or masks over multiple key frames <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b18">18]</ref>. Video SnapCut <ref type="bibr" target="#b2">[3]</ref> uses overlapping local classifiers that predict the foreground probability, which are propagated and refined over time. This approach was extended to a combination of local and global classifiers <ref type="bibr" target="#b38">[38]</ref> to improve robustness. Dondera et al. <ref type="bibr" target="#b10">[11]</ref> apply the spectral clustering method of <ref type="bibr" target="#b22">[22]</ref> on a graph of super-pixels in a 3D video volume. An initial segmentation is obtained without additional input, the user can then add constraints to correct the solution. Labels are then inferred using a conditional random field formulation.</p><p>Recently, Fan et al. <ref type="bibr" target="#b13">[13]</ref> proposed a method that propagates masks using nearest neighbor fields, and then refines the result with active contours on classified edge maps. As this is currently the top performing method in many cases, we use it as a basis for our comparisons.</p><p>These approaches solve the mask propagation locally, which makes enforcing global spatio-temporal constraints difficult. As opposed to this, our method has the benefits of a fully global solve, while operating on a reduced space, which yields a highly efficient solution.</p><p>Unsupervised Approaches By making certain assumptions about the application scenario, some methods have presented fully unsupervised techniques for video segmentation. These approaches use features such as clustering of point trajectories <ref type="bibr" target="#b6">[7]</ref>, motion characteristics <ref type="bibr" target="#b23">[23]</ref>, appearance <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b19">19]</ref>, or occlusion cues <ref type="bibr" target="#b33">[33]</ref> to hypothesize foreground object locations. Faktor et al. <ref type="bibr" target="#b12">[12]</ref> achieved state-ofthe-art results by diffusing such hypotheses on a non-local graph. Similarly, Wang et al. <ref type="bibr" target="#b36">[36]</ref> aggregate spatio-temporal saliency information <ref type="bibr" target="#b25">[25]</ref> to infer the object appearance model. While automatic video segmentation methods certainly have advantages, they are only valid in restricted use cases where the desired "foreground" regions have notably different characteristics. For general purpose, high quality video segmentation, we instead focus on the user-assisted case, but note that our method could be combined with any existing automatic foreground model.</p><p>Bilateral Space Bilateral filtering has been widely used for edge-adhering image processing operations <ref type="bibr" target="#b24">[24]</ref>. Chen et al. <ref type="bibr" target="#b7">[8]</ref> introduced the bilateral grid as a data structure to speed up bilateral filtering. This approach lifts pixels into a higher-dimensional space based on position and color, after which bilateral filtering can be performed as a standard Gaussian filter. The advantage is that the resolution of the bilateral grid can be significantly lower than the number of input pixels, thereby providing an effective means of ac-  celeration. This idea was later generalized to high dimensional simplexes <ref type="bibr" target="#b0">[1]</ref>, and has been used beyond filtering operations for edge preserving painting <ref type="bibr" target="#b7">[8]</ref>, and accelerating stereo matching <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">31]</ref>. Our method draws inspiration from these approaches. In particular, we extend the work of <ref type="bibr" target="#b3">[4]</ref>, and describe an energy that when solved using graph cuts, can be used to achieve a high quality video segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Let V : Ω → R 3 be a color video, defined on a finite discrete domain Ω ⊂ R 3 . Given some user input as a set of known foreground and background pixels, F G, BG ⊂ Ω, we seek a binary mask M : Ω → {0, 1} that labels each pixel of the video either as background or foreground.</p><p>Our approach makes use of a bilateral grid <ref type="bibr" target="#b7">[8]</ref>, Γ , consisting of regularly sampled vertices v ∈ Γ . The mask M is computed in four main stages, (summarized in Figure 2): by lifting pixels into a higher dimensional feature space (Section 3.1), splatting them onto regularly sampled vertices (Section 3.2), computing a graph cut label assignment (Section 3.3), and slicing vertex labels back into pixel space (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lifting</head><p>The first step is to embed each pixel p = [x, y, t] T in a higher d-dimensional feature space, for example by concatenating YUV pixel color and spatial and temporal coordinates:</p><formula xml:id="formula_0">b(p) = [c y , c u , c v , x, y, t] T ∈ R 6<label>(1)</label></formula><p>In this bilateral space, Euclidean distance encodes both spatial proximity and appearance similarity. We evaluated a number of feature spaces, generalized as the concatenation of appearance features A(p) and position features P(p), and interestingly found that state-of-the-art results can be achieved by simply extending traditional 5D bilateral features with a temporal dimension, which is very efficient due to the low dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Splatting</head><p>Instead of labeling each lifted pixel b(p) directly, we resample the bilateral space using a regular grid <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> and compute labels on the vertices of this grid. The process of accumulating values on the bilateral space vertices is known as "splatting". For each vertex v ∈ Γ , a weighted sum of lifted pixels b(p) is computed as:</p><formula xml:id="formula_1">S(v) = w(v, b(p)) · (p) (2) wherep = (✶ F G (p), ✶ BG (p), 1)<label>(3)</label></formula><p>and ✶ × (p) is an indicator function that is 1 iff p ∈ ×.</p><p>The weight function w(v, b(p)), determines the range and influence that each lifted pixel b(p) has on the vertices of Γ . Prior work has used a nearest neighbor (NN) indicator <ref type="bibr" target="#b3">[4]</ref> or multi-linear interpolation weights <ref type="bibr" target="#b7">[8]</ref>. Importantly, these approaches have limited support, (1 nonzero vertex for each pixel using NN, and 2 d−1 for multi-linear), which is necessary for computation and memory efficiency. The NN approach is the fastest, but can lead to blocky artifacts, while the multi-linear interpolation is slower, but generates higher quality results. We propose an adjacent interpolation that provides a good compromise between the two, yielding high quality results, but with a linear growth in the number of non-zero weights as a function of feature space dimension, as opposed to the exponential growth of the multi-linear case ( <ref type="figure" target="#fig_2">Figure 3)</ref>.</p><p>The idea behind adjacent weighting is that with multilinear interpolation, weights quickly decrease for vertices that differ from the nearest neighbor N b(p) in many dimensions. More precisely,</p><formula xml:id="formula_2">w l (v, b(p)) ≤ 0.5 |v−Nb(p)| 0<label>(4)</label></formula><p>presents an upper bound for the weight, because each factor of the linear interpolation is smaller than 0.5 if for that dimension v i is not the integer value that b i (p) was rounded to. We use this bound to skip weight computation where the result would have been small anyway:</p><formula xml:id="formula_3">w a (v, b(p)) = d i=1 v i − N b(p) if v ∈ A b(p) 0 otherwise<label>(5)</label></formula><p>We found that interpolation between the nearest neighbor and vertices that differ in only one dimension (the set of adjacent vertices A b(p) ) already produces significantly better results than hard nearest neighbor assignments with only a minor increase in runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Graph Cut</head><p>We now seek binary labels α, that mark each vertex v as foreground, α v = 1, or background, α v = 0.</p><p>We compute these labels by constructing a graph G = (Γ, E) where the vertices are the vertices in the bilateral grid, and edges connect immediate neighbors (e.g., 4 neighbors when d = 2, 6 neighbors when d = 3, . . . ). We then define an energy based on the assumption that the label assignment is smooth in bilateral space:</p><formula xml:id="formula_4">E(α) = v∈Γ θ v (v, α v ) + λ (u,v)∈E θ uv (u, α u , v, α v ) (6)</formula><p>θ v is the unary term, θ uv is the pairwise term, and λ is a weight that balances the two.</p><p>The unary term θ v models deviations from the supplied user input. As we invert the splatting step to retrieve final pixel labels, the splatted value S BG (v) expresses the total cost of assigning v to foreground, α v = 1, and S F G (v) the cost of assigning it to background, α v = 0, respectively.</p><formula xml:id="formula_5">θ v (v, α v ) = (1 − α v ) · S F G (v) + α v · S BG (v)<label>(7)</label></formula><p>The pairwise term θ uv attempts to ensure that neighboring vertices are assigned the same label. In order to derive θ uv , we consider that the bilateral space graph G is equivalent to a densely connected pixel graph, where edge weight between pixels assigned to the same vertex is set to infinity (as it is impossible to assign them different labels in bilateral space). The edge weight between other pixels is then approximated by the distance of their respective vertices. With that in mind, it becomes clear that the weights between vertices need to be scaled by the total number of points S # (u) and S # (v) that have been assigned to the two vertices (we can retrieve S # (u) and S # (v) from the homogeneous (3rd) coordinate in <ref type="figure">Equation 4</ref>). That way, assigning different labels to two vertices is (approximately) equivalent to assigning the labels to all the original points and our pairwise term can be written as: <ref type="figure">)</ref> is a high-dimensional Gaussian kernel where the diagonal matrix Σ scales each dimension to balance color, spatial and temporal dimensions:</p><formula xml:id="formula_6">θ uv (u, α u , v, α v ) = g(u, v) · S # (u) · S # (v) · [α u = α v ] (8) where g(u, v</formula><formula xml:id="formula_7">g(u, v) = e − 1 2 (u−v) T Σ −1 (u−v)<label>(9)</label></formula><p>This formulation also reduces the complexity of the graph cut due to the fact that all vertices without any assigned pixels, S # (v) = 0, are now completely excluded from any computation and thus need no representation in the graph. We can now efficiently apply a max-flow computation to find the vertex labeling with minimal energy <ref type="bibr" target="#b4">[5]</ref>.</p><p>Connectivity analysis So far we have assumed that increased connectivity leads to higher quality results. We validate this by conducting experiments where we compute a graph cut segmentation on a per-pixel (not bilateral) graph, as in <ref type="bibr" target="#b5">[6]</ref>. We begin with just local neighbor edges (4 neighbors on a 2D graph), and increase the connectivity by connecting all points in an n × n window ( <ref type="figure">Figure 4</ref>). This plot clearly shows that increasing connectivity leads to better results, but at an increased running time, as was also observed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b20">20</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Slicing</head><p>Given the foreground and background labels of the bilateral vertices, the final mask M is retrieved by slicing, i.e. interpolating grid labels at the positions of the lifted pixels in the output frame. We generally use the same interpolation scheme for both splatting and slicing, although a even more precise adjustment of the quality/speed trade-off is possible by choosing different interpolations. Our runtime (BV SQ) <ref type="figure">Figure 4</ref>. Mask propagation on a pixel-level graph with increasing neighborhood sizes w. Error decreases with larger neighborhoods at the expense of larger runtimes. Our approach (BVSQ) is shown for comparison. We obtain lower error than even large window sizes, while being much faster as well.</p><formula xml:id="formula_8">M(p) = v∈Γ w(v, b(p)) · L(v)<label>(10)</label></formula><p>Finally, we post-process each frame with a simple 3 × 3 median filter in order to remove minor high frequency artifacts that arise due to the solution being smooth in bilateral space, but not necessarily pixel space, however we note that a more sophisticated method like the geodesic active contours of <ref type="bibr" target="#b13">[13]</ref> could also be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Evaluation</head><p>Implementation Our approach is implemented in Matlab, with C++ bindings for most time consuming routines. All our experiments were performed on a Mac Pro with a 3.5 GHz 6-Core Intel Xeon E5 CPU and 16 GB RAM. The measured timings include the complete pipeline except for IO-operations. Unlike many other approaches, we do not rely on pre-computed flows, edge maps or other information.</p><p>Parameters We evaluate two different sets of settings, one tuned for quality, BVS Q , and the other for speed, BVS S : Our method can predict temporally global segmentations, and higher temporal resolutions allow for compensating for large degrees of object motion. However, this did not improve result quality on the benchmarks due to limited object motion, and the testing strategy of <ref type="bibr" target="#b13">[13]</ref>, where a single keyframe is propagated forward by multiple frames. In cases where user input is distributed temporally, e.g., in the interactive interface, we use a higher temporal grid size of N = 5, . . . , 15.</p><formula xml:id="formula_9">BVS Q (</formula><p>We set the pairwise weight to λ = 0.001 for all results. The lifting stage also allows for different feature dimensions to be scaled independently of each other (Σ in <ref type="figure">Equation 9</ref>). For all results, we scale by 0.01, 0.5, 1.3, 1.5 the temporal (t), spatial (xy), the intensity (c y ) and the chroma (c u c v ) dimensions respectively, but we didn't notice any particular dependency on the unary edge factor or the dimension scaling. All parameters could be tuned to achieve better results per benchmark or even per video, but we leave them fixed in all tests to represent a more real-world scenario.</p><p>Runtime Comparing runtime is difficult, with different code bases and levels of optimization, however, we give some average runtimes from our observations as a rough idea of the expected computational complexity. As many existing video segmentation methods take even up to one hour for a single frame, we compare only with the following fastest state-of-the-art methods: SEA: SeamSeg <ref type="bibr" target="#b29">[29]</ref>, JMP: JumpCut <ref type="bibr" target="#b13">[13]</ref>, NLC: Non-Local Consensus Voting <ref type="bibr" target="#b12">[12]</ref>, and HVS: Efficient Hierarchical Graph-Based Video Segmentation <ref type="bibr" target="#b14">[14]</ref>. Our method computes 480p masks in as little as 0.15 seconds <ref type="table">(Table 1</ref>) which is roughly an order of magnitude faster than all other approaches. Even if we trade speed for quality, our method still takes significantly less time than the second-fastest approach. Furthermore, the two most expensive steps, i.e. lifting and slicing, can be trivially parallelized since their output values only depends on color and position of individual pixels. Splatting can also be performed on concurrent threads, simply augmenting the grid with a small number of accumulators at bilateral vertices. The only stage that is not easily parallelizable is graphcut, which anyway has small runtime due to the size and sparsity of the bilateral grid. Therefore we would expect a tuned GPU implementation to report substantial performance gains.  <ref type="table">Table 2</ref>. IoU score (higher is better) on a representative subset of the DAVIS benchmark <ref type="bibr" target="#b26">[26]</ref>, and the average computed over all 50 sequences.</p><formula xml:id="formula_10">BVS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Evaluation</head><p>In order to evaluate our approach with respect to existing methods, we focus on the task of mask propagation, which has been widely used by previous work. Given a manual segmentation of the first frame, each method predicts subsequent frames, without any additional user input. Using this approach, we measured the performance on three different benchmark datasets.</p><p>DAVIS The dataset of Perazzi et al. <ref type="bibr" target="#b26">[26]</ref> comprises a total of 50 high-resolution sequences alternating a wide range of object segmentation challenges such as occlusions, fast-motion and appearance changes. The dataset comes with per-frame, per-pixel manual annotations. Table 2 summarizes the results for a representative subset of DAVIS sequences and the average performance over the entire dataset. The full, per-sequence, evaluation can be found in the benchmark. While our approach scales linearly with image resolution, not all algorithms that we compare to are able to handle the full 1080p resolution, so we run comparisons on downscaled 480p versions of these sequences. We report the widely used intersection-over-union (IoU) metric, averaged over all frames in each sequence. As may be seen in <ref type="table">Table 2</ref>, our method outperforms all other methods, achieving the best score on most of the videos and the best average score overall. Even with the faster, but less accu-  <ref type="table">Table 3</ref>. Errors (lower is better) on the JumpCut benchmark for two transfer distances and several different methods as reported by <ref type="bibr" target="#b13">[13]</ref>.</p><p>rate configuration, our approach still performs comparably or better than several concurrent approaches <ref type="bibr" target="#b26">[26]</ref>.</p><p>JumpCut The recent method of Fan et al. <ref type="bibr" target="#b13">[13]</ref> includes a dataset consisting of 22 videos with medium resolution and good per-frame ground truth masks. In addition to the methods mentioned above, we compare to RB: RotoBrush, based on SnapCut <ref type="bibr" target="#b13">[13]</ref>, and DA: Discontinuity-aware video object cutout <ref type="bibr" target="#b38">[38]</ref>. As we do not have access to implementations for all methods reported on this dataset, we instead adapt our method to conform to the same testing strategy and error metric used in <ref type="bibr" target="#b13">[13]</ref>. That is, propagating masks from multiple keyframes 0, 16, . . . , 96, over different transfer distances <ref type="bibr">(1, 4, 8, 16 frames)</ref>, and reporting error as follows:</p><formula xml:id="formula_11">Err = 100 n n i=1</formula><p># error pixels in i-th frame # foreground pixels in i-th frame <ref type="bibr" target="#b10">(11)</ref> Overall, our method performs best on this benchmark, closely followed by JumpCut <ref type="table">(Table 3)</ref>.</p><p>We note that our approach uses a simple refinement step (3x3 median filter). However, we conducted an experiment using an active contour refinement, similar to JumpCut, and our result improved to 2.45 on average, with a running time . This plot shows how IoU (higher is better) decreases when a single mask is propagated over increasing numbers of frames. Our method degrades favorably when compared to other approaches. The NLC approach stays constant as it is an automatic method that doesn't depend on the input of the first frame.</p><p>of only 1s per frame. We additionally observe that many methods degrade in quality over long sequences, as errors accumulate over time. In contrast, our method scores better on long videos, experiencing less drift of the object region than other approaches ( <ref type="figure" target="#fig_5">Figure 5</ref>).</p><p>SegTrack For the sake of completeness we also present an evaluation on the popular benchmark of Tsai et al. <ref type="bibr" target="#b34">[34]</ref>. We additionally compare to: FST: Fast Object Segmentation in Unconstrained Video <ref type="bibr" target="#b23">[23]</ref>, DAG: Video object segmentation through spatially accurate and temporally dense extraction of primary object regions <ref type="bibr" target="#b37">[37]</ref>, TMF: Video segmentation by tracking many figure-ground segments <ref type="bibr" target="#b20">[20]</ref>, and KEY: Key-segments for video object segmentation <ref type="bibr" target="#b19">[19]</ref>. In this case, it can be seen that our method clearly struggles to compete with existing approaches. This is most likely due to a combination of factors related to the low quality and resolution of the input videos, which lead to many mixed pixels that confuse the bilateral model. We also note that many of these methods were optimized with this dataset in mind, using different parameter settings per sequence. Instead, we use the same parameter settings for all three datasets. We also believe that the more recent datasets from JumpCut and our additional videos provide a more contemporary representation of video segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Interactive Segmentation</head><p>It is important to note that while our method scores well on these two higher-resolution benchmarks, the real advantage is the fast running time, when used in an interactive framework. To demonstrate this, we built a simple prototype editor <ref type="figure" target="#fig_7">(Figure 7)</ref> in Matlab that allows a user to draw strokes on an image to mark foreground or background re-  <ref type="table">Table 4</ref>. Comparison of our method on the SegTrack dataset <ref type="bibr" target="#b34">[34]</ref>, using the IoU metric (higher is better).</p><p>gions. After every stroke, the newly marked pixels are splatted to the bilateral grid and a global spatio-temporally solution is computed. Finally, the mask is sliced from the current frame and its outline is overlaid on the image. Please see the supplemental video for an example of this interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In summary, we have shown how simple and wellunderstood video segmentation techniques leveraging graph cuts can yield state-of-the-art results when performed in bilateral space.</p><p>There are many exciting avenues for extending the research in this area. For example, one could consider alternate, more descriptive feature spaces in the lifting step. We made some initial experiments with using patches, and obtained marginally better results, but at the expense of higher running time. Additionally, while the bilateral representation can handle some degree of motion, it does not explicitly account for camera or object motion. One possibility is to warp pixels using their optical flow before splatting. Our initial experiments indicated that due to the instability of flow, such methods were unreliable; sometimes leading to large improvements in quality, but in other times made the results worse. These methods also rely on precomputing optical flow, which is costly. Nonetheless, explicitly exploring scene motion is a promising venue to future work.</p><p>Despite this, we believe that the method as presented here has many attractive qualities. It is simple to implement, parallelizable, and fast, all without sacrificing quality. This efficiency gain is not only vital to providing faster feedback to users, but is also important for extending to low computational power (mobile) devices, or large scale (cloud-based) problems, which will hopefully enable new applications. <ref type="figure">Figure 6</ref>. Qualitative video segmentation results from three sequences of DAVIS <ref type="bibr" target="#b26">[26]</ref> (horsejump, stroller and soapbox). The segmentation is computed non-interactively, given the first frame as initialization. Our method demonstrates robustness to challenging scenarios such as complex objects, fast-motion, and occlusions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Our pipeline, demonstrated on a 1D example. Pixels are lifted into a 2D feature space (a), with two user assigned labels (red and green highlighted pixels). Values are accumulated on the vertices of a regular grid (b), a graph cut label assignment is computed on these vertices (c), and finally pixel values are sliced at at their original locations (c), showing the final segmentation (again, red and green boundaries).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Different interpolation schemes. Adjacent interpolation scales significantly better to higher dimensionality when compared to multi-linear interpolation, with only a small reduction in quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5</head><label>5</label><figDesc>Figure 5. This plot shows how IoU (higher is better) decreases when a single mask is propagated over increasing numbers of frames. Our method degrades favorably when compared to other approaches. The NLC approach stays constant as it is an automatic method that doesn't depend on the input of the first frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Our interactive segmentation editor. Very simple input (a) is sufficient to infer an accurate foreground mask (b) and track it over time. As a new object enters the scene (c), the user can choose to add it to the foreground with an additional input stroke (d). The mask is then automatically propagated to to the other frames (e-h) without further corrections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Q BVS S SEA JMP NLC HVSTable 1. Approximate running time per frame for a number of fast methods with code available. Ours is roughly an order of magnitude faster than prior methods, and scales linearly with image size. NLC has mostly constant running time because it uses a fixed number of superpixels.</figDesc><table>480p 
0.37s 
0.15s 
6s 
12s 
20s 
5s 
1080p 
1.5s 
0.8s 
30s 
49s 
20s 
24s 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gaussian KD-trees for fast high-dimensional filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keyframe-based tracking for rotoscoping and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video SnapCut: robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast bilateral-space stereo for synthetic defocus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in N-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Real-time edge-aware image processing with the bilateral grid. SIGGRAPH</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video Matting Using Multiframe Nonlocal Matting Laplacian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video matting of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive video segmentation using occlusion boundaries and temporally coherent superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dondera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">784</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Video Segmentation by Non-Local Consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<idno>21.1-21.12</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">JumpCut: Non-Successive Mask Transfer and Interpolation for Video Cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Coarse-to-Fine Semantic Video Segmentation Using Supervoxel Trees. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classifier Based Graph Construction for Video Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic Graph Cuts for Efficient Inference in Markov Random Fields. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2079" to="2088" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical temporal consistency for image-based graphics applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video object cut and paste</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="595" to="600" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Fast Approximation of the Bilateral Filter Using a Signal Processing Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LIVEcut: Learningbased interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interactive Segmentation of High-Resolution Video Content Using Temporally Coherent Superpixels and Graph Cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scheuermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jachalsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-Time Spatiotemporal Stereo Matching Using the Dual-Cross-Bilateral Grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Dodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">GrabCut&quot;: interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>SIG-GRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causal Video Object Segmentation From Persistence of Occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Motion Coherent Tracking with Multi-label MRF optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cohen. Interactive video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="585" to="594" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discontinuityaware video object cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
