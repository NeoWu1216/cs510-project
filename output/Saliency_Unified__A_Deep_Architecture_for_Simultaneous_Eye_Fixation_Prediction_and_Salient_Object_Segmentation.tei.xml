<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Saliency Unified: A Deep Architecture for simultaneous Eye Fixation Prediction and Salient Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><forename type="middle">S S</forename><surname>Kruthiventi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vennela</forename><surname>Gudisa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaley</forename><forename type="middle">H</forename><surname>Dholakiya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Saliency Unified: A Deep Architecture for simultaneous Eye Fixation Prediction and Salient Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human eye fixations often correlate with locations of salient objects in the scene. However, only a handful of approaches have attempted to simultaneously address the related aspects of eye fixations and object saliency. In this work, we propose a deep convolutional neural network (CNN) capable of predicting eye fixations and segmenting salient objects in a unified framework. We design the initial network layers, shared between both the tasks, such that they capture the object level semantics and the global contextual aspects of saliency, while the deeper layers of the network address task specific aspects. In addition, our network captures saliency at multiple scales via inceptionstyle convolution blocks. Our network shows a significant improvement over the current state-of-the-art for both eye fixation prediction and salient object segmentation across a number of challenging datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Among the various striking features of the human visual system, the ability to discriminate and selectively pay attention to a few regions in the scene over others, distinctly sets it apart. This phenomenon of selective visual attention has been a topic of interest for researchers in the fields of both neuroscience and computer vision over the past few decades <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Modelling this focus of attention, also termed as visual saliency, not only gives an insight into human vision, but also has various applications such as image retargetting <ref type="bibr" target="#b2">[3]</ref>, object recognition <ref type="bibr" target="#b3">[4]</ref>, visual tracking <ref type="bibr" target="#b4">[5]</ref>, foveated video compression <ref type="bibr" target="#b5">[6]</ref> etc.</p><p>Computational models for visual saliency often aim to solve one of the two problems -Predict the locations where an observer will fixate while free-viewing an image; Detect and segment the objects in a scene which grab our immediate attention. Eye fixation locations are considered to be indicative of the bottom-up visual attentional mechanism in humans. Models to predict fixation locations output a saliency map -'a topographical map representing the conspicuity of each pixel in the image' <ref type="bibr" target="#b6">[7]</ref>. The second task of salient object segmentation requires the generation of a pixel-accurate binary map indicating the presence of striking objects in the image. An example image with eye fixation and salient object segmentation maps generated by our model along with the ground-truth maps are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Recent studies have shown that the two tasks of eye fixation prediction and salient object segmentation are correlated <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Human eye fixations are often found to be guided by the locations of salient objects in the scene. This hypothesis of task correlation is bolstered by the work of Li et al. <ref type="bibr" target="#b8">[9]</ref> who used a simple eye fixation based model for segmenting salient objects in an image and achieved stateof-the-art results. Nevertheless, only a handful of the works in visual saliency attempt to solve these two problems together <ref type="bibr" target="#b8">[9]</ref>. In this work, we propose a deep network model which performs both these tasks simultaneously.</p><p>Early approaches for modeling saliency were driven by manually crafting features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> for over-segmented image regions and estimating their saliency using machine learning or optimization methods. However, recent advances in deep learning and the availability of large datasets have enabled models to perform end-to-end learning. Specifically, the success of Convolutional Neural Networks (CNNs) for various computer vision tasks, has led to a shift in focus from a paradigm of devising innovative features and techniques of combining them, to that of learning complex representations from data directly.</p><p>In this work, we propose a deep convolutional architecture for simultaneously predicting the human eye fixations and segmenting salient objects in an image. Our network has a branched architecture, where the shared layers, common to both the tasks, are designed to extract the crucial factors for saliency such as object level semantics and global context. The layers specialized for each of the tasks tap features from these shared layers using multi-scale convolution modules and process them further to obtain the final predictions. Our deep network has been evaluated on multiple datasets for both the tasks and is shown to achieve state-ofthe-art performance across multiple metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we discuss a few important works in the areas of visual saliency and deep convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual Saliency</head><p>The classic work of Itti et al. <ref type="bibr" target="#b1">[2]</ref> considers low-level features such as color and edge orientation at multiple scales which are combined using a neural network to predict saliency maps. Bruce et al. <ref type="bibr" target="#b11">[12]</ref> attributed saliency to image patches using the criterion of maximizing the selfinformation derived from color based features. Another landmark work in saliency, by Harel et al. <ref type="bibr" target="#b12">[13]</ref>, obtained pixel saliency values by calculating equilibrium distribution of Markov chains constructed over image maps generated from low level features. In addition to low-level features, Judd et al. <ref type="bibr" target="#b13">[14]</ref> proposed a learning based approach which also uses high-level features from person and face detectors. Borji et al. <ref type="bibr" target="#b14">[15]</ref> examined the role of additional high-level descriptors such as the presence of text and cars in predicting saliency.</p><p>While early saliency works were primarily aimed towards generating saliency maps for predicting eye fixation locations, the works by Liu et al. <ref type="bibr" target="#b15">[16]</ref> and Achanta et al. <ref type="bibr" target="#b16">[17]</ref> introduced the notion of object level saliency. These works defined immediate attention-grabbing objects in a scene as salient objects and formulated the problem of salient object segmentation to predict a pixel-accurate binary mask of the salient objects in a given test image. Achanta et al. <ref type="bibr" target="#b16">[17]</ref> proposed a frequency domain approach for segmenting salient objects using low-level features of color and luminance. Perazzi et al. <ref type="bibr" target="#b9">[10]</ref> obtained an abstract image representation having homogeneous regions by removing unnecessary details and assigned saliency scores based on the factors of a region's uniqueness and spatial distribution. Segmentation by assigning saliency scores to over-segmented image regions (super-pixels) using various priors (background <ref type="bibr" target="#b17">[18]</ref>, objectness <ref type="bibr" target="#b18">[19]</ref>) has been another popular approach.</p><p>There have been only a few works in visual saliency which have explored the relation between eye fixations and salient objects. Mishra et al. <ref type="bibr" target="#b19">[20]</ref> segmented salient objects using fixation points as identification markers on objects and found an optimal contour around the fixation points. Li et al. <ref type="bibr" target="#b8">[9]</ref> proposed a salient object segmentation model which used the saliency maps from existing fixation prediction algorithms to classify image regions marked by an object proposal algorithm as a salient object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Convolutional Nets</head><p>Deep Convolutional Networks, popularized by the seminal work of Krizhevsky et al. <ref type="bibr" target="#b20">[21]</ref>, brought a paradigm shift in vision research from hand-crafting of features to learning them from data. While initial deep networks were aimed towards image classification, they were successfully adapted to pixel-level image tasks like semantic object segmentation <ref type="bibr" target="#b21">[22]</ref> and depth estimation <ref type="bibr" target="#b22">[23]</ref>. Fully convolutional nets are a particular flavour of CNNs designed to make structured predictions on the image grid. They were first used by Long et al. <ref type="bibr" target="#b21">[22]</ref> in their work of semantic object segmentation. Long et al. converted fully connected layers in existing image classification nets into convolutional layers for obtaining pixel-level predictions. Further, they introduced novel deconvolutional layers for making predictions at the original image resolution. Alternately, Chen et al. <ref type="bibr" target="#b23">[24]</ref>, adopted a simpler approach for retaining spatial resolution to the extent possible, by removing stride in some of the constituent layers of the network. They also introduced convolutional layers with holes which allowed the filters to have receptive fields larger than their kernel sizes. In our network, we use these layers with holes in order to capture the global context of the scene.</p><p>Recently, in the realm of salient object segmentation, Zhao et al. <ref type="bibr" target="#b24">[25]</ref> proposed a multi-context approach using deep convolutional networks. They considered two different networks operating in parallel, over a subsampled and upsampled image patch around each superpixel. The network operating on subsampled image region was considered to capture global context while the network operating on upsampled region captured local context. The output features from the two networks were concatenated for determining the saliency of the corresponding superpixel.</p><p>In eye fixation prediction, Liu et al. <ref type="bibr" target="#b25">[26]</ref> took an approach similar to that of Zhao et al. <ref type="bibr" target="#b24">[25]</ref> by considering multiple convolutional networks, each operating at a particular scale in the image pyramid representation. This construction of multiple CNNs was termed as Multiresolution-CNN and was found to be efficient at characterizing the low-level and high-level semantics of the image.</p><p>In contrast to the works of <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, our model captures the semantic context at various levels efficiently through a single network, by leveraging intermediate representations in the deep feature hierarchy for detecting saliency. The multi-scale aspects of saliency are captured through convolutional kernels of different sizes operating in parallel.</p><formula xml:id="formula_0">CONV-1 2 Conv. 1 Max-pool CONV-2 2 Conv. 1 Max-pool CONV-3 3 Conv. 1 Max-pool CONV-4 3 Conv. 1 Max-pool CONV-5 3 Conv. 1 Max-pool CONV-6 3 Conv. 1 Max-pool Fix.Conv 3 Conv.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seg.Conv</head><p>3 Conv. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fix.Final</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><p>We propose a fully convolutional deep network with a branched architecture for simultaneously predicting eye fixations and segmenting salient objects. Layers specialized to these two tasks branch out from a central shared pipeline in the network. This shared pipeline comprises of a series of 6 convolution blocks (shown in gray bounding box in <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><p>Inspired from VGG-16 <ref type="bibr" target="#b26">[27]</ref>, the layers in first five blocks (CONV-1 to CONV-5) of the shared pipeline have small kernels of spatial size 3 × 3. Small kernels allow the network to have a very deep architecture with a low memory requirement while making the model more discriminative. All the five convolution blocks (CONV-1 to CONV-5) end with a maxpool layer and every convolutional layer in the network is followed by a ReLU non-linear activation. The architectural details of these 5 convolution blocks are described in <ref type="table">Table 1</ref>. In the VGG-16 network, the spatial dimensions of the data blob are halved after each block using a stride of 2 in the block's max-pool layer. This strategy of spatially subsampling the data blob is crucial in classification networks for keeping the computational demand low, as data blobs tend to have a large number of channels (usually &gt; 1000) at fully connected layers to cater to the large number of classes. However, for fully convolutional networks, the spatial resolution of the output blob is also important as they are primarily trained for per-pixel recognition tasks. We retain the spatial resolution of the data blob at 1/8 times that of the original image after the third convolution block (CONV-3). We accomplish this by reducing the stride value from 2 to 1 in the max-pool layers of fourth and fifth convolution blocks (CONV-4, CONV-5).</p><p>During training, the first five convolution blocks (CONV-1 to CONV-5) of our network are initialized with weights of the VGG-16 network, which was originally trained over 1.3 million images of the ImageNet <ref type="bibr" target="#b27">[28]</ref> dataset. In the VGG-16 net, the filters of the fifth block (CONV-5) were trained to operate on a data blob with a resolution of 1/16 times the original image, unlike our network where the blob has a resolution of 1/8 times the image. We handle this scale mismatch by introducing holes of size 2 in filters of the fifth block which doubles their receptive field <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>. This allows the convolutional layers in fifth block to operate on the blob at the scale that they were originally trained for. We refer the readers to <ref type="bibr" target="#b23">[24]</ref> for a more elaborate description of convolutional layers with holes.</p><p>Capturing Global Context: Saliency is the distinctive quality of an entity which makes it stand out from its neighbors and captures our immediate attention <ref type="bibr" target="#b29">[30]</ref>. Efficient detection of these salient regions in an image would require the model to capture the global context of the image before assigning scores to its individual regions. To facilitate this, we employ convolutional layers with very large receptive fields in the sixth convolution block (CONV-6). Each of the layers in this block operates with filters of kernel size 5 and hole size 5, thus achieving an effective receptive field of 21 × 21. Similar to the fourth and fifth convolution blocks, the sixth block also ends with a max-pool layer of stride 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Salient Object Segmentation</head><p>Salient object segmentation consists of two sub-tasks: detecting the salient objects in the image and determining the spatial extent of the object by identifying its boundaries. While detection of a salient object requires the image regions to be characterized using contextually rich semantic features, the task of finding the object's spatial extent requires lower level semantics like colour, contrast, texture and part composition. These two kinds of features are referred to as global and local contexts in a recent deep saliency work <ref type="bibr" target="#b24">[25]</ref>, where two different networks are used for extracting them. However, previous studies <ref type="bibr" target="#b30">[31]</ref> on deep architectures have shown that early layers in a convolutional network capture low-level image aspects while the later lay-ers capture high-level semantics. Our model captures features from both the local and global contexts efficiently using this inherent feature hierarchy present in deep networks.</p><p>Recently, Hariharan et al. <ref type="bibr" target="#b31">[32]</ref> have shown that information of interest for pixel-level tasks is spread across all the layers of a convolutional network. They introduced the concept of hypercolumns, defined as the concatenation of features corresponding to a spatial location across all the layers of the deep network. These features were shown to be effective for fine-grained localization tasks. We extract features from the max-pool layers of CONV-2, CONV-4, CONV-5 and CONV-6 blocks for the task of salient object segmentation.</p><p>The features from these blocks are tapped using multiscale convolution kernels and are concatenated together. Li et al. <ref type="bibr" target="#b32">[33]</ref> and Zhao et al. <ref type="bibr" target="#b24">[25]</ref> have observed that saliency can be captured better when semantics are considered across multiple scales by upsampling and downsampling image patches. Inspired by the recent success of GoogLeNet <ref type="bibr" target="#b33">[34]</ref>, we capture this multi-scale semantic information using inception modules. Each inception module operates on its input feature maps with filters of different receptive fields i.e., 1×1, 3×3 and 5×5 capturing information from multiple scales. In order to reduce the computational costs, we replace the usual 5 × 5 kernel with a 3 × 3 kernel with 2 holes which will result in an effective receptive field of 5 × 5. We also reduce the number of channels in the inputs to 3 × 3 and 5 × 5 layers in the inception modules, using a 1 × 1 layer, similar to <ref type="bibr" target="#b33">[34]</ref>. Apart from reducing the computational load, these 1 × 1 layers aid in introducing additional non-linearity.</p><p>Using inception modules to extract features from the intermediate layers, we obtain a multi-scale representation of the hierarchical deep features. Also, having multiple pathways via the inception modules (INCP-1 to INCP-5) provides a plausible solution to the issue of vanishing gradients while back-propagating error through the network <ref type="bibr" target="#b33">[34]</ref>.</p><p>The concatenated output from the inception modules is fed to a block (Seg.Conv) with three convolutional layers each of a kernel size 3 × 3. The resulting output is fed to a 1 × 1 convolutional layer (Seg.Final) to predict the object saliency map at a spatial resolution of 1/8 times the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Predicting Eye Fixations</head><p>The second task of predicting eye fixation saliency maps requires the model to estimate the saliency score for every pixel in a given test image. The ground-truth saliency map for this task is generated by blurring the observer fixation locations on the image with a Gaussian kernel of a constant variance <ref type="bibr" target="#b7">[8]</ref>. This blurring is done to take care of the noise in eye tracker equipment and the saccade landing of the observer. These saliency maps generally tend to be blurry, and do not have sharp boundaries unlike the groundtruth of  <ref type="table">Table 1</ref>. Architectural details of the proposed deep convolutional network for segmenting salient objects and predicting eye fixations salient object segmentation <ref type="bibr" target="#b7">[8]</ref>.</p><p>The assignment of saliency scores requires characterizing local regions in an image with semantic features while incorporating the global context of the entire scene <ref type="bibr" target="#b28">[29]</ref>. The layers in CONV-6 block, owing to their large receptive fields (21 × 21), can provide contextually rich semantic features necessary for estimating the saliency score of local image regions. Following the multi-scale approach described earlier for salient object segmentation, we tap the max-pool layer of the CONV-6 block using an inception module with layers of receptive fields: 1 × 1, 3 × 3 and 5 × 5. The output from this inception module is fed to a block with three convolutional layers (Fix.Conv) of spatial size 3 × 3, which is followed by a 1 × 1 convolution layer (Fix.Final) for estimating the fixation saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Refining Saliency Maps</head><p>Our network predicts the saliency maps at a sub-sampled resolution of 1/8 times the original image resolution.</p><p>Ground-truth saliency maps for eye fixations are usually smooth. Hence, we directly interpolate the network's fixation saliency output, using bi-cubic interpolation to obtain the final saliency map.</p><p>In the case of salient object segmentation, the groundtruth maps are binary and have sharp edges at the object boundaries. Since the network's salient object predictions are coarse in resolution, we use the fully connected Conditional Random Field (CRF) formulation of Phillip et al. <ref type="bibr" target="#b34">[35]</ref> to obtain the final pixel-accurate segmentation prediction.</p><p>We construct a dense graph on the image grid at its original resolution by considering each pixel as a node. The unary costs for a node to take the labelssalient and background are defined using the network's object saliency map prediction. The object saliency map is bicubic interpolated to the original image resolution and transformed using a sigmoid activation to obtain a pixel-level saliency scores. This saliency score of a pixel is assumed to be the unary cost for the corresponding graph node to take the background label. The additive inverse of this object saliency map is taken to be the unary cost for the salient label.</p><p>We use the pair-wise formulation of Phillip et al. <ref type="bibr" target="#b34">[35]</ref> for defining the pair-wise cost between two nodes. This formulation connects every pixel in the image to every other pixel with an edge resulting in a densely connected graph. The pair-wise cost for two nodes taking different labels is defined as a function of the corresponding pixels' color similarity and spatial proximity. Specifically, the pair-wise cost ψ(i, j) for two nodes i, j is defined as</p><formula xml:id="formula_1">ψ(i, j) = w a exp appearance kernel − |p i − p j | 2 σ 2 ap − |I i − I j | 2 σ 2 ai +w s smoothness kernel exp − |p i − p j | 2 σ 2 sp</formula><p>Here w a , w s indicate the relative weights and σ ap , σ ai , σ sp are the standard deviation values of the Gaussian kernels in the appearance and smoothness terms. p i , p j are the position vectors and I i , I j are the RGB vectors of the pixels i, j.</p><p>The overall energy to be minimized, which is a combination of unary φ u (i) and pair-wise ψ p (i, j) terms, can be expressed as</p><formula xml:id="formula_2">E G =   ∀i∈V G φ u (i) + ∀(i,j)∈C G ψ p (i, j)  </formula><p>where V G and C G denote the nodes and edges in the constructed dense graph respectively.</p><p>Every pixel is binary classified into the labels of salient and background by minimizing the above energy using the approach of mean-field approximation <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Network Training</head><p>We train the proposed network on MSRA10K <ref type="bibr" target="#b16">[17]</ref> and SALICON <ref type="bibr" target="#b37">[38]</ref> datasets. MSRA10K dataset comprises of 10, 000 images picked from a variety of scenarios -natural scenes, animals, indoor, outdoor, etc. Each of these images is provided with a pixel-accurate ground truth binary mask indicating the salient object and is used for training the network for segmentation task. SALICON is a saliency dataset with 15, 000 images where eye fixation annotations are simulated through mouse movements of users on blurred images. The authors of <ref type="bibr" target="#b37">[38]</ref> show that the mousecontingent saliency annotations strongly correlate with actual eye-tracker annotations. We use the SALICON dataset for training the network to predict eye fixations .</p><p>For training the network, we use a mini-batch of 8 images in each iteration, 4 of which have segmentation ground truth and the rest have fixation ground truth. CONV-1 to CONV-6 blocks, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, are shared between both the tasks of salient object segmentation and eye fixation prediction and are trained for both the tasks simultaneously using all the images in the batch. The layers of the network specialized to each of the tasks are trained using only those images in the batch having the corresponding ground-truth.</p><p>The first five convolution blocks (CONV-1 to CONV-5) in the network are initialized from the weights of VGG-16 <ref type="bibr" target="#b26">[27]</ref>. The weights in all the other convolutional layers and inception blocks are initialized from zero mean Gaussian with a standard deviation of 0.01 and the biases are set to 0. The layers in (CONV-1 to CONV-5), whose weights are initialized from VGG-16, are trained with a learning rate of 5 × 10 −8 while the rest of the layers in the network are trained with a higher learning rate of 5 × 10 −7 .</p><p>Before feeding the input images and ground-truth maps to the network, we scale the images such that the larger dimension is 417 and zero pad along the smaller dimension to bring the image to a fixed size of 417 × 417 pixels. The network is trained using stochastic gradient descent with a momentum of 0.9. The entire training procedure takes about 1 day for completion on Nvidia TITAN X GPU with deeplab version <ref type="bibr" target="#b23">[24]</ref> of caffe deep learning framework <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets and Evaluation</head><p>We evaluate the proposed approach on multiple datasets with images from a wide-variety of scenarios and varying resolutions, number of objects and level of background clutter. A good model should perform well consistently, on most of the datasets. PASCAL-S <ref type="bibr" target="#b8">[9]</ref>, DUT-OMRON <ref type="bibr" target="#b39">[40]</ref>, iCoSeg <ref type="bibr" target="#b40">[41]</ref> and ECSSD <ref type="bibr" target="#b41">[42]</ref> datasets consisting of 850, 5168, 643 and 1000 images respectively are used for evaluating the model on the task of salient object segmentation. PASCAL-S <ref type="bibr" target="#b8">[9]</ref>, DUT-OMRON <ref type="bibr" target="#b39">[40]</ref>, MIT1003 <ref type="bibr" target="#b13">[14]</ref> Image SF <ref type="bibr" target="#b9">[10]</ref> PCA <ref type="bibr" target="#b35">[36]</ref> DRFI <ref type="bibr" target="#b10">[11]</ref> SMD <ref type="bibr" target="#b36">[37]</ref> MDF <ref type="bibr" target="#b32">[33]</ref> Proposed Proposed GT (CNN) (CNN+CRF) and IS <ref type="bibr" target="#b42">[43]</ref> datasets consisting of 850, 5168, 1003 and 235 images respectively are used for evaluating fixation prediction. We used Mean Absolute Error (MAE) and Weighted F β -Measure to evaluate the performance of our network for salient object segmentation. Earth Mover's Distance (EMD), Normalized Scanpath Saliency (NSS) and the shuffled-Area Under Curve (s-AUC) were used for evaluating the performance on eye-fixation prediction. We briefly describe each of these metrics in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Salient Object Segmentation</head><p>Mean Absolute Error (MAE) : MAE is computed as the mean of pixel-wise absolute difference between the continuous object saliency map and the binary ground-truth . <ref type="bibr" target="#b43">[44]</ref> evaluates a binarized map with respect to ground truth based on weighted precision and recall values. It combines these two values into a single number by taking their weighted harmonic mean. Similar to other works, we consider β 2 = 0.3, thereby giving more importance to precision.</p><formula xml:id="formula_3">Weighted F β -Measure (F w β ) : Weighted F β -measure</formula><p>For binarizing the object saliency map to obtain the salient object segmentation, we follow the procedure described in <ref type="bibr" target="#b44">[45]</ref>. Initially, the saliency map is binarized by thresholding at various intermediate values in the range of [0 255] and the F w β is computed for each of them. We use the mean and maximal F w β values to evaluate the salient object segmentation capabilities of a model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Predicting Eye Fixations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Salient Object Segmentation</head><p>The quantitative results obtained by the proposed method on PASCAL-S, DUT-OMRON, iCoSeg and ECSSD datasets for salient object segmentation are shown in <ref type="table">Table 2</ref>. We compare our model against the methods -SF <ref type="bibr" target="#b9">[10]</ref>, PCA <ref type="bibr" target="#b35">[36]</ref>, DRFI <ref type="bibr" target="#b10">[11]</ref>, SMD <ref type="bibr" target="#b36">[37]</ref> and MDF <ref type="bibr" target="#b32">[33]</ref>with respect to the metrics discussed in Sec. 5.2.1.</p><p>As evident from the table, our model achieves state of the art results on all the datasets across these metrics. The Image ITTI <ref type="bibr" target="#b1">[2]</ref> GBVS <ref type="bibr" target="#b12">[13]</ref> AWS <ref type="bibr" target="#b45">[46]</ref> BMS <ref type="bibr" target="#b46">[47]</ref> eDN <ref type="bibr" target="#b47">[48]</ref> Junting <ref type="bibr" target="#b48">[49]</ref> Proposed GT qualitative results for salient object segmentation are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. As shown in the figure, our method performs well in a variety of challenging cases, e.g., multiple disconnected objects (fourth row), low contrast between object and background (second row), cluttered background (sixth row). We can also see that our method captures local features like edges and boundaries quite well (fifth row) compared to other methods. In the first image, while most of the existing methods fail to predict the person's legs as salient, our model correctly identifies the entire person as salient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Predicting Eye-Fixations</head><p>The quantitative results obtained by the proposed method on PASCAL-S, DUT-OMRON, MIT1003 and IS datasets for eye-fixation prediction are shown in <ref type="table">Table 3</ref>. We quantify our results in terms of the previously discussed EMD, NSS and s-AUC metrics. Results illustrate that our method outperforms existing methods with respect to NSS and EMD by a huge margin and achieves state of art results with respect to s-AUC as well.</p><p>The qualitative results for eye fixation prediction are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. The proposed network is able to detect saliency arising from faces of both humans (third row) and animals (fourth row) efficiently. From the images in fifth and sixth rows, we also note that our model can correctly detect the text and sign boards as salient. Our network also captures multiple salient objects (third row) and weighs their relative importance in the scene appropriately.</p><p>In addition to the above four datasets, we further evaluate our model on another large-scale test set -SALICON which comprises of 5000 test images. We obtain the metric scores on this dataset by submitting our fixation predictions to the SALICON challenge website. Results, shown in <ref type="table">Table. 4</ref> illustrate that our method outperforms the winner of LSUN Saliency Challenge 2015, JuntingNet <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>, a CNN based model, by a significant margin. For both the tasks of eye fixation prediction and salient object segmentation, our model has been evaluated in a cross-dataset manner i.e., the train and test sets have been taken from different datasets. Inspite of this, our model performs consistently well across various metrics on both the tasks highlighting its generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Simultaneous versus Independent Training</head><p>In order to understand the effect of simultaneously training the network, we train relevant parts of the network independently for the tasks of salient object segmentation and eye fixation prediction. We compare the results from these independently trained models with the proposed simultaneously trained model on DUT-OMRON dataset. Results, shown in <ref type="table">Table.</ref> 5, illustrate that simultaneously training the network retains the performance of independently trained models across MAE, sAUC metrics while giving a small improvement across F w β , EMD, NSS metrics. Also, a model which can simultaneously predict eye fixations and segment salient objects is computationally efficient compared to independent models as the former shares the low-level feature computations for both tasks.  <ref type="table">Table 3</ref>. Quantitative results of our approach on eye fixation prediction compared against other state-of-the-art methods on PASCAL-S, DUT-OMRON, MIT1003 and IS datasets. The best results are shown in red and the second best in blue. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSUN Saliency Challenge 2015 -SALICON</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have proposed a novel deep convolutional architecture capable of simultaneously predicting human eye fixations and segmenting the salient objects in an image. Our network captures the global context, which is crucial for saliency, through layers with large receptive fields and handles multi-scale aspects of saliency using inception modules. Also, our network has a branched architecture to efficiently capture both the low-level and highlevel semantics necessary for salient object segmentation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustrative images (a) with their corresponding eye fixation predictions (b), groundtruth (c) and salient object segmentation predictions (d), groundtruth (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Architecture overview of the proposed network for simultaneously predicting human eye fixations and segmenting salient objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results of our approach along with other state-of-the-art methods for salient object segmentation. Proposed (CNN) refers to the results from the CNN alone, whereas the Proposed (CNN+CRF) refers to the final binary segmentation results obtained after refining the maps obtained from CNN using CRF. The object saliency maps of other state-of-the-art methods and Proposed (CNN) are thresholded such that their F w β values are maximized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Earth Mover's Distance (EMD) : EMD considers the ground-truth and predicted saliency maps to be two probability distributions and measures the cost of transforming one distribution to the other. Normalized Scanpath Saliency (NSS) : Normalized Scanpath Saliency is the average of the response values at eye fixation locations in a model's saliency map, normalized to have zero mean and unit standard deviation. shuffled -Area Under Curve (s-AUC) : sAUC is the area under the ROC curve of true positives vs. false positives during the binary classification of fixation and non-fixation points using saliency map at various thresholds. The nonfixation points are taken from fixations on other images in the dataset to tackle the issue of centre-bias in eye fixations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of our approach along with other state-of-the art methods for eye fixation prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 2. Quantitative results of our approach on salient object segmentation compared against other state-of-the art methods on PASCAL-S, DUT-OMRON, iCoSeg and ECSSD datasets. Proposed (CNN) refers to the results using the CNN alone, whereas the Proposed (CNN+CRF) refers to the final results obtained after refining the CNN's output using CRF. The best results are shown in red and the second best in blue. ↑ indicates higher scores on the metric are better and ↓ indicates lower scores on the metric are better.</figDesc><table>Dataset 

Metric 
SF 
PCA 
DRFI 
SMD 
MDF 
Proposed 
Proposed 
[10] 
[36] 
[11] 
[37] 
[33] 
(CNN) 
(CNN+CRF) 

MAE ↓ 
0.26 
0.25 
-
0.21 
0.15 
0.12 
0.10 
PASCAL-S [9] 
mean F w 
β ↑ 
0.33 
0.35 
-
0.49 
0.64 
0.75 
0.77 
max F w 
β ↑ 
0.43 
0.49 
-
0.56 
0.68 
0.78 
0.77 
MAE ↓ 
0.27 
0.21 
0.14 
0.17 
0.09 
0.09 
0.07 
DUT-OMRON [40] 
mean F w 
β ↑ 
0.30 
0.34 
0.48 
0.45 
0.59 
0.65 
0.68 
max F w 
β ↑ 
0.42 
0.46 
0.58 
0.53 
0.61 
0.69 
0.68 
MAE ↓ 
0.25 
0.20 
0.14 
0.14 
0.10 
0.10 
0.08 
iCoSeg [41] 
mean F w 
β ↑ 
0.36 
0.42 
0.58 
0.61 
0.67 
0.76 
0.79 
max F w 
β ↑ 
0.54 
0.58 
0.67 
0.68 
0.74 
0.77 
0.79 
MAE ↓ 
0.29 
0.25 
0.16 
0.17 
0.11 
0.08 
0.06 
ECSSD [42] 
mean F w 
β ↑ 
0.33 
0.39 
0.59 
0.58 
0.74 
0.85 
0.88 
max F w 
β ↑ 
0.46 
0.54 
0.71 
0.67 
0.77 
0.87 
0.88 

Dataset 
Metric 
ITTI 
GBVS 
AWS 
BMS 
eDN 
MrCNN JuntingNet 
Proposed 
[2] 
[13] 
[46] 
[47] 
[48] 
[26] 
[49] 

s-AUC ↑ 
0.64 
0.65 
0.67 
0.67 
0.65 
-
0.69 
0.72 
PASCAL-S [9] 
EMD ↓ 
1.21 
1.16 
1.38 
1.32 
1.29 
-
1.03 
0.73 
NSS ↑ 
1.30 
1.36 
1.12 
1.28 
1.42 
-
1.90 
2.22 
s-AUC ↑ 
0.78 
0.81 
0.78 
0.79 
0.80 
-
0.83 
0.83 
DUT-OMRON [40] 
EMD ↓ 
1.47 
1.32 
1.62 
1.58 
1.56 
-
1.37 
1.03 
NSS ↑ 
1.54 
1.71 
1.51 
1.66 
1.33 
-
2.03 
3.02 
s-AUC ↑ 
0.66 
0.66 
0.69 
0.69 
0.66 
0.71 
0.68 
0.73 
MIT1003 [14] 
EMD ↓ 
2.33 
2.19 
2.54 
2.40 
2.39 
2.30 
1.91 
1.49 
NSS ↑ 
1.06 
1.17 
1.07 
1.19 
1.24 
1.28 
1.60 
2.08 
s-AUC ↑ 
0.66 
0.67 
0.72 
0.71 
0.61 
-
0.65 
0.70 
IS [43] 
EMD ↓ 
1.30 
1.22 
1.49 
1.43 
1.49 
-
1.11 
0.77 
NSS ↑ 
1.50 
1.58 
1.58 
1.74 
1.27 
-
1.72 
2.30 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Methods-AUC↑ CC↑ AUC-Borji↑ NSS↑Table 4. Quantitative results of our approach on SALICON Test set compared against JuntingNet -the winner of LSUN 2015 Saliency Challenge. The best results are shown in red.</figDesc><table>Proposed 
0.76 
0.78 
0.88 
2.61 
JuntingNet [49] 
0.67 
0.60 
0.83 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Simultaneous vs. Independent TrainingSegmentationFixation Method MAE ↓ F w β ↑ sAUC ↑ EMD ↓ NSS ↑Table 5. Quantitative Results on DUT-OMRON dataset when the networks are trained simultaneously versus independently for the tasks of eye fixation prediction and salient object segmentation. The best results are shown in red.</figDesc><table>Simul. 
0.07 
0.68 
0.83 
1.03 
3.02 
Indp. 
0.07 
0.67 
0.83 
1.07 
2.80 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We evaluate our method on four datasets of eye fixation prediction and salient object segmentation and show that it outperforms the existing state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work was supported by Defence Research and Development Organization (DRDO), Government of India. We thank Nvidia for their hardware grant and Google for the travel grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MUM</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminant saliency for visual recognition from cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Saliency-based discriminant tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic foveation for video compression using a neurobiological model of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1304" to="1318" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Saliency map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What is a salient object? a dataset and a baseline model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="742" to="756" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Boosting bottom-up and top-down visual features for saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Learning to detect a salient object</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object saliency using a background prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Salient object detection via objectness measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions On</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="653" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Active visual segmentation</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting eye fixations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deepfix: A fully convolutional neural network for predicting human eye fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02927</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual salience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3327</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08663</idno>
		<title level="m">Visual saliency based on multiscale deep features</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient inference in fully connected CRFs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.5644</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What makes a patch distinct?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Salient object detection via structured matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">iCoSeg: Interactive co-segmentation with intelligent scribble guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual saliency based on scale-space analysis in the frequency domain,&quot; Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="996" to="1010" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the relationship between optical variability, visual saliency, and eye fixations: A computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Leborán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">R</forename><surname>Fdez-Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Saliency detection: A boolean map approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">End-to-end convolutional network for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giró-I Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01422</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
