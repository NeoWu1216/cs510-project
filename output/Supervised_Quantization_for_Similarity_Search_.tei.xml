<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Quantization for Similarity Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised Quantization for Similarity Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the problem of searching for semantically similar images from a large database. We present a compact coding approach, supervised quantization. Our approach simultaneously learns feature selection that linearly transforms the database points into a low-dimensional discriminative subspace, and quantizes the data points in the transformed space. The optimization criterion is that the quantized points not only approximate the transformed points accurately, but also are semantically separable: the points belonging to a class lie in a cluster that is not overlapped with other clusters corresponding to other classes, which is formulated as a classification problem. The experiments on several standard datasets show the superiority of our approach over the state-of-the art supervised hashing and unsupervised quantization algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Similarity search has been a fundamental research topic in machine learning, computer vision, and information retrieval. The goal, given a query, is to find the most similar item from a database, e.g., composed of N d-dimensional vectors. The recent study shows that the compact coding approach, including hashing and quantization, is advantageous in terms of memory cost, search efficiency, and search accuracy.</p><p>The compact coding approach converts the database items into short codes in which the distance is efficiently computed. The objective is that the similarity computed in the coding space is well aligned with the similarity that is computed based on the Euclidean distance in the input space, or that comes from the given semantic similarity (e.g., the data items from the same class should be similar). The solution to the former kind of similarity search is unsupervised compact coding, such as hash- * This work was partly done when Xiaojuan Wang and Ting Zhang were interns at MSR. They contributed equally to this work.</p><p>ing <ref type="bibr">[1, 5-7, 9-12, 16, 20, 22, 28, 29, 34, 36-38]</ref> and quantization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>. The solution to the latter problem is supervised compact coding, which is our interest in this paper.</p><p>Almost all research efforts in supervised compact coding focus on developing hashing algorithms to preserve semantic similarities, such as LDA Hashing <ref type="bibr" target="#b29">[30]</ref>, minimal loss hashing <ref type="bibr" target="#b23">[24]</ref>, supervised hashing with kernels <ref type="bibr" target="#b20">[21]</ref>, FastHash <ref type="bibr" target="#b17">[18]</ref>, triplet loss hashing <ref type="bibr" target="#b24">[25]</ref>, and supervised discrete hashing <ref type="bibr" target="#b26">[27]</ref>. In contrast, there is less study in quantization, which however already shows the superior performance for Euclidean distance and cosine-based similarity search. This paper makes a study on the quantization solution to semantic similarity search.</p><p>Our main contributions are as follows: (i) We propose a supervised composite quantization approach. To the best of our knowledge, our method is the first attempt to explore quantization for semantic similarity search. The advantage of quantization over hashing is that the number of possible distances is significantly higher, and hence the distance approximation, accordingly the similarity search accuracy, is more accurate. (ii) Our approach jointly optimizes the quantization and learns the discriminative subspace where the quantization is performed. The criterion is the semantic separability: the points belonging to a class lie in a cluster that is not overlapped with other clusters corresponding to other classes, which is formulated as a classification problem. (iii) Our method significantly outperforms many stateof-the-art methods in terms of search accuracy and search efficiency under the same code length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There are two main research issues in supervised hashing: how to design hash functions and how to preserve semantic similarity. In essence, most algorithms can adopt various hash functions, e.g., an algorithm using a linear hash function usually can also use a kernel hash function. Our review of the supervised hashing algorithms focuses on the semantic similarity preserving manners. We roughly divide them into three categories: pairwise similarity preserving, multiwise similarity preserving, and classification.</p><p>Pairwise similarity preserving hashing aligns the similarity over each pair of items computed in the hash codes with the semantic similarity in various manners. Representative algorithms include LDA Hashing <ref type="bibr" target="#b29">[30]</ref>, minimal loss hashing <ref type="bibr" target="#b23">[24]</ref>, binary reconstructive embedding <ref type="bibr" target="#b14">[15]</ref>, supervised hashing with kernels <ref type="bibr" target="#b20">[21]</ref>, two-step hashing <ref type="bibr" target="#b18">[19]</ref>, FastHash <ref type="bibr" target="#b17">[18]</ref>, and so on. The recent work <ref type="bibr" target="#b3">[4]</ref>, supervised deep hashing, designs deep neural network as hash functions to seek multiple hierarchical non-linear feature transformations, and preserves the pairwise semantic similarity by maximizing the inter-class variations and minimizing the intra-class variations of the hash codes.</p><p>Multiwise similarity preserving hashing formulates the problem by maximizing the agreement of the similarity orders over more than two items between the input space and the coding space. The representative algorithms include order preserving hashing <ref type="bibr" target="#b33">[34]</ref>, which directly aligns the rank orders computed from the input space and the coding space, triplet loss hashing <ref type="bibr" target="#b24">[25]</ref>, listwise supervision hashing <ref type="bibr" target="#b31">[32]</ref>, and so on. Triplet loss hashing and listwise supervision hashing adopt different loss functions to align the similarity order in the coding space and the semantic similarity over triplets of items. The recent proposed deep semantic ranking based method <ref type="bibr" target="#b39">[40]</ref> preserves multilevel semantic similarity between multilabel images by jointly learning feature representations and mappings from them to hash codes.</p><p>The recently-developed supervised discrete hashing (SDH) algorithm <ref type="bibr" target="#b26">[27]</ref> formulates the problem using the rule that the classification performance over the learned binary codes is as good as possible. This rule seems inferior compared with pairwise and multiwise similarity preserving, but yields superior search performance. This is mainly thanks to its optimization algorithm (directly optimize the binary codes) and scalability (not necessarily do the sampling as done in most pairwise and multiwise similarity preserving algorithms). Semantic separability in our approach, whose goal is that the points belonging to a class lie in a cluster that is not overlapped with other clusters corresponding to other classes, is formulated as a classification problem, which can also be optimized using all the data points.</p><p>Our approach is a supervised version of quantization. The quantizer we adopt is composite quantization <ref type="bibr" target="#b38">[39]</ref>, which is shown to be a generalized version of product quantization <ref type="bibr" target="#b7">[8]</ref> and cartesian k-means <ref type="bibr" target="#b25">[26]</ref>, and achieves better performance. Rather than performing the quantization in the input space, our approach conducts the quantization in a discriminative space, which is jointly learned with the composite quantizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Formulation</head><p>Given a d-dimensional query vector q ∈ R d and a search database consisting of N d-dimensional vectors X = {x n } N n=1 with each point x n ∈ R d associated with a class label, denoted by a binary label vector y n ∈ {0, 1} C in which the 1-valued entry indicates the class label of x n , the goal is to find K vectors from the database X that are nearest to the query so that the found vectors share the same class label with the query. This paper is interested in the approximate solution: converting the database vectors into compact codes and then performing the similarity search in the compact coding space, which has the advantage of lower memory cost and higher search efficiency.</p><p>Modeling. We present a supervised quantization approach to approximate each database vector with a vector selected or composed from a dictionary of base items. Then the database vector is represented by a short code composed of the indices of the selected base items. Our approach, rather than directly quantizing the database vectors in the original space, learns to transform the database vectors to a discriminative subspace with a matrix P ∈ R d×r , and then does the quantization in the transformed space. We propose to adopt the state-of-the-art unsupervised quantization approach: composite quantization <ref type="bibr" target="#b38">[39]</ref>. Composite quantization approximates a vector x using the sum of M elements with each selected from a dictionary, i.e., x = M m=1 c mkm , where c mkm is selected from the mth dictionary with K elements C m = [c m1 c m2 · · · c mK ], and encodes x by a short code (k 1 k 2 · · · k M ). Our approach uses the sum to approximate the transformed vector, which is formulated by minimizing the approximation error,</p><formula xml:id="formula_0">P T x −x 2 2 = P T x − M m=1 c mkm 2 2 .<label>(1)</label></formula><p>We learn the transformation matrix P such that the quantized data points are semantically separable: the points belonging to the same class lie in a cluster, and the clusters corresponding to different classes are disjointed. We solve the semantic separation problem by finding C linear decision surfaces to divide all the points into C clusters 1 , each corresponding to a class, which is formulated as a classification problem given as follows,</p><formula xml:id="formula_1">N n=1 ℓ(y n , W Tx n ) + λ W 2 F ,<label>(2)</label></formula><p>where λ is the parameter controlling the regularization term W 2 F ; W = [w 1 w 2 · · · w C ] ∈ R r×C ; ℓ(·, ·) is a classification loss function to penalize the cases where the point x n is not assigned to the cluster corresponding to y n based on the C associated decision functions {w T kx n } C k=1 . In this paper, we adopt the regression loss:</p><formula xml:id="formula_2">ℓ(y n , W Tx n ) = y n − W Tx n 2 2<label>(3)</label></formula><p>1 C linear decision surfaces can divide the points into more than C clusters.</p><p>The proposed approach combines the quantization with the feature selection, and jointly learns the quantization parameter and the transform matrix. The overall objective function is given as follows,</p><formula xml:id="formula_3">min W,P,C,{bn} N n=1 ,ǫ N n=1 y n − W T Cb n 2 2 + λ W 2 F + γ N n=1 Cb n − P T x n 2 2 (4) s. t. M i =j b T ni C T i C j b nj = ǫ, n = 1, 2, · · · , N,</formula><p>where γ is the parameter controlling the quantization term;</p><formula xml:id="formula_4">Cb n is the matrix form of M m=1 c mk n m and b n = [b T n1 b T n2 · · · b T nM ] T ; b nm ∈ {0,</formula><p>1} K is an indicator vector with only one entry being 1, indicating that the corresponding dictionary element is selected from the mth dictionary. The equality constraint,</p><formula xml:id="formula_5">M i =j b T ni C T i C j b nj = M i =j c T ik n i c jk n j = ǫ, called constant inter-dictionary- element-product, is introduced from composite quantiza- tion [39] for fast distance computation (reduced from O(d) to O(M )) in the search stage, which is presented below.</formula><p>Querying. The search process is similar to that in composite quantization <ref type="bibr" target="#b38">[39]</ref>. Given a query q, after transformation, the approximate distance between q (represented as q ′ = P T q) and a database vector x (represented as Cb = M m=1 c mkm ) is computed as</p><formula xml:id="formula_6">q ′ − M m=1 c mkm 2 2 = (5) M m=1 q ′ − c mkm 2 2 − (M − 1) q ′ 2 2 + M i =j c T iki c jkj .</formula><p>Given the query q ′ , the second term −(M − 1) q ′ 2 2 in the right-hand side of Equation 5 is a constant for all database vectors. Meanwhile, the third term M i =j c T iki c jkj , which is equal to ǫ thanks to the introduced constant constraint, is also a constant. Hence these two constant terms can be ignored, as they do not affect the sorting results. As a result, it is enough to compute the distances between q ′ and the selected dictionary elements {c mkm } M m=1 :</p><formula xml:id="formula_7">{ q ′ − c mkm 2 2 } M m=1 .</formula><p>We precompute a distance table of length M K recording the distances between q ′ and the dictionary elements in all the dictionaries before examining the distance between q ′ and each approximated pointx in the database. Then computing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimization</head><p>Our problem (4) consists of five groups of unknown variables: classification matrix W, transformation matrix P, dictionaries C, binary indicator vectors {b n } N n=1 , and the constant ǫ. We follow <ref type="bibr" target="#b38">[39]</ref> and combine the constraints</p><formula xml:id="formula_8">M i =j b T ni C T i C j b nj = ǫ into</formula><p>the objective function using the quadratic penalty method:</p><formula xml:id="formula_9">ψ(W, P, C, {bn} N n=1 , ǫ) = N n=1 yn − W T Cbn 2 2 + λ W 2 F + γ N n=1 Cbn − P T xn 2 2 + µ N n=1 ( M i =j b T ni C T i Cjbnj − ǫ) 2 ,<label>(6)</label></formula><p>where µ is the penalty parameter. We use the alternative optimization technique to iteratively solve the problem, with each iteration updating one of W, P, ǫ, C, and {b n } N n=1 while fixing the others. The initialization scheme and the iteration details are presented as follows.</p><p>Initialization. The transformation matrix P is initialized using principal component analysis (PCA). We use the dictionaries and codes learned from product quantization <ref type="bibr" target="#b7">[8]</ref> in the transformed space to initialize C and {b n } N n=1 for the shortest code (16 bits) in our experiment, and we use the dictionaries and codes learned in the shorter code to do the initialization for longer code with setting the additional dictionary elements to zero and randomly initializing the additional binary codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W-</head><p>Step. With C and {b n } N n=1 fixed, W is solved by the regularized least squares problem, resulting in a closedform solution:</p><formula xml:id="formula_10">W * = (XX T + λI r ) −1X Y T ,<label>(7)</label></formula><p>whereX = [Cb 1 · · · Cb N ] ∈ R r×N , Y = [y 1 · · · y N ] ∈ R C×N , and I r is an identity matrix of size r × r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P-</head><p>Step. With C and {b n } N n=1 fixed, the transformation matrix P is solved using the normal equation:</p><formula xml:id="formula_11">P * = (XX T ) −1 XX T ,<label>(8)</label></formula><formula xml:id="formula_12">where X = [x 1 · · · x N ] ∈ R d×N . ǫ-</formula><p>Step. With C and {b n } N n=1 fixed, the objective function is a quadratic function with respect to ǫ, and it is easy to get the optimal solution to ǫ.</p><formula xml:id="formula_13">ǫ * = 1 N N n=1 M i =j b T ni C T i C j b nj .<label>(9)</label></formula><p>C-Step. With other variables fixed, the problem is an unconstrained nonlinear optimization problem with respect to C. We use the quasi-Newton algorithm and specifically the L-BFGS algorithm, the limited version of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. The implementation is publicly available 2 . The derivative with respect to C and the objective function value need to be fed into the solver. L-BFGS is an iterative algorithm and we set its maximum iterations to 100. The partial derivative with respect to C m is :</p><formula xml:id="formula_14">∂ψ ∂C m = N n=1 [2W(W T Cb n − y n )b T nm + (10) 2γ(Cb n − P T x n )b T nm + 4µ( M i =j b T ni C T i C j b nj − ǫ)( M l=1,l =m C l b nl )b T nm ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B-</head><p>Step. The optimization problem with respect to {b n } N n=1 could be decomposed to N subproblems,</p><formula xml:id="formula_15">ψ n (b n ) = ||y n − W T Cb n || 2 2 + γ||Cb n − P T x n || 2 2 + µ( M i =j b T ni C T i C j b nj − ǫ) 2 .<label>(11)</label></formula><p>b n is a binary-integer-mixed vector, and thus the optimization is NP-hard. We use the alternative optimization technique again to solve the M subvectors {b nm } M m=1 iteratively. With {b nl } M l=1,l =m fixed, we exhaustively check all the elements in the dictionary C m , finding the element such that ψ n (b n ) is minimized, and accordingly set the corresponding entry of b nm to be 1 and all the others to be 0.</p><p>Convergence. Every update step in the algorithm assures that the objective function value weakly decreases after each iteration, and the empirical results show that the algorithm takes a few iterations to converge. <ref type="figure" target="#fig_1">Figure 1</ref> shows the convergence curves on NUS-WIDE and ImageNet with 16 bits, which indicates that our algorithm gets converged in a few iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions</head><p>Connection with supervised sparse coding. It is pointed in <ref type="bibr" target="#b38">[39]</ref> that composite quantization is related to sparse coding: the binary indicator vector b is a special sparse code, containing only M non-zero entries (valued as 1) and each non-zero entry distributed in a subvector. The proposed supervised quantization approach is close to supervised sparse coding <ref type="bibr" target="#b22">[23]</ref>, which introduces supervision to learn the dictionary and the sparse codes, but different from it in the motivation and the manner of imposing the supervision: our approach adopts the supervision to help separate the data 2 http://users.iems.northwestern.edu/ nocedal/lbfgsb.html points into clusters with each corresponding to a class; our approach imposes the supervision on the approximated data points while supervised sparse coding imposes the supervision on the sparse codes.</p><p>Classification loss vs. rank loss. There are some hashing approaches exploring the supervision information through rank loss <ref type="bibr" target="#b32">[33]</ref>, such as the triplet loss in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>, and the pairwise loss in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref>. In general, compared with the classification loss, those two rank losses might be more helpful to learn the compact codes as they directly align the rank order in the coding space with the given semantic rank information. However, they yield a larger number of loss terms, e.g., O(N 2 ) for pairwise loss and O(N 3 ) for triplet loss, requiring prohibitive computational cost which makes the optimization difficult and infeasible. Therefore, sampling is usually adopted for training, which however makes the results not as good as expected. A comparison with triplet loss is shown in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets and settings</head><p>Datasets. We perform the experiments on four standard datasets: CIFAR-10 <ref type="bibr" target="#b12">[13]</ref>, MNIST <ref type="bibr" target="#b16">[17]</ref>, NUS-WIDE <ref type="bibr" target="#b1">[2]</ref>, and ImageNet <ref type="bibr" target="#b2">[3]</ref>. The CIFAR-10 dataset consists of 60, 000 32 × 32 color tinny images, and includes 10 classes with 6, 000 images per class. We represent each image by a 512-dimensional GIST feature vector available on the website 3 . The dataset is split into a query set with 1, 000 samples and a training set with all the remaining samples as done in <ref type="bibr" target="#b26">[27]</ref>.</p><p>The MNIST dataset consists of 70, 000 28×28 greyscale images of handwritten digits from '0' to '9'. Each image is represented by the raw pixel values, resulting in a 784-dimensional vector. We split the dataset into a query set with 1, 000 samples and a training set with all remaining samples as done in <ref type="bibr" target="#b26">[27]</ref>.</p><p>The NUS-WIDE dataset contains 269, 648 images collected from Flickr, with each image containing multiple semantic labels from 81 concept labels. The 500-dimensional bag-of-words features provided in <ref type="bibr" target="#b1">[2]</ref> are used. Following <ref type="bibr" target="#b26">[27]</ref>, we collect 193,752 images that are from the 21 most frequent labels for evaluation, including sky, clouds, person, water, animal, grass, building, window, plants, lake, ocean, road, flowers, sunset, relocation, rocks, vehicles, snow, tree, beach, and mountain. For each label, 100 images are uniformly sampled as the query set, and the remaining images are used as the training set.</p><p>The dataset ILSVRC 2012 <ref type="bibr" target="#b2">[3]</ref>, named as ImageNet in this paper, contains over 1.2 million images of 1, 000 categories. We use the provided training set as the retrieval database and the provided 50, 000 validation images as the query set since the ground-truth labeling of the test set is not publicly available. Similar to <ref type="bibr" target="#b26">[27]</ref>, we use the 4096dimensional feature extracted from the convolution neural networks (CNN) in <ref type="bibr" target="#b13">[14]</ref> to represent each image.</p><p>Evaluation criteria. We adopt the widely used mean average precision (MAP) criterion, defined as MAP =</p><formula xml:id="formula_16">1 Q Q i=1 AP (q i ),</formula><p>where Q is the number of queries, and AP is computed as AP (q) = 1 L R r=1 P q (r)δ(r). Here L is the number of true neighbors for the query q in the R retrieved items, where R is the size of the database except that R is 1500 on the ImageNet dataset for evaluation efficiency. P q (r) denotes the precision when top r data points are returned, and δ(r) is an indicator function which is 1 when the rth result is a true neighbor and otherwise 0. A data point is considered as a true neighbor when it shares at least one class label with the query.</p><p>Besides the search accuracy, we also report the search efficiency by evaluating the query time under various code lengths. The query time contains the query preprocessing time and the linear scan search time. For hashing algorithms, the query preprocessing time refers to query encoding time; for unsupervised quantization algorithms, the query preprocessing time refers to distance lookup table construction time; for our proposed method, the query preprocessing time includes feature transformation time and distance lookup table construction time. For all methods, we use C++ implementations to test the query time on a 64bit windows server with 48 GB RAM and 3.33 GHz CPU.</p><p>Parameter settings. There are three trade-off parameters in the objective function (6): γ for the quantization loss term, µ for penalizing the equality constraint term, and λ for the regularization term. We select γ and µ via validation. We choose a subset of the training set as the validation set (the size of the validation set is the same to that of the query set), and the best parameters γ and µ are chosen so that the average search performance in terms of MAP, by regarding the validation vectors as queries, is the best. It is feasible that the validation set is a subset of the training set, as the validation criterion is not the objective function but the search performance <ref type="bibr" target="#b38">[39]</ref>. The empirical analysis about the two parameters will be given in Section 6.3. The parameter λ is set to 1, which already shows the satisfactory performance. We set the dimension r of the discriminative subspace to 256. We do not tune r and λ for saving time while we think that tuning it might yield better performance. We choose K = 256 to be the dictionary size as done in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>, so that the resulting distance lookup tables are small and each subindex fits into one byte.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison</head><p>Methods. Our method, denoted by SQ, is compared with seven state-of-the-art supervised hashing methods: supervised discrete hashing (SDH) <ref type="bibr" target="#b26">[27]</ref>, FastHash <ref type="bibr" target="#b34">[35]</ref>, supervised hashing with kernels (KSH) <ref type="bibr" target="#b20">[21]</ref>, CCA-ITQ <ref type="bibr" target="#b5">[6]</ref>, semi-supervised hashing (SSH) <ref type="bibr" target="#b30">[31]</ref>, minimal loss hashing (MLH) <ref type="bibr" target="#b23">[24]</ref>, and binary reconstructive embedding (BRE) <ref type="bibr" target="#b14">[15]</ref>, as well as the state-of-the-art unsupervised quantization method, composite quantization (CQ) <ref type="bibr" target="#b38">[39]</ref>. To the best of our knowledge, there do not exist supervised quantization algorithms. We use the public implementations for all the algorithms except that we implement SSH by ourselves as we do not find the public code, and follow the corresponding papers/authors to set up the parameters. For FastHash, we adopt hinge loss as loss function in the binary code inference step and boosted tree as classifier in the hash function learning step, which is suggested by the author to achieve the best performance. Implementation details. It is infeasible to do the training over the whole training set for the pairwise-similarity-based hashing algorithms (SSH, BRE, MLH, KSH, FastHash), as discussed in <ref type="bibr" target="#b26">[27]</ref>. Therefore, for CIFAR-10, MNIST, and NUS-WIDE, following the recent work <ref type="bibr" target="#b26">[27]</ref>, we randomly sample 5000 data points from the training set to do the optimization for the pairwise similarity-based algorithms, and use the whole training set for SDH and CCA-ITQ. For Im-ageNet, we use as many training samples for optimization as possible if the 256G RAM in our server is enough for optimization: 500, 000 for CCA-ITQ, 100, 000 for SDH, 10, 000 for the remaining hashing methods. There are two hashing algorithms, KSH and SDH, that adopt the kernelbased representation, i.e., select h anchor points {a j } h j=1 and use φ(x) = [exp(−||x − a 1 || 2 2 /2σ 2 ) . . . exp(−||x − a h || 2 2 /2σ 2 )] T ∈ R h to represent x. Our approach also uses the kernel-based representation for CIFAR-10, MNIST, and NUS-WIDE. Following <ref type="bibr" target="#b26">[27]</ref>, h = 1000 and σ is chosen based on the rule σ = 1 N N n=1 min{ x n − a j 2 } h j=1 .  <ref type="figure" target="#fig_2">Figure 2</ref>. It can be seen that our approach, SQ, achieves the best performance, and SDH is the second best. In comparison with SDH, our approach gains large improvement on CIFAR-10 and NUS-WIDE, e.g., 23.66% improvement on CIFAR-10 with 64 bits, and 4.65% improvement on NUS-WIDE with 16 bits. It is worth noting that on these two datasets, the performance of SQ with 16 bits is even much better than that of SDH with 128 bits. Our approach gets relatively small improvement over SDH on MNIST. The reason might be that SDH already achieves a high performance, and it is not easy to get a large improvement further. Compared with the unsupervised quantization algorithm, composite quantization (CQ), whose performance is lower than most of the supervised hashing algorithms, our approach obtains significant improvement, e.g., 42.57% improvement on CIFAR-10 with 16 bits, 46.14% on MNIST with 16 bits, and 15.39% on NUS-WIDE with 16 bits. This shows that learning with supervision indeed benefits the search performance.</p><p>The result on ImageNet is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. The performance of our approach again outperforms other algorithms, and CQ is the second best. The reason might be the powerful discrimination ability of the original CNN features. To achieve a comprehensive analysis, we provide the Euclidean baseline (see <ref type="figure" target="#fig_3">Figure 3</ref>) that simply computes the distances between the query and the database vectors using the original CNN features and returns the top R retrieved items. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, our proposed SQ also outperforms the Euclidean baseline by a large margin, and CQ is a little lower than the baseline. This shows that our approach is able to learn better quantizer through the supervision though it is known that the CNN features are already good. The best supervised hashing algorithm, SDH, uses the kernel-based representation in our experiment as suggested in its original paper <ref type="bibr" target="#b26">[27]</ref>. To further verify the superiority of our approach over SDH, we also report the result of SDH without using the kernel representation (denoted by "SDH-Linear" in <ref type="figure" target="#fig_3">Figure 3</ref>), and find that it is still lower than our approach. This further shows the effectiveness of quantization: quantization has much more different differences compared with hashing, which has only a few Hamming distances for the same code length.</p><p>Search efficiency. We report the query time of our proposed approach SQ, the unsupervised quantization method CQ, and the supervised hashing method SDH, which outperforms other supervised hashing algorithms in our experiments. <ref type="figure" target="#fig_4">Figure 4</ref> shows the search performance and the corresponding query time under the code length of 16, 32, 64, and 128 on the four datasets.</p><p>Compared with CQ, our proposed SQ obtains much higher search performance for the same query time. It can be seen that on CIFAR-10, MNIST, and NUS-WIDE, SQ takes more time than CQ under the code length of 16 and 32, and less time under the code length of 128: SQ takes extra time to do feature transformation; the querying process, however, is carried out in a lower-dimensional transformed subspace, therefore the search efficiency is still comparable to CQ. It can also be observed that SQ takes almost equal time as CQ on ImageNet. This is because CQ also takes time to do feature transformation here and the querying process is carried out in the 256-dimensional PCA subspace (it is cost prohibitive to tune the parameter of CQ on high-dimensional large-scale dataset). Compared with SDH, SQ outperforms SDH for the same query time on ImageNet and NUS-WIDE. For example, SQ with 32 bits outperforms SDH with 16 bits by a margin of 40.82% on ImageNet, and SQ with 16 bits outperforms SDH with 128 bits by a margin of 2% on NUS-WIDE, while they take almost the same query time.</p><p>On CIFAR-10, SQ with 16 bits outperforms SDH with 128 bits by 12.4% while taking slightly more time (0.16 milliseconds), and this trend indicates that for the same query time, SQ could also obtain higher performance. On MNIST, SQ achieves the same performance as SDH while taking slightly more query time. The reason is that the query preprocessing time of SQ (mainly refers to distance lookup table construction time here) is relatively long compared with the linear scan search time on the small-scale database. In real-word scenarios, retrieval tasks that require quantization solution usually are conducted on large-scale databases, and the scale usually is at least 200, 000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Empirical analysis</head><p>Classification loss vs. triplet loss. We empirically compare the performances between the proposed formulation (4) that uses the classification loss for semantic separation, and an intuitive formulation that uses triplet loss <ref type="table">Table 1</ref>: MAP comparison of classification loss (denoted by "c-loss") and triplet loss (denoted by "t-loss"). to discriminate a semantically similar pair and a semantically dissimilar pair. The triplet loss formulation is written as (i,j,l) [||Cb i − Cb j || 2 2 − ||Cb i − Cb l || 2 2 + ρ] + . The triplet (i, j, l) is composed of three points where i and j are from the same class and l is from a different class; ρ ≥ 0 is a constant indicating the distance margin; [·] + = max(0, ·) is the standard hinge loss function.</p><p>We optimize the formulation with triplet loss using the alternative optimization algorithm similar to that for optimizing problem (4). The parameters γ and µ are chosen through validation. It is infeasible to do the optimization with all the triplets. Therefore we borrow the idea of active set, and select the triplets that are most likely to trigger the hinge loss at each iteration, which is efficiently implemented by maintaining an approximate nearest neighbor list for each database vector.</p><p>The results on CIFAR-10 and MNIST under various code lengths are shown in <ref type="table">Table 1</ref>. It is observed that the results with classification loss are much better than those with triplet loss. It seems to us that the triplet loss is better than classification loss, as the search goal is essentially to rank similar pairs before dissimilar pairs, which is explicitly formulated in triplet loss. The reason of the lower performance of triplet loss most likely lies in the difficulty of the optimization (e.g., too many (O(N 3 )) loss terms results in the sampling technique used for training, which makes the <ref type="figure">Figure 5</ref>: Illustration of the effect of γ and µ on the search performance in the validation sets of CIFAR-10, MNIST, NUS-WIDE, and ImageNet with 16 bits. γ ranges from 1e-7 to 1e+2 and µ ranges from 1e-1 to 1e+2. <ref type="table">Table 2</ref>: MAP comparison of the formulation with feature transformation (denoted by "with fea.") and that without feature transformation (denoted by "no fea.").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Methods <ref type="formula" target="#formula_0">16</ref>  results not as good as expected).</p><p>Feature transformation. Our approach learns the feature transformation matrix P, and quantizes the database vectors in the learned discriminative subspace. To verify the effectiveness of feature transformation in our formulation (4), we empirically compare the performances between the proposed formulation and the formulation that does not learn feature transformation. We take CIFAR-10 and MNIST as examples and the results are shown in <ref type="table">Table 2</ref>. As shown, SQ significantly outperforms the formulation that does not learn feature transformation, which indicates the importance of feature transformation in our proposed formulation.</p><p>The Effect of γ and µ. We empirically show how the parameters γ (for controlling the quantization loss term) and µ (for penalizing the equality constraint term) affect the search performance on the validation set, where the parameters are tuned to select the best combination. We report the performances with 16 bits in <ref type="figure">Figure 5</ref>, by varying γ from 1e-7 to 1e+2 and µ from 1e-1 to 1e+2. It can be seen from <ref type="figure">Figure 5</ref> that the overall perfor-mances do not depend much on µ and the performances change a lot when varying the γ. This is reasonable because γ controls the quantization loss, and µ is introduced for accelerating the search. The best search performances on CIFAR-10, MNIST, NUS-WIDE, and ImageNet are obtained with (γ, µ) = (0.01, 0.1), (γ, µ) = (1e-7, 10), (γ, µ) = (1e-5, 0.1), and (γ, µ) = (1, 100) respectively. We can see that the best MAP values 0.6132, 0.9449, and 0.5466 on the validation sets are close to the values 0.6045, 0.9329, and 0.5452 on the query sets of CIFAR-10, MNIST, and NUS-WIDE, and that the MAP value 0.5372 on the validation set is different from the value 0.5039 on the query set of Ima-geNet. The reason might be that the validation set (sampled from the training set) and the query set (the validation set provided in ImageNet) are not of the same distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we present a supervised compact coding approach, supervised quantization, to semantic similarity search. To the best of our knowledge, our approach is the first attempt to study the quantization for semantic similarity search. The superior performance comes from two points: (i) The distance differentiation ability of quantization is stronger than that of hashing. (ii) The learned discriminative subspace is helpful to find a semantic quantizer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>M m=1 q ′ − c mkm 2 2 takes only O(M ) distance table lookups and O(M ) addition operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Convergence curves of our algorithm on NUS-WIDE and ImageNet with 16 bits. The vertical axis represents the value of the objective function (6) and the horizontal axis corresponds to the number of iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Search performance (in terms of MAP) comparison of different methods on CIFAR-10, MNIST, and NUS-WIDE with code length of 16, 32, 64, and 128. Search accuracy. The results on CIFAR-10, MNIST, and NUS-WIDE with the code length of 16, 32, 64, and 128, are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Search performance (in terms of MAP) comparison of different methods on ImageNet with code length of 16, 32, 64, and 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Query time comparison of SQ, CQ, and SDH under various code lengths on CIFAR-10, MNIST, NUS-WIDE, and ImageNet. The vertical axis represents the search performance, and the horizontal axis corresponds to the query time cost (milliseconds). The markers from left to right on each curve indicate the code length of 16, 32, 64, and 128 respectively.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.cs.toronto.edu/ kriz/cifar.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by the National Basic Research Program of China (973 Program) under Grant 2014CB347600.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hashing with binary autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raziperchikolaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2475" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast image search for learned metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Revisiting kernelized locality-sensitive hashing for improved large-scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Que</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4933" to="4941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable graph hashing with feature transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2248" to="2254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random maximum margin hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Isotropic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kernelized locality-sensitive hashing for scalable image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2130" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast supervised hashing with decision trees for highdimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A general two-step approach to learning-based hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2552" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discrete graph hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3419" to="3427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hamming distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cartesian k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inductive hashing on manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1562" to="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hashing on nonlinear manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1839" to="1851" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ldahash: Improved matching with smaller descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="78" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large-scale search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning hash codes with listwise supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hashing for similarity search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<idno>abs/1408.2927</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Order preserving hashing for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast neighborhood graph search using cartesian concatenation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2128" to="2135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multidimensional spectral hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing. In NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complementary hashing for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1631" to="1638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Composite quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="838" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
