<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simultaneous Clustering and Model Selection for Tensor Affinities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuoguang</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loong-Fah</forename><surname>Cheong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Chuan</forename><surname>Toh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simultaneous Clustering and Model Selection for Tensor Affinities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the number of clusters remains a difficult model selection problem. We consider this problem in the domain where the affinity relations involve groups of more than two nodes. Building on the previous formulation for the pairwise affinity case, we exploit the mathematical structures in the higher order case. We express the original minimal-rank and positive semi-definite (PSD) constraints in a form amenable for numerical implementation, as the original constraints are either intractable or even undefined in general in the higher order case. To scale to large problem sizes, we also propose an alternative formulation, so that it can be efficiently solved via stochastic optimization in an online fashion. We evaluate our algorithm with different applications to demonstrate its superiority, and show it can adapt to varying levels of unbalancedness of clusters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In graph clustering that is conventionally set forth, it is assumed that relationship between data points can be captured by a pairwise measure of affinity. However, in many graph systems from computer vision, it does not make sense to talk about affinities between a pair of data points. We can best illustrate the issue with the classic problem of vanishing point estimation <ref type="bibr" target="#b25">[26]</ref>: two lines trivially define a point and thus there does not seem to exist any useful measure of affinity between two lines, whereas the degree to which three lines are coincident conceivably constitutes a much more useful measure of affinity for this estimation problem. Other examples in computer vision include multi-structure fitting <ref type="bibr" target="#b33">[34]</ref>, texture segmentation <ref type="bibr" target="#b13">[14]</ref>, etc. In all these problems, clustering must be performed on the basis of higher order affinities. Therefore, hypergraph clustering has been increasingly gaining attention. Hypergraphs are made up of hyperedges, which encode higher order affinities among a subset of data whereby the set size can be more than two.</p><p>In spite of this increased interest, the problem of model selection connected with any clustering problem has generally received less attention partly because it is very hard to * S. Yang worked on this project as a research engineer at NUS. provide a formal definition of what a cluster is. This lacuna is even more glaring in the case of the higher order setting. Recently Li et al. <ref type="bibr" target="#b17">[18]</ref> has proposed a simultaneous clustering and model selection (SCAMS) method to solve this problem for the case of pairwise affinities. This is based on an indicator matrix formulation, together with the generic low rank and sparsity constraints that capture the notion of good clusters without making overly strong domain-specific assumptions about the clusters. In this paper, we will develop a non-obvious generalization of SCAMS to hypergraphs. Before that, we briefly review its essential points.</p><p>Given an affinity matrix A with non-negative entries, S-CAMS introduces the following problem:</p><formula xml:id="formula_0">min − ⟨A, G⟩, s.t. G ∈ S+, diag(G) = 1, rank(G) = R, G ∈ {0, 1} N ×N ,<label>(1)</label></formula><p>where ⟨·, ·⟩ is the Frobenius inner product, which is defined as ⟨A, G⟩ = ∑ i,j A(i, j)G(i, j), S + is the PSD cone, diag(·) are the diagonal entries of the matrix, R is the number of clusters, and G is a binary relationship matrix encoding the pairwise relationships between elements. More specifically, if the i-th and j-th elements belong to the same cluster, G(i, j) = 1; otherwise, G(i, j) = 0. G can be factorized as G = ZZ T , where Z is an indicator matrix whose rows indicate to which group a point belongs. Intuitively, given A, we approximate it with G with ideal attributes, in which "1" means similar and "0" means different.</p><p>With the group number R being unknown, SCAMS instead solves the following problem:</p><formula xml:id="formula_1">min − ⟨A, G⟩ + λrank(G) + γ||G||0, s.t. G ∈ S+, diag(G) = 1, G ∈ {0, 1} N ×N ,<label>(2)</label></formula><p>where || · || 0 is the ℓ 0 norm, which counts the number of nonzero elements. In this problem, the first term −⟨A, G⟩ keeps G as close to the affinity matrix A as possible; the second term rank(G) seeks a simplest model, i.e. the clustering result with the smallest cluster number; the third term reflects the sparse nature of an ideal affinity matrix and avoids the trivial solution of G with entries being all ones; the PSD constraint and {0, 1} integer constraint help to discover the authentic sparse structure underlying the data, removing spurious connections and filling in missing links.</p><p>There are various non-trivial difficulties when we want to generalize the preceding SCAMS algorithm to hypergraphs. Given an affinity tensor A with non-negative entries, a direct extension would be to solve for a binary relationship tensor G in a similar manner. However, even though we expect that the low rank condition to be retained for the tensor, it is in general difficult to minimize a tensor rank as conventionally defined. There is no straightforward algorithm to determine the rank of a specific given tensor; in fact, this problem is NP-hard <ref type="bibr" target="#b15">[16]</ref>. Moreover, while it is prevalent to use the nuclear norm as a convex surrogate of a matrix rank, it is NP-hard to find the nuclear norm of a tensor rank <ref type="bibr" target="#b7">[8]</ref>. Thus the natural generalization of matrix rank to tensor rank will not work. Another serious issue is as follows. In the formulation set forth by SCAMS, the structure of G arising from its specific form of G = ZZ T is captured by the PSD constraint. In the higher order case of tensor, the PSD constraint breaks down: firstly, there is no common definitions of tensor eigenvalue and eigenvector, and even with these definitions, positive semi-definiteness can only be defined on tensors of even orders <ref type="bibr" target="#b3">[4]</ref>. Thus, there is a fundamental difficulty to define a meaningful notion of positive semi-definiteness. Lastly, there is also the practical issue of scalability: a K-order affinity tensor with N nodes results in N K unknowns in G. This can easily reach a prohibitively large number, rendering the problem intractable due to both time and memory constraint. All these represent significant obstacles when we extend SCAMS to the hypergraphs.</p><p>The SCAMS formulation has revealed and exploited the particular structure present in the clustering and model selection problem for the pairwise affinity case. We believe that there are also interesting structures in the higher order case, and it is the objective of this paper to arrive at a more complete knowledge of these structures, as well as to articulate the constraints in a form that circumvents the three problems mentioned in the preceding paragraph. Specifically, we make three main contributions in solving these three problems. First, to solve the tensor rank minimization problem, we unfold the tensor in a special way so that the problem becomes a matrix rank minimization problem. Note that it is not the same as the n-rank methods which will be reviewed in Section 1.2. By leveraging on the special structure of the binary relationship tensor, we show that the rank of the matrix obtained in this way is exactly the same as the tensor rank. Next, to handle the problem of the PSD constraint, we articulate the constraint on G in a different way: we regard G as the sum of the outer-products of several 1-D tensors (vectors), i.e., G = ∑ R r=1 z r • z r • · · · • z r , where • represents the vector outer product and z r is an indicator vector. The upshot is that the constraint on G can then be transformed into many small-sized rank-1 matrix approximation problems with the PSD constraints. Lastly, for huge problems, we propose an alternative formulation to signifi-cantly reduce the number of unknowns based on the special structure of the binary relationship in the tensor, and most importantly, to keep the size of the unknowns constant as the order increases. We solve this problem via stochastic optimization in an online fashion without having to construct the affinity tensor in the first place, thereby bypassing any potential memory bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Notations</head><p>We mostly follow the terminology of tensors used in <ref type="bibr" target="#b15">[16]</ref> for our paper. An K-mode (or order/way) tensor is written as X ∈ R I1×I2×...×I K . For example, a vector is a 1-mode tensor and a matrix is a 2-mode tensor. The (i 1 , i 2 , · · · , i K )-th entry of X is denoted as</p><formula xml:id="formula_2">X (i 1 , i 2 , · · · , i K ), where 1 ≤ i k ≤ I k and 1 ≤ k ≤ K.</formula><p>We also define fibers (vectors) as higher order analogue of matrix rows and columns by fixing every index but one, denoted as X (:, i 2 , · · · , i K ). Similarly, we define slices (matrices) as two-dimensional sections of a tensor by fixing all but two indices, denoted as X (:, :, · · · , i K ).</p><p>A tensor is called cubical if every mode is of the same size, i.e., X ∈ R I×I×...×I . A cubical tensor is called supersymmetric if its entries remain constant under any permutation of the indices. An K-mode tensor X ∈ R I1×I2×...×IK is rank one if it can be written as the outer product of K vectors, i.e., X = a (1) • a <ref type="bibr" target="#b1">(2)</ref> • · · · • a (K) . The rank of a tensor, denoted as rank(X ), is defined as the smallest number of rank-1 tensors that generate X as their sum.</p><p>Let</p><formula xml:id="formula_3">H = (V, E, A) denote a hypergraph with V = {v i } N i=1 the set of the N nodes, E = {e i } L i=1</formula><p>the set of the L hyperedges containing subsets of nodes, and A ∈ R N ×N ×...×N an affinity tensor storing the weights of the hyperedges. It is customary to neglect the directedness of the hyperedges, if any, and thus, it follows that A is supersymmetric. The weight A(e i ) ≥ 0 is an entry in A which represents the affinity of the nodes in the hyperedge e i . A(e i ) = 0 suggests that the nodes in e i are completely dissimilar, and thus likely to be disconnected, while A(e i ) &gt; 0 means there is the possibility for these nodes in e i to be clustered into the same group. The larger the value, the more likely these nodes in e i should be in the same group. The degree of a hyperedge e i is the number of nodes in it and denoted as |e i |. The order of the corresponding affinity tensor is thus K = max L i=1 (|e i |).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Related Works</head><p>Current approaches to hypergraph clustering can be roughly divided into the projection and generalization methods. The projection methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36]</ref> transform the hypergraph into a graph by mapping the higher order affinities to pairwise ones, after which conventional graph clustering method (such as spectral clustering <ref type="bibr" target="#b22">[23]</ref>) is applied. The generalization methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref> extend the graph clustering methods and the attendant matrix analysis to hyergraphs and tensor analysis.</p><p>Choosing the number of clusters is still a difficult model-selection problem, especially in the hypergraph setting. A general approach to this problem is based on the information-theoretic principle <ref type="bibr" target="#b12">[13]</ref>, which balances the goodness of fit against the complexity of the model. This principle is not algorithm specific, and thus can be applied to any clustering method. The major drawback of this kind of methods is that the results are often sensitive to choice of parameters. Alternatively, graph based model selection algorithms can be applied via the projection approach. That is, we first project the hypergraph onto a graph, after which, say the spectral clustering (SC) approach <ref type="bibr" target="#b26">[27]</ref> can be used to determine the number of zero eigenvalues of the Laplacian matrix of the affinity graph. Note that the SC approach usually does not deliver the number of clusters. One has to rely on the existence of large gaps between pairs of consecutive eigenvalues to suggest that number. This gap is not always apparent unless the clusters are weakly connected to each other. What is more, when the clusters are unbalanced, the SC approach faces limitations that are well-documented <ref type="bibr" target="#b21">[22]</ref>. In the hypergraph setting, these issues are exacerbated by the need to perform the averaging operation in the projection step-such averaging favors large clusters as vertices in these clusters have many edges joining vertices of the same cluster. Our method works in the hypergraph space directly and does not suffer from the aforementioned issues. Furthermore, our method performs a tight joint optimization of clustering and model selection in one single step, whereas the two approaches mentioned above usually adopt separate criteria to measure the goodness of the classification and to determine the number of clusters.</p><p>Our method is based on an extension of SCAMS <ref type="bibr" target="#b17">[18]</ref> to the higher order setting. As SCAMS is related to correlation clustering (CC) <ref type="bibr" target="#b2">[3]</ref>, our method is also closely related to higher order CC <ref type="bibr" target="#b13">[14]</ref>. The difficulty of CC or its higher order variant is in determining a proper threshold to distinguish between "similar" and "dissimilar". While <ref type="bibr" target="#b13">[14]</ref> learns this threshold in the image segmentation task, both SCAMS and our method are completely unsupervised.</p><p>The tensor rank minimization component of our algorithm is closely related to tensor decompositions. Two early works, based on the higher-order extensions of the matrix singular value decomposition, have very much shaped research directions in this field: CANDECOMP/PARAFAC (CP) <ref type="bibr" target="#b6">[7]</ref> decomposes a tensor as a sum of rank-1 tensors, whereas Tucker decomposition <ref type="bibr" target="#b32">[33]</ref> is a higher-order form of principal component analysis (PCA). Building upon Tucker decomposition, the low n-rank tensor approximation <ref type="bibr" target="#b9">[10]</ref> has many practical ramifications due to its computational feasibility. Basically, the n-rank method "unfolds" a tensor by stacking all its fibers along every direction and forms several matrices (This operation is called matricization <ref type="bibr" target="#b15">[16]</ref>). Thus, a n-mode tensor will result in n matrices, and the n-rank method performs low rank minimization by minimizing the sum of the rank of these n matrices. Our method is different from the n-rank method in the unfolding procedure, which in our case is governed by the physical meaning of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hypergraph Clustering with Unknown</head><p>Group Numbers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem Formulation</head><p>The goal is to cluster N nodes in a hypergraph into R groups, where the group number R is unknown a priori and needs to be estimated. To formulate the problem, we denote {z r ∈ {0, 1} N } R r=1 as the indicator vectors of the R clusters, whose entries indicate if the points belong to the r-th cluster, i.e., if the i-th point belongs to the r-th cluster, z r (i) = 1; otherwise, z r (i) = 0. In the standard definition for clusters, each node must belong to at least one cluster but cannot be in multiple clusters. Thus, there is one and only one '1' at the same i-th position of all z r , r = 1 . . . R.</p><p>To generalize SCAMS as presented in <ref type="formula" target="#formula_0">(1)</ref> to the tensor case, we have the following problem:</p><formula xml:id="formula_4">min − ⟨A, G⟩, s.t. G = R ∑ r=1 zr • zr • · · · • zr, zr ∈ {0, 1} N , G ∈ {0, 1} N ×N ×···×N , R ∑ r=1 zr = e,<label>(3)</label></formula><p>where e is an all one vector of size N . The last constraint ensures that one node belongs to one and only one cluster, and we call it the exclusivity constraint. Note that (1) is a special case of (3) whereby the tensor is 2-mode. One may however observe that the PSD constraint in (1) is different from the first constraint in (3) (we call it the rank-1 sum constraint). Indeed, when the tensor is 2-mode, the rank-1 sum constraint leads to the PSD constraint since the sum of PSD matrices is PSD. Even though the reverse is generally not true, it can be readily proven that, together with the {0, 1}-integer constraint and the exclusivity constraint in <ref type="formula" target="#formula_4">(3)</ref>, a PSD G can be always factorized into the form of</p><formula xml:id="formula_5">G = ∑ R r=1 z r • z r .</formula><p>The good news is that the PSD constraint on tensor is not really required but rather the mathematical form of G = ∑ R r=1 z r • z r • · · · • z r should be enforced. This formulation lends itself to a more tractable solution, as we will show in the following.</p><p>Let {G i } nt i=1 be all the slice matrices extracted from G in all directions. Note that there are in total n t = N K−2 ( K 2 ) slices 1 . However, since A and G are assumed to be supersymmetric, we only need to deal with the slice matrices in one direction; i.e. n = N K−2 slices. When K ≥ 3, if we carefully examine the slice in G, it can only be either a rank-1 matrix or a 0 matrix. Formally, we have</p><formula xml:id="formula_6">1 ( a b ) = a! b!(a−b)! is the Choose function.</formula><p>Theorem 1. For any supersymmetric tensor X ∈ R N ×N ×...×N of order at least 3, if it has the form</p><formula xml:id="formula_7">X = ∑ R r=1 zr • zr • · · · • zr, ∑ R r=1 zr = e, zr ∈ {0, 1} N ,<label>(4)</label></formula><p>any slice of X can only be either a rank-1 matrix z r • z r or a 0 matrix. The proof follows from the fact that any slice in X has the form of</p><formula xml:id="formula_8">∑ R i=1 c i · z r • z r , where c i ∈ {0,</formula><p>1} depends on the assignments of the fixed indices and</p><formula xml:id="formula_9">∑ R i=1 c i ∈ {0, 1}.</formula><p>The detailed proof is provided in the supplementary material.</p><p>From Theorem 1, it is clear that there are exactly R + 1 types of slices, namely, z 1 • z 1 , . . . , z R • z R , or a 0 matrix, and each of them is a PSD matrix of rank no larger than 1. Thus, we can transform <ref type="formula" target="#formula_4">(3)</ref> to</p><formula xml:id="formula_10">min − ⟨A, G⟩, s.t. rank( G) = R, diag(G) = 1, G ∈ {0, 1} N ×...×N , Gi ∈ S+, rank(Gi) ≤ 1, i = 1, 2, . . . n,<label>(5)</label></formula><p>where G = [vec(G 1 ) vec(G 2 ) . . . vec(G n )] and vec(·) vectorizes a matrix to a column vector. Note that G, G and {G i } n i=1 are the same variable in different forms: G is the tensor form; G is the unfolding of the tensor G in matrix form; and {G i } n i=1 is the slice matrices form. It is also necessary to note that the unfolded form G is not the commonly used matricization of a tensor in n-rank estimation <ref type="bibr" target="#b14">[15]</ref>. In (5), rank( G) represents model complexity. Essentially it is the number of types of rank-1 matrices G i , which should be equal to the number of clusters, i.e., the tensor rank of G. Writing G in these various forms permits the derivation of a tractable optimization scheme.</p><p>Since R is unknown, we instead solve</p><formula xml:id="formula_11">min − ⟨A, G⟩ + λrank( G) + γ∥ G∥0, s.t. G ∈ [0, 1] N ×N ×...×N , diag(G) = 1, Gi ∈ S+, rank(Gi) ≤ 1, i = 1, 2, . . . n,<label>(6)</label></formula><p>where ∥ G∥ 0 is used to avoid the trivial solution and recover the underlying sparsity structure as in SCAMS <ref type="bibr" target="#b17">[18]</ref>. Note that the {0, 1} integer constraint is also relaxed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Solver</head><p>For ease of representation, we let W = −A and unfold it in the same form of G as W. We solve <ref type="formula" target="#formula_11">(6)</ref>  </p><p>where J = [vec(J 1 ) vec(J 2 ) . . . vec(J n )], g is the indicator function of the convex set [0, 1] N N ×n , which returns 0 if it is in the set, ∞ otherwise, and unfolding diag(·) are those entries of the unfolded form H corresponding to the diagonal entries of the tensor.</p><p>The augmented Lagrangian function is</p><formula xml:id="formula_13">L =⟨ W, G⟩ + λrank( G) + γ∥ H∥0 + g( H) + ⟨Y1, G − H⟩+ 1 2µ ∥ G − H∥ 2 F + ⟨Y2, G − J⟩ + 1 2µ ∥ G − J∥ 2 F , s.t.unfolding diag( H) = 1, Ji ∈ S+, rank(Ji) ≤ 1, i = 1, 2, . . . n,<label>(8)</label></formula><p>where Y 1 and Y 2 are the Lagrange parameters, and µ &gt; 0 is a penalty parameter. The function can be minimized with respect to G, H and {J i } n i=1 alternatingly, by fixing the other two variables, and then updating the Lagrange multipliers Y 1 and Y 2 . The subproblems in ADMM are:</p><formula xml:id="formula_14">Solving G. min G ∥ G− 1 2 ( H+ J−µ( W+Y1 +Y2))∥ 2 F +λµrank( G). (9)</formula><p>Solving H.</p><formula xml:id="formula_15">min H ∥ H − ( G + µY1)∥ 2 F + 2µγ∥ H∥0 + g( H), s.t. unfolding diag( H) = 1.<label>(10)</label></formula><p>Solving J.</p><formula xml:id="formula_16">min J ∥Ji − (Gi + µY2i)∥ 2 F , s.t. Ji ∈ S+, rank(Ji) ≤ 1,<label>(11)</label></formula><p>where G i is extracted from the i-th column of G and reorganized into a square matrix. Similarly Y 2i is extracted from the i-th column of Y 2 and reorganized. These three subproblems can be solved similarly as in <ref type="bibr" target="#b17">[18]</ref> and all of them have closed-form solutions. The overall framework is provided in the supplementary material.</p><p>In general, there is no convergence result for ADMM applied to non-convex problems, but it often works well in practice( <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref>, etc.). Empirically our algorithm has strong convergence behavior indeed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Constrained Boolean Tensor Decomposition</head><p>We also extend the Constrained Boolean Matrix Factorization of <ref type="bibr" target="#b17">[18]</ref> to tensor cases, for the purpose of decomposing the tensor G obtained from the preceding section into its constituent indicator vectors z 1 , ...z R . Here "Boolean" means that the matrix/tensor contains only 0's and 1's.</p><p>Before we define our Boolean Tensor Decomposition problem, we first introduce the necessary notations and Boolean operators. We use the superscript b to distinguish Boolean variables from the normal ones. ∨ is the OR operator applied element-wise, and defined as the normal sum but with 1 + 1 = 1. ⊕ is the Exclusive-OR operator applied element-wise, and defined as the normal sum but with 1 + 1 = 0. Finally, we denote | · | as the norm of Boolean tensor and defined it as the number of 1's in it, i.e.,</p><formula xml:id="formula_17">|X b | = ∑ i1,i2,··· ,i K X b (i 1 , i 2 , · · · , i K )</formula><p>. Now we define the problem as Problem 1. Constrained Boolean Tensor Decomposition (CBTD) with the rank-1 sum and exclusivity constraints. Given a Boolean tensor G b ∈ {0, 1} N ×N ×···×N of order K, and an upper bound R 0 , find Boolean vectors</p><formula xml:id="formula_18">{z 1 , z 2 , . . . , z R }, R ≤ R 0 satisfying min |G b ⊕G b |, s.t.G b = ∨ R r=1 zr • zr • · · · • zr, R ∑ r=1 zr = e,<label>(12)</label></formula><p>where e is an all-one vector of length N . Note that the exclusivity constraint can be interpreted as an orthonormal constraint under Boolean algebra.</p><p>To solve this problem, we follow the Asso algorithm <ref type="bibr" target="#b20">[21]</ref> and its constrained variant <ref type="bibr" target="#b17">[18]</ref> via a heuristic approach of generating the candidate columns using pairwise association accuracies. More specifically, we generate a matrix D with its entries being the association accuracy as defined in association rule mining <ref type="bibr" target="#b1">[2]</ref>, so that the candidate vectors can be extracted from D and used to constructG b in a greedy fashion. We extend the association accuracy to tensor cases as follows. Without loss of generality, let's consider the (K − 1)-mode tensorG b (i, :, · · · , :). Due to the exclusivity of {z r } R r=1 , there exists an unique cluster p such that z p (i) = 1 andG b (i, :, · · · , :) = z p • · · · • z p K−1 .</p><p>Similarly, there exists an unique cluster q such thatG b (j, : , · · · , :) = z q • · · · • z q K−1 . If p = q, ⟨G b (i, :, · · · , :),G b (j, :</p><p>, · · · , :)⟩ = ∥G b (i, :, · · · , :)∥ 2 F = ∥G b (j, :, · · · , :)∥ 2 F , and z p (i) = z p (j) = 1; otherwise, ⟨G b (i, :, · · · , :),G b (j, : , · · · , :)⟩ = 0, and z p (j) = z q (i) = 0, which means z p (i) ̸ = z p (j). The key aspect of this analysis is that if ⟨G b (i, :, · · · , :),G b (j, :, · · · , :)⟩ = ∥G b (j, :, · · · , :)∥ 2 F , it is likely that z p (i) = z p (j). By this intuition, we construct D(i, j) = ⟨G b (i, :, · · · , :), G b (j, :, · · · , :)⟩ ∥G b (j, :, · · · , :)∥ 2</p><formula xml:id="formula_19">F ,<label>(13)</label></formula><p>which is the association accuracy for rule G b (j, :, · · · , :) ⇒ G b (i, :, · · · , :). Note that ∀i, D(i, i) = 1.</p><p>If G b has the perfect form of ∑ R r=1 z r • z r • · · · • z r , the set of columns of D should be exactly {z 1 , ...z R }. However, G b is often contaminated by some noise (e.g. it is not binary) or outliers and thus is not perfect. In this case, the closer D(i, j) is to 1, the more likely G b (i, :, · · · , :) and G b (j, :, · · · , :) are generated by the same vector z r with some noise, and the more likely that if z r (i) = 1, then z r (j) = 1, i.e., i and j belong to the same cluster r. We could set a threshold τ to get a binary matrix D b which contains all the candidate items for {z 1 , ...z R }, and choose the ones that could best describe G b by the greedy Algorithm.</p><p>To reduce the probability that the same position of z r contains multiple 1's and violates the Boolean orthonormal  <ref type="formula" target="#formula_0">(12)</ref> is not reduced in this loop, break, end if end for end for return Z * (i.e. exclusivity) constraint, we only retain as candidate those columns which are sufficiently different from the selected columns (based on some threshold t d ) for the next iteration. The full details are presented in Algorithm 1, in which the input R 0 is selected as the rank of G.</p><formula xml:id="formula_20">i = arg min i |G b ⊕G b |, whereG b = ∨ z∈Z ∪ D b (:,i) z • z • · · · • z; Z ← D b (:, i); delete all j-th columns with ⟨D b (:,i),D b (:,j)⟩ ∥D b (:,i)∥∥D b (:,j)∥ &gt; t d from D b . if ∥G − ∨ z∈Z z • z • · · · • z∥ 2 F &lt; e, Z * = Z; e = ∥G − ∨ z∈Z z • z • · · · • z∥ 2 F , end if if D b is empty or</formula><p>Since we only approximately enforce the Boolean orthonormal constraint, it is possible for a row of Z = {z r } R r=1 to contain multiple 1's in the same positions of z r . Usually, these constitute a very small proportion. Thus, most points can be uniquely assigned to clusters and the clusters are adequately populated. As a result, we can resolve the assignment conflict by a simple post-processing step as follows. We postpone the cluster assignment of all those points with conflicts. Assuming the resultant clustering is Y = {Y 1 , . . . , Y R } and that there is an unassigned data point i, we assign the point i to the group Y R ′ with whose members it has the largest affinities; that is R ′ = arg max r ∑ j∈Yr,e∈Eij A(e), where E ij contains all hyperedges that including node i and j and A is the affinity tensor as defined in Section 1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">An Alternative Solver via Stochastic Optimization</head><p>When the order of the problem increases, the number of unknowns in the previous formulation can rapidly reach a very large number. As an example, multiple homography fitting has order 5; even as few as 100 points leads to 10 10 unknowns. What is worse, we can easily exceed the memory limit of a modern desktop computer if we pre-compute and store the affinity tensor even for problem with moderate order and number of points. This motivates us to develop an alternative solver which is scalable to large problems.</p><p>Recall from Theorem 1 that there are exactly R + 1 types of slices (including a 0 slice) in the optimal solution for G. Thus, instead of solving for the tensor G, we solve for the R nonzero slices G j . While R is generally unknown, we do know that R is usually a small number. Thus, we give an upper bound R 0 to R:</p><formula xml:id="formula_21">min G 1 n n ∑ i=1 R 0 min j=1 ⟨Wi, Gj⟩ + γ R 0 ∑ j=1 ||Gj|| 2 0 , s.t. G ∈ {0, 1} N N ×R 0 , Gj ∈ S+, rank(Gj) ≤ 1, j = 1, ...R0, S GeR 0 = eN ,<label>(14)</label></formula><p>where W i is a slice from W,</p><formula xml:id="formula_22">G = [vec(G 1 ) vec(G 2 ) . . . vec(G R0 )]</formula><p>, e R0 and e N are all-one vectors of size R 0 and N respectively, and S ∈ {0, 1} N ×N N is a selection matrix with S(i, (N + 1)i − N ) = 1, i = 1, ..., N and other elements being 0. Essentially, Svec(G j ) selects the diagonal elements of G j and organise them into a column vector. Recall that Theorem 1 implies that the (i, i) diagonal elelment of each slice can be a 0 or 1; the constraint S Ge R0 = e N requires that the (i, i) element must be a 1 in at least one and only one of the slice G j . In <ref type="bibr" target="#b13">(14)</ref>, ∥ · ∥ 2 0 is the square of the ℓ 0 -norm. Numerically, squaring the ℓ 0 -norm enlarges the range of values spanned by the second term in <ref type="bibr" target="#b13">(14)</ref>, making <ref type="bibr" target="#b13">(14)</ref> less sensitive to the setting of γ. Compared to the unsquared ℓ 0 norm in (6), we need this stretching here because the second term only contains R 0 × N × N entries and thus its ℓ 0 -norm can only sum up to R 0 × N × N , whereas that in (6) can attain a much larger value of n × N × N . Note that as a consequence of the squaring, this penalty term would also favour a partitioning with more balanced clusters.</p><p>Note that in <ref type="bibr" target="#b13">(14)</ref>, G has very few columns, more exactly, R 0 of them; R 0 can be considered as an upper bound of the tensor rank of the original G. Comparing to <ref type="bibr" target="#b5">(6)</ref>, the number of unknowns is significantly reduced (from N 2 n to N 2 R 0 ), and importantly, it does not increase as the order increases.</p><p>For notational convenience, we denote the first term in the objective function as</p><formula xml:id="formula_23">f ( G) = 1 n ∑ n i=1 ℓ i ( G), and ℓ i ( G) = min R0 j=1 ⟨ W j i , G ⟩ , where W j i ∈ R N N ×R0</formula><p>with its j-th column given by vec(W i ) and 0 elsewhere. Now the problem is in a form suited for optimization by the stochastic ADMM <ref type="bibr" target="#b23">[24]</ref> 2 in an online fashion. In particular, we randomly obtain one slice W i from W for each iteration, solve the following constituent subproblems once, and then initiate the next iteration with a different slice from W, repeating the above until convergence.</p><p>Again we introduce the intermediate variables H and J and solve</p><formula xml:id="formula_24">min G 1 n n ∑ i=1 ℓi( G) + γ R 0 ∑ j=1 ||Hj|| 2 0 , s.t. Gj ∈ S+, G = H, G = J, H ∈ {0, 1} N N ×R 0 , rank(Gj) ≤ 1, j = 1, ...R0, S JeR 0 = eN .<label>(15)</label></formula><p>Using the notion of approximate augmented Lagrangian defined by <ref type="bibr" target="#b23">[24]</ref> and applying it to problem <ref type="bibr" target="#b14">(15)</ref>, we obtain</p><formula xml:id="formula_25">L k = ℓ k ( G k ) + ⟨ ℓ ′ k+1 ( G k ), G ⟩ + γ R 0 ∑ j=1 ||Hj|| 2 0 + 1 2µ || G − H|| 2 F + ⟨ Y1, G − H ⟩ + 1 2µ || G − J|| 2 F + ⟨ Y2, G − J ⟩ + 1 2µ ||S JeR 0 − eN || 2 F + ⟨ Y3, S JeR 0 − eN ⟩ + || G − G k || 2 F 2η k+1 s.t. H ∈ {0, 1} N N ×R 0 , Gj ∈ S+, rank(Gj) ≤ 1, j = 1, ...R0,<label>(16)</label></formula><p>where ℓ ′ k+1 is the subgradient of ℓ k+1 and || G − G k || is the proximal term which is scaled by a time-varying stepsize η k+1 . The subproblems we need to solve become:</p><formula xml:id="formula_26">Solving G. min G ∥ G − P∥ 2 F , s.t. Gj ∈ S+, rank(Gj) ≤ 1, j = 1 . . . R0,<label>(17)</label></formula><p>where P = ( 1</p><formula xml:id="formula_27">µ H+ 1 µ J+ 1 η k+1 G k −Y1−Y2−ℓ ′ k+1 ( G k ))/( 2 µ + 1 η k+1 ).</formula><p>Solving H.</p><formula xml:id="formula_28">min H ∥ H − Q∥ 2 F + 2µγ R 0 ∑ j=1 ||Hj|| 2 0 , s.t. H ∈ {0, 1} N N ×R 0 ,<label>(18)</label></formula><p>where Q = G + µY1. Solving J.</p><formula xml:id="formula_29">min J ⟨ Y2, G − J ⟩ + 1 2µ ∥ G − J∥ 2 F + ⟨ Y3, S JeR 0 − eN ⟩ + 1 2µ ∥S JeR 0 − eN ∥ 2 F<label>(19)</label></formula><p>Since subproblem <ref type="bibr" target="#b16">(17)</ref> can be solved in a similar manner as <ref type="bibr" target="#b10">(11)</ref> and <ref type="formula" target="#formula_0">(19)</ref> is linear in J, the only difficulty lies in <ref type="bibr" target="#b17">(18)</ref>. Evidently, <ref type="bibr" target="#b17">(18)</ref> is separable in the columns of H j . We solve each column H j individually by the following theorem:</p><formula xml:id="formula_30">Theorem 2.</formula><p>Let a ∈ R m be a given vector such that its elements a 1 ≥ a 2 ≥ · · · ≥ a m . Consider problem: <ref type="bibr" target="#b19">(20)</ref> where e is an all-one vector. The components of x * are also in a descending order and there exists an integer 0 ≤ s ≤ m such that x * i = 1 for i ≤ s and x * i = 0 for i &gt; s. This theorem can be proved by contradiction. The detailed proof is provided in the supplementary material.</p><formula xml:id="formula_31">x * = arg min ∥x − a∥ + ρ(e T x) 2 , s.t. x ∈ {0, 1} m ,</formula><p>The stochastic ADMM is close to but not exactly the same as ADMM; details of the algorithm are provided in the supplementary material. Since the problem is solved in an online fashion, we only need to randomly construct a slice in each iteration; thus this algorithm is memory-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we compare our method with various model selection methods, including those based on pairwise affinity (acting upon projected 2D graph) and those based on higher order affinity. For the former category, we adopt the widely used clique expansion algorithm <ref type="bibr" target="#b35">[36]</ref> to get the projected 2D graph. We then apply the basic gap heuristic (GH) <ref type="bibr" target="#b19">[20]</ref> followed by normalized cut <ref type="bibr" target="#b29">[30]</ref>, the results of which are taken as baseline. We also choose SCAMS <ref type="bibr" target="#b17">[18]</ref> as the representative of the state-of-the-art 2D graph methods for comparison. We denote these two methods as CExp+GH and CExp+SCAMS respectively. For the latter category, we choose the Ensembles of Affinity Relations (EAR) method <ref type="bibr" target="#b18">[19]</ref>, which is based on a generalization of the game-theoretic approach <ref type="bibr" target="#b5">[6]</ref>. To obtain the final clusterings of <ref type="bibr" target="#b18">[19]</ref>, we need to further construct a 2D graph based on the probabilities output of this method and apply GH and normalized cut. We denote our methods as SCAMSTA and SCAMSTA-SADMM (the stochastic ADMM version). In all experiments, the upper bound R 0 is given as R + 2, where R is the groundtruth group number. To evaluate the performance of the algorithms, we adopt the F1-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Line clustering</head><p>We first investigate the performance of the various methods on the task of line clustering in the 2D space. In this task, we randomly generate lines within the region [−0.5, 0.5] 2 , with all lines passing through the origin. To generate the affinities, we measure how well a triplet of points can be fitted to a line. Thus the order of the affinity tensor in this problem is three. To examine how noise can affect the performance of the various algorithms, we fix the number of lines to three and sample 10, 30 and 60 points from them respectively, resulting in a total of N = 100 points and three unbalanced clusters overlapping with each other to some extent. We perturb the sampled points by adding Gaussian noise N (0, σ), and consider 11 different noise levels: σ = 0, 0.005, · · · , 0.05. Since larger noise leads to more ambiguous affinities, we should reduce the influence of the data term (the first term in (6)) as noise level increases. This is actually implemented by increasing the weightage of the sparsity term in <ref type="bibr" target="#b5">(6)</ref>. For this experiment, we set the parameters λ = 2N and γ = 2σ for SCAMSTA and γ = 4σ × 10 −4 for SCAMSTA-SADMM. Similarly, we set λ = 2 and γ = 2σ for SCAMS. For EAR, we set ϵ = 1 10 , where the '10' is obtained from the number of points of the smallest cluster. The test runs 20 times and the average F1-measures are reported in <ref type="figure" target="#fig_1">Figure 1 (a)</ref>. We also evaluate our method under varying number of lines. In this experiment, we fix σ = 0.005, and thus the parameter settings correspond to that noise level. We gradually increase  <ref type="figure" target="#fig_1">Figure 1 (b)</ref>.</p><p>As can be seen from <ref type="figure" target="#fig_1">Figure 1 (a)</ref>, the performances of SCAMSTA and SCAMSTA-SADMM are close, and they are significantly better than those of the other methods at all noise levels. In <ref type="figure" target="#fig_1">Figure 1</ref> (b), SCAMSTA and SCAMSTA-SADMM also consistently outperform the other methods. When we examined the estimated numbers of clusters produced by the different approaches, the performance gap is even more remarkable. We find that our approaches yield 75% correct number estimation among all test runs, while those of the other approaches are less than 30%. The reason for this performance gap is that the projection step required to produce the conventional 2D graph representation loses information. This is especially detrimental to performance when there are unbalanced clusters, as the genuine affinities in the small cluster are averaged out to small values by the clique expansion step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Vanishing point estimation</head><p>We further evaluate our methods in dealing with real world problems. In this subsection, we tackle the vanishing point estimation problem 3 given multiple detected lines in the image plane. Since two lines always intersect at a point (perhaps at infinity), we need the third line to determine to what extent the triplet of lines intersect at a common point. Specifically, the affinity between three lines is computed as the sum of the distances from the three lines to the fitted intersection point, and the order of the affinity tensor in this problem is 3. Actually, this is a dual problem to the line clustering problem in the preceding experiment.</p><p>In this experiment, we evaluate the performance of the algorithms on the York Urban Dataset <ref type="bibr" target="#b11">[12]</ref>. This dataset consists of 102 images with line segments hand-assigned to the vanishing points and pre-selected to conform to the Manhattan assumption <ref type="bibr" target="#b8">[9]</ref>. It contains both indoor and outdoor scenes. Besides the other methods, we further compare with the J-Linkage clustering method <ref type="bibr" target="#b31">[32]</ref>, which is used in the state-of-the-art vanishing point estimation method <ref type="bibr" target="#b30">[31]</ref>. However, J-Linkage usually needs a good sampling, and thus a fully computed affinity tensor (our input) -which means all possible model candidates are sampled -leads to very bad result (F1-measure &lt; 0.6). Thus, the J-Linkage results reported here are instead obtained using the RCM sampling <ref type="bibr" target="#b24">[25]</ref> to generate the input samples. We first carry out our evaluation under this almost noiseless setting, for which we set parameters λ = 2N and γ = 0.05 for SCAMSTA, γ = 5 × 10 −5 for SCAMSTA-SADMM, λ = 2 and γ = 0.08 for SCAMS, the inlier threshold of J-Linkage is set to 0.05, and ϵ = 1 c for EAR, where c is the number of points of the smallest cluster obtained from the groundtruth. The first row of <ref type="table" target="#tab_0">Table 1</ref> reports the F1-measures averaged over the 102 images. As can be seen, our methods significantly outperform the others, being the ones with mean F1-measure greater than 0.9 and standard deviation less than 0.1. Note that the stochastic version degrades slightly compared to the original one.</p><p>The above scenario is not very realistic as the line segments are pre-selected and hand-labeled. Thus, we next recreate a more realistic environment by having the Canny line detector generate the line segments in the images. This setting brings much more noise (e.g. orientations of lines might be perturbed) and outlier lines (e.g. lines belonging to additional vanishing points). To evaluate the algorithms, we match the detected line segments to the pre-selected ones and compute the F1-measure of only those line segments which can be matched (as only those have groundtruth). The last row of <ref type="table" target="#tab_0">Table 1</ref> reports the F1-measures averaged over the 102 images. The F1-measures of all methods drop due to the noise and outliers except J-Linkage, probably due to the RCM sampling. Nevertheless, both SCAMSTA and its stochastic variant perform consistently better than the other methods; SCAMSTA outperforms the best of other methods by 0.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Multiple homography fitting</head><p>In this subsection, we estimate multiple planar homographies on real images. Given the matched points in an image pair, at least 4 correspondences are needed to fit a homography model; a 5-th point is need to determine the goodness of the fitted model, meaning that the order in this problem is at least 5. With such an order and the attendant large number of unknowns, the ADMM version of SCAMSTA is impractical. Thus, we only compare the stochastic AD-MM version with the others. Unfortunately, EAR cannot return a result in a reasonable time (i.e. 24 hours), and it often crashes with unknown reasons, so its result is also not listed. For a better comparison, we add the state-of-the-art multi-structure fitting method with label cost (LabelCost) <ref type="bibr" target="#b10">[11]</ref>, since multiple homography estimation is usually formulated as a multi-structure fitting problem.</p><p>In this experiment, we evaluate the performances of the algorithms on the AdelaideRMF dataset <ref type="bibr" target="#b33">[34]</ref>. This dataset consists of 17 image pairs with matched points available for multiple homography fitting. In each iteration of SCAMSTA-SADMM, we adopt the RCM sampling <ref type="bibr" target="#b24">[25]</ref> to sample 3 correspondences, as (5 − 2) samples are required to generate a slice. All the generated slices are also used to construct the projected 2D graph needed by other methods; using the same slices makes sure that the same amount of information are provided for each methods. We set the label cost to 7000 for all labels, parameters γ = 10 −5 for SCAMSTA-SADMM, and λ = 2 and γ = 0.16 for S-CAMS. The F1-measures averaged over 17 instances are reported in <ref type="table" target="#tab_1">Table 2</ref>. It is observed that our method again outperforms the others significantly, being the only one with mean F1-measure greater than 0.8. An extra experiment on multiple fundamental matrix fitting is provided in the supplementary material, which shows similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a general and robust algorithm to perform simultaneous clustering and model selection given an affinity tensor as input. This is a more general setting for solving clustering problems and the proposed algorithm subsumes the previous SCAMS algorithm for the matrix case. To solve this problem, we impose the low-rank and sparsity conditions on the affinity tensor to reveal the underlying clusters hidden therein. With careful observation and design, we transform the tensor rank minimization and rank-1 sum constraint into matrix forms such that the problem becomes solvable. To tackle the scalability issue, we also provide an alternative slice formulation which lends itself to a stochastic ADMM solution. Experiments on different applications show its scalability to large problems and the advantage of our approach in dealing with unbalanced clusters and detecting small clusters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>by Alternating Direction Method of Multipliers(ADMM) [5] with three block variables G, H and {J i } n i=1 : min ⟨ W, G⟩ + λrank( G) + γ∥ H∥0 + g( H), s.t. unfolding diag( H) = 1, G = H, G = J, Ji ∈ S+, rank(Ji) ≤ 1, i = 1, 2, . . . n,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>The AssoCBTD algorithm Input: G, R 0 . Output: A set of indicator vectors Z * = {z 1 , . . . , z R }.Initialize: Z = ∅, e = ∞, t d = 0.1, and construct G b from G with rounding threshold t b = 0.5. for τ = 0.1, 0.2, . . . , 1 do Construct D by<ref type="bibr" target="#b12">(13)</ref> and obtain the Boolean matrix D b with threshold τ . for k = 1, 2, ..., R 0 do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Comparison on line clustering. Left: F1-measure on clustering the lines perturbed with increasing noise levels. Right: F1-measure on clustering varying number of lines. the line number from 2 to 8, and the average F1-measures over 20 runs are reported in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Vanishing point estimation (F1-measure) on York Urban Dataset SADMM Pre-selected 0.8487 ± 0.1849 0.8884 ± 0.1083 0.8434 ± 0.1515 0.7831 ± 0.1010 0.9426 ± 0.0739 0.9274 ± 0.0793 Detected 0.6602 ± 0.1617 0.7993 ± 0.1966 0.6570 ± 0.1837 0.7936 ± 0.1504 0.8273 ± 0.1760 0.8035 ± 0.1938</figDesc><table>Method 
CExp-GH 
CExp-SCAMS 
EAR 
J-Linkage 
SCAMSTA 
SCAMSTA-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Multiple homography fitting (F1-measure) on Adelai-deRMF dataset</figDesc><table>Method 
Label-
Cost 

CExp-
GH 

CExp-
SCAMS 

SCAMSTA-
SADMM 
mean ± 
variance 

0.7871± 
0.1183 

0.6980± 
0.1796 

0.7827± 
0.1925 

0.8418 ± 
0.1071 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that its convergence result is not applicable due to the non-convex problems in our formulation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Under projective geometry, a set of parallel lines in 3D scene is projected onto a set of image lines meeting at a common point known as the vanishing point. Identifying these vanishing points provides strong 3D cues for inferring structure of the scene.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by Singapore PSF grant 1321202075.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond pairwise clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining association rules between sets of items in large databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlation clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximating symmetric positive semidefinite tensors of even order</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barmpoutis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Vemuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="434" to="464" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to hypergraph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1312" to="1327" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of individual differences in multidimensional scaling via an n-way generalization of eckart-young decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="319" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The convex geometry of linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="805" to="849" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Manhattan world: Orientation and outlier detection by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1063" to="1088" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multilinear singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1253" to="1278" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization with label costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Isack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric information criterion for model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kanatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="189" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image segmentation using higher-order correlation clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilinear operators for higher-order decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<idno>SAND2006-2081</idno>
	</analytic>
	<monogr>
		<title level="m">Sandia National Laboratories</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient hypergraph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SCAMS: simultaneous clustering and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust clustering as ensembles of affinity relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The discrete basis problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miettinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mielikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1348" to="1362" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fundamental limitations of spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clustering with hypergraphs: The case for large hyperedges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Purkait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new approach to vanishing point detection in architectural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="647" to="655" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Computational and Applied Math</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-way clustering using super-symmetric non-negative tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Augmented lagrangian alternating direction method for matrix separation based on low-rank factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Methods and Software</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="263" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-iterative approach for fast and accurate vanishing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tardif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust multiple structures estimation with j-linkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fusiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic and hierarchical multi-structure geometric model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning with hypergraphs: Clustering, classification, and embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multilevel spectral hypergraph partitioning with arbitrary vertex sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D F</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on CAD of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
