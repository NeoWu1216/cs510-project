<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimizing Global Loss Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
							<email>vijay.kumar@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<email>gustavo.carneiro@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<email>ian.reid@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimizing Global Loss Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent innovations in training deep convolutional neural network (ConvNet) models have motivated the design of new methods to automatically learn local image descriptors. The latest deep ConvNets proposed for this task consist of a siamese network that is trained by penalising misclassification of pairs of local image patches. Current results from machine learning show that replacing this siamese by a triplet network can improve the classification accuracy in several problems, but this has yet to be demonstrated for local image descriptor learning. Moreover, current siamese and triplet networks have been trained with stochastic gradient descent that computes the gradient from individual pairs or triplets of local image patches, which can make them prone to overfitting. In this paper, we first propose the use of triplet networks for the problem of local image descriptor learning. Furthermore, we also propose the use of a global loss that minimises the overall classification error in the training set, which can improve the generalisation capability of the model. Using the UBC benchmark dataset for comparing local image descriptors, we show that the triplet network produces a more accurate embedding than the siamese network in terms of the UBC dataset errors. Moreover, we also demonstrate that a combination of the triplet and global losses produces the best embedding in the field, using this triplet network. Finally, we also show that the use of the central-surround siamese network trained with the global loss produces the best result of the field on the UBC dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The design of effective local image descriptors has been instrumental for the application of computer vision methods in several problems involving the matching of local image patches, such as wide baseline stereo <ref type="bibr" target="#b20">[21]</ref>, structure from motion <ref type="bibr" target="#b21">[22]</ref>, image classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>, just to name a few. Over the last decades, numerous hand-crafted <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref> and automatically learned <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> local image descriptors have been proposed and used in the applications above. Despite their conceptual differ-ences, these two types of local descriptors are formed based on similar goals: descriptors extracted from local image patches of the same 3-D location of a scene must be unique (compared with descriptors from different 3-D locations) and robust to brightness and geometric deformations. Given the difficulty in guaranteeing such goals for hand-crafted local descriptors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>, the field has gradually focused more on the automatic learning of such local descriptors, where an objective function that achieves the goals above is used in an optimisation procedure. In particular, the most common objective function minimises the distance between the descriptors from the same 3-D location (i.e., same class) extracted under varying imaging conditions and different viewpoints and, at the same time, maximises that distance between patches from different 3-D locations (or different classes) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>The more recently proposed approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36</ref>] based on deep ConvNets <ref type="bibr" target="#b17">[18]</ref> optimise slightly new objective functions that have the same goal as mentioned above. Specifically, Zagoruyko and Komodakis <ref type="bibr" target="#b35">[36]</ref> and Han et al. <ref type="bibr" target="#b11">[12]</ref> minimise a pairwise similarity loss of local image patches using a siamese network <ref type="bibr" target="#b1">[2]</ref> (see <ref type="figure">Fig. 1-(b)</ref>), where the patches can belong to the same or different classes (a class is for example a specific 3-D location). Dosovitskiy et al. <ref type="bibr" target="#b9">[10]</ref> minimise a multi-class classification loss ( <ref type="figure">Fig. 1-(c)</ref>), where the model outputs the classification of a single input patch into one of the many descriptor classes (estimated in an unsupervised manner). Moreover, <ref type="bibr">Masci et</ref> al. <ref type="bibr" target="#b19">[20]</ref> propose a siamese network trained with a pairwise loss that minimises the distance (in the embedded space) between patches of the same class and maximises the distance between patches of different classes ( <ref type="figure">Fig. 1-(b)</ref>). Even though these methods show substantial gains compared to the previous state of the art in public benchmark datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, we believe that the loss functions and network structures being explored for this task can be improved. For instance, the triplet network <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> (see <ref type="figure">Fig. 1</ref>-(d)) has been shown to improve the siamese network on several classification problems, and the training of the siamese and triplet networks can involve loss functions based on global classification results, which has the potential to generalise better.</p><p>In this paper, we propose the use of the triplet net- <ref type="bibr">Figure 1</ref>. Comparison between different types of loss functions, network architectures and input/output types used by training methods of local image descriptor models. The metric learning loss to learn a linear transform G is represented in (a) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref> (please see text for the definition of Sw and S b ) and produces a feature embedding; in (b) we show the siamese network <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36</ref>] that can be trained with different loss functions and input types, where δ(.) denotes the Dirac delta function, y is the data label, Net(x) represents the ConvNet response for input x (similarly for Net(x i ,x j )), and the output can be an embedding (i.e., Net(x)) or a pairwise similarity estimation (i.e., Net(x i ,x j )); the classification network in (c) can be used when classes of local image descriptors can be defined <ref type="bibr" target="#b9">[10]</ref> and used in a multiclass classification problem; and in (d) the recently proposed triplet network <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> is displayed with different loss functions and input types, where x + represents a point belonging to the same class as x and x − a point from a different class of x (this triplet net produces in general an embedding). Note that our proposed global loss (embedding) in (b) and (d) takes the whole training set as input and minimises the variance of the distance of points belonging to the same and different classes and at the same time, minimise the mean distance of points belonging to the same class and maximise the mean distance of points belonging to different classes. The global loss (pairwise similarity) in (b) is similarly defined (please see text for more details).</p><p>work <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>  <ref type="figure">(Fig. 1-(d)</ref>) and a new global loss function to train local image descriptor learning models that can be applied to the siamese and triplet networks ( <ref type="figure">Fig. 1</ref>-(b),(d)). The global loss to produce a feature embedding minimises the variance of the distance between descriptors (in the embedded space) belonging to the same and different classes, minimises the mean distance between descriptors belonging to the same class and maximises the mean distance between descriptors belonging to different classes ( <ref type="figure">Fig. 1-(b)</ref>,(d)). For the case of pairwise similarity in siamese networks, this global loss minimises the variances of the pairwise similarity between descriptors belonging to the same and different classes, maximises the mean similarity between descriptors belonging to the same class and minimises the mean similarity between descriptors belonging to different classes ( <ref type="figure">Fig. 1-(b)</ref>). We first extend the siamese network <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36</ref>] to a triplet network, trained with a triplet loss <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> and regularised by the proposed global loss (embedding). Then we take the siamese network <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref> and train it exclusively with the global loss (pairwise similarity). Finally, we take the central-surround siamese network <ref type="bibr" target="#b35">[36]</ref>, which is the cur-rent state-of-the-art model for the problem of local image descriptor learning, and train it with the global loss (pairwise similarity). We show on the public benchmark UBC dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref> that: 1) the triplet network shows better classification results than the siamese network <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref>; 2) the combination of the triplet and the global loss functions improves the results produced by the triplet loss from item (1) above, resulting in the best embedding result in the field for the UBC dataset; and 3) the global loss function used to train the central-surround siamese network <ref type="bibr" target="#b35">[36]</ref> produces the best classification result on the UBC dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we first discuss metric learning methods, which form the basis for several local image descriptor learning approaches. Then, we discuss relevant local image descriptor learning methods recently proposed in the field, and highlight our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Metric Learning</head><p>In general, metric learning (see <ref type="figure">Fig. 1</ref>-(a)) assumes the existence of a set of points represented by {x i } N i=1 , with x i ∈ R n and a respective set of labels {y i } N i=1 , with y i ∈ {1, ..., C}, and the goal is to find a Mahalanobis distance with parameter W. For example, the square distance between x i and x j is <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref>:</p><formula xml:id="formula_0">d W = (x i − x j ) ⊤ W(x i − x j ),<label>(1)</label></formula><p>where the factorisation of the matrix W = GG ⊤ (with G ∈ R n×m ) allows us to formulate the following optimisation problem:</p><formula xml:id="formula_1">G * = arg max G tr (G ⊤ S w G) −1 (G ⊤ S b G) , with S k = ij Y k (x i − x j )(x i − x j ) ⊤ , Y w = Y, Y b = 1 − Y, and Y ij = 1 if y i = y j and Y ij = 0,</formula><p>otherwise. This optimisation is solved using the generalised Eigenvalue problem, which generates a linear feature transform that effectively produces a feature embedding. The method above has been extended in many ways, such that: 1) it can handle multimodal distributions in <ref type="bibr" target="#b30">[31]</ref>; 2) it optimises K nearest neighbour classification, which is formulated as a softmax loss minimisation and estimates a linear transform with eigenvalue decomposition <ref type="bibr" target="#b10">[11]</ref>; 3) it optimises a large margin re-formulation of the problem in (1) using semidefinite programming <ref type="bibr" target="#b33">[34]</ref>; 4) it can use a prior for W, which regularises the training and gets around the cubic complexity issues of the previous methods <ref type="bibr" target="#b8">[9]</ref>; and 5) it can be extended to large problems using equivalence constraints <ref type="bibr" target="#b16">[17]</ref>. However, the main issue is the fact that (1) leads to a linear transformation that is unlikely to handle some of the difficult (and usually more interesting) learning problems.</p><p>Extending (1) to a non-linear transformation can be done by re-formulating S k such that it involves inner products, which can then be kernelised <ref type="bibr" target="#b30">[31]</ref>, and the optimisation is again solved with generalised Eigenvalue problem <ref type="bibr" target="#b30">[31]</ref>. Alternatively, this non-linear transform can be learned with a ConvNet using a siamese network <ref type="bibr" target="#b1">[2]</ref> that minimises a pairwise loss <ref type="bibr" target="#b5">[6]</ref>  <ref type="figure">(Fig. 1-(b)</ref>) by reducing the distance of patches (in the embedded space) belonging to the same class and increasing the distance of patches from different classes, similarly to the objective function derived from (1). Note that this siamese network can produce either an embedding or a pairwise similarity estimation, depending on the architecture and loss function. This siamese network has been extended to a triplet network that uses a triplet loss <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>  <ref type="figure">(Fig. 1-(d)</ref>), which has been shown not only to produce the best classification results in several problems (e.g., STL10 <ref type="bibr" target="#b6">[7]</ref>, LineMOD <ref type="bibr" target="#b12">[13]</ref>, Labelled Faces in the Wild), but also to produce effective feature embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Local Image Descriptor</head><p>In the past, many local image descriptor learning methodologies have been proposed, with most based on the linear or non-linear distance metric learning, and explored in different ways <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>. However, these methods have been shown to produce significantly worse classification results on the UBC dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref> than the recently proposed siamese deep ConvNets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref> (note that the UBC dataset is a benchmark dataset that has been used to compare local image descriptors). Even though the triplet network <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> has been demonstrated to improve the results produced by the siamese networks, it has yet to be applied to the problem of local image descriptor learning. Finally, another relevant method is the discriminative unsupervised learning of local descriptors <ref type="bibr" target="#b9">[10]</ref>, which uses a single deep ConvNet to classify input local patches into many classes, which are generated in an unsupervised manner ( <ref type="figure">Fig. 1-(c)</ref>). However, the latter method has not been applied to the UBC dataset mentioned above. It is also important to notice that none of the deep ConvNets methods above use the whole training set in a global loss function during the learning process, which can improve the generalisation ability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As mentioned above in Sec. 2.1, we assume the existence of a training set of image patches and their respective classes, i.e.,</p><formula xml:id="formula_2">{(x i , y i )} N i=1</formula><p>, with x i ∈ R n and y i ∈ {1, ..., C} (note that we use n as the patch size to simplify the notation, but the extension to a matrix representation for x is trivial). The first goal of our work is to use a triplet network and respective triplet loss (defined below in detail) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> to produce a feature embedding f (x, θ f ) defined by f : R n × R k → R m , where θ f ∈ R k denotes the network parameters (weights, biases and etc.). The second goal is to design a new global loss function to train the triplet <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> and siamese networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref>, where we are particularly interested in the 2-channel 2-stream network, represented by a multiresolution central-surround siamese network. Essentially, the siamese network can form a feature embedding, like the one above, or a pairwise similarity estimator, represented with g(x i , x j , θ g ), which is defined by g : R n ×R n ×R k → R. In this section, we first explain the siamese and triplet networks, then we describe the proposed global loss function, and we also present the models being proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Siamese and Triplet Networks</head><p>The siamese network <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref> is trained with a two-tower deep ConvNet ( <ref type="figure">Fig. 1-(b)</ref>), where the weights on both towers are initialised at the same values and during stochastic gradient descent, they receive the same gradients (i.e., the weights on both towers are tied). We consider the following definition of a deep ConvNet:</p><formula xml:id="formula_3">f (x, θ f ) = f out • r L • h L • f L • ... • r 1 • h 1 • f 1 (x),<label>(2)</label></formula><p>where the parameter θ f is formed by the network weight matrices, bias vectors, and normalisation parameters for each layer l ∈ {1, ..., L}, f l (.) represents the pre-activation function (i.e., the linear transforms in the convolutional layers), h l (.) represents a normalisation function, and r l (.) is a non-linear activation function (e.g., ReLU <ref type="bibr" target="#b22">[23]</ref>). Also note that f l = [f l,1 , ..., f l,n l ] is an array of n l pre-activation functions, representing the number of features in layer l. A siamese network is then represented by two identical deep ConvNets trained using pairs of labelled inputs, where one possible loss function (called pairwise loss) minimises the distance between embedded features of the same class and maximises the distance between embedded features of different classes, as follows <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>:</p><formula xml:id="formula_4">J s 1 (x i , x j ,θ f ) = δ(y i − y j ) f (1) (x i , θ f ) − f (2) (x j , θ f ) 2 − (1 − δ(y i − y j )) f (1) (x i , θ f ) − f (2) (x j , θ f ) 2 ,<label>(3)</label></formula><p>where δ(.) is the Dirac delta function and f <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_5">(x, θ f ) is con- strained to equal to f (2) (x, θ f ).</formula><p>Alternatively, the siamese network can be trained as a pairwise similarity estimator, with a pairwise similarity loss that can be defined as:</p><formula xml:id="formula_6">J s 2 (x i , x j , θ g ) =δ(y i − y j )(1/(κ + g(x i , x j , θ g ))) + (1 − δ(y i − y j ))g(x i , x j , θ g ),<label>(4)</label></formula><p>where the ConvNet g(x i , x j , θ g ) returns the similarity between the descriptors x i and x j , with κ representing a small positive constant. Note that the loss functions used by recently proposed methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12]</ref> are conceptually similar to (4), but not exactly the same, where the idea is to produce a ConvNet g(x i , x j , θ g ) that returns large similarity values when the inputs belong to the same class and small values, otherwise. It is important to emphasise that the local descriptor learning model that currently produces the smallest error on the UBC dataset (Central-surround two-stream network) consists of a siamese network, trained with a loss similar to (4), where the input patch is sampled twice at half the resolution of the input image: one sample containing the whole patch is input to the surround stream and another sample containing a sub-patch at the centre of the original patch is input to the central stream <ref type="bibr" target="#b35">[36]</ref>. The output of these two streams are combined to obtain a similarity score. The triplet network <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>  <ref type="figure">(Fig. 1-(d)</ref>) is an extension of the siamese network that is trained with triplets at the input (which produces an embedding) using the triplet loss function, as follows:</p><formula xml:id="formula_7">J t 1 (x, x + , x − , θ f ) = max 0, 1 − f (1) (x, θ f ) − f (3) (x − , θ f ) 2 f (1) (x, θ f ) − f (2) (x + , θ f ) 2 + m ,<label>(5)</label></formula><p>where m is the margin, x + and x belong to the same class, x − and x are from different classes, and f (1) (.), f <ref type="bibr" target="#b1">(2)</ref> (.) and f (3) (.) are constrained to be the same network. Note that the losses in <ref type="formula" target="#formula_4">(3)</ref> and <ref type="formula" target="#formula_7">(5)</ref> are apparently similar, but they have a noticeable difference, which is the fact that a triplet of similar and dissimilar inputs gives context for the optimisation process, as opposed to the pairwise loss that the siamese network minimises (same class) or maximises (different classes) as much as possible for each pair independently <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global Loss function</head><p>The siamese and triplet networks presented in Sec. 3.1 typically contain a large number of parameters, which means that a large number of pairs or triplets must be sampled from the training data such that a robust model can be learned. However, sampling all possible pairs or triplets from the training dataset can quickly become intractable, where the majority of those samples may produce small costs in (3)-(5), resulting in slow convergence <ref type="bibr" target="#b25">[26]</ref>. An alternative is to have a smart sampling strategy, where one must be careful to avoid focusing too much on the hard training cases because of the possibility of overfitting <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref>. In this paper, we propose a simple, yet effective, loss function that can overcome these drawbacks.</p><p>The main idea behind our proposed loss function is the avoidance of the over-or under-sampling problems mentioned above with the assumption that the distances (or similarities) between descriptors of the same class (i.e., matching pairs) or different classes (i.e., non-matching pairs) are samples from two distinct distributions. This allows us to formulate a loss function (for the embedding case) that globally tries to: 1) minimise the variance of the two distributions and the mean value of the distances between matching pairs, and 2) maximise the mean value of the distances between non-matching pairs. <ref type="figure" target="#fig_0">Fig. 2-(a)</ref> depicts the reasoning behind the design of the proposed global loss function, which is defined for the feature embedding case by:</p><formula xml:id="formula_8">J g 1 ({x i } N i=1 ,{x + i } N i=1 , {x − i } N i=1 , θ f ) = (σ 2+ + σ 2− ) + λ max 0, µ + − µ − + t ,<label>(6)</label></formula><formula xml:id="formula_9">where µ + = N i=1 d + i /N, µ − = N i=1 d − i /N , σ 2+ = N i=1 (d + i − µ + ) 2 /N, σ 2− = N i=1 (d − i − µ − ) 2 /N ,<label>with</label></formula><p>µ + and σ 2+ denoting the mean and variance of the matching pair distance distribution, µ − and σ 2− representing the mean and variance of the non-matching pair dis-</p><formula xml:id="formula_10">tance distribution, d + i = f (1) (xi,θ f )−f (2) (x + i ,θ f ) 2 2 4 , d − i = f (1) (xi,θ f )−f (3) (x − i ,θ f ) 2 2 4</formula><p>, λ is a term that balances the importance of each term, t is the margin between the mean of the matching and non-matching distance distributions and N is the size of the training set. Note in <ref type="formula" target="#formula_8">(6)</ref>, that we assume a triplet network (i.e., f (1) (.), f <ref type="bibr" target="#b1">(2)</ref> (.) and f (3) (.) are the same network), where the squared Euclidean distances of the matching and non-matching pairs of the i th triplet are constrained to be 0 ≤ d + i , d − i ≤ 1 because of the division by 4, and the normalisation layer enforces that the norm of the embedding is 1.</p><p>Given that we use SGD for the optimisation process, we need to derive the gradient of the global loss function, as follows:</p><formula xml:id="formula_11">∂J g 1 ∂f (x i ) = − 1 2N 2 (d + i − µ + )f (x + i ) + (d − i − µ − )f (x − i ) + λ(f (x + i ) − f (x − i ))1((µ − − µ + ) &lt; t) , ∂J g 1 ∂f (x + i ) = − 1 2N 2 (d + i − µ + )f (x i ) + f (x i )1((µ − − µ + ) &lt; t) , ∂J g 1 ∂f (x − i ) = − 1 2N 2 (d − i − µ − )f (x i ) − f (x i )1((µ − − µ + ) &lt; t)<label>(7)</label></formula><p>where the dependence on θ f and the channel index f (.) are dropped to simplify the notation, and 1(a) is an indicator function with value 1 when a is true. It is important to note that the gradient ∂J t 1 /∂f (x i ) of the triplet loss in (5) depends only on the i th triplet of the training set, whereas the gradient ∂J g 1 /∂f (x i ) of the global loss in <ref type="formula" target="#formula_11">(7)</ref> depends on µ + and µ − , which in turn depends on the statistics of the samples in the whole training set. This dependence on global training set statistics has the potential to suppress the spurious gradients computed from outliers and thus improving the generalisation of the trained model. This global loss can be slightly modified to train a siamese network that estimates pairwise similarities, where the objective consists of: 1) minimising the variance of the two distributions and the mean value of the similarities between non-matching pairs, and 2) maximising the mean value of the similarities between matching pairs. <ref type="figure" target="#fig_0">Fig. 2-(b)</ref> shows the idea behind the design of the proposed pairwise similarity global loss function, which is defined by:</p><formula xml:id="formula_12">J g 2 = ({x i } N i=1 ,{x + i } N i=1 , {x − i } N i=1 , θ f ) = (σ 2+ + σ 2− ) + λ max 0, m − (µ + − µ − ) ,<label>(8)</label></formula><p>where g(x,x, θ g ) produces a similarity score between</p><formula xml:id="formula_13">x andx, µ + = N i=1 g(x i , x + i , θ g )/N, µ − = N i=1 g(x i , x − i , θ g )/N , σ 2+ = N i=1 (g(x i , x + i ) − µ + ) 2 /N, σ 2− = N i=1 (g(x i , x − i ) − µ − ) 2 /N ,</formula><p>with µ + and σ 2+ denoting the mean and variance of the matching pair similarity distribution, µ − and σ 2− representing the mean and variance of the non-matching pair similarity distribution, λ is a term that balances the importance of each term, m is the margin between the mean of the matching and non-matching similarity distributions and N is again the size of the training set (note that we are abusing the notation with the re-definition of µ + , µ − , σ 2+ , and σ 2− ). The gradient of this global loss function is derived as</p><formula xml:id="formula_14">∂J g 2 ∂g(x i , x + i , θ g ) = 2 N (g(x i , x + i , θ g ) − µ + ) − 1 2 1((µ + − µ − ) &lt; m) ∂J g 2 ∂g(x i , x − i , θ g ) = 2 N (g(x i , x − i , θ g ) − µ − ) + 1 2 1((µ + − µ − ) &lt; m) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proposed Models</head><p>We propose four models for the the problem of local image descriptor learning. The first model consists of a triplet network trained with the triplet loss in (5), which produces an embedding -this is labelled as TNet, TLoss. The second model is a triplet network that also produces an embedding and uses the following loss function that combines the original triplet loss (5) and the proposed global loss (6) for the learning process:</p><formula xml:id="formula_15">J tg 1 ({x i } N i=1 , {x + i } N i=1 , {x − i } N i=1 ) = γ j J t 1 (x j , x + j , x − j ) + J g 1 ({x i } N i=1 , {x + i } N i=1 , {x − i } N i=1 ),<label>(10)</label></formula><p>-this model is labelled as TNet, TGLoss. The third model is a siamese network that produces a similarity estimation of an input pair of local patches, but the model is trained with the siamese global loss defined in (8) -this model is labelled as SNet, GLoss. Finally, the fourth model is the centralsurround siamese network model described in Sec. 3.1 that also produces the pairwise similarity estimator of an input pair of local patches and is trained with the global loss (8) -this model is labelled as CS SNet, GLoss. Note that for the first two models that produce the embedding, the comparison between local descriptors is done based on the ℓ 2 norm of the distance in the embedded feature space. In terms of the ConvNet structure, we use an architecture similar to the one described by Zagoruyko and Komodakis <ref type="bibr" target="#b35">[36]</ref>. Specifically, the triplet network has the following structure: B(96,7,3)-P(2,2)-B(192,5,1)-P(2,2)-B <ref type="figure" target="#fig_0">(256,3,1)-B(256,1,1)-B(256,1,1)</ref>. The siamese network has the following architecture: B(96,7,3)-P(2,2)-B(192,5,1)-P(2,2)-B <ref type="figure" target="#fig_0">(256,3,1)-B(256,1,1)-C(1,1,1)</ref>.</p><p>Furthermore, the central-surround siamese network has the following structure: B(95,5,1)-P(2,2)-B(96,3,1)-P(2,2)-B <ref type="figure" target="#fig_0">(192,3,1)-B(192,3,1</ref>) and the final block that combines the outputs of the two input streams has components B(768,2,1)-C <ref type="figure">(1,1,1)</ref>. In the description above, P(p, q) is a max pooling layer of size p × p and stride q, and B(n, k, s) is a block with the components C(n, k, s)-bnorm(n), where C(n, k, s) is a convolutional layer with n filters of kernel size k and stride s, bnorm(n) is the batch normalisation unit [15] with 2n parameters. Each B and C is followed by a rectified linear unit <ref type="bibr" target="#b22">[23]</ref> except the final layer. Finally, the output feature from the exmedding networks are normalised to have unit norm, as mentioned in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Toy Problem</head><p>To illustrate the robustness of the proposed global loss function to outliers, we generated a toy dataset in two dimensions with two classes (80 samples from two Gaussian distributions) represented by two distinct cloud of points, as indicated by the red and green points in <ref type="figure" target="#fig_1">Fig. 3-(a)</ref>. We introduce outliers by switching the labels of randomly selected points (i.e., we switch the labels of 5% of the training set, or 4 samples). We generate a set of triplets from this training set and train a ConvNet that maps the input points to an output embedding space with 128 dimensions with the following structure: B(256,2,1)-B(512,1,1)-C(128,1,1), where the output is normalised to have unit norm and these blocks are defined in Sec. 3.3. Three separate trainings are run: the first training uses the triplet loss function in (5), the second uses a combination of the triplet and global losses in <ref type="bibr" target="#b9">(10)</ref>, and the third uses only the global loss in <ref type="bibr" target="#b5">(6)</ref>. To ensure a fair comparison, we run the experiments with identical settings, where the only difference is the loss function. We evaluate the models learned from each loss function by computing the embedding of a grid of points from the input space, and labelling them based on the label of the nearest neighbour from the training set, found in the embedding space. <ref type="figure" target="#fig_1">Figure 3-(b)</ref> shows the input space labelled according to the nearest neighbour classifier run in the embedding space generated by the triplet loss. Similarly, <ref type="figure" target="#fig_1">Fig. 3-(c)</ref> shows the same result for the combined triplet and global losses and <ref type="figure" target="#fig_1">Fig. 3-(d)</ref> displays the results for the global loss. In general, it is clear that outliers affect more the classifier in (b), which seems to be over-fitting the training data. Such labelling mistakes are reduced when we use the combination of the triplet and global losses as show in <ref type="figure" target="#fig_1">Fig. 3-(c)</ref>. The label map in <ref type="figure" target="#fig_1">Fig. 3-(d)</ref> generated by the embedding that uses global loss is coherent even at the locations, where outliers can be found in the training set, indicating that the global loss function is robust to outliers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first describe the dataset used for assessing our proposed models, then we explain the model setup, followed by a presentation of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">UBC Benchmark Dataset</head><p>The experiments are based on the performance evaluation of local image patches using the standard UBC benchmark dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref>, which contains three sets: Yosemite, Notre Dame, and Liberty. Using these sets, we run six combinations of training and testing sets, where we use one set for training and another for testing. Each one of this sets has more than 450, 000 local image patches (with normalised orientation and scale) of size 64 × 64 sampled using a Difference of Gaussians (DoG) detector. In each of these sets there are more than 100, 000 patch classes that are determined based on their 3-D locations obtained from multi-view stereo depth maps. These patch classes are used to produce 500, 000 pairs of matching (i.e., from the same class) and non-matching (i.e., different classes) image patches. Each model is assessed using the false positive at 95% recall (FPR95) on each of the six combinations of training and testing sets, the mean over all combinations, and the receiver operating characteristic (ROC) curve also for each of the six combinations. The test set contains 100, 000 pairs with equal number of matching and non-matching pairs and is chosen as specified in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training Setup and Implementation Details</head><p>The model training is based on stochastic gradient descent (SGD) that involves: 1) the use of a learning rate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Proposed Models Zagoruyko et al. <ref type="bibr" target="#b35">[36]</ref> Xufeng et al. <ref type="bibr" target="#b11">[12]</ref> Train Test   <ref type="bibr" target="#b35">[36]</ref>. The training set for the triplet and siamese networks consists of a set of 250, 000 triplets, which are sampled randomly from the aforementioned set of 500, 000 pairs of matching and nonmatching image patches, where it is important to make sure that the triplet contains one pair of matching image patches and one patch that belongs to a different class of this pair. The mini-batch of the SGD optimisation consists of 250 triplets (randomly picked from this 250K set of triplets), which is used to compute the global loss in <ref type="formula" target="#formula_8">(6)</ref> and <ref type="formula" target="#formula_12">(8)</ref>.</p><p>Our Matlab implementation takes ≈ 56 hours for training a model and processes 16K images/sec during testing on a GTX 980 GPU.</p><p>The triplet networks TNet-TLoss and TNet-TGLoss use the three towers of ConvNets (see <ref type="figure">Fig. 1</ref>) to learn an embedding of size 256 (we choose this number of dimensions based on the feature dimensionality of the models in <ref type="bibr" target="#b35">[36]</ref>, which also have 256 dimensions before the fully connected layer). During testing, only one tower is used (all three towers are in fact the same after training) to compute the embedded features, which are compared based on the ℓ 2 norm of the distance between these embedded features. The network weights for the TNet-TLoss network are initialised randomly and trained for 100 epochs, whereas the weights for the TNet-TGLoss network are trained for 50 epochs after being initialised using the weights from TNet-TLoss network trained for 50 epochs (the initialisation from the TNet-TLoss model trained with early stopping provided a good initialisation for TNet-TGLoss). This number of epochs for training is decided based on the convergence obtained in the training set with respect to the loss function. Moreover, the margin parameter m = 0.01 in (5) and γ = 1, t = 0.4 and λ = 0.8 in (10) are estimated via cross validation. For the siamese networks SNet-GLoss and CS-SNet-GLoss, the weights are randomly initialised and trained for 80 epochs (again, based on the convergence of the training set). Finally, m = 1 and λ = 1 in (8) are also estimated via cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on UBC Benchmark Dataset</head><p>Tables 1 and 2 summarises the performance of the proposed models and compares them with the current state-ofthe-art methods for the UBC dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref> using the FPR95 on each of the six combinations of training and testing sets, and the mean over all combinations. Note that we separate the results in terms of the comparison of descriptors obtained by pairwise similarity methods (Tab. 1) and embedding (Tab. 2). We also show the ROC curves for each of the six combinations of training and testing sets in <ref type="figure">Fig. 4</ref> for our proposed models, in addition to the current state-ofthe-art models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>From the results in Tab. 2 and <ref type="figure">Fig. 4</ref>  <ref type="figure">Figure 4</ref>. ROC curves on the UBC benchmark dataset for our proposed models, and the current state-of-the-art descriptors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>. Note that for our models, we use the test set specified in <ref type="bibr" target="#b0">[1]</ref> to compute these curves, and the numbers in the legends represent the FPR95 values.</p><p>the proposed triplet network trained with a combination of the triplet and global losses (i.e., the TNet-TGLoss) shows the best result in the field in terms of feature embedding. The pairwise similarity results in Tab. 1 and <ref type="figure">Fig. 4</ref> indicate that our centre-surround siamese network trained with global loss (i.e., the CS SNet, GLoss) produces a result that is almost half of the previous state-of-the-art result, i.e., the 2ch-2stream <ref type="bibr" target="#b35">[36]</ref>.</p><p>Similar to <ref type="bibr" target="#b35">[36]</ref>, we notice that the siamese networks trained with the pairwise similarity loss achieve better classification performance compared to the feature embeddings produced by the triplet loss, but the dependence of the siamese networks on pairwise inputs is a potential issue during inference in terms of complexity. For instance, the ℓ 2 distance norm computation between feature embeddings can be significantly simplified to a cosine distance dot product, since the descriptor norms are equal to 1, while the siamese networks have to measure the similarity using the final fully connected (FC) layer of the network (assuming the features before that FC layer have been pre-computed). Even though pairwise similarity methods tend to perform better than feature embedding approaches, according to our results and also the results from <ref type="bibr" target="#b35">[36]</ref>, it is interesting to notice that our feature embedding model TNet-TGLoss performs better than Siam network <ref type="bibr" target="#b35">[36]</ref> and the 512d-F(512) network <ref type="bibr" target="#b11">[12]</ref>, with both representing examples of pairwise similarity methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have presented new methods for patch matching based on learning using triplet and siamese networks trained with a combination of triplet loss and global loss applied to mini-batches -this is the first time such global loss and triplet network have been applied in patch matching. This new loss overcomes a number of the issues that have previously arisen when using triplet loss, most notably slow or even unreliable convergence.</p><p>We argue that the superior results provided by our models are due to the better regularisation provided by the global loss, as shown in Sec. 4. We have shown our models to be very effective on the UBC benchmark dataset, delivering state-of-the-art results.</p><p>A natural extension of our models is the use of the global loss with the triplet network, but our preliminary results (not shown in this paper) indicate that this model does not produce better results than the ones in <ref type="table">Table 2</ref>. We plan to extend this method to other applications, such as pre-training in visual class recognition problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The objective of the proposed global loss is to reduce the proportion of false positive and false negative classification, which in the graph is represented by the area of the blue shaded region, assuming that the green curve indicates the distribution of distances in (a) or similarities in (b) between matching pairs (with mean µ + and variance σ 2+ ) and the red curve denotes the distribution of non-matching pair distances in (a) and similarities in (b) (with mean µ − and variance σ 2− ). Our proposed global loss for feature embedding (a) reduces the area mentioned above by minimising σ 2+ , σ 2− and µ + and maximising µ − . For the pairwise similarity in (b), the global loss minimises σ 2+ , σ 2− and µ − and maximises µ + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration to compare the robustness of different loss functions to outliers: (a) training data with outliers, (b-d) classification of points in the input space based on the nearest neighbour classifier run in the embedding space learned with the triplet loss (b), the combined triplet and global losses (c), and the global loss (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>of 0.01 that gradually (automatically computed based on the number of epochs set for training) decreases after each epoch until it reaches 0.0001; 2) a momentum set at 0.9, 3) weight decay of 0.0005, and 4) data augmentation by rotating the pair of patches by 90, 180, and 270 degrees, and flipping the images horizontally and vertically (i.e., augmented 5 times: 3 rotations and 2 flippings)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1. Pairwise similarity results: False Positive Rate at 95% recall (FPR95) on UBC benchmark dataset, where bold numbers indicate the best results on the dataset. Note that for our models, we use the test set specified in<ref type="bibr" target="#b0">[1]</ref> to compute these values.</figDesc><table>CS SNet, 
GLoss 

SNet, 
GLoss 

2ch-
2stream 
2-ch Siam 
siam-
2stream 

4096d-
F(512) 

512d-
F(512) 
Liberty 

Notredame 0.77 
1.84 
1.9 
3.03 4.33 3.05 
3.87 
4.75 

Liberty 
Yosemite 

3.09 
6.61 
5.00 
7 
14.89 9.02 
10.88 13.58 

Notredame Liberty 

3.69 
6.39 
4.85 
6.05 8.77 6.45 
6.9 
8.84 

Notredame Yosemite 

2.67 
5.57 
4.10 
6.04 13.23 10.44 8.39 
11 

Yosemite 
Liberty 

4.91 
8.43 
7.2 
8.59 13.48 11.51 10.77 13.02 

Yosemite 

Notredame 1.14 
2.83 
2.11 
3.05 5.75 5.29 
5.67 
7.7 
mean 
2.71 
5.28 
4.19 
5.63 10.07 7.63 
7.75 
9.82 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>, we observe that Train: Liberty Test: Notredame Train: Notredame Test: Yosemite Train: Yosemite Test: Liberty</figDesc><table>false positive rate 

0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 

true positive rate (recall) 

0.7 

0.75 

0.8 

0.85 

0.9 

0.95 

1 

CS SNet, GLoss: 0.77% 
SNet, GLoss: 1.85% 
TNet, TGLoss: 3.91% 
TNet, TLoss: 4.47% 
Simonyan etal: 7.22% 
2ch-2stream, Zagoruyko etal: 1.90% 
Siam, Zagoruyko etal: 3.93% 
Siam-2stream-l2 Zagoruyko etal: 4.54% 

false positive rate 

0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 

true positive rate (recall) 

0.7 

0.75 

0.8 

0.85 

0.9 

0.95 

1 

CS SNet, GLoss: 2.67% 
SNet, GLoss: 5.58% 
TNet, TGLoss: 9.47% 
TNet, TLoss: 10.96% 
Simonyan etal: 10.08% 
2ch-2stream, Zagoruyko etal: 4.09% 
Siam, Zagoruyko etal: 13.61% 
Siam-2stream-l2 Zagoruyko etal: 13.02% 

false positive rate 

0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 

true positive rate (recall) 

0.7 

0.75 

0.8 

0.85 

0.9 

0.95 

1 

CS SNet, GLoss: 4.91% 
SNet, GLoss: 8.43% 
TNet, TGLoss: 13.45% 
TNet, TLoss: 13.90% 
Simonyan etal: 14.58% 
2ch-2stream, Zagoruyko etal: 7.20% 
Siam, Zagoruyko etal: 12.64% 
Siam-2stream-l2 Zagoruyko etal: 12.84% 

false positive rate 

0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 

true positive rate (recall) 

0.7 

0.75 

0.8 

0.85 

0.9 

0.95 

1 

Train: Liberty Test: Yosemite 

CS SNet, GLoss: 3.09% 
SNet, GLoss: 6.62% 
TNet, TGLoss: 10.65% 
TNet, TLoss: 11.82% 
Simonyan etal: 11.18% 
2ch-2stream, Zagoruyko etal: 5.00% 
Siam, Zagoruyko etal: 12.50% 
Siam-2stream-l2 Zagoruyko etal: 13.24% 

false positive rate 

0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 

true positive rate (recall) 

0.7 

0.75 

0.8 

0.85 

0.9 

0.95 

1 

Train: Notredame Test: Liberty 

CS SNet, GLoss: 3.69% 
SNet, GLoss: 6.39% 
TNet, TGLoss: 9.92% 
TNet, TLoss: 10.78% 
Simonyan etal: 12.42% 
2ch-2stream, Zagoruyko etal: 4.85% 
Siam, Zagoruyko etal: 10.35% 
Siam-2stream-l2 Zagoruyko etal: 8.79% 

false positive rate 

0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 

true positive rate (recall) 

0.7 

0.75 

0.8 

0.85 

0.9 

0.95 

1 

Train: Yosemite Test: Notredame 

CS SNet, GLoss: 1.14% 
SNet, GLoss: 2.84% 
TNet, TGLoss: 5.43% 
TNet, TLoss: 5.85% 
Simonyan etal: 6.82% 
2ch-2stream, Zagoruyko etal: 2.11% 
Siam, Zagoruyko etal: 5.44% 
Siam-2stream-l2 Zagoruyko etal: 5.58% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>This research was supported by the Australian Research Council through the Centre of Excellence in Robotic Vision, CE140100016, and through Laureate Fellowship FL130100102 to IDR</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.cs.ubc.ca/mbrown/patchdata/patchdata.html" />
		<title level="m">Ubc patch dataset</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative learning of local image descriptors. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="57" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The automatic design of feature spaces for local image descriptors using an ensemble of non-linear feature extractors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local discriminant embedding and its variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="846" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analysis of singlelayer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient response maps for real-time detection of textureless objects. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="876" to="888" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6622</idno>
		<title level="m">Deep metric learning using triplet network</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Metric and kernel learning using a linear transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="519" to="547" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, 1999. The proceedings of the seventh IEEE international conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Descriptor learning for omnidirectional image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Migliore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Registration and Recognition in Images and Videos</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Robust widebaseline stereo from maximally stable extremal regions. Image and vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Locally planar patch features for real-time structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems</title>
		<imprint>
			<publisher>MIT</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Local grayvalue invariants for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="534" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors using convex optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling the world from internet photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boosting binary keypoint descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
