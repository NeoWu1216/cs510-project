<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mining 3D Key-Pose-Motifs for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<email>wangchunyu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Cooperative Medianet Innovation Center Key Lab. of Machine Perception (MoE)</orgName>
								<orgName type="department" key="dep2">Sch&apos;l of EECS</orgName>
								<orgName type="laboratory">Nat&apos;l Eng. Lab. for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<email>yizhou.wang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Cooperative Medianet Innovation Center Key Lab. of Machine Perception (MoE)</orgName>
								<orgName type="department" key="dep2">Sch&apos;l of EECS</orgName>
								<orgName type="laboratory">Nat&apos;l Eng. Lab. for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<email>yuille@stat.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">Departments of Cognitive Science and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>UCLA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">John Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mining 3D Key-Pose-Motifs for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing an action from a sequence of 3D skeletal poses is a challenging task. First, different actors may perform the same action in various styles. Second, the estimated poses are sometimes inaccurate. These challenges can cause large variations between instances of the same class. Third, the datasets are usually small, with only a few actors performing few repetitions of each action. Hence training complex classifiers risks over-fitting the data. We address this task by mining a set of key-pose-motifs for each action class. A key-pose-motif contains a set of ordered poses, which are required to be close but not necessarily adjacent in the action sequences. The representation is robust to style variations. The key-pose-motifs are represented in terms of a dictionary using soft-quantization to deal with inaccuracies caused by quantization. We propose an efficient algorithm to mine key-pose-motifs taking into account of these probabilities. We classify a sequence by matching it to the motifs of each class and selecting the class that maximizes the matching score. This simple classifier obtains state-ofthe-art performance on two benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition from RGB videos <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b15">16]</ref> is an important task with many applications, such as intelligent surveillance, sports video analysis and humancomputer interactions. Although this task has attracted a lot of attention, the recognition performance is unsatisfactory because of the considerable variations in the appearance and the scales of the people performing the same action. In addition, 2D images will be very dependent on viewpoint and the same action can vary as the perspective changes. The introduction of RGB-D cameras and the corresponding pose estimation algorithms <ref type="bibr" target="#b18">[19]</ref> makes it possible to obtain 3D human poses and hence study pose-based action recognition. This leads to progress on the appearance and viewpoint variations <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b24">[25]</ref> [27] <ref type="bibr" target="#b22">[23]</ref> but there are other challenges remaining unsolved. First, different actors often perform the same action in differing styles. Second, the 3D poses are sometimes inaccurate because they are usually estimated from noisy depth maps. The two challenges together cause large intra-class variations making two instances of the same class far apart. It is not plausible to compare two instances directly. Third, most benchmarks only provide only a few sequences for each action which makes training complex classifiers sensitive to over-fitting.</p><p>Psychological studies, however, show that humans can effortlessly recognize actions from pose sequences despite all these challenges <ref type="bibr" target="#b8">[9]</ref>. Indeed some actions can be classified from a single key-pose <ref type="bibr" target="#b30">[31]</ref>. This suggests that we can perform action classification using a set of key poses rather than using the whole pose sequence. The key poses are close but not necessarily adjacent in the sequences and hence allows short and variable gaps between the poses. These gaps help deal with style variations. Also the representation is robust to outlier poses as they have little effect on the representation as long as the key poses are accurate.</p><p>Inspired by the psychological studies, we propose to mine a set of key-pose-motifs for each action class. We define a motif to be a short sequence of poses which are nearby but not necessarily adjacent in the original sequences. A motif is called a key-pose-motif of a particular class if it appears in a sufficient number of sequences of that class. We mine several key-pose-motifs for each action class and classify an input sequence by finding the action class whose key-pose-motifs best match the sequence. Observe that this approach also has the ability to detect the start and finish of an action sequence by inspecting the matching results, although that is not the focus of this paper.</p><p>To mine the key-pose-motifs, we need to quantize the continuous pose sequences (by a dictionary) into discrete sequences where each pose of the sequence is represented by a symbol of the dictionary. We use clustering algorithm to learn the dictionary. We use the activated simplices method proposed in <ref type="bibr" target="#b23">[24]</ref> to learn the dictionary. Each symbol in the dictionary is an activated simplex consisting of a set of bases which represents data by their convex combinations. Typically, each pose is quantized (represented) by the closest simplex. However, in our work, to reduce the influence of quantization error (e.g., two similar poses are quantized by different simplices), we use soft-quantization so that a pose is represented by several symbols with a probability for each (based on the distance to the symbol). Hence each pose is represented by a probability vector (its dimension is the same of as the number of symbols in the dictionary) and a sequence of poses is represented by a probability matrix where each column is the probability vector of the corresponding pose. Standard sequential pattern mining algorithms do not deal with this type of probabilistic data so we propose a novel, and efficient, algorithm to mine the key-pose-motifs which is one of our contributions.</p><p>We classify a test sequence by matching it to the keypose-motifs of each class and select the class by maximizing the matching score. The classifier is interpretable because we can visualize the mined key-pose-motifs and the matched positions in the sequence. So when there are misclassifications, we can easily detect why and where the failures happen. In experiments we use action-units consisting of short sequences of poses (e.g., neighboring three poses compose an action-unit and the original pose sequence is transformed to an action-unit sequence). But for ease of exposition, we will describe our method using poses only (and introduce action-units in the experiment section).</p><p>The paper is organized as follows. Section 2 reviews related work. Sections 3, 4 and 5 describe the proposed action representation (action-snippets), the key-pose-motif mining algorithm and the action classification method respectively. The remainder of the paper is devoted to experiments followed by a conclusion of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we provide a brief overview of the related work on human pose based action recognition. We also review the existing sequential pattern mining algorithms and discuss how they differ from our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human pose based on action recognition</head><p>We classify the existing work into three categories according to how temporal cues are modeled. The first class of work <ref type="bibr" target="#b7">[8]</ref> [3] <ref type="bibr" target="#b22">[23]</ref> ignores the temporal information and treat the poses in a sequence independently. They usually adopt "bag of poses" <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b22">[23]</ref> or majority voting <ref type="bibr" target="#b2">[3]</ref> schemes for classification. At the other extreme, the second class of work <ref type="bibr" target="#b29">[30]</ref> classifies pose sequences by modeling all poses in a sequence, for example by Hidden Markov Models <ref type="bibr" target="#b29">[30]</ref> or by dynamic time warping <ref type="bibr" target="#b17">[18]</ref>. Another line of work <ref type="bibr" target="#b24">[25]</ref> [27] <ref type="bibr" target="#b11">[12]</ref> encodes restricted temporal pose structures, for example, using temporal pyramid matching <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b11">[12]</ref> or by modeling neighboring pose changes <ref type="bibr" target="#b24">[25]</ref>.</p><p>Our proposed key-pose-motifs models the temporal structures of a set of key poses. However, our approach differs from <ref type="bibr" target="#b24">[25]</ref> [27] <ref type="bibr" target="#b11">[12]</ref> as key-pose-motifs are more flexible because they allow gaps between neighboring poses which makes them robust to speed variations. Besides the temporal structures are automatically learned from training sequences rather than manually designed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sequential pattern mining</head><p>Sequential pattern mining is the task of discovering frequent subsequences as patterns in a sequence database. There a lot of work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32]</ref> when the sequence database is certain (or deterministic) rather than probabilistic. The main challenge of sequential pattern mining is that it is computationally infeasible to examine all possible subsequences to determine the frequent ones, most early sequential pattern mining methods are based on A-prior algorithm <ref type="bibr" target="#b1">[2]</ref> where the main idea is that the sub-sequences of frequent patterns are also frequent. So we can generate candidate longer frequent patterns from the shorter ones which can substantially reduce the search space to be examined. However, those methods can not deal with the situations where the sequences are uncertain. Consider the amount of noise in our data, it is important for us to use soft-quantization which means that standard mining methods do not apply.</p><p>There are some work <ref type="bibr" target="#b32">[33]</ref> [13] which address the task when the database is uncertain. But the uncertainty in these work <ref type="bibr" target="#b32">[33]</ref>  <ref type="bibr" target="#b12">[13]</ref> occurs at different levels as ours. Muhammad et al. <ref type="bibr" target="#b12">[13]</ref> propose a method to mine sequential patterns when there is uncertainty about which sequence an item is associated with (but this is not equivalent to our problem). Zhao et al. <ref type="bibr" target="#b32">[33]</ref> solve a similar problem as ours. However, it is impossible to incorporate the gap constraints using their framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Action Representation</head><p>We represent an action by a sequence of 3D poses {y <ref type="bibr" target="#b0">(1)</ref> , · · · , y (m) } where each pose y is a high dimensional vector consisting of a set of body joint locations. This is a natural and intrinsic representation as it conforms to studies of how humans understand actions <ref type="bibr" target="#b3">[4]</ref>.</p><p>To mine key-pose-motifs, we first quantize the poses using a dictionary of discrete symbols. The classic quantization method is k-means which represents a pose by its nearest symbol in the vocabulary. However, small perturbations of poses, due to inaccuracies in poses, can cause the poses to be assigned to different symbols which will hurt classification performance.</p><p>We propose instead to use an alternative quantization method, namely the activated simplices method <ref type="bibr" target="#b23">[24]</ref>. The simplicial model consists of a mixture of activated simplices S = {s 1 , · · · , s K } where each simplex has several bases. A simplex represents poses by convex combinations of the bases which will form a convex hull. All the data which are close to the convex hull will be regarded as similar. For each action class, we learn a dictionary of activated simplices and combine them to make our dictionary of symbols. There are two reasons for choosing the simplicial model. First, it was shown to be robust to the inaccuracies in poses <ref type="bibr" target="#b23">[24]</ref>. Slightly distorted poses will be projected to the same simplex. Second, the simplicial model is more semantically meaningful. In the simplicial model, semantically similar poses are projected to different positions of the same simplex and they all have small projection errors. This is because activated simplices are able to represent the local linearity of poses, as described in <ref type="bibr" target="#b23">[24]</ref>. We will compare the two quantization methods in the experiment section.</p><p>To make the representation more robust to inaccurate poses, we use soft-quantization to assign each pose to all symbols. More precisely, We represent a pose using all of the K symbols in the dictionary and associate each symbol with a probability p i which measures its distance to the pose</p><formula xml:id="formula_0">p i = e −dist(s i ,y)</formula><p>K j=1 e −dist(s j ,y) (dist(s j , y) measures the distance of the point y to the convex hull formed by the activated simplex s j . see <ref type="bibr" target="#b23">[24]</ref> for the definition). Intuitively, small distances induce large probabilities and vice versa. Finally, each pose y (i) in a sequence is represented by a probability vector p (i) = [p i 1 , · · · , p i K ] T and a sequence of poses is represented by a matrix P = (p <ref type="bibr" target="#b0">(1)</ref> , · · · , p (m) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Mining Key-pose-motifs</head><p>We propose an efficient algorithm to mine key-posemotifs from the probability matrices, representing the softassignment of poses to symbols, as described above. We first give formal definitions of the terms which are going to be used in this paper. <ref type="table">Table 1</ref>: An example probabilistic sequence of length five which is defined on a vocabulary of five symbols (each column sums to one). The most probable sequence is (s 1 , s 2 , s 5 , s 1 , s 1 ) but there are 95 other possibilities.</p><formula xml:id="formula_1">❳ ❳ ❳ ❳ ❳ ❳ ❳ ❳ ❳ ❳ symbols time 1 2 3 4 5 s 1 0.7 0 0.3 0.9 1 s 2 0.1 0.8 0.0 0.1 0 s 3 0.1 0.1 0.1 0 0 s 4 0.1 0.0 0.2 0 0 s 5 0.0 0.1 0.4 0 0 Definition 4.1 (Deterministic sequence). T = (t (1) , · · · , t (m) )</formula><p>is a deterministic sequence of length m containing symbols chosen from the dictionary, i.e., t (i) ∈ S, at each of the m time-stamps.</p><formula xml:id="formula_2">Definition 4.2 (Probabilistic sequence). P = (p (1) , · · · , p (m) ) is a probabilistic sequence of length m where each item p (i) is a K dimensional vector spec- ifying the soft-assignment of the input pose y (i) to the K symbols in S. For example, p (i)</formula><p>j represents the probability that the pose y (i) is s j . <ref type="table">Table 1</ref> shows an example probabilistic sequence of length 5. Definition 4.3 (Probabilistic support). Given a deterministic sequence T and a probabilistic sequence P, the probabilistic support from P to T is a scalar value η between zero and one which measures how well T can be matched to P. The formal definition is given in section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.4 (Key-pose-motif).</head><p>A key-pose-motif of an action class is a deterministic sequence whose probabilistic support averaged over all (probabilistic) sequences from that class is larger than a threshold ǫ. A motif of length m is called an m-motif.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Overview:</head><p>The task of key-pose-motif mining is to find out all the key-pose-motifs from a datasets of pose sequences. Each pose sequence is soft-quantized and hence is represented by a probability matrix. Algorithm 1 shows the algorithm. Initially, each symbol in the dictionary S is a candidate 1-motif. We compute the average probabilistic supports for the candidates and remove the ones whose average supports are smaller than the threshold ǫ. Then we expand the 1-motifs to get candidate 2-motifs and continue recursively to get higher-order motifs. We repeat the process until no larger motifs can be generated. There are two main components in the algorithm. The first computes the probabilistic supports for the candidate motifs which is discussed in section 4.1. The second expands the k-motifs to get candidate k +1-motifs which is discussed in section 4.2.</p><p>Algorithm 1 Key-pose-motif Mining Algorithm</p><formula xml:id="formula_3">1: T 1 ={1-motifs} 2: for (k = 2; T k−1 = ∅; k++) do 3: T k = expand(T k−1 ) 4: for (i = 1; i ≤ |T k |; i++) do 5: support=0 6: for (j = 1; j ≤ |D|; j++) do 7: support=support+η(t k i , D j ) 8:</formula><p>If support |D| ≤ ǫ 9: </p><formula xml:id="formula_4">T k ← T k − {t k i } 10:</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Probabilistic Support</head><p>Unlike the existing sequential mining algorithms such as <ref type="bibr" target="#b19">[20]</ref>, the inputs are probabilistic sequences. It is non-trivial to decide whether a key-pose-motif appears in a probabilistic sequence. We propose to compute the probabilistic support of the motif in the sequence as follows.</p><p>Let a deterministic sequence (e.g., a candidate motif) be T = (t <ref type="bibr" target="#b0">(1)</ref> , · · · , t (m) ) and a probabilistic sequence be P = (p <ref type="bibr" target="#b0">(1)</ref> , · · · , p (n) ). The length m of the deterministic sequence is usually much smaller than the length n of the probabilistic. The probabilistic support of P for T measures how well T can be matched to P. Formally, we search for a mapping M (i) ∈ {1 · · · n}, i ∈ {1 · · · m} which maps each item in T to an item (location) in P with the constraint M (i) &lt; M (i + 1) and M (i + 1) − M (i) ≤ g. Here g is the maximum gap constraint which prevents neighboring poses in a motif from matching to positions which are far away from each other in the sequence.</p><p>We define the probabilistic support η(T, P) as follows:</p><formula xml:id="formula_5">η(T, P) = max M m i=1 p (M (i)) t (i) s.t. M (i) &lt; M (i + 1), M (i + 1) − M (i) ≤ g<label>(1)</label></formula><p>We now show that this objective function can be optimized efficiently by dynamic programming. Let T (1 : n 1 ) = (t <ref type="bibr" target="#b0">(1)</ref> , · · · , t (n1) ) and P(1 : n 2 ) = (p <ref type="bibr" target="#b0">(1)</ref> , · · · , p (n2) ). Let f (T (1 : n 1 ), P(1 : n 2 )) denote the probabilistic support of matching T (1 : n 1 ) to P(1 : n 2 ) with the condition that t (n1) is matched to p (n2) . Hence f (T (1 : n 1 ), P(1 : n 2 )) can be computed by:  Using Eq.</p><p>(2), we can efficiently compute a probabilistic support matrix f (, ) of dimension m × n. We iterate over the last row of the matrix and find out the maximum value which is η(T, P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The Expansion Algorithm</head><p>The expansion algorithm enables us to mine key-posemotifs by efficiently searching over the enormous space of deterministic sequences. It exploits the fact that larger sequences are compositions of smaller sequences. Suppose a sequence T = (t <ref type="bibr" target="#b0">(1)</ref> , · · · , t (k+1) ) is a key-pose-motif, then we can guarantee that its head and tail sub-sequences T head = (t <ref type="bibr" target="#b0">(1)</ref> , · · · , t (k) ) and T tail = (t <ref type="bibr" target="#b1">(2)</ref> , · · · , t (k+1) ) are also key-pose-motifs (note that, by equation <ref type="formula" target="#formula_5">(1)</ref>, these subsequences must have higher support than the sequence and hence their supports will be above threshold). Unlike existing methods which do not have the maximum gap constraints <ref type="bibr" target="#b32">[33]</ref>, it is not guaranteed that all the other sub-sequences, e.g., (t (1) , t <ref type="bibr" target="#b2">(3)</ref> , · · · , t (k) ) are also keypose-motifs. For example, suppose there is a sequence (1, 2, 3, 4, 5) and the maximum gap is set to be one, then the sequence (1, 3, 5) is supported by the input sequence but the sub-sequence (1, 5) is not.</p><p>We derive our expansion algorithm based on these ideas. Let F T k = {T k 1 , T k 2 , · · · , T k |F T k | } denote the set of kmotifs mined in the last iteration. For each pair of motifs in F T k , for example, T k 1 and T k 2 , we compare the last k − 1 items of T k 1 with the first k − 1 items of T k 2 . If they are all equal, then we generate a k + 1-motif candidate T k+1   Methods Acc (%) Year HON4D <ref type="bibr" target="#b13">[14]</ref> 82.15 2013 Tran <ref type="bibr" target="#b20">[21]</ref> 84.54 2013 Wang <ref type="bibr" target="#b23">[24]</ref> 88.10 2014 Du <ref type="bibr" target="#b5">[6]</ref> 89  Proof. If a sequence is a key-pose-motif, then its head and tail subsequences must also be key-pose-motifs by equation (1). It means that the two subsequences have already been mined. Then in the expansion stage, the two subsequences will generate the larger candidate key-posemotif for sure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Action Recognition: Inference Algorithm</head><p>We propose a simple classifier which works by matching a test sequence to the key-pose-motifs of each class. During the matching process each motif will get a probabilistic support, given by equation <ref type="bibr" target="#b0">(1)</ref>. We classify the sequence to be the class that gets the largest average probabilistic support over all motifs of that class. Ideally, a sequence of a particular class should have large supports for the key-pose-motifs mined for that class and small supports for the key-posemotifs of other classes. <ref type="figure" target="#fig_3">Figure 2</ref> shows an example.</p><p>The classifier is simple because it has no parameters. In addition, it is also interpretable. We can visualize the mined key-pose-motifs and the matched poses in a sequence. This helps us spot why and where failures may happen. <ref type="figure" target="#fig_3">Figure 2</ref> shows that we can even use this model for action localization, i.e. to coarsely find the start and end of an action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We conduct experiments on four most popular action recognition benchmarks, i.e. the MSR-Action3D dataset <ref type="bibr" target="#b10">[11]</ref>, the UTKinect dataset <ref type="bibr" target="#b29">[30]</ref>, the MSR Daily Activity3D dataset <ref type="bibr" target="#b26">[27]</ref> and the Florence dataset <ref type="bibr" target="#b16">[17]</ref>. We first compare our method with the state-of-the-arts on the four datasets respectively. Then we present diagnostic analysis of the method on the MSR-Action3D dataset.</p><p>Action-units. In our experiment, we concatenate l consecutive poses together to form an action-unitŷ (i) = [y (i) · · · y (i+l−1) ] and represent an action by a sequence of action-units: A = {ŷ <ref type="bibr" target="#b0">(1)</ref> , · · · ,ŷ (m−l−1) }. Consecutive <ref type="table">Table 3</ref>: The state-of-the-art action recognition accuracies using single split on the MSR-Action3D Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc (%) Year Vemulapalli <ref type="bibr" target="#b22">[23]</ref> 89.48 2014 Wang <ref type="bibr" target="#b24">[25]</ref> 90.22 2013 Wang <ref type="bibr" target="#b23">[24]</ref> 91.30 2014 Luo <ref type="bibr" target="#b11">[12]</ref> 96.70 2013 Ours 99.36 2015 <ref type="table">Table 4</ref>: Action recognition accuracies on the UTKinect Dataset using the "leave-one-sequence-out" criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc (%) Year Maxime <ref type="bibr" target="#b4">[5]</ref> 91.5 2014 Xia <ref type="bibr" target="#b29">[30]</ref> 90.92 2012 Ours 93.47 2015 action-units have overlaps. The proposed mining and classification algorithms are readily applicable to the actionunit sequences. The activated simplices are also learned on action-units. The use of action-units is more robust to outlier poses because a single inaccurate pose will not significantly affect the action-unit. We evaluate the influence of this pre-processing in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">On The MSR-Action3D Dataset</head><p>The MSR-Action3D dataset provides 557 pose sequences of ten subjects performing 20 actions. There are about 50 frames in each sequence. This is a challenging dataset because first many actions in the dataset are similar and second the pose sequences of the same action can have large variations due to either 3D pose estimation inaccuracies and performing style variations.</p><p>While learning the simplicial model, we set the number of bases to be 400 by cross-validation. We obtain about 200 activated simplices whose average dimension is about five. While mining the key-pose-motifs, we decrease the minimum support threshold ǫ from 1 to 0 with the step size of 0.05 until we obtain about 50 key-pose-motifs for each class. The number 50 is set by cross validation.</p><p>Most existing works choose five subjects for training and the remaining five subjects for testing, e.g. in <ref type="bibr" target="#b10">[11]</ref>, and report the result based on a single split. However, it is shown in <ref type="bibr" target="#b14">[15]</ref> that the way how the data are split (i.e. choosing which five subjects for training) can have large influence on the results. To make the results more comparable, in this work, we experiment with all 252 possible subject splits and report the average accuracy. <ref type="figure" target="#fig_9">Figure 4</ref> shows the classification confusion matrix of a certain split.</p><p>Comparison with the State-of-the-arts. <ref type="table" target="#tab_0">Table 2</ref> compares our method with the state-of-the-art methods using the protocol of "average over all splits". We can see that our method outperforms the state-of-the-art methods <ref type="bibr" target="#b13">[14]</ref> [21] [24] <ref type="bibr" target="#b5">[6]</ref>. In addition, our method is also the simplest in terms of both the features and the classifiers. Note that the method proposed in <ref type="bibr" target="#b23">[24]</ref> uses the activated simplices as classifiers directly using a nearest-neighbor algorithm. We can see that we improve the performance by mining keypose-motifs on activated simplices. Du et al. <ref type="bibr" target="#b5">[6]</ref> use deep learning techniques to learn an end-to-end classifier which achieves the current best performance.</p><p>Since some methods only provide results for a single split, we also provide these numbers. However, note that they are not directly comparable as the methods may choose different five subjects for training. The accuracies of our method using single split criterion: (1) the accuracy is 95.88% when we use subjects 1, 2, 3, 4, 5 for training; (2) the accuracy is 97.44% when we use subjects 1, 3, 5, 7, 9 for training; (3) the best accuracy of a single split is 99.36%. <ref type="table">Table 3</ref> shows the state-of-the-art results using the single split criterion.</p><p>Comparison with Baselines. The first baseline is the Direct Matching Method. Given a test sequence of actionunits, we compute the dynamic time warping based matching scores between the sequence and the training sequences of all classes. The class that achieves the largest average matching score is the predicted class. The method achieves an accuracy of 88%. The result is not satisfactory which is mainly because it cannot deal with large intra-class variations effectively. The second baseline uses the proposed key-pose-motifs mining method. However, each action-unit is quantized into only one symbol (It is not a probabilistic representation). We name this method as the Deterministic Mining Method. The deterministic mining method achieves an accuracy of 91% which is lower than our method, but is already higher than the state of the arts. We also compared with a method which uses probabilistic key-pose-motif mining but uses k-means to obtain the probabilistic data. The method achieves an accuracy of 88.7% which is lower than ours. This is mainly because k-means based quantization is not meaningful in the sense that semantically close poses will have similar probabilistic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">On The UTKinect Dataset</head><p>The UTKinect dataset <ref type="bibr" target="#b29">[30]</ref> was captured using a single stationary Kinect. There are ten action including walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, clap hands. There are ten subjects involved in the experiment with each subject performing each action twice. There are 199 sequences in total.</p><p>We learn 200 bases and end up with about 120 activated simplices. The number of bases in the simplices is five  <ref type="bibr" target="#b26">[27]</ref> 68.00 2012 Skeletons HON4D <ref type="bibr" target="#b13">[14]</ref> 80.00 2014 depth Actionlet <ref type="bibr" target="#b26">[27]</ref> 85.75 2012 Skeletons + depth Xia <ref type="bibr" target="#b28">[29]</ref> 88.20 2013 Skeletons + depth on average. We use the "leave-one-sequence-out" evaluation criterion where one sequence is used for testing and the rest of the sequences are used for training. We repeat the process for all sequences (199 in total) and report the average accuracy. <ref type="table">Table 4</ref> shows the results. We can see that our approach outperforms the state-of-the-arts. The main reason may be because our method suffers less from noisy poses because it is only the key poses rather than all poses are useful for classification. In other words, the model will be accurate as long as the key poses are accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">On the MSR Daily Activity3D Dataset</head><p>The Daily Activity3D dataset is captured by a Kinect device and it provides both depth map and 3D skeleton sequences. It includes 16 activities: drink, eat, read book, call cellphone, write on a paper, use laptop, use vacuum cleaner, cheer up, sit still, toss paper, play game, lay down on sofa, walk, play guitar, stand up and sit down. For some actions, each subject performs them in both "sitting" and "standing" poses. In total, there are 320 sequences. This is a rather challenging dataset and very few work have demonstrated good performance on it. In particular, the 3D joint locations are very noisy when the performer stands close to the sofa or sits on the sofa. Considering the large amount of noises in 3D skeletons, most methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> combine both depth maps and 3D joint locations for action recognition. Note that the depth map is relatively more accurate in this dataset. Our method only uses 3D skeletons which is a more challenging problem.</p><p>We use the cross-subject evaluation method to compare our method with the state-of-the-arts. However, it is worth noting that the dataset doesn't specify which five subjects to use for training. We report the result when training on subjects 1-5. We also report the average recognition result over all 252 possible splits. <ref type="table" target="#tab_1">Table 5</ref> shows the results on this dataset using a single split evaluation criterion. Our method outperforms the stateof-the-art methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> which use only 3D skeletons or depth maps. Some methods which use more information (e.g., combine the 3D skeletons and depth maps) achieve slightly better performance than ours. The average recognition accuracy over all 252 splits for our method is 79%. <ref type="table">Table 6</ref>: Action recognition accuracy on the Florence dataset using leave-one-actor-out setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy (%) Year Lorenzo et al. <ref type="bibr" target="#b16">[17]</ref> 82.15 2013 Raviteja et al. <ref type="bibr" target="#b22">[23]</ref> 90.88 2014 Tran et al. <ref type="bibr" target="#b4">[5]</ref> 87.04 2014 Our Approach 92.25 2015</p><p>The result is promising given the amount of noises in the dataset. To the best of our knowledge, no previous methods have reported the average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">The Florence Dataset</head><p>The dataset <ref type="bibr" target="#b16">[17]</ref> was captured using a Kinect camera at the University of Florence. It includes nine activities: wave, drink from a bottle, answer phone, clap, tight lace, sit down, stand up, read watch, and bow. During acquisition, ten subjects were asked to perform the above actions for two or three times. This resulted in a total of 215 activity samples.</p><p>Following the data suggestion, we adopt a leave-one-actorout protocol: we train the classifier using all the sequences from nine out of ten actors and test on the remaining one. We repeat this procedure for all actors and compute the average classification accuracy values of the ten actors. We set the number of bases for each class to be 50 (450 in total) by cross-validation. <ref type="table">Table 6</ref> compares our method with the state-of-art methods on this dataset. Our approach achieves the highest recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Diagnostic Analysis</head><p>Reasons behind the performance. We observe in experiments that given a test sequence of a certain class, the corresponding class of key-pose-motifs usually obtain very large probabilistic supports while other key-pose-motifs obtain small supports. See <ref type="figure" target="#fig_8">Figure 3</ref>. We can also tell from the figure which actions are easy to differentiate and which are not. For example, in the eighth sub-figure (row 2, column 3) of <ref type="figure" target="#fig_8">Figure 3</ref>, we can see that the classes of 7, 8 and 9 are ambiguous because they all get large supports when the test sequences are from class 8. The three classes are "draw x", "draw tick" and "draw circle" actions respectively which are in fact very similar.</p><p>Influence of the Parameters. We evaluate the three main parameters in the proposed method: the gap g in the maximum gap constraints, the number of mined key-posemotifs for each class and the number of poses l in an actionunit. To save time, we only use the first ten splits out of the 252 splits and report the average recognition accuracy. <ref type="figure" target="#fig_10">Figure 5</ref> shows the influence of the gap constraints. We can see that there is large performance improvement by allowing gaps between consecutive poses. One of the reasons  might be that it can deal with the speed variations among different sequences. In other words, the key poses might appear at various positions of different sequences and allowing gaps between consecutive poses helps reduce the influence of the variations. The best performance is achieved when the maximum gap is 20. After that, increasing the gap will degrade the performance a little bit. This may be because too large a gap encourages the model to find keypose-motifs that are not meaningful.</p><p>We also evaluate the influence of the number of keypose-motifs. We adjust the minimum probabilistic support threshold ǫ and obtain a desired number of key-pose-motifs. <ref type="figure" target="#fig_10">Figure 5</ref> shows the result. First, using more key-pose-motifs will consistently improve the performance when the number is smaller than 80. This is reasonable because the model becomes more representative using more key-pose-motifs. However, when the number is too large, then many motifs which only appear in a few sequences are also mined which makes the model over representative. In other words, the motifs become less discriminative because they might be able to represent sequences of other classes well which degrades the action recognition performance.</p><p>The use of action-units improves the performance. See <ref type="figure" target="#fig_10">Figure 5</ref>. We can see that there is a large performance improvement by putting more poses in an action-unit. But the change is not significant after the number exceeds 9.</p><p>Robust to Occlusion. We now evaluate the robustness of the method to inaccurate poses which are mainly caused by occlusions. We synthesize a set of data by randomly perturbing the 3D poses in the MSR-Action3D dataset. In particular, we set some joint locations of several poses in a pose sequence as zero to simulate occlusion. We control the number of perturbed joints and frames. See <ref type="figure" target="#fig_11">Figure 6</ref> for the results. In the worst case, when 9 joints (about 45%) of 11 frames (about 20%) are contaminated, the performance only drops by about 2%. The results justify that our method is robust to the occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose a simple and interpretable method for action recognition. By mining the key-pose-motifs which are not necessarily adjacent in the original sequence, we obtain a compact representation which is robust to intra-class variations. We evaluate the model on two benchmark datasets and show that it outperforms the state-of-the-arts. Moreover, the model is easy to interpret and we can spot where and why failures happen. In our future work, we would like to improve the discriminative power of the method by mining discriminative key-pose-motifs which can match to the sequences of its class very well but will not match to the sequences of other classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview: (a) a sequence of training poses. (b) the poses quantized by a simplicial model. (c) mined keypose-motifs. (d) a sequence is classified by matching it to the motifs of each class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>f</head><label></label><figDesc>(T (1 : n 1 ),P(1 : n 2 )) = p (n2) t (n 1 ) × max i∈{n2−g,··· ,n2−1} f (T (1 : n 1 − 1), P(1 : i))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Left: matching the mined key-pose-motifs of Classes 1, 2 and 3 to a sequence of Class 1 (length 54). Each line segment defines the matching regions of each motif and its y-axis value gives the matching score. Right: a long sequence is composed by sequences from classes 1, 2 and 3, respectively (lengths 54, 34 and 40). We match the keypose-motifs of the three classes to the long sequence. This shows that motifs can be used to roughly detect the starts and ends of actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>T k 1 with the last item of T k 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Example 4. 1 (</head><label>1</label><figDesc>Expansion). Suppose we have three 3motifs F T 3 = {(1, 2, 3), (2, 3, 4), (3, 4, 6)}. Then by joining (1, 2, 3), (2, 3, 4) we obtain (1, 2, 3, 4) and by joining</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Proposition 4. 1 .</head><label>1</label><figDesc>The above expansion algorithm will not leave out any key-pose-motifs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>The x-axis is the index of the 20 classes of key-pose-motif-models and the y-axis is the average matching scores (standard deviations) which are computed when matching a set of test sequences of a particular class to those models. For example, in the first sub-figure, we match a set of sequences of class 1 to the 20 classes of models and obtain the average matching scores and standard deviations for each model. The motif model of class 1 gets the largest average score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Confusion matrix on the MSR-Action3D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Influence of the maximum gap constraint, the number of mined key-pose-motifs in each class and the number of poses in an action-unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>The influence of the number of occluded joints and the number of affected frames in a test sequence on action recognition accuracies on the MSR-action3D dataset. See section 6.1 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The state-of-the-art action recognition accuracies over all 252 splits on the MSR-Action3D Dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Action recognition accuracies on the MSR Daily Activity3D Dataset.</figDesc><table>Methods 
Acc (%) Year 
Features 
Ours 
83.47 
2015 
Skeletons 
Actionlet </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining sequential patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1215</biblScope>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human activity recognition using multidimensional indexing. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ben-Arie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1091" to="1104" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Top-down influences on stereoscopic depth-perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bülthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bülthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="257" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3-d human action recognition by shape analysis of motion trajectories on riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing action at a distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="726" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic key pose selection for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Roca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Group sparsity and geometry constrained dictionary learning for action recognition from depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1809" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining sequential patterns from probabilistic databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muzammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="210" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A discussion on the validation tests employed to compare human action recognition methods using the msr action3d dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Padilla-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Chaaraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flórez-Revuelta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.7390</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discovering discriminative action parts from mid-level video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1242" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing actions from depth cameras as weakly aligned multi-part bag-of-poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="479" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human action recognition using dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sempena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">U</forename><surname>Maulidevi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Aryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electrical Engineering and Informatics (ICEEI), 2011 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mining sequential patterns: Generalizations and performance improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse spatio-temporal representation of joint shape-motion cues for human action recognition in depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Q</forename><surname>Ly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing and Communication Technologies, Research, Innovation, and Vision for the Future (RIVF)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
	<note>IEEE RIVF International Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Activity recognition using the dynamics of the configuration of interacting objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">633</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Recognizing actions in 3d by action-snippets and activated simplices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Making action recognition robust to occlusions and viewpoint changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Özuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="635" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatio-temporal depth cuboid similarity feature for activity recognition using depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2834" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2030" to="2037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spade: An efficient algorithm for mining frequent sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="60" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mining probabilistically frequent sequential patterns in large uncertain databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1171" to="1184" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Knowledge and Data Engineering</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
