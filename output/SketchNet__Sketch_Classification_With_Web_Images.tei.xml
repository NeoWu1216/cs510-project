<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SketchNet: Sketch Classification with Web Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Information Security (SKLOIS)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siliu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Information Security (SKLOIS)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
							<email>zhangchangqing@tju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
							<email>rwq.renwenqi@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Information Security (SKLOIS)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
							<email>caoxiaochun@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Information Security (SKLOIS)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SketchNet: Sketch Classification with Web Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, we present a weakly supervised approach that discovers the discriminative structures of sketch images, given pairs of sketch images and web images. In contrast to traditional approaches that use global appearance features or relay on keypoint features, our aim is to automatically learn the shared latent structures that exist between sketch images and real images, even when there are significant appearance differences across its relevant real images. To accomplish this, we propose a deep convolutional neural network, named SketchNet. We firstly develop a triplet composed of sketch, positive and negative real image as the input of our neural network. To discover the coherent visual structures between the sketch and its positive pairs, we introduce the softmax as the loss function. Then a ranking mechanism is introduced to make the positive pairs obtain a higher score comparing over negative ones to achieve robust representation. Finally, we formalize above-mentioned constrains into the unified objective function, and create an ensemble feature representation to describe the sketch images. Experiments on the TU-Berlin sketch benchmark demonstrate the effectiveness of our model and show that deep feature representation brings substantial improvements over other state-of-the-art methods on sketch classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sketch can be seen as the abstract representation of expressing some types of ideas. Moreover, sketch image can convey information that is hard to describe using text without requiring a tremendous amount of effort (e.g. Sketch2photo <ref type="bibr" target="#b6">[7]</ref>). With the popularity of touch devices, sketch has been attracted more and more researches' attentions in the computer vision and graphic field. There exist * corresponding author (a) A group of cat images collected from website (b) Some samples of sketch images drawn by human <ref type="figure">Figure 1</ref>. Motivation of our proposed method. Humans draw sketch images with their prior knowledges, which would generate a large variations for the same category (b). Sketch classification can be seen as the reverse processing of drawing sketch. We introduce the real images (a) as the references, which can bridge the representation gap between sketch images in the same category.</p><p>various interesting applications including sketch based image retrieval <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>, human-computer interaction <ref type="bibr" target="#b9">[10]</ref>, and other relevant works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. While most existing methods use global appearance <ref type="bibr" target="#b12">[13]</ref> or interesting points <ref type="bibr" target="#b0">[1]</ref> based feature representations to model sketches, recent work demonstrate that the effective way of using localized part based representation with embedding structure information <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29]</ref>. Based on the extracted low-level hand-crafted local features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13]</ref>, one way is following the process of image matching <ref type="bibr" target="#b29">[30]</ref> to find the most-similar image. The other way is based on the paradigm of bag of features <ref type="bibr" target="#b16">[17]</ref>, and then train the classifiers <ref type="bibr" target="#b3">[4]</ref> of categories using histogram representations.</p><p>One of the biggest limitations of traditional methods is the sketch image itself as shown in <ref type="figure">Fig.1 (b)</ref>: 1) Without the texture and color informations, distinct categories appear to be similar e.g. tire and donut. 2) Different drawing styles can cause the variation of sketches, which would further enlarge the intra-similarities. 3) The existing localized sketch descriptors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1]</ref> are sensitive to the view perspectives and some appearance cues (the stripes of tigers) of drawn . Sketch classification with real image embedding using SketchNet. Given a sketch image of an object, e.g., donut, we find its initialized category predictions (Donut, Tire and Wheel) based on the pre-trained sketch model. Then the test pairs are constructed based on their visual similarity. Conditioned on these test pairs, we use our SketchNet model to achieve the prediction scores of each test image pair. Finally, all the predictions are merged to make the decision to identify the sketch category.</p><p>sketches. Furthermore, the ability of learning algorithms to train the classification models are also influenced by the handcrafted features and the capacity of classifiers (SVM <ref type="bibr" target="#b3">[4]</ref>) to memorize feature information.</p><p>So, how can we achieve robust visual representation for the sketch image, and develop the classifiers to memorize more feature information? To this end, we need to automatically identify the discriminative visual patterns in each sketch image whose appearance may be significant differences. This is a challenging task considering the abovementioned factors and the limited useful information of sketch images. Fortunately, there exists large amounts of web image data online with weakly supervised labels as shown in <ref type="figure">Fig.1 (a)</ref>, which would be helpful to identify the latent discriminative structure of sketch images via discovering the coherent appearance between the sketch and real images of the same category. This assumption is supported by the process of human recognizing sketches: we can classify sketch images even without seeing any sketch images. Given the task, deep Convolutional Neural Network (CNN) <ref type="bibr" target="#b19">[20]</ref> is proved to be a valuable model choice since it allows for large scale feature representation learning.</p><p>Specifically, in this paper, we propose a novel deep convolutional neural network, named SketchNet for sketch classification. To construct the auxiliary repository, the real images are collected from the web which covers all the sketch categories in the TU-Berlin sketch dataset <ref type="bibr" target="#b11">[12]</ref>. To extract the real reference images for each training sketch, we first train an preliminary model based on Alexnet <ref type="bibr" target="#b19">[20]</ref> following the fine-tuning process. Afterwards, the top K predicted category labels of each training sketch is extracted based on the pre-trained Alexnet model. For each training sketch, we find the most visual similar real images from the image sets of top predicted categories to construct the training pairs. Thus, the sketch with the real images which are in the same class is used to generate the positive im-age pair while the sketch with the real images which are in distinct classes are defined as negative image pair. Next, a triplet is constructed based on the positive and negative pairs. Our proposed SketchNet contains three subnetworks: R-Net is used to extract features from the real images. S-Net is applied on the sketch images. And the C-Net is proposed to discover the common structures between real images and sketches. To guarantee the positive pair achieves a higher prediction score on the corresponding category than the negative pair, we customize a novel loss function based on the extracted features. In the test time as shown in <ref type="figure" target="#fig_0">Fig.  2</ref>, we feed the test pairs composing of sketches and their similar real images into SketchNet to set the category predictions. Finally, the predictions are merged together to achieve the final results. Extensive experiments are conducted on the TU-Berlin sketch dataset <ref type="bibr" target="#b11">[12]</ref> and the experimental results show that our SketchNet can significantly improve the performance of sketch classification by introducing the real images as the references.</p><p>The contributions of our proposed method can be summarized as: (1) We propose a novel sketch feature representation learning method named SketchNet based on the deep convolutional neural network to address the problem of sketch classification. (2) To learn the discriminative feature representation, we propose to construct the image pairs to discover the shared structure between sketch and real images. (3) Our method boosts the benchmark of sketch classification, achieving the state-of-the-art performance in terms of classification evaluation metrics. (4) We have collected a new real image dataset which includes the corresponding real images of TU-Berlin sketch benchmark <ref type="bibr" target="#b11">[12]</ref>. This dataset will be released at the author's websites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we review the most related work on sketch classification. There has been a large body of re-search on analyzing sketch classification. Most of the works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29</ref>] firstly extract the low-level features from the sketch image, and then model the sketch representation by using bag of feature model. These methods are limited to a specific class of sketch with small variations, with similar shape appearances. Moreover, in <ref type="bibr" target="#b20">[21]</ref> the authors proposed an ensemble matching method for sketch recognition. This method is firstly construct the star-graph with bag of features representation, then employ the SVM to train the category classifiers. However, this method is still sensitive to the variations of the sketches. On sketch retrieval, Eitz et al. <ref type="bibr" target="#b13">[14]</ref> propose to extract several kinds of descriptors to construct the bag of features model, and then evaluate the performance of these descriptors on a large scale dataset. While Cao et al. <ref type="bibr" target="#b1">[2]</ref> propose a flip invariant feature representation named FISH for sketch retrieval by embedding the symmetry structure of the sketch. But all the existed methods are based on the traditional low-level features which are still sensitive to the sketch variations.</p><p>With the development of deep learning, we have witnessed the successful of applying the deep learning framework on image recognition and retrieval. The generative and discriminative power of deep features have been used to build deep generative model shapes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, the authors propose to use DBN to generate the hand-written digits, which has achieved notable performance. While Eslami et al. <ref type="bibr" target="#b15">[16]</ref> develop an object shape model method by using the boltzmann machine. These methods are able to efficiently handle the intra-class variations. Moreover, Wu et al. <ref type="bibr" target="#b34">[35]</ref> propose a novel shape representation named 3D shapeNets, which focuses on modeling the 3D shapes. In this method <ref type="bibr" target="#b34">[35]</ref>, the authors propose to recognize the object from the depth images and then construct a 3D shapenets to capture the structure of 3D shapes. Different from the existing works based on deep learning on modeling the object shape, our method is focused on the sketch image which has large intra-class variations and large inter-class similarity. Another related work is Siamese network <ref type="bibr" target="#b7">[8]</ref> which has been widely used in text classification <ref type="bibr" target="#b32">[33]</ref> and speech feature classification <ref type="bibr" target="#b5">[6]</ref>. This network contains two identical sub-convolutional networks shared the weighting parameters. The goal of this network is to make the output vectors similar when the input pairs have the same label, and dissimilar for the input pairs that are labeled dissimilar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SketchNet</head><p>In this section we introduce our proposed SketchNet on the task of sketch classification. The training sketch image is represented as S =  where C represent the number of categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Triplet construction by embedding real images</head><p>Our goal is to discover the shared structures between sketch images and real images, and then based on the shared feature representation learn a model to predict the label for any new sketch images. This is a challenge problem: the real images are not provided any localized information of objects so we must automatically discover the relevant regions in each images that correspond to the sketch image. Moreover, the appearance of these real images may change drastically caused by views, occlusion and scales. To address above-mentioned problems, we exploit the fact that for each sketch image, the nearest real images which are in the same category would have the coherent appearance or changing smoothly. To this end, each sketch image is matched to its most similar real images from the auxiliary repository to construct the image pairs. After that, we develop a triplet from these image pairs which is composed of the sketch image, the same category real image (positive image) and different category real image (negative image). However, considering the ratio between relevant images and irrelevant ones, it would generate a large number of negative pairs which makes the training data be computation inefficient and unbalanced.</p><p>Considering that problem, Alexnet <ref type="bibr" target="#b19">[20]</ref> is firstly trained by mixing the sketches and real images as the training data. We follow the fine-tune process to train this sketch model which initialize the Alexnet with the pre-trained model on ImageNet <ref type="bibr" target="#b10">[11]</ref>. Specifically, we set the number of categories to be 250 and the softmax loss function is employed. The learning rate is initialized as 0.001. When the sketch model is obtained defined as F, we extract the prediction </p><formula xml:id="formula_0">S cov1 11 × 11 /4 96× 54× 54 S P ool1 3 × 3/2 96× 27× 27 S cov2 5 × 5/1 256× 27× 27 S P ool2 3 × 3/2 256× 13× 13 S cov3 3 × 3/1 384× 13× 13 R-Net R cov1 11 × 11 /4 96× 54× 54 R P ool1 3 × 3/2 96× 27× 27 R cov2 5 × 5/1 256× 27× 27 R P ool2 3 × 3/2 256× 13× 13 R cov3 3 × 3/1 384× 13× 13 R cov4 3 × 3/1 384× 13× 13 C-Net C cov1 3 × 3/1 384× 13× 13 C cov2 3 × 3/1 384× 13× 13 C cov3 3 × 3/1 256× 13× 13 C P ool1 3 × 3/2 256× 6× 6 fca 1 × 1/1 4096 × 1 × 1 fc b 1 × 1/1 4096 × 1 × 1</formula><p>score of each sketch denoting as P i ∈ R 1×C . We sort the prediction scores and extract the top K incorrected categories P top i to construct the negative image pairs. In all our experiments, we set the K =5by considering the computation efficiently and the performance of classification model.</p><p>As discussed aforementioned, a triplet is composed of the sketch images, positive and negative real images. To obtain the positive real images for each sketch, we use the nearest neighborhood methods as Eq. 1 to extract the top 5 similarity real images from the positive training image set. In the same method, we also extract the top 5 similarity real images from each negative category training set. In total, there are 5 positive real images and 25 negative real images which would generate about C 1 5 ×C 1 25 = 125 triplets for each training sketch images. In the following all the experiments, we apply this method to generate the training data.</p><formula xml:id="formula_1">{r i 1 , ..., r i j } =argmin tj ∈P top i ||F fc7 (s i ) −F fc7 (r j )|| 2 ,<label>(1)</label></formula><p>where F fc7 (·) denotes the features of the layer fc 7 in Alexnet. | |·| | 2 represents the Euclidean distance. {r i 1 , ..., r i j } represents the related training images for s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SketchNet architecture</head><p>There exists significant appearance gap between sketch images and real images, thus the traditional neural network can not be directly transferred to our problem. To handle those differences, we customize the AlexNet <ref type="bibr" target="#b19">[20]</ref> to develop a novel neural network.</p><p>Specifically, our proposed SketchNet is composed of S-Net, R-Net and C-net. S-Net is defined to extract the fea-tures from the sketch images. With the consideration of limited information on the sketch images, we set three convolution layers and two pooling layers as shown in <ref type="figure" target="#fig_4">Fig. 4</ref> in the green bounding box. Similarly, R-Net is used to extract the features from positive and negative real images. To achieve the relative similarities, R-Net should be able to extract different features from positive and negative images. Furthermore, one more convolution layers are added on the R-Net comparing with S-Net. There are four convolution layers and two pooling layers as shown in <ref type="figure" target="#fig_4">Fig. 4</ref> in the blue bounding box. The reason is that in the step of loss back propagation, sketch images should be more sensitive to the loss. C-Net is developed to merge the feature maps between sketch and real images. We concatenate the layer of "R conv4" and "S cov3" as the input of C-Net. Moreover, two full connected layers are added in this net. Thus, three convolution layers and one pooling layers constitute the C-Net as shown in <ref type="figure" target="#fig_4">Fig. 4</ref> in the red bounding box. We also make the C-Net in the structure of Siamese architecture <ref type="bibr" target="#b7">[8]</ref> to let positive pair have a higher classification score comparing with the negative ones. The overall architecture between layers are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. Furthermore, the specific parameters of each layer are displayed in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The reasons to choose three nets to form the SketchNet are: firstly, the corresponding real images of each sketch include more appearance and color information than sketch images, we need to use different number of convolution layers to extract their features to narrow the gap between sketches and real images. Moreover, from the loss back propagation perspective, sketch images should have more impact on the loss than real images to determine the common latent structures. Secondly, considering that positive and negative real images should have distinct structures with the sketch image, we design their nets sharing the same parameters to make sure that positive image pair could generate the higher scores in the prediction. Last but not the least, to adopt our task of sketch classification, we design the novel loss function to discover the discriminative features of different training pairs. Specifically, we need the learned features to be able to get smaller intra-class variances and bigger inter-class differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss function</head><p>We propose to use different loss functions to adopt to different tasks using our SketchNet. For sketch classification, we develop a novel loss function which is composed of two components. The first component is the traditional loss function softmax which has demonstrated its strong ability on image classification. In our neural network, we use this loss function to achieve the prediction score of the input images. Specially, we can use this loss function to obtain the initialization prediction results on positive pair and negative   pairs. The definition of softmax is following:</p><formula xml:id="formula_2">L c (x i ,y i ,W c )=−logP (y i = k|x i ,W c ) (2) = −log e −f k (x i ,Wc)</formula><p>C l=1 e −f l (x i ,Wc) where x i represent the input image data and y i denotes the label of the input image x i , and C is the number of sketch categories. k is the category label of current input image and W c denotes the weights in the fully connected layers which is used to map the extract deep features to the labels.</p><p>Based on the softmax loss function we can separately extract the predictions of positive and negative pair images. However, there do not exist evidence that the score of positive pair could be higher than the negative ones. To this end, we introduce the second loss function which is based on the prediction scores of the positive and negative image pairs. In this loss function, we constrain the score of correct category label should be higher than all the other labels, which could be seen as a ranking loss. We set p + as the prediction score of positive image pair and p − as the prediction score of negative pair. Then the loss function is defined as:</p><formula xml:id="formula_3">L r (p i + , p i − ,y i )=max(0, 1 − (p − − p + ))<label>(3)</label></formula><p>This loss function is the variation of ranking loss which is the convex. Finally, the loss function of our proposed SketchNet for sketch classification is:</p><formula xml:id="formula_4">L SketchN et = L r + λ * Lc<label>(4)</label></formula><p>where λ is the weights parameters to balance the values of two components in the objective function. Moreover, the loss function is convex function which can be easily computed its gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Testing of SketchNet</head><p>In the test time, we have obtained the SketchNet model to extract the sketch feature representations. However, the category of test sketch images are unknown, we can not develop the triplet as the training time. Different from the training process as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, in the test time, our SketchNet shrink to contain One R-Net, S-Net and One C-Net as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. So, a sketch image is firstly fed into the pre-trained Alexnet model to achieve the top 5 prediction labels. After that, we extract 5 similar real images for each prediction label from auxiliary repository to develop the sketch pairs. In total, each test sketch image would generate 25 image pairs. One of the test pairs on sketch classification are shown in <ref type="figure">Fig. 5</ref>. Afterwards, we can adaptive the pre-trained sketchnet model to extract augmented feature representation of test sketch images. The feature maps of each layer could be used as the feature representation of test images.</p><p>The straight way to get the prediction results of the test sketch is to sum the feature maps of the last layer of test image pairs. However, this would generate the biased results since in the training time we only use a small of image pairs comparing to the possible combinations. We need to learn a metric that could make the positive pairs achieve a higher prediction scores on the correct label than the negative ones. Specifically, A validation image set composed of sketch images and all the auxiliary real images is extracted from the training part. Similarly, we generate the test pairs which  <ref type="figure">Figure 5</ref>. Examples of generating test pairs. The first column image shows the test sketch image "cake". The top 5 prediction results are displayed in the second columns and the rest columns of images correspond to the related real images which are used to construct the test pairs. contains the sketch and its nearest real images. Then, the feature map is constructed based on the prediction scores of each test pair. After that we apply the metric SVM <ref type="bibr" target="#b25">[26]</ref> based on the feature maps of full connected layers (FC b ) for each image pairs to compute the metric weighting. Finally, the prediction score combining with the learned metric parameters are used to propagate the label of test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive evaluations of the proposed method on the task of sketch classification. We implement the proposed method based on the opensource Caffe <ref type="bibr" target="#b18">[19]</ref>. We change sketch images from one channel to three by copying themselves three times. In all experiments, our networks are trained by stochastic gradient descent with 0.9 momentum. We initiate learning rate to be 0.0001 and decrease it by 0.1 after finishing about 30 epochs. The weight decay parameter is 0.0005. The training time is proportional to the total number of triplets and the number of epochs. Overall training it takes approximately 2 days based on a PC with 2.8GHz CPU and GTX TITAN Black GPU. The mini-batch size of images is 64. Considering the total number of triplets is very large, the training time is sensible.</p><p>The ImageNet ILSVRC-2012 dataset <ref type="bibr" target="#b26">[27]</ref> is utilized to pre-train the CNN model by optimizing multinomial logistic regression objective function in the image classification task. This dataset contains about 1.2 million training images and 50, 000 validation images, roughly 1, 000 images in each of 1, 000 categories. We use the pre-trained parameters of convolutional layers and fully-connected layers to initialize the CNN part of SketchNet in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>TU-Berlin sketch benchmark 1 <ref type="bibr" target="#b11">[12]</ref> is proposed for sketch classification and recognition. There are 250 object categories in this dataset, which cover mostly daily objects in the life, e.g. car, horse. For each category, 80 sketch images are collected without any common templates and each sketch was defined by the person drawing it. After the dataset was constructed, there was a phase where humans tried to recognize the sketches and the mean recognition accuracy over all the 250 categories is 73%.</p><p>Furthermore, we also propose a new sketch dataset which extends the TU-Berlin sketch benchmark by introducing the real images for each category. This extended dataset can be used for sketch classification and retrieval. To construct this dataset, for each sketch category we find its corresponding class in the ImageNet and directly collect these real images to extend. While for these categories which are not overlapping with ImageNet, we collect the real images by using Google image search based on their category labels. Finally, we collect 191, 067 real images in total, and average 764 images for each category. To guarantee the wide variety of real images, we do not add any constrains on collecting the real images.</p><p>Evaluation criteria: In our experiment, we use the aforementioned dataset and measure the performance using the following criteria: we use the average precision (AP) to calculated the classification accuracy for each category. Then to compare with other existing work on sketch classification, Mean Average Precision (MAP) is computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sketch classification</head><p>In this section, to validate the effectiveness of our SketchNet, we show the sketch classification results on the TU-Berlin sketch benchmark. We firstly split this dataset into training and testing parts. Following the related work <ref type="bibr" target="#b11">[12]</ref>, 9 kinds of training splits are collected by selecting different number of sketch images for each categories which are {8, 16, 24, 32, 40, 48, 56, 65, 72}. The training sketch images for each class are randomly chosen from the dataset. And the rest images of this dataset are used as the test. To train the metric parameters as discussed in Sec. 3.4, we randomly select 20% training data as the validation dataset.</p><p>Each training sketch image is firstly used to construct a triplet, and we would obtain 125 triplets for each sketch image. Similarly, the test sketch image is asked to construct the test pair by computing the similarity between sketch and real images. Specially, in the test time, we extract the top 5 prediction categories and for each category 5 related real images are extracted to construct 25 test pairs. All the parameters of SketchNet are defined as shown in <ref type="table" target="#tab_0">Table 1</ref>. Furthermore, S-Net and R-Net is initialized based on the two We select two kinds of baselines to compare with our method. We firstly compare our method with these existing neural network which only use the sketch images as the training images. We also compare the propose SketchNet with other methods on sketch classification based on handcrafted features <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>. Specially, we select Alexnet <ref type="bibr" target="#b19">[20]</ref>, GoogLeNet <ref type="bibr" target="#b31">[32]</ref>, Network in Network <ref type="bibr" target="#b22">[23]</ref> and VGGnet <ref type="bibr" target="#b4">[5]</ref> as the baselines which use the deep features. And for these methods using shadow features, we select the methods proposed in <ref type="bibr" target="#b28">[29]</ref> which is used the fisher vector as the sketch representation. Moreover, four kinds of feature representations are selected. We also select the method described in <ref type="bibr" target="#b11">[12]</ref> which uses the soft mapping and hard mapping. Since there do not exist the open source code for these hand-crafted methods, we copy their reported classification results in the corresponding papers <ref type="bibr" target="#b28">[29]</ref>. While for the deep features, we implement their corresponding neural network by treating the sketch images as the training data with softmax as the loss function. Considering the limited number of training images, we adopt the fine-tune strategy to train the neural network based on the ImageNet model using Caffe. We also show the classification result without using metric learning (SketchNet no metric). We first sum the predictions of test pairs, and then find the maximal response label as the test image label.</p><p>The comparison results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Several observations can be drawn from the results. First of all, for all the cases, the proposed SketchNet consistently outperforms the baselines reaching 80.42%. This can demonstrate that real images as the reference plays an important role on judging the category of sketches. Moreover, our proposed label propagation method can help making decisions. Secondly, we could find that these very deep methods e.g. GoogLenet <ref type="bibr" target="#b31">[32]</ref> and VGGnet <ref type="bibr" target="#b4">[5]</ref> do not achieve much better classification accuracies comparing with Alexnet <ref type="bibr" target="#b19">[20]</ref>. For example, when the number of training images for each category is 32, Alexnet obtains 68.12% comparing with GoogLenet up to 67.48% and VGGnet 65.54%. While the number grows to 72, they all get higher results than AlexNet. The main reason may be the limited number of training images on each category which may cause over-fitting. Thirdly, the classification performance is improving with the number of training images growing. Obviously, our SkechtNet can gain better performance even with smaller amount of training images as shown in <ref type="table" target="#tab_2">Table 2</ref>. Furthermore, when the extra real image are added into the neural network, the classification accuracy is significantly improved. For example, for comparison the impact caused by introducing the real images as the context information, we can find that SketchNet obtain a higher accuracy comparing with AlexNet. Comparing with these baselines without discovering the correlation between sketch and real images, our proposed method based on triplets is able to effectively improve the classification accuracy. Comparing with Alexnet(mixed real images) whose training data are mixed of sketch and real images, our SketchNet achieves about 7% improvement. It is interesting to observe that the overall performance of hand-crafted features on this dataset is worse than the results based on the deep features. Last but not the least, the performance of using deep features is beyond human with enough training images. For example, for our SketchNet, we achieve 73.54% when the number of training images for each category is 40. While for VGGnet the number of training images is 56. This can further demonstrate the representation ability of learned deep features.</p><p>We believe three possible reasons may explain such observations: (i) Similar with the human to recognize the con-  <ref type="figure">Figure 6</ref>. Examples of correctly classified sketches on test case with the real images as the references. The first column shows the test sketch, and the top 5 category predictions are displayed from the top row to the bottom in sequence. Specifically, each row of real image represents the nearest visual similarity images in their category. tent of sketch images, our proposed SketchNet can imitate this process by introducing the most similarity real images. (ii) With our designed loss function, our SketchNet can not only discover the latent structure between sketches and real images, but also obtain the discriminative feature representation. The second component of our proposed loss function can be seen as ranking regularization to serve as feature selection. (iii) As the experiment results show, deep features have a significantly discriminative ability than the traditional hand-crafted feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>Although the overall sketch classification performance can be boosted by introducing the real image as the context, there are still existing some limitations influencing the performance of specific categories. Some results are displayed in <ref type="figure">Fig. 6</ref> and <ref type="figure" target="#fig_7">Fig. 7</ref>. As shown in <ref type="figure" target="#fig_7">Fig. 7</ref>, we can observe that because of the drawing perspective, the sketch "cake" is more similar with "submarine" than a cake. The second factor is the real images in the repository, which need cover the object in multi-views. In this case, we can not find the same view perspective real images from the cake image set. This demonstrates that the repository should include real images of different views. Last but not the least, the aspect ratio of sketch images also contain important categorical information. However in our work we ask the images show be fixed to 256 × 256 which lead to some unexpected situations. As shown in <ref type="figure" target="#fig_7">Fig. 7</ref>, the top five prediction includes trombone which is dissimilar with the query sketch. Furthermore, from the classification results we can observe that Human can recognize almost sketches without sketch training data. While for computer, with enough training sketches it can achieve much better classification accuracy even beyond human on sketch classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a novel method to develop sketch feature representation, named SketchNet based  SketchNet. The first column shows the test sketch image "cake". The top 5 prediction results are displayed in the second columns and the rest columns of images are shown the related real images which are used to construct the test pairs. on the deep convolutional neural network. We use the real web images as the reference to discover the latent discriminative structures of sketch images. To that end, a triplet is constructed as the input of our SketchNet. Moreover, we customize the novel neural network with our defined loss function to extract the discriminative feature representation of sketch images. In the step of testing, we introduce a metric learning method to merge the scores of test pairs. Empirical evaluations on sketch classification show that the proposed method has achieved superior performance gains over state-of-the-arts. Nevertheless, there exist many other factors e.g. view perspectives which influence the performance of classification. This can be solved by introducing the 3D shape models which would be our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>Figure 2. Sketch classification with real image embedding using SketchNet. Given a sketch image of an object, e.g., donut, we find its initialized category predictions (Donut, Tire and Wheel) based on the pre-trained sketch model. Then the test pairs are constructed based on their visual similarity. Conditioned on these test pairs, we use our SketchNet model to achieve the prediction scores of each test image pair. Finally, all the predictions are merged to make the decision to identify the sketch category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>{s 1 , ..., s N }∈R 1×N where N denotes the number of training sketches. The corresponding real images dataset is denoted as R = {r 1 , ..., r M }∈ R 1×M where M is the number of real images. The category label of these real images are T = {t 1 , ..., t N }∈R C×M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Triplet generation for each training sketch. Given the sketch image, top 5 category predictions (dog, cat, zebra, lion and cow) are extracted based on the pre-trained sketch model. For each sketch image, we find one positive real image from the corresponding category real image set and one negative image from the predicted category set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of our SketchNet. As described in Section 3.2, there are three components R-Net (in the blue box), S-Net(in the green box) and C-Net (in the red box) to construct our neural network. Given the training triplet, we apply the sketch into the S-Net and real images into the R-net. In particular, positive and negative images share the same R-net. Then, the positive and negative sketch pairs are fed into the C-Net. Finally, we use our defined loss function to back propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Examples of incorrect classification case based on our</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Configurations of the SketchNet with input images of size 256 × 256</figDesc><table>Net 
Type 
Filiter Size/Stride Output Size 

S-Net 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc>The comparison classification results on TU-Berlin sketch benchmark</figDesc><table>Methods 
8 
16 
24 
32 
40 
48 
56 
64 
72 
SketchNet 
58.04% 64.43% 67.89% 72.01% 73.54% 75.18% 76.08% 77.33% 80.42% 
SketchNet(no metric) 
55.69% 64.37% 66.20% 71.19% 69.57% 73.62% 73.43% 76.50% 77.41% 
AlexNet(mixed real images) 
51.96% 59.22% 63.80% 65.97% 68.58% 69.80% 70.46% 72.31% 73.25% 
AlexNet [20] 
54.8% 
62.3% 
67.6% 
68.12% 69.86% 71.65% 72.62% 74.02% 75.02% 
GoogLeNet [32] 
52.01% 59.61% 62.45% 67.48% 69.19% 
70.5% 
71.5% 
72.4% 75.25% 
NIN [23] 
51.4% 
61.9% 
65.50% 68.05% 70.61% 71.50% 72.02% 73.82% 74.40% 
VGGNet [5] 
53.85% 60.65% 63.05% 65.54% 67.34% 69.54% 73.83% 75.17% 76.53% 
FisherVector size 24 (SP) [29] 
43% 
52% 
56% 
59% 
62% 
65% 
66% 
67% 
68% 
FisherVector size 24 [29] 
41% 
50% 
53% 
56% 
60% 
62% 
64% 
64% 
65% 
FisherVector size 16 (SP) [29] 
44% 
50% 
55% 
57% 
60% 
63% 
64% 
65% 
66% 
FisherVector size 16 [29] 
39% 
45.5% 
50% 
53% 
56% 
59% 
60% 
61% 
62% 
Eitz et al. [12] (SVM soft) 
33% 
41% 
44% 
46% 
50% 
51% 
54% 
55% 
55% 
Eitz et al. [12] (SVM hard) 
32% 
37% 
42% 
45.5% 
48% 
49% 
50.8% 
53% 
53% 
Eitz et al. [12] (Knn soft) 
26% 
31% 
34.8% 
36% 
39% 
40.5% 
42% 
43% 
44% 
Eitz et al. [12] (knn hard) 
22% 
26% 
28% 
31% 
33% 
34.5% 
35% 
36% 
37.5% 

different pre-trained sketch models. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Top 5 nearest real Images Top PredictionTop 5 nearest real Images</figDesc><table>Van 

Test Sketch 

Van 

Truck 

SUV 

Picktruck 

Bus 

Moon 

Ship 

Sailboat Mouth 

Canoe 

Sail boat 

Test Sketch 

Top 
Prediction 

Hot air 
ballon 

Test Sketch 

Top 5 nearest real Images 

Top 
Prediction 

Hot air 
ballon Binoculars Toilet 

Arm 
chair 

Lobster 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by National Natural Science Foundation of China (No.61422213 and U1536203) and "Strategic Priority Research Program" of the Chinese Academy of Sciences (XDA06010701).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sym-fish: A symmetry-aware flip invariant sketch histogram shape descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sketch-based image matching using angular partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalechale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Naghdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSMC, Part A</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="41" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting speaker-specific information with a regularized siamese deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sketch2photo: internet image montage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">124</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">icandraw: using sketch recognition and corrective feedback to assist a user in drawing human faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hammond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCHI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="44" to="52" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A descriptor for large scale image retrieval based on sketched feature lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Symposium on Sketch-Based Interfaces and Modeling</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sketch-based image retrieval: Benchmark and bag-offeatures descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1624" to="1636" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photosketcher: interactive sketch-based image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CGA</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="56" to="66" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The shape boltzmann machine: a strong model of object shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sketch recognition by ensemble matching of structured features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep human parsing with active template regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2402" to="2414" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">network.arXivpreprintarXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching-cnn meets knn: quasiparametric human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1419" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="775" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">Imagenet large scale visual recognition challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shapeshop: Sketch-based solid modeling with blobtrees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wyvill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Jorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH courses, number 43</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sketch classification and classification-driven analysis using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Data-driven visual similarity for cross-domain image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">154</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning discriminative projections for text similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCNLL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sketch-based 3d shape retrieval using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1875" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">3d shapenets for 2.5 d object recognition and next-best-view prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5670</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
