<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraB: Visual Saliency via Novel Graph Model and Background Priors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">eBay Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">eBay Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GraB: Visual Saliency via Novel Graph Model and Background Priors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an unsupervised bottom-up saliency detection approach by exploiting novel graph structure and background priors. The input image is represented as an undirected graph with superpixels as nodes. Feature vectors are extracted from each node to cover regional color, contrast and texture information. A novel graph model is proposed to effectively capture local and global saliency cues. To obtain more accurate saliency estimations, we optimize the saliency map by using a robust background measure. Comprehensive evaluations on benchmark datasets indicate that our algorithm universally surpasses state-of-the-art unsupervised solutions and performs favorably against supervised approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans are able to rapidly identify the visually distinctive objects in a scene. This fundamental capability has long been studied in neuroscience and cognitive psychology. In the computer vision community, researchers focus on similar tasks to determine regions that attract attention from a human perception system. The selected regions contain finer details of interest and can be used for extraction of intermediate and higher level information. Therefore, a fast and robust saliency detection algorithm can benefit various other vision tasks.</p><p>The literature of saliency map estimation is vast. However, most existing approaches can be categorized into unsupervised (typically bottom-up) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19]</ref> and supervised (typically bottom-up, but more recent approaches are a combination of top-down and bottom-up) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> approaches.</p><p>While supervised approaches are able to automatically integrate multiple features and in general achieve better performance than unsupervised methods, it is still expensive to perform the training process, especially data collection. Also, compared to traditional special-purpose object detectors (e.g. pedestrian detection) where objects un- der the same class share some consistency, the salient objects from two images are often found vastly different in terms of visual appearance, especially when the object can be anything. Furthermore, the process of generating pixelwise ground truth annotations itself is expensive and laborintensive, and sometimes may even be impossible considering the scale of today's massive long-tailed visual repositories. This is typically the case in large e-commerce scenarios. A fast saliency technique can be an essential preprocessing step for background removal or object/product detection and recognition in large ecommerce applications.</p><p>In this paper, we propose an unsupervised bottom-up saliency estimation approach. Our method is based on the remarkable success of the spectral graph theory. We focus on the core elements of spectral clustering algorithms. Specifically, we introduce a new graph model which captures local/global contrast and effectively utilizes the boundary prior. Inspired by ISOMAP manifold learning <ref type="bibr" target="#b30">[31]</ref>, we introduce geodesic distance to calculate the weight matrix. This constraint maximally enforces the background connectivity prior. Furthermore, we exploit . Nodes in the graph are superpixels. Weights are based on color and texture features. Groups of background seeds are selected for initial saliency based on their influence via the graph structure. Inconsistent groups of background seeds are eliminated. This estimated saliency map is refined by passing it again through the system. This process is repeated at multiple scales and results are fused.</p><p>boundary prior for selecting seeds to perform an initial background query. The resulting saliency map is further used to generate seeds to perform another query to obtain the final saliency map. As we will demonstrate empirically, the proposed method universally outperforms state-of-theart unsupervised methods (e.g. GMR <ref type="bibr" target="#b35">[36]</ref> ) by a large margin, and in some cases even excels supervised methods (e.g. DRFI <ref type="bibr" target="#b13">[14]</ref>). Our claim is that the proposed graph model provides more desirable characteristics for saliency detection and achieves unprecedented balance between computational complexity and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The core of our work is closely related to graph-based manifold ranking as in <ref type="bibr" target="#b35">[36]</ref>, geodesic distance as in <ref type="bibr" target="#b31">[32]</ref>, boundary prior sampling as in <ref type="bibr" target="#b18">[19]</ref> and multi-scale fusion as in <ref type="bibr" target="#b34">[35]</ref>.</p><p>Supervised vs. Unsupervised Unsupervised methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19]</ref> aim at separating salient objects by extracting cues from the input image only. To date, various low-level features have been shown to be effective for saliency detection, such as color contrast, edge density <ref type="bibr" target="#b27">[28]</ref>, backgroundness <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref>, objectness <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, focus <ref type="bibr" target="#b14">[15]</ref>, etc. By eliminating the requirement of training, unsupervised methods can be easily integrated into various applications. In contrast, supervised approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> acquire visual knowledge from ground truth annotations. Recent advances in deep learning show promising results on benchmark datasets <ref type="bibr" target="#b19">[20]</ref>. However, it is expensive to collect the hand-labeled images and set up the learning framework.</p><p>Graph-based Models Graph-based approaches have gained great popularity due to the simplicity and efficiency of graph algorithms. Harel et al. <ref type="bibr" target="#b9">[10]</ref> proposed the graph based visual saliency (GBVS), a graph-based saliency model with multiple features to extract saliency information. Chang et al. <ref type="bibr" target="#b5">[6]</ref> present a computational framework by constructing a graphical model to fuse objectness and regional saliency. Yang et al. <ref type="bibr" target="#b35">[36]</ref> rank the similarity of superpixels with foreground or background seeds via graph-based manifold ranking. This method is further improved by Li et al. to generate pixel-wise saliency maps via regularized random walks ranking <ref type="bibr" target="#b18">[19]</ref>.</p><p>Center vs. Background Prior Recently, more and more bottom-up methods prefer to use the image boundary as the background seeds. This boundary prior is more general than previously used center prior, which assumes that the saliency object tend to appear near the image center <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. Wei et al. <ref type="bibr" target="#b31">[32]</ref> define the saliency of a region to be the length of its shortest path to the virtual background node. In <ref type="bibr" target="#b38">[39]</ref>, a robust background measure is proposed to characterize the spatial layout of an image region with respect to the boundary regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph Construction</head><p>Our approach is based on building an undirected weighted graph for superpixels. We first segment the input image I into n superpixels S = {s 1 ,s 2 , ..., s n } via the Simple Linear Iterative Clustering (SLIC) <ref type="bibr" target="#b1">[2]</ref> algorithm. For each superpixel s, we extract color and texture information to form a regional feature descriptor r. A metric is proposed to calculate the edge weight between two given descriptors. Next, we construct a graph G =( V, E) (see <ref type="figure" target="#fig_0">Fig. 1</ref>) where V is a set of nodes corresponding to superpixels S, and edges E are constructed using the proposed graph model. E is quantified by a weight matrix W =[ w ij ] n×n where the weights are calculated using distances between extracted feature descriptors. In Sec. 3.1, we describe our newly proposed graph model and in Sec. 3.2 we show how to extract regional features and calculate the weight matrix W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposed Graph Model</head><p>Given a set of superpixels S, we start by building a kregular graph where each node is only connected to its immediate neighbors. We define the adjacency matrix of the initial graph G to be A =[ a ij ] n×n .I fa ij =1 , then the nodes s i and s j are adjacent, otherwise a ij =0 .A s G is undirected we require a ij = a ji . B∈Sdenotes a set of boundary nodes containing |B| superpixels on the four borders of the input image. For robust purposes, we only choose to use three borders, and the selection of borders is described in Sec. 4.1. We subsequently add edges to the initial graph G to build a new graph model with the following rules: 1) Each node is connected to both its immediate neighbors and 2-hop neighbors; 2) We add edges to connect each node to boundary nodes on the four sides of the image. The weight for each edge is divided by the number of boundary nodes; 3) Any pair of nodes on the image boundary is considered to be connected. We denote the above three rules by R 1 ,R 2 and R 3 , and the final edge set E = {E 1 , E 2 , E 3 } can be obtained as:</p><formula xml:id="formula_0">R 1 : E 1 = {(s i ,s j )|s i ,s j ∈S,a ij =1} ∪{(s i ,s k )|s k ∈S,a kj =1}, w ij = weight(r i ,r j ). R 2 : E 2 = {(s i ,s j )|s i ∈S,s j ∈B}, w ij = weight(r i ,r j )/|B|. R 3 : E 3 = {(s i ,s j )|s i ,s j ∈B}, w ij = weight(r i ,r j ).<label>(1)</label></formula><p>The structure of our graph model is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Since neighboring superpixels are more likely to be visually similar, R 1 enables us to effectively utilize local neighborhood relationships between the superpixel nodes. R 2 connects each node to all boundary nodes, enforcing the global contrast constraint. Since the number of boundary superpixels may be large, we average the edge weights, making the total contribution of boundary nodes equivalent to only one single superpixel. R 3 enforces the graph to be a closedloop. Combined with R 2 which connects each superpixel to boundary nodes. R 3 further reduces the geodesic distance of two similar superpixels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction</head><p>In this section, we detail the process of extracting feature descriptors from each superpixel. This process is crucial to the estimation of the final saliency map as the edge weights are calculated by comparing the feature descriptors of two nodes. A good feature descriptor should exhibit high contrast between salient and non-salient regions. In our work, we mainly adopt two kinds of features: color and texture. For color features, we consider mean color values and color histograms in the CIELAB <ref type="bibr" target="#b11">[12]</ref> color space for each superpixel. For texture features, we use responses from the Leung-Malik (LM) filter bank <ref type="bibr" target="#b17">[18]</ref>. Let v lab ,h lab ,h tex be the mean L*a*b* color, L*a*b* histogram and max LM response histogram of superpixel s, we define the distance between two superpixels as:</p><formula xml:id="formula_1">dist(r i ,r j )=λ 1 ||v lab i − v lab j || + λ 2 χ 2 (h lab i ,h lab j ) + λ 3 χ 2 (h tex i ,h tex j ).<label>(2)</label></formula><p>where r =(v, h lab ,h tex ) is the combined feature descriptor for superpixel s, λ 1 ,λ 2 and λ 3 are weighting parameters,</p><formula xml:id="formula_2">χ 2 (h 1 ,h 2 )= K i=1 2(h 1 (i) − h 2 (i)) 2 h 1 (i)+h 2 (i)</formula><p>is the chi-squared distance between histograms h 1 and h 2 with K being the number of bins. The edge weights can be obtained by the 3. Select three borders as query seeds as described in Sec. 4.1 and obtain query vector y =[y 1 ,y 2 , ..., y n ] T . 4. Acquire initial saliency estimation S † using Eq. <ref type="formula" target="#formula_7">(7)</ref>, Eq. <ref type="formula" target="#formula_8">(8)</ref> and Eq. (9). 5. Optimize S † using Eq. (10) and re-apply Eq. <ref type="formula" target="#formula_7">(7)</ref> to obtain the foreground estimation. Apply Eq. (4) and average results across different levels to obtain final saliency map S ‡ .</p><p>Result: A saliency map S ‡ with the same size as the input image</p><p>Gaussian similarity function:</p><formula xml:id="formula_3">weight(r i ,r j )= ⎧ ⎪ ⎨ ⎪ ⎩ exp(−dist(r i ,r j )/σ 2 ) if a ij =1, min ρ1=ri,ρ2=ri+1,...,ρm=rj m−1 ρ=1 weight(ρ k ,ρ k+1 ) if a ij =0.</formula><p>(3) where σ is a constant. In the above equation, the second condition considers the shortest path between nodes i, j.As can be seen from Eq.(2), our approach is completely based on intrinsic cues of the input image. Without any prior knowledge of size of the salient object, we adopt the L-layer Gaussian pyramid for robustness. The lth-level pyramid I l is obtained as:</p><formula xml:id="formula_4">I l (x, y)= 2 s=−2 2 t=−2 ω(s, t)I l−1 (2x + s, 2y + t),l 1. (4)</formula><p>where I 0 is the original image, ω(s, t) is a Gaussian weighting function (identical at all levels). The number of superpixels n l = |S l | for the l-th level pyramid I l is set as:</p><formula xml:id="formula_5">n l = n l−1 2 2(l−1)<label>(5)</label></formula><p>Next, we extract multiscale features r l and build weight matrices W l for each level. The final saliency estimation is conducted on each level independently and the output saliency map is combined using results from all levels (see Sec. 5.2 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Background Priors</head><p>Given the weighted graph, we can take either foreground or background nodes as queries <ref type="bibr" target="#b35">[36]</ref>. The resulting saliency map is calculated based on its relevance to the queries. Our algorithm is based on background priors, which consists of two parts: the boundary prior and the connectivity prior. The first prior is based on the observation that the salient object seldom touches the image borders. Compared to the center prior <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref> which assumes that the salient object always stays at the center of an image, the boundary prior is more robust, which is validated on several public datasets <ref type="bibr" target="#b31">[32]</ref>. In our work, we choose three out of four borders as background seeds to perform queries <ref type="bibr" target="#b18">[19]</ref>. This is because the foreground object may completely occupy one border of an image, which is commonly seen in portrait photos. Therefore, eliminating one border which tends to have a very distinct appearance generates more accurate results. The second prior is based on the insight that background regions are usually large and homogeneous. Therefore, the superpixels in the background can be easily connected to each other. This prior is also applicable for images with a shallow depth of field, where the background region is out of focus. The rest of this section is organized as follows: Sec. 4.1 elaborates the detailed steps of the initial background query and Sec. 4.2 illustrates a refinement scheme based on the connectivity prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Query via the Boundary Prior</head><p>To provide more accurate saliency estimations, we first compare the four borders of the image and remove one with the most distinctive color distribution. We combine boundary superpixels together to form a single region, and use Eq.</p><p>(2) to compute the distance of any two of the four regions {B top , B bottom , B lef t , B right }. The resulting 4 × 4 matrix is summed column-wise, and the maximum column corresponds to the boundary to be removed.</p><p>Once the query boundaries are obtained, we can label the corresponding superpixels to be background. More formally, we build a query vector y =[ y 1 ,y 2 , ..., y n ] T , where y i =1if s i belongs one of the four query boundaries, otherwise y i =0 . Given the weight matrix W =[ w ij ] n×n computed in Sec. 3.2, we can obtain the degree matrix D = diag(d 1 ,d 2 , ..., d n ), where d i = j w ij . Let f be the ranking function assigning rank values f =[f 1 ,f 2 , ..., f n ] T which could be obtained by solving the following minimization problem:</p><formula xml:id="formula_6">f † =arg min f 1 2 ⎛ ⎝ n ij=1 w ij f i √ d i − f j d j 2 +μ n i=1 || f i −y i || 2 ⎞ ⎠ .<label>(6)</label></formula><p>where μ is a controlling parameter. The optimized solution is given in <ref type="bibr" target="#b37">[38]</ref> as:</p><formula xml:id="formula_7">f † =(D − W μ +1 ) −1 y.<label>(7)</label></formula><p>Three ranking results f † (b) will be achieved after applying Eq. <ref type="formula" target="#formula_7">(7)</ref>, where b corresponds one of the three borders. Since the ranking results show the background relevance of each  <ref type="bibr" target="#b10">[11]</ref>, RTV = texture smoothing using relative total variation <ref type="bibr" target="#b33">[34]</ref>, EBR = erroneous boundary removal <ref type="bibr" target="#b18">[19]</ref>, RPCA = robust PCA <ref type="bibr" target="#b4">[5]</ref>, LAB = CIELAB color <ref type="bibr" target="#b11">[12]</ref>, HIST = L*a*b* histogram, LM = Leung-Malik filter bank <ref type="bibr" target="#b17">[18]</ref>, LBP = local binary patterns <ref type="bibr" target="#b25">[26]</ref>, AVE = simple averaging, HS = hierarchical saliency <ref type="bibr" target="#b34">[35]</ref>, GMR = graph based manifold ranking <ref type="bibr" target="#b35">[36]</ref>, BC = boundary connection, GEO = geodesic distance. Methods included in the final pipeline are marked in bold. node, we still need to calculate their complement values to obtain the foreground-based saliency:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><formula xml:id="formula_8">S i (b)=1− f † i (b),i =1, 2, ..., n.<label>(8)</label></formula><p>The results are then put into element-wise multiplication to calculate the saliency map:</p><formula xml:id="formula_9">S † = b S i (b).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Refinement</head><p>In this section, we seek to optimize the result from the previous section. The optimized result will be used as foreground query by applying Eq. (7) again. The cost function is designed to assign 1 to salient region value and 0 to background region. The optimized result is then obtained by minimizing the following cost function <ref type="bibr" target="#b38">[39]</ref>:</p><formula xml:id="formula_10">f ‡ = arg min f ⎛ ⎝ n i=1 F i (f i −1) 2 + n i=1 B i f 2 i + i,j w ij (f i −f j ) 2 ⎞ ⎠ .</formula><p>(10) Where F i and B i are foreground and background probabilities, F i &gt; mean(S i ) and B i &lt; mean(S i ). The three terms are all squared errors and the optimal result is computed by least-square. The newly obtained f is a binary indicator vector and can be used as seed for foreground queries. By re-applying Eq. <ref type="formula" target="#formula_7">(7)</ref>, we obtain the final saliency map</p><formula xml:id="formula_11">S ‡ =(D − W μ +1 ) −1 f ‡ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Parameter Setup</head><p>We empirically set parameters in all experiments. λ 1 , λ 2 and λ 3 in Eq. (2) are set to 0.25, 0.45 and 0.3, respectively. In our experiment, we use a 3 level pyramid, hence l =3 in Eq. (4). The constant σ in Eq. (3) and μ in Eq. (6) are empirically chosen and σ 2 =0 .1, 1/(μ +1) = 0.99. Our method is implemented using Matlab on a machine with Intel Core i7-980X 3.3 GHz CPU and 16GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>We start by modifying the GMR framework proposed by <ref type="bibr" target="#b35">[36]</ref>. We experiment different design options among five categories: preprocessing, sampling, features, scaling and graph structure. The individual components are added to the original GMR framework and quantitative evaluations are conducted on the entire ECSSD dataset <ref type="figure" target="#fig_5">(Fig. 5 and Fig.  8)</ref>.</p><p>Preprocessing The input images are often composed of objects at various scales with diverse texture details. Therefore, it is important to remove detrimental or unwanted content. We choose two edge-preserving filters for testing: guided image filtering <ref type="bibr" target="#b10">[11]</ref> and imaging smoothing via relative total variation <ref type="bibr" target="#b33">[34]</ref>. The first method performs edgepreserving smoothing while the second method extracts important structure from texture based on inherent variation and relative total variation measures. Quantitative evaluations suggest that both methods are able to improve the saliency detection results with similar performance.</p><p>Sampling Our method estimates saliency by using boundary superpixels as queries. If the foreground object touches one or more boundaries of the image, then the query results may be problematic. Therefore, it is important to smartly choose boundary superpixels as seeds. We tested two schemes for sampling boundary superpixels: erroneous boundary removal and robust principle component analysis. The details of the first method is illustrated in Sec. 4.1. The second method is based on the recently proposed rank minimization model <ref type="bibr" target="#b4">[5]</ref>. We randomly sample 25% of all superpixels on each border, and repeat this step n times. This results in 4n set of query seeds. For each set we apply Eq. (7) to estimate saliency values for all superpixels. We unroll each resulting image into a vector and stack them into a matrix P . The low rank matrix A can be recovered from the corrupted data matrix P = A + E by solving the following convex optimization problem:  where ||·|| * denotes the nuclear norm, ||·|| 1 denotes the sum of absolute values of matrix entries, λ is a positive weighting parameter and E is a sparse error matrix. In our experiment, we set n =5and perform the query 20 times for each image to get the initial saliency map S † . Evaluation on the complete ECSSD dataset shows that RPCA achieves better precision than erroneous boundary removal.</p><formula xml:id="formula_12">min A,E ||A|| * + λ||E|| 1 .<label>(11)</label></formula><p>Features As stated in Sec. 3.2, we associate each superpixel with a feature vector to calculate the weight matrix W . A good feature descriptor should exhibit high contrast between salient and non-salient regions. In our experiment, we mainly test four different features: mean L*a*b* value <ref type="bibr" target="#b11">[12]</ref>, L*a*b* histogram, responses from the LM filter bank and local binary patterns (LBP) <ref type="bibr" target="#b25">[26]</ref>. Among these features, the mean L*a*b* value is shown to be effective in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. According to Jiang et al. <ref type="bibr" target="#b13">[14]</ref>, the L*a*b* histogram is the most important regional feature in their feature integration framework. We are able to achieve satisfactory precision using the first two features. The LM filter response gives better overall recall. LBP feature seems to be not as effective as LM texture features in our case. Therefore, we linearly combine the first three features together to form the final feature vector.</p><p>Scaling In the saliency detection literature, hierarchical models are often adopted for robustness purpose <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14]</ref>. Our first experiment is to build an image pyramid, apply our algorithm to each layer and simply average all maps (Sec. 3.1). We subsequently test the approach proposed in <ref type="bibr" target="#b34">[35]</ref>. This method differs from naive multi-layer fusion by selecting optimal weights for each region using hierarchical inference. Due to the proposed tree structure, the saliency inference can efficiently be conducted using belief propagation.</p><p>Graph Structure We use the model proposed by <ref type="bibr" target="#b35">[36]</ref> as a baseline to test variations on the graph structure. The reference model enforces rule R 1 and R 3 in Sec. 1 and adopts Euclidean distance as the weighting metric. We conduct experiments on both graph structures (Sec. 3.1) and distance metrics (Sec. 3.2). Quantitative evaluations show a major performance improvement compared to other methodologies.</p><p>Combination We have presented 5 different strategies to facilitate more accurate saliency estimation. However, it is difficult to test all permutations and analyze the interactions between different methods. Therefore, how to optimally combine these methods still remains non-trivial. For example, the use of guided filter and multiscale averaging alone improves the recall scores. However, when combined together the performance drops slightly. Also, we choose not to use RPCA-based boundary sampling and belief-propagation based multi-layer fusion due to speedaccuracy tradeoffs. In our final model we choose not to perform any texture smoothing and employ the multiscale averaging scheme due to its simplicity and efficacy. The color histogram based erroneous boundary removal scheme is used for generating the initial queries. The methods we choose to include in the final pipeline are marked in bold in <ref type="table" target="#tab_1">Table 1</ref>. At the core of our algorithm is the newly proposed graph model and geodesic distance metric as they offer significant performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-Art</head><p>Datasets In the experiments, we qualitatively and quantitatively compare the proposed approach with eight stateof-the-art approaches, including DRFI <ref type="bibr" target="#b13">[14]</ref>, DSR <ref type="bibr" target="#b21">[22]</ref>, GMR <ref type="bibr" target="#b35">[36]</ref>, HS <ref type="bibr" target="#b34">[35]</ref>, LMLC <ref type="bibr" target="#b32">[33]</ref>, MC <ref type="bibr" target="#b12">[13]</ref>, RC <ref type="bibr" target="#b6">[7]</ref>, SF <ref type="bibr" target="#b26">[27]</ref>. It is important to note that besides DRFI, all other methods are unsupervised. The evaluation is conducted on three challenging datasets: ECSSD, THUS10K and Jud-dDB. The Extended Complex Scene Saliency Dataset (EC-SSD) <ref type="bibr" target="#b34">[35]</ref> contains 1000 semantically meaningful but structurally complex images from the BSD dataset <ref type="bibr" target="#b2">[3]</ref>, PASCAL VOC <ref type="bibr" target="#b8">[9]</ref> and the Internet. The binary masks for the salient objects are produced by 5 subjects. THUS10K <ref type="bibr" target="#b7">[8]</ref> contains 10000 images with pixel-level ground-truth labelings from the large dataset (20,000+ images) proposed by Liu et al. <ref type="bibr" target="#b23">[24]</ref>. The JuddDB dataset <ref type="bibr" target="#b3">[4]</ref> is created from the MIT saliency benchmark <ref type="bibr" target="#b15">[16]</ref>, mainly for checking generality of salient object detection models over real-world scenes with multiple objects and complex background. Additionally, we compared with all saliency object segmentation methods mentioned in <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b36">[37]</ref> on the PASCAL-S dataset, including CPMC+GBVS <ref type="bibr" target="#b22">[23]</ref>, CPMC+PatchCut <ref type="bibr" target="#b36">[37]</ref>, GBVS+PatchCut <ref type="bibr" target="#b36">[37]</ref>, RC <ref type="bibr" target="#b6">[7]</ref>, SF <ref type="bibr" target="#b26">[27]</ref>, PCAS <ref type="bibr" target="#b24">[25]</ref> and FT <ref type="bibr" target="#b0">[1]</ref>. The PASCAL-S is proposed to avoid the dataset design bias, where the image selection process deliberately emphasizes the concept of saliency <ref type="bibr" target="#b22">[23]</ref>.</p><p>Evaluation We follow the canonical precision-recall curve and F-measure methodologies to evaluate the performance of our algorithm using the toolbox provided by <ref type="bibr" target="#b20">[21]</ref>. The PR-curve and F-measure comparisons are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. Specifically, the PR curve is obtained by binarizing the saliency map using varying thresholds from 0 to 255, as mentioned in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. F-measure is obtained using the metric proposed by <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_13">F β = (1 + β 2 )P recision × Recall β 2 P recision + Recall<label>(12)</label></formula><p>Here, the precision and recall rates binarized using an adaptive threshold determined as two times the mean saliency of a given image. We set β to 0.3 to emphasize the precision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19]</ref>. As can be seen in <ref type="figure" target="#fig_5">Fig. 5</ref>, our method significantly outperforms all seven unsupervised methods by a large margin. Specifically, our method achieved an improvement of 6% in comparison with the baseline GMR model on the challenging ECSSD dataset. Also, our method is highly competitive  when compared to DRFI on all three datasets. It is worth noting that DRFI takes around 24 hours for training and 10 seconds for testing given a typical 400×300 image <ref type="bibr" target="#b13">[14]</ref>, whereas our method is fully unsupervised and only takes 800 milliseconds to process a similar image. Furthermore, DRFI takes 2500 images for training and extracts more than 20 different features, while our method is purely based on the input image and only uses 3 simple features. In other words, our method is much more efficient than DRFI yet still capable of maintaining competitive accuracy. The efficacy of our graph model is self-evident. Quantitative evaluations on PASCAL-S [23] <ref type="figure" target="#fig_7">(Fig. 7)</ref> show that our method achieves higher precision, recall and F-measure scores compared to the state-of-the-art CPMC+GBVS algorithm presented in <ref type="bibr" target="#b22">[23]</ref>. Also, our method performs favorably against the more recent Patch-Cut method <ref type="bibr" target="#b36">[37]</ref> and clearly above all other saliency algorithms. Again, our method is training-free and performs much faster than CPMC+GBVS and PatchCut. (CPMC+GBVS takes around 30s to process a 400 × 300 image, according to our experiment; PatchCut takes around 10s for segmenting a 200 × 200 image, as reported by <ref type="bibr" target="#b36">[37]</ref>. Both methods require extra training/example data).</p><p>Our evaluation does not include some of the latest deep-learning methods. The crux of this paper is to propose a novel heuristic model which is able to achieve similar performance to supervised methods like DRFI or CPMC+GBVS without preparing expensive training data. This provides simplicity and easy-to-use generality in many practical applications where computing power is limited and ground truth annotations are very expensive or impossible to acquire. <ref type="figure" target="#fig_6">Fig. 6</ref> shows a few saliency maps for qualitative evaluation. We note that the proposed algorithm uniformly highlights the salient regions and preserves fine object boundaries than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present a novel unsupervised saliency estimation method based on a novel graph model and background priors. Our graph model incorporates local and global contrast and naturally enforces the background connectivity constraint. The proposed feature distance metrics effectively and efficiently combines local color and texture cues to represent the intrinsic manifold structure. We further optimize the background seeds by exploiting a boundary query and refinement scheme, achieving state-of-the-art results. Our future work includes theoretical analysis on the proposed graph model and its potential towards building better clustering algorithms. Also, we would like to accelerate our algorithm via parallel computing, as large-scale spectral clustering has been trivially accomplished in high-performance graphics hardware <ref type="bibr" target="#b29">[30]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our novel graph structure with superpixels as nodes. The purple and blue lines represent connections to first and second order neighbors, respectively. The green lines indicate that each node is connected to the boundary nodes on four sides of the image. The red lines show that the all boundary nodes are connected among themselves. See Sec. 3.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Pipeline of the proposed algorithm, divided into three parts: Graph Construction (Sec. 3), Query Selection (Sec. 4.1) and Refinement (Sec. 4.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The effect of our graph model described in Sec. 3.1. From left to right: input image, result using the graph structure proposed by<ref type="bibr" target="#b35">[36]</ref>, result obtained using our graph model. Our model performs better since it encodes background consistency, global contrast and local contrast.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples where geodesic distance generate more accurate results. From left to right: input image, results without enforcing the geodesic distance constraints, results with geodesic constraints. Geodesic distance avoids missing parts due to color bleeding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Quantitative PR-curve and F-measure evaluation of 9 approaches on 3 datasets. The rows from top to bottom correspond to ECSSD, THUS10K and JuddDB, respectively. Clearly, our approach excels all other unsupervised approaches and performs favorably against a powerful supervised approach (DRFI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative evaluation. DRFI is one of the best supervised approaches. All other approaches shown here are unsupervised. Our model is closely related to GMR, but gives much better performance. See Sec. 5.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Quantitative PR-curve and F-measure evaluation of 7 methods on the PASCAL-S dataset. Note that our method achieves similar or better F-measure as more compute expensive methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Quantitative PR-curve on different design options mentioned in Sec. 5.2. The baseline method (GMR) and final combined method (GraB) are added to both figures for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Algorithm 1: Visual Saliency via Novel Graph Model and Background PriorsData: Input image I and related parameters 1. Apply SLIC<ref type="bibr" target="#b1">[2]</ref> and separate input image I into n superpixels S = {s 1 ,s 2 , ..., s n }, establish graph structure with Eq. (1). 2. Calculate W and D using Eq. (2) and Eq. (3).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on adding different components to the baseline GMR<ref type="bibr" target="#b35">[36]</ref> algorithm (Sec. 5.2). All results correspond to ECSSD. GF = guided filter</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>We thank Jimei Yang and Professor Ming-Hsuan Yang from UC Merced for sharing the PatchCut data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The berkeley segmentation dataset and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<ptr target="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What is a salient object? a dataset and a baseline model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="742" to="756" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Robust principal component analysis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JACM</publisher>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusing generic objectness and visual saliency for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="914" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="569" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salient object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Guided image filtering. PAMI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparative analysis of the quantization of color spaces on the basis of the cielab color-difference formula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Vorhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="154" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Saliency detection via absorbing markov chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Salient region detection by ufo: Uniqueness, focusness and objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1976" to="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIT Technical Report</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust saliency detection via regularized random walks ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contextual hypergraph modeling for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3328" to="3335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What makes a patch distinct</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple method for detecting salient regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2363" to="2371" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A unified approach to salient object detection via low rank matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="853" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long term video segmentation through pixel level spectral clustering on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian saliency via low and mid level cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1689" to="1698" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Structure extraction from texture via relative total variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Patchcut: Data-driven object segmentation via local shape transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1770" to="1778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<title level="m">Ranking on data manifolds. NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
