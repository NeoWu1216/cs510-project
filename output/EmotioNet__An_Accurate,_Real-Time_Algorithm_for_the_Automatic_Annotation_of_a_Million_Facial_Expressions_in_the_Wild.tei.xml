<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fabian Benitez-Quiroz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Electrical and Computer Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprakash</forename><surname>Srinivasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Electrical and Computer Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Electrical and Computer Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* These authors contributed equally to this paper.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research in face perception and emotion theory requires very large annotated databases of images of facial expressions of emotion. Annotations should include Action Units (AUs) and their intensities as well as emotion category.</head><p>This goal cannot be readily achieved manually. Herein, we present a novel computer vision algorithm to annotate a large database of one million images of facial expressions of emotion in the wild (i.e., face images downloaded from the Internet). First, we show that this newly proposed algorithm can recognize AUs and their intensities reliably across databases. To our knowledge, this is the first published algorithm to achieve highly-accurate results in the recognition of AUs and their intensities across multiple databases. Our algorithm also runs in real-time (&gt;30 images/second), allowing it to work with large numbers of images and video sequences. Second, we use WordNet to download 1,000,000 images of facial expressions with associated emotion keywords from the Internet. These images are then automatically annotated with AUs, AU intensities and emotion categories by our algorithm. The result is a highly useful database that can be readily queried using semantic descriptions for applications in computer vision, affective computing, social and cognitive psychology and neuroscience; e.g., "show me all the images with happy faces" or "all images with AU 1 at intensity c."</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Basic research in face perception and emotion theory cannot be completed without large annotated databases of images and video sequences of facial expressions of emotion <ref type="bibr" target="#b6">[7]</ref>. Some of the most useful and typically needed annotations are Action Units (AUs), AU intensities, and emotion categories <ref type="bibr" target="#b7">[8]</ref>. While small and medium size databases can be manually annotated by expert coders over several months <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5]</ref>, large databases cannot. For example, even if it were possible to annotate each face image very fast by an expert coder (say, 20 seconds/image) <ref type="bibr" target="#b0">1</ref> , it would take 5,556 hours to code a million images, which translates to 694 (8hour) working days or 2.66 years of uninterrupted work.</p><p>This complexity can sometimes be managed, e.g., in image segmentation <ref type="bibr" target="#b17">[18]</ref> and object categorization <ref type="bibr" target="#b16">[17]</ref>, because everyone knows how to do these annotations with minimal instructions and online tools (e.g., Amazon's Mechanical Turk) can be utilized to recruit large numbers of people. But AU coding requires specific expertise that takes months to learn and perfect and, hence, alternative solutions are needed. This is why recent years have seen a number of computer vision algorithms that provide fully-or semiautomatic means of AU annotation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>The major problem with existing algorithms is that they either do not recognize all the necessary AUs for all applications, do not specify AU intensity, are too computational demanding in space and/or time to work with large database, or are only tested within databases (i.e., even when multiple databases are used, training and testing is generally done within each database independently).</p><p>The present paper describes a new computer vision algorithm for the recognition of AUs typically seen in most applications, their intensities, and a large number <ref type="bibr" target="#b22">(23)</ref> of basic and compound emotion categories across databases. Additionally, images are annotated semantically with 421 emotion keywords. (A list of these semantic labels is in the Supplementary Materials.)</p><p>Crucially, our algorithm is the first to provide reliable recognition of AUs and their intensities across databases and runs in real-time (&gt;30 images/second). This allows us to automatically annotate a large database of a million facial expressions of emotion images "in the wild" in about 11 hours in a PC with a 2.8 GHz i7 core and 32 Gb of RAM.</p><p>The result is a database of facial expressions that can be readily queried by AU, AU intensity, emotion category, or  <ref type="figure">Figure 1</ref>: The computer vision algorithm described in the present work was used to automatically annotate emotion category and AU in a million face images in the wild. These images were downloaded using a variety of web search engines by selecting only images with faces and with associated emotion keywords in WordNet <ref type="bibr" target="#b14">[15]</ref>. Shown above are three example queries. The top example is the results of two queries obtained when retrieving all images that have been identified as happy and fearful by our algorithm. Also shown is the number of images in our database of images in the wild that were annotated as either happy or fearful. The next example queries show the results of retrieving all images with AU 4 or 6 present, and images with the emotive keyword "anxiety" and "disaproval." emotion keyword, <ref type="figure">Figure 1</ref>. Such a database will prove invaluable for the design of new computer vision algorithms as well as basic, translational and clinical studies in social and cognitive psychology, social and cognitive neuroscience, neuromarketing, and psychiatry, to name but a few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">AU and Intensity Recognition</head><p>We derive a novel approach for the recognition of AUs. Our algorithm runs at over 30 images/second and is highly accurate even across databases. Note that, to date, most algorithms have only achieved good results within databases. The major contributions of our proposed approach is that it achieves high recognition accuracies even across databases and runs in real time. This is what allows us to automati-cally annotate a million images in the wild. We also categorize facial expressions within one of the twenty-three basic and compound emotion categories defined in <ref type="bibr" target="#b6">[7]</ref>. Categorization of emotion is given by the detected AU pattern of activation. Not all images belong to one of these 23 categories. When this is the case, the image is only annotated with AUs, not emotion category. If an image does not have any AU active, it is classified as a neutral expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Face space</head><p>We start by defining the feature space employed to represent AUs in face images. Perception of faces, and facial expressions in particular, by humans is known to involve a combination of shape and shading analyses <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Shape features thought to play a major role in the per- ception of facial expressions of emotion are second-order statistics of facial landmarks (i.e., distances and angles between landmark points) <ref type="bibr" target="#b15">[16]</ref>. These are sometimes called configural features, because they define the configuration of the face. Let s ij = s T ij1 , . . . , s T ijp T be the vector of landmark points in the j th sample image (j = 1, . . . , n i ) of AU i, where s ijk ∈ R 2 are the 2D image coordinates of the k th landmark, and n i is the number of sample images with AU i present. These face landmarks can be readily obtained with state-of-the-art computer vision algorithms. Specifically, we combine the algorithms defined in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref> to automatically detect the 66 landmarks shown in <ref type="figure" target="#fig_0">Figure 2a</ref>. Thus, s ij ∈ R 132 . All training images are then normalized to have the same inter-eye distance of τ pixels. Specifically,ŝ ij = c s ij , where c = τ / l − r 2 , l and r are the image coordinates of the center of the left and right eye, . 2 defines the 2-norm of a vector,ŝ ij = ŝ T ij1 , . . . ,ŝ T ijp T and we used τ = 300.</p><p>The location of the center of each eye can be readily computed as the geometric mid-point between the landmarks defining the two corners of the eye. Now, define the shape feature vector of configural features as,</p><formula xml:id="formula_0">x ij = d ij12 , . . . , d ijp−1 p , θ T 1 , . . . , θ T p T ,<label>(1)</label></formula><p>where d ijab = ŝ ija −ŝ ijb 2 are the Euclidean distances between normalized landmarks, a = 1, . . . , p − 1, b = a + 1, . . . , p, and θ a = (θ a1 , . . . , θ aqa ) T are the angles defined by each of the Delaunay triangles emanating from the normalized landmarkŝ ija , with q a the number of Delaunay triangles originating atŝ ija and qa k=1 θ ak ≤ 360 o (the equality holds for non-boundary landmark points). Specifically, we use the Delaunay triangulation of the face shown in <ref type="figure">Figure</ref> 2b. Note that since each triangle in this figure can be defined by three angles and we have 107 triangles, the total number of angles in our shape feature vector is 321. More generally, the shape feature vectors x ij ∈ R p(p−1)/2+3t , where p is the number of landmarks and t the number of triangles in the Delaunay triangulation. With p = 66 and t = 107, we have x ij ∈ R 2,466 .</p><p>Next, we use Gabor filters centered at each of the normalized landmark pointsŝ ijk to model shading changes due to the local deformation of the skin. When a facial muscle group deforms the skin of the face locally, the reflectance properties of the skin change (i.e., the skin's bidirectional reflectance distribution function is defined as a function of the skin's wrinkles because this changes the way light penetrates and travels between the epidermis and the dermis and may also vary their hemoglobin levels <ref type="bibr" target="#b0">[1]</ref>) as well as the foreshortening of the light source as seen from a point on the surface of the skin.</p><p>Cells in early visual cortex in humans can be modelled using Gabor filters <ref type="bibr" target="#b3">[4]</ref>, and there is evidence that face perception uses this Gabor-like modeling to gain invariance to shading changes such as those seen when expressing emotions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. Formally, let</p><formula xml:id="formula_1">g (ŝ ijk ; λ, α, φ, γ) = exp s 2 1 + γ 2 s 2 2 2σ 2 cos 2π s 1 λ + φ ,<label>(2)</label></formula><formula xml:id="formula_2">withŝ ijk = (ŝ ijk1 ,ŝ ijk2 )</formula><p>T , s 1 =ŝ ijk1 cos α +ŝ ijk2 sin α, s 2 = −ŝ ijk1 sin α +ŝ ijk2 cos α , λ the wavelength (i.e., number of cycles/pixel), α the orientation (i.e., the angle of the normal vector of the sinusoidal function), φ the phase (i.e., the offset of the sinusoidal function), γ the (spatial) aspect ratio, and σ the scale of the filter (i.e., the standard deviation of the Gaussian window).</p><p>We use a Gabor filter bank with o orientations, s spatial scales, and r phases.</p><formula xml:id="formula_3">We set λ = {4, 4 √ 2, 4 × 2, 4(2 √ 2), 4(2 × 2)} = {4, 4 √ 2, 8, 8 √ 2,</formula><p>16} and γ = 1, since these values have been shown to be appropriate to represent facial expressions of emotion <ref type="bibr" target="#b6">[7]</ref>. The values of o, s and r are learned using cross-validation on the training set. This means, we use the following set of possible values α = {4, 6, 8, 10}, σ = {λ/4, λ/2, 3λ/4, λ} and φ = {0, 1, 2} and use 5-fold cross-validation on the training set to determine which set of parameters best discriminates each AU in our face space.</p><p>Formally, let I ij be the j th sample image with AU i present and define</p><formula xml:id="formula_4">g ijk = (g (ŝ ijk ; λ 1 , α 1 , φ 1 , γ) * I ij , . . . ,<label>(3)</label></formula><formula xml:id="formula_5">g (ŝ ij1 ; λ 5 , α o , φ r , γ) * I ij ) T ,</formula><p>as the feature vector of Gabor responses at the k th landmark points, where * defines the convolution of the filter g(.) with the image I ij , and λ k is the k th element of the set λ defined above; the same applies to α k and φ k , but not to γ since this is always 1.</p><p>We can now define the feature vector of the Gabor responses on all landmark points for the j th sample image with AU i active as</p><formula xml:id="formula_6">g ij = g T ij1 , . . . , g T ijp T .<label>(4)</label></formula><p>These feature vecotros define the shading information of the local patches around the landmarks of the face and their dimensionality is g ij ∈ R 5×p×o×s×r . Finally, putting everything together, we obtained the following feature vectors defining the shape and shading changes of AU i in our face space,</p><formula xml:id="formula_7">z ij = x T ij , g T ij T , j = 1, . . . , n i .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Classification in face space</head><p>Let the training set of AU i be</p><formula xml:id="formula_8">D i = { (z i1 , y i1 ) , . . . , (z ini , y ini ) ,<label>(6)</label></formula><formula xml:id="formula_9">(z ini+1 , y ini+1 ) , . . . , (z i ni+mi , y i ni+mi )},</formula><p>where y ij = 1 for j = 1, . . . , n i , indicating that AU i is present in the image, y ij = 0 for j = n i + 1, . . . , n i + m i , indicating that AU i is not present in the image, and m i is the number of sample images that do not have AU i active.</p><p>The training set above is also ordered as follows. The set</p><formula xml:id="formula_10">D i (a) = {(z i1 , y i1 ) , . . . , (z i nia , y i nia )}<label>(7)</label></formula><p>includes the n ia samples with AU i active at intensity a (that is the lowest intensity of activation of an AU), the set</p><formula xml:id="formula_11">D i (b) = { (z i nia+1 , y i nia+1 ) , . . . ,<label>(8)</label></formula><formula xml:id="formula_12">(z i nia+n ib , y i nia+n ib )}</formula><p>are the n ib samples with AU i active at intensity b (which is the second smallest intensity), the set</p><formula xml:id="formula_13">D i (c) = { (z i nia+n ib +1 , y i nia+n ib +1 ) , . . . ,<label>(9)</label></formula><p>(z i nia+n ib +nic , y i nia+n ib +nic )} are the n ic samples with AU i active at intensity c (which is the next intensity), and the set <ref type="bibr" target="#b9">(10)</ref> (z i nia+n ib +nic+n id , y i nia+n ib +nic+n id )} are the n id samples with AU i active at intensity d (which is the highest intensity we have in the databases we used), and n ia + n ib + n ic + n id = n i .</p><formula xml:id="formula_14">D i (d) = { (z i nia+n ib +nic+1 , y i nia+n ib +nic1 ) , . . . ,</formula><p>Recall that an AU can be active at five intensities, which are labeled a, b, c, d, and e <ref type="bibr" target="#b7">[8]</ref>. In the databases we will use in this paper, there are no examples with intensity e and, hence, we only consider the four other intensities.</p><p>The four training sets defined above are subsets of D i and are thus represented as different subclasses of the set of images with AU i active. This observation directly suggests the use of a subclass-based classifier. In particular, we use Kernel Subclass Discriminant Analysis (KSDA) <ref type="bibr" target="#b24">[25]</ref> to derive our algorithm. The reason we chose KSDA is because it can uncover complex non-linear classification boundaries by optimizing the kernel matrix and number of subclasses, i.e., while other kernel methods use crossvalidation on the training data to find an appropriate kernel mapping, KSDA optimizes a class discriminant criterion that is theoretically known to separate classes optimally wrt Bayes. This criterion is formally given by</p><formula xml:id="formula_15">Q i (ϕ i , h i1 , h i2 ) = Q i1 (ϕ i , h i1 , h i2 )Q i2 (ϕ i , h i1 , h i2 ), with Q i1 (ϕ i , h i1 , h i2</formula><p>) responsible for maximizing homoscedasticity (i.e., since the goal of the kernel map is to find a kernel space F where the data is linearly separable, this means that the subclasses will need to be linearly separable in F, which is the case when the class distributions share the same variance), and Q i2 (ϕ i , h i1 , h i2 ) maximizes the distance between all subclass means (i.e., which is used to find a Bayes classifier with smaller Bayes error 2 ).</p><p>Thus, the first component of the KSDA criterion presented above is given by,</p><formula xml:id="formula_16">Q i1 (ϕ i , h i1 , h i2 ) = 1 h i1 h i2 hi1 c=1 hi1+hi2 d=hi1 tr (Σ ϕi ic Σ ϕi id ) tr Σ ϕ 2 i ic tr Σ ϕ 2 i id ,<label>(11)</label></formula><p>where Σ ϕi il is the subclass covariance matrix (i.e., the covariance matrix of the samples in subclass l) in the kernel space defined by the mapping function ϕ i (.) : R e → F, h i1 is the number of subclasses representing AU i is present in the image, h i2 is the number of subclasses representing <ref type="bibr" target="#b1">2</ref> To see this recall that the Bayes classification boundary is given in a location of feature space where the probabilities of the two Normal distributions are identical (i.e., p(z|N (µ 1 , Σ 1 )) = p(z|N (µ 2 , Σ 2 )), where N (µ i , Σ i ) is a Normal distribution with mean µ i and covariance matrix Σ i . Separating the means of two Normal distributions decreases the value where this equality holds, i.e., the equality p(x|N (µ 1 , Σ 1 )) = p(x|N (µ 2 , Σ 2 )) is given at a probability values lower than before and, hence, the Bayes error is reduced.</p><p>AU i is not present in the image, and recall e = 3t + p(p − 1)/2 + 5 × p × o × s × r is the dimensionality of the feature vectors in the face space defined in Section 2.1.</p><p>The second component of the KSDA criterion is,</p><formula xml:id="formula_17">Q i2 (ϕ i , h i1 , h i2 ) = hi1 c=1 hi1+hi2 d=hi1+1 p ic p id µ ϕi ic − µ ϕi id 2 2 ,<label>(12)</label></formula><p>where p il = n l /n i is the prior of subclass l in class i (i.e., the class defining AU i), n l is the number of samples in subclass l, and µ ϕi il is the sample mean of subclass l in class i in the kernel space defined by the mapping function ϕ i (.).</p><p>Specifically, we define the mapping functions ϕ i (.) using the Radial Basis Function (RBF) kernel,</p><formula xml:id="formula_18">k(z ij1 , z ij2 ) = exp − z ij1 − z ij2 2 2 υ i ,<label>(13)</label></formula><p>where υ i is the variance of the RBF, and j 1 , j 2 = 1, . . . , n i + m i . Hence, our KSDA-based classifier is given by the solution to,</p><formula xml:id="formula_19">υ * i , h * i1 , h * i2 = arg max υi,hi1,hi2 Q i (υ i , h i1 , h i2 ).<label>(14)</label></formula><p>Solving for (14) yields the model for AU i, <ref type="figure" target="#fig_2">Figure 3</ref>. To do this, we first divide the training set D i into five subclasses. The first subclass (i.e., l = 1) includes the sample feature vectors that correspond to the images with AU i active at intensity a, that is, the D i (a) defined in <ref type="bibr" target="#b6">(7)</ref>. The second subclass (l = 2) includes the sample subset <ref type="bibr" target="#b7">(8)</ref>. Similarly, the third and fourth subclass (l = 2, 3) include the sample subsets (9) and (10), respectively. Finally, the five subclass (l = 5) includes the sample feature vectors corresponding to the images with AU i not active, i.e.,</p><formula xml:id="formula_20">D i (not active) = { (z i ni+1 , y i ni+1 ) , . . . ,<label>(15)</label></formula><p>(z i ni+mi , y i ni+mi )}.</p><p>Thus, initially, the number of subclasses to define AU i active/inactive is five (i.e., h i1 = 4 and h i2 = 1).</p><p>Optimizing <ref type="formula" target="#formula_0">(14)</ref> may yield additional subclasses. To see this, note that the derived approach optimizes the parameter of the kernel map υ i as well as the number of subclasses h i1 and h i2 . This means that our initial (five) subclasses can be further subdivided into additional subclasses. For example, when no kernel parameter υ i can map the non-linearly separable samples in D i (a) into a space where these are linearly separable from the other subsets, D i (a) is further divided into two subsets D i (a) = {D i (a 1 ), D i (a 2 )}. This division is simply given by a nearest-neighbor clustering. Formally, let the sample z i j+1 be the nearest-neighbor to z ij , then the division of D i (a) is readily given by,   <ref type="formula" target="#formula_0">(14)</ref> to further subdivide each subclass into additional subclasses to find the kernel mapping that (intrinsically) maps the data into a kernel space where the above Normal distributions can be separated linearly and are as far apart from each other as possible.</p><p>The same applies to D i (b), D i (c), D i (d) and D i (not active). Thus, optimizing <ref type="bibr" target="#b13">(14)</ref> can result in multiple subclasses to model the samples of each intensity of activation or non-activation of AU i, e.g., if subclass one (l = 1) defines the samples in D i (a) and we wish to divide this into two subclasses (and currently h i1 = 4), then the first new two subclasses will be used to define the samples in D i (a), with the fist subclass (l = 1) including the samples in D i (a 1 ) and the second subclass (l = 2) those in D i (a 2 ) (and h i1 will now be 5). Subsequent subclasses will define the samples in D i (b), D i (c), D i (d) and D i (not active) as defined above. Thus, the order of the samples as given in D i never changes with subclasses 1 through h i1 defining the sample feature vectors associated to the images with AU i active and subclasses h i1 + 1 through h i1 + h i2 those representing the images with AU i not active. This end result is illustrated using a hypothetical example in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Then, every test image I test can be readily classified as follows. First, its feature representation in face space z test is computed as described in Section 2.1. Second, this vector is projected into the kernel space obtained above. Let us call this z ϕ test . To determine if this image has AU i active, we find the nearest mean,</p><formula xml:id="formula_21">j * = arg min j z ϕi test −µ ϕi ij 2 , j = 1, . . . , h i1 +h i2 .<label>(17)</label></formula><p>If j * ≤ h i1 , then I test is labeled as having AU i active; otherwise, it is not. The classification result in <ref type="formula" target="#formula_0">(17)</ref>   <ref type="figure">I test is b, c and d, respectively.</ref> Of course, if j * &gt; h i1 , the images does not have AU i present and there is no intensity (or, one could say that the intensity is zero).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EmotioNet: Annotating a million face images in the wild</head><p>In the section to follow, we will present comparative quantitative results of the approach defined in Section 2. These results will show that the proposed algorithm can reliably recognize AUs and their intensities across databases. To our knowledge, this is the first published algorithm that can reliably recognize AUs and AU intensities across databases. This fact allows us to now define a fully automatic method to annotate AUs, AU intensities and emotion categories on a large number of images in "the wild" (i.e., images downloaded from the Internet). In this section we present the approach used to obtain and annotate this large database of facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Selecting images</head><p>We are interested in face images with associated emotive keywords. To this end, we selected all the words derived from the word "feeling" in WordNet <ref type="bibr" target="#b14">[15]</ref>.</p><p>WordNet includes synonyms (i.e., words that have the same or nearly the same meaning), hyponyms (i.e., subordinate nouns or nouns of more specific meaning, which defines a hierarchy of relationships), troponymys (i.e., verbs of more specific meaning, which defines a hierarchy of verbs), and entailments (i.e., deductions or implications that follow logically from or are implied by another meaningthese define additional relationships between verbs).</p><p>We used these noun and verb relationships in WordNet to identify words of emotive value starting at the root word "feeling." This resulted in a list of 457 concepts that were then used to search for face images in a variety of popular web search engines, i.e., we used the words in these concepts as search keywords. Note that each concept includes a list of synonyms, i.e., each concept is defined as a list of one or more words with a common meaning. Example words in our set are: affect, emotion, anger, choler, ire, fury, madness, irritation, frustration, creeps, love, timidity, adoration, loyalty, etc. A complete list is provided in the Supplementary Materials.</p><p>While we only searched for face images, occasionally non-face image were obtained. To eliminate these, we checked for the presence of faces in all downloaded images with the standard face detector of <ref type="bibr" target="#b20">[21]</ref>. If a face was not detected in an image by this algorithm, the image was eliminated. Visual inspection of the remaining images by the authors further identify a few additional images with no faces in them. These images were also eliminated. We also eliminated repeated and highly similar images. The end result was a dataset of about a million images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image annotation</head><p>To successfully automatically annotate AU and AU intensity in our set of a million face images in the wild, we used the following approach. First, we used three available databases with manually annotated AUs and AU intensities to train the classifiers defined in Section 2. These databases are: the shoulder pain database of <ref type="bibr" target="#b11">[12]</ref>, the Denver Intensity of Spontaneous Facial Action (DISFA) dataset of <ref type="bibr" target="#b13">[14]</ref>, and the database of compound facial expressions of emotion (CFEE) of <ref type="bibr" target="#b6">[7]</ref>. We used these databases because they provide a large number of samples with accurate annotations of AUs an AU intensities. Training with these three datasets allows our algorithm to learn to recognize AUs and AU intensities under a large number of image conditions (e.g., each database includes images at different resolutions, orientations and lighting conditions). These datasets also include a variety of samples in both genders and most ethnicities and races (especially the database of <ref type="bibr" target="#b6">[7]</ref>). The resulting trained system is then used to automatically annotate our one million images in the wild.</p><p>Images may also belong to one of the 23 basic or compound emotion categories defined in <ref type="bibr" target="#b6">[7]</ref>. To produce a facial expression of one of these emotion categories, a person will need to activate the unique pattern of AUs listed in <ref type="table" target="#tab_2">Table 1</ref>. Thus, annotating emotion category in an image is as simple as checking whether one of the unique AU activation patterns listed in each row in <ref type="table" target="#tab_2">Table 1</ref> is present in the image. For example, if an image has been annotated as having AUs 1, 2, 12 and 25 by our algorithm, we will also annotated it as expressing the emotion category happily surprised.</p><p>The images in our database can thus be searched by AU, AU intensity, basic and compound emotion category, and WordNet concept. Six examples are given in <ref type="figure">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We provide extensive evaluations of the proposed approach. Our evaluation of the derived algorithm is divided  into three sets of experiments. First, we present comparative results against the published literature using withindatabases classification. This is needed because, to our knowledge, only one paper <ref type="bibr" target="#b19">[20]</ref> has published results across databases. Second, we provide results across databases where we show that our ability to recognize AUs is comparable to that seen in within database recognition. And, third, we use the algorithm derived in this paper to automatically annotate a million facial expressions in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Within-database classification</head><p>We tested the algorithm derived in Section 2 on three standard databases: the extended Cohn-Kanade database (CK+) <ref type="bibr" target="#b10">[11]</ref>, the Denver Intensity of Spontaneous Facial Action (DISFA) dataset <ref type="bibr" target="#b13">[14]</ref>, and the shoulder pain database of <ref type="bibr" target="#b11">[12]</ref>.</p><p>In each database, we use 5-fold-cross validation to test how well the proposed algorithm performs. These databases include video sequences. Automatic recognition of AUs is done at each frame of the video sequence and the results compared with the provided ground-truth. To more accurately compare our results with state-of-the-art algorithms, we compute the F1 score, defined as, F1 score = 2 Precision×Recall Precision+Recall , where Precision (also called positive predictive value) is the fraction of the automatic annotations of AU i that are correctly recognized (i.e., number of correct recognitions of AU i / number of images with detected AU i), and Recall (also called sensitivity) is the number of correct recognitions of AU i over the actual number of images with AU i.</p><p>Comparative results on the recognition of AUs in these three databases are given in <ref type="figure" target="#fig_5">Figure 4</ref>. This figure shows comparative results with the following algorithms: the Hierarchical-Restricted Boltzmann Machine (HRBM) algorithm of <ref type="bibr" target="#b21">[22]</ref>, the nonrigid registration with Free-Form Deformations (FFD) algorithm of <ref type="bibr" target="#b9">[10]</ref>, and the l p -norm algorithm of <ref type="bibr" target="#b25">[26]</ref>. Comparative results on the shoulder database can be found in the Supplementary Materials. These were not included in this figure because the papers that report results on this database did not disclose F1 values. Comparative results based on receiver operating characteristic (ROC) curves are in the Supplementary Materials.</p><p>Next, we tested the accuracy of the proposed algorithm in estimating AU intensity. Here, we use three databases that include annotations of AU intensity: CK+ <ref type="bibr" target="#b10">[11]</ref>, DISFA <ref type="bibr" target="#b13">[14]</ref>, and CFEE <ref type="bibr" target="#b6">[7]</ref>. To compute the accuracy of AU intensity estimation, we code the four levels of AU intensity a-d as 1-4 and use 0 to represent inactivity of the AU, then compute Mean Error = n −1 n i=1 |Estimated AU intensity − Actual AU intensity|, n the number of test images. Additional results (e.g., successful detection rates, ROCs) as well as additional comparisons to state-of-the-art methods are provided in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Across-database classification</head><p>As seen in the previous section, the proposed algorithm yields results superior to the state-of-the-art. In the present section, we show that the algorithm defined above can also recognize AUs accurately across databases. This means that we train our algorithm using data from several databases and test it on a separate (independent) database. This is an extremely challenging task due to the large variability of filming conditions employed in each database as well as the high variability in the subject population.</p><p>Specifically, we used three of the above-defined databases -CFEE, DISFA and CK+ -and run a leave-onedatabase out test. This means that we use two of these databases for training and one database for testing. Since there are three ways of leaving one database out, we test all three options. We report each of these results and their average in <ref type="figure" target="#fig_6">Figure 5a</ref>. <ref type="figure" target="#fig_6">Figure 5b</ref> shows the average Mean Error of estimating the AU intensity using this same leaveone-database out approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">EmotioNet database</head><p>Finally, we provide an analysis of the used of the derived algorithm on our database of a million images of facial expressions described in Section 3. To estimate the accuracy of these automatic annotations, we proceeded as follows. First, the probability of correct annotation was obtained by computing the probability of the feature vector z ϕ test to belong to subclass j * as given by <ref type="bibr" target="#b16">(17)</ref>. Recall that j * specifies the subclass closest to z ϕ test . If this subclass models samples of AU i active, then the face in I test is assumed to have AU i active and the appropriate annotation is made. Now, note that since this subclass is defined as a Normal distribution, N (Σ ij * , µ ij * ), we can also compute the probability of z ϕ test belonging to it, i.e., p (z ϕ test |N (Σ ij * , µ ij * )). This allows us to sort the retrieved images as a function of their probability of being correctly labeled. Then, from this ordered set, we randomly selected 3, 000 images in the top 1/3 of the list, 3, 000 in the middle 1/3, and 3, 000 in the bottom 1/3.</p><p>Only the top 1/3 are listed as having AU i active, since these are the only images with a large probability p (z ϕ test |N (Σ ij * , µ ij * )). The number of true positives over the number of true plus false positives was then calculated in this set, yielding 80.9% in this group. Given the heterogeneity of the images in our database, this is considered a really good result. The other two groups (middle and bottom 1/3) also contain some instances of AU i but recognition there would only be 74.9% and 67.2%, respectively, which is clearly indicated by the low probability computed by our algorithm. These results thus provide a quantitative measure of reliability for the results retrieved using the system summarized in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented a novel computer vision algorithm for the recognition of AUs and AU intensities in images of faces. Our main contributions are: 1. Our algorithm can reliably recognize AUs and AU intensities across databases, i.e., while other methods defined in the literature only report recognition accuracies within databases, we demonstrate that the algorithm derived in this paper can be trained using several databases to successfully recognize AUs and AU intensities on an independent database of images not used to train our classifiers. 2. We use this derived algorithm to automatically construct and annotate a large database of images of facial expressions of emotion. Images are annotated with AUs, AU intensities and emotion categories. The result is a database of a million images that can be readily queried by AU, AU intensity, emotion category and/or emotive keyword, <ref type="figure">Figure 1</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) Shown here are the normalized face landmarkŝ s ij (j = 1, . . . , 66) used by the proposed algorithm. Fifteen of them correspond to anatomical landmarks (e.g., corners of the eyes, mouth and brows, tip of the nose, and chin). The others are pseudo-landmarks defined about the edge of the eyelids, mouth, brows, lips and jaw line as well as the midline of the nose going from the tip of the nose to the horizontal line given by the center of the two eyes. The number of pseudo-landmarks defining the contour of each facial component (e.g., brows) is constant. This guarantees equivalency of landmark position across people. (b) The Delaunay triangulation used by the algorithm derived in the present paper. The number of triangles in this configuration is 107. Also shown in the image are the angles of the vector θ a = (θ a1 , . . . , θ aqa ) T (with q a = 3), which define the angles of the triangles emanating from the normalized landmarkŝ ija .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>D i (a 1 ) = {(z i1 , y i1 ) , . . . , z i na/2 , y i na/2 }(16)D i (a 2 ) = { z i na/2+1 , y i na/2+1 , . . . , (z i na , y i na )}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>In the hypothetical model shown above, the sample images with AU 4 active are first divided into four subclasses, with each subclass including the samples of AU 4 at the same intensity of activation (a-d). Then, the derived KSDA-based approach uses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>also provides intensity recognition. If the samples represented by subclass l are a subset of those in D i (a), then the identified intensity is a. Similarly, if the samples of subclass l are a subset of those in D i (b), D i (c) or D i (d), then the intensity of AU i in the test image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. The first two examples in this figure show samples returned by our system when retrieving images classified as "happy" or "fearful." The two examples in the middle of the figure show sample images obtained when the query is AU 4 or 6. The final two examples in this figure illustrate the use of keyword searches using WordNet words, specifically, anxiety and disapproval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Cross-validation results within each database for the method derived in this paper and those in the literature. Results correspond to (a) CK+, (b) DISFA, and (c) shoulder pain databases. (d) Mean Error of intensity estimation of 16 AUs in three databases using our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>(a). Leave-one-database out experiments. In these experiments we used three databases (CFEE, DISFA, and CK+). Two of the databases are used for training, and the third for testing, The color of each bar indicates the database that was used for testing. Also shown are the average results of these three experiments. (b) Average intensity estimation across databases of the three possible leave-one out experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Listed here are the prototypical AUs observed in each basic and compound emotion category.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Expert coders typically use video rather than still images. Coding in stills is generally done by comparing the images of an expressive face with the neutral face of the same individual.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. Supported by NIH grants R01-EY-020834 and R01-DC-014498 and a Google Faculty Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multispectral skin color modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Angelopoulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Molana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">635</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3515" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using structural and semantic voxel-wise encoding models to investigate face representation in human cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdel-Ghaffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="422" to="422" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by twodimensional visual cortical filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1160" to="1169" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video and image based emotion recognition challenges in the wild: Emotiw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th ACM Intl. Conf. on Multimodal Interaction</title>
		<meeting>of the 17th ACM Intl. Conf. on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICMI 2015</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1454" to="1462" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A dynamic texturebased approach to recognition of facial actions and their temporal models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions onPattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference on</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Painful data: The unbc-mcmaster shoulder pain expression archive database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A model of the perception of facial expressions of emotion by humans: Research overview and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1589" to="1608" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emotion perception in emotionless face images suggests a norm-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The utility of surface reflectance for the recognition of upright and inverted faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nederhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action unit detection with segment-based svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Capturing global semantic relationships for facial action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kuiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Von Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kernel optimization in discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">C</forename><surname>Hamsici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="631" to="638" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A lp-norm mtmkl framework for simultaneous detection of multiple facial action units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2207" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
