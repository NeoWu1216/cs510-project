<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Detection from Video Tubelets with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
							<email>kkang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wlouyang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Object Detection from Video Tubelets with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Convolution Neural Networks (CNNs) have shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. For object detection, particularly in still images, the performance has been significantly increased last year thanks to powerful deep networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN features (R-CNN)). The lately introduced ImageNet [6] task on object detection from video (VID) brings the object detection task into the video domain, in which objects' locations at each frame are required to be annotated with bounding boxes. In this work, we introduce a complete framework for the VID task based on still-image object detection and general object tracking. Their relations and contributions in the VID task are thoroughly studied and evaluated. In addition, a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task. Code is available at https://github.com/ myfavouritekk/vdetlib.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has been widely applied to various computer vision tasks such as image classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, semantic segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>, human pose estimation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4]</ref>, etc. Over the past few years, the performance of object detection in ImageNet and PASCAL VOC has been increased by a significant margin with the success of deep Convolutional Neural Networks (CNN). State-of-the-art methods for object detection train CNNs to classify region proposals into background or one of the object classes. However, these methods focus on detecting objects in still images. The lately introduced ImageNet challenge on object detection from video brings up a new question on how to solve the object detection problem for videos effectively and robustly. At each frame of a video, the algorithm is required to annotate bounding boxes and confidence scores on objects of  Tracking is able to relate boxes of the same object. However, due to occlusions, appearance changes and pose variations, the tracked boxes may drift to non-target objects. Object detectors should be incorporated into tracking algorithm to constantly start new tracks when drifting occurs. each class. Although there have been methods on detecting objects in videos, they mainly focus on detecting one specific class of objects, such as pedestrians <ref type="bibr" target="#b30">[31]</ref>, cars <ref type="bibr" target="#b18">[19]</ref>, or humans with actions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref>. The ImageNet challenge defines a new problem on detecting general objects in videos, which is worth studying. Similar to object detection in still images being able to assist tasks including image classification <ref type="bibr" target="#b28">[29]</ref>, object segmentation <ref type="bibr" target="#b4">[5]</ref>, and image captioning <ref type="bibr" target="#b7">[8]</ref>, accurately detecting objects in videos could possibly boost the performance of video classification, video captioning and related surveillance applications. By locating objects in the videos, the semantic meaning of a video could also be described more clearly, which results in more robust performance for video-based tasks.</p><p>Existing methods for general object detection cannot be applied to solve this problem effectively. Their performance may suffer from large appearance changes of objects in videos. For instance in <ref type="figure" target="#fig_1">Figure 1 (a)</ref>, if a cat faces the camera at first and then turns back. Its back image cannot be effectively recognized as a cat because it contains little texture information and is not likely to be included in training samples. The correct recognition result needs to be inferred from information in previous and future frames, because the appearance of an object in video is highly correlated. Since an object's location might change in the video, the location correspondences across the video should be recovered such that the correlated image patches could be well aligned into trajectories for extracting the temporal information. Besides, temporal consistency of recognition results should be regularized <ref type="figure" target="#fig_1">(Figure 1 (a)</ref>). The detection scores of a bounding-box tubelet representing an object should not change dramatically across the video.</p><p>Such requirements motivate us to incorporate object tracking into our detection framework. Deep CNNs have shown impressive performance on object tracking <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22]</ref>, which outperform previous methods by a large margin. The large number of tracking-by-detection methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26]</ref> for multi-pedestrian tracking have shown that temporal information could be utilized to regularize the detection results. However, directly utilizing object tracking cannot effectively solve the VID problem either <ref type="figure" target="#fig_1">(Figure 1 (b)</ref>). In our experiments, we have noticed that directly using still-image object detectors on object tracking results has only 37.4% mean average precision (mean AP) compared to 45.3% on object proposals. The performance difference results from detectors' sensitivity to location changes and the box mismatch between tracks and object proposals. To solve this problem, we proposed a tubelet box perturbation and maxpooling process to increase the performance from 37.4% to 45.2%, which is comparable to the performance of image object proposal with only 1/38 the number of boxes.</p><p>In this work, we propose a multi-stage framework based on deep CNN detection and tracking for object detection in videos. The framework consists of two main modules: 1) a tubelet proposal module that combines object detection and object tracking for tubelet object proposal; 2) a tubelet classification and re-scoring module that performs spatial max-pooling for robust box scoring and temporal convolution for incorporating temporal consistency. Object detection and tracking work closely in our framework. On one hand, object detection produces high-confidence anchors to initiate tracking and reduces tracking failure by spatial maxpooling. On the other hand, tracking also generates new proposals for object detection and the tracked boxes act as anchors to aggregate existing detections.</p><p>The contribution of this paper is three-fold. 1) A complete multi-stage framework is proposed for object detection in videos. 2) The relation between still-image object detection and object tracking, and their influences on object detection from video are studied in details. 3) A special temporal convolutional neural network is proposed to incorporate temporal information into object detection from video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>State-of-the-art methods for detecting objects of general classes are mainly based on deep CNNs. Girshick et al. <ref type="bibr" target="#b9">[10]</ref> proposed a multi-stage pipeline called Regions with Convolutional Neural Networks (R-CNN) for training deep CNN to classify region proposals for object detection. It decomposes the detection problem into several stages including bounding-box proposal, CNN pre-training, CNN fine-tuning, SVM training, and bounding box regression. Such framework has shown good performance and was adopted by other methods. Szegedy et al. <ref type="bibr" target="#b29">[30]</ref> proposed the GoogLeNet with a 22-layer structure and "inception" modules to replace the CNN in the R-CNN, which won the ILSVRC 2014 object detection task. Ouyang et al. <ref type="bibr" target="#b22">[23]</ref> proposed a deformation constrained pooling layer and a box pre-training strategy, which achieves an accuracy of 50.3% on the ILSVRC 2014 test set. To accelerate the training of the R-CNN pipeline, Fast R-CNN <ref type="bibr" target="#b8">[9]</ref> is proposed, where each image patch is no longer wrapped to a fixed size before being fed into the CNN. Instead, the corresponding features are cropped from the output feature map of the last convolutional layer. In the Faster R-CNN pipeline <ref type="bibr" target="#b27">[28]</ref>, the bounding box proposals were generated by a Region Proposal Network (RPN), and the overall framework can thus be trained in an end-to-end manner. However, these pipelines are for object detection in still images. When these methods are applied to videos, they might miss some positive samples because the objects might not be of their best poses in each frame of the videos.</p><p>Object localization and co-localization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>, which have mainly focused on the YouTube Object Dataset (YTO) <ref type="bibr" target="#b26">[27]</ref>, seems to be a similar topic to the VID task. However, there are crucial differences between the two problems. 1) Goal: The (co)locolization problem assumes that each video contains only one known (weakly supervised setting) or unknown (unsupervised setting) class and only requires localizing one of the objects in each test frame. In VID, however, each video frame contains unknown numbers of objects instances and classes. The VID task is closer to real-world applications. 2) Metrics: Localization metric (CorLoc <ref type="bibr" target="#b6">[7]</ref>) is usually used for evaluation in (co)locolization, while mean average precision (mean AP) is used for evaluation on the VID task. The mean AP is more challenging to evaluate overall performances on different classes and thresholds. With these differences, the VID task is more difficult and closer to real-world scenarios. The previous works on object (co)localization in videos cannot be directly applied to VID.</p><p>There have also been methods on action localization. At each frame of human action video, the system is required to annotate a bounding box for the human action of interest. The methods that are based on action proposals are related to our work. Yu and Yuang et al. <ref type="bibr" target="#b37">[38]</ref> proposed to generate action proposals by calculating actionness scores and solving a maximum set coverage problem. Jain et al. <ref type="bibr" target="#b12">[13]</ref> adopted the Selective Search strategy on super-voxels to generate tubulet proposals and proposed new features to differentiate human actions from background movements. In <ref type="bibr" target="#b10">[11]</ref>, candidate regions are fed into two CNNs to learn feature representations, which is followed by a SVM to make prediction on actions using appearance and motion cues. The regions are then linked across frames based on the action predictions and their spatial overlap.</p><p>Object tracking has been studied for decades <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref>. Recently, deep CNNs have been used for object tracking and achieved impressive tracking accuracy <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref>. Wang et al. <ref type="bibr" target="#b35">[36]</ref> proposed to create an object-specific tracker by online selecting the most influential features from an ImageNet pre-trained CNN, which outperforms state-ofthe-art trackers by a large margin. Nam et al. <ref type="bibr" target="#b21">[22]</ref> trained a multi-domain CNN for learning generic representations for tracking objects. When tracking a new target, a new network is created by combining the shared layers in the pretrained CNN with a new binary classification layer, which is online updated. However, even for the CNN-based trackers, they might still drift in long-term tracking because they mostly utilize the object appearance information within the video without semantic understanding on its class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we will introduce the task setting for object detection from video and give a detailed description of our framework design. The general framework of video object detection system is shown in <ref type="figure" target="#fig_3">Figure 2</ref>. The framework has two main modules: 1) a spatio-temporal tubelet proposal module and 2) a tubelet classification and re-scoring module. The two major components will be elaborated in Section 3.2 and Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task setting</head><p>The ImageNet object detection from video (VID) task is similar to image object detection task (DET) in still images. There are 30 classes, which is a subset of 200 classes of the DET task. All classes are fully labeled for each video clip. For each video clip, algorithms need to produce a set of annotations (f i , c i , s i , b i ) of frame number f i , class label c i , confidence scores s i and bounding boxes b i . The evaluation protocol for the VID task is the same as DET task. Therefore, we use the conventional mean average precision (mean AP) on all classes as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatio-temporal tubelet proposal</head><p>Objects in videos show temporal and spatial consistency. The same object in adjacent frames has similar appearances and locations. Using either existing object detection methods or object tracking methods alone cannot effectively solve the VID problem. On one hand, a straightforward application of image object detectors is to treat videos as a collection of images and detect objects on each image individually. This strategy focuses only on appearance similarities and ignores the temporal consistency. Thus the detection scores on consecutive frames usually have large fluctuations <ref type="figure" target="#fig_1">(Figure 1 (a)</ref>). On the other hand, generic object tracking methods track objects from a starting frame and usually online update detectors using samples from currently tracked bounding boxes. The detectors in tracking methods mainly focus on samples within the video and usually tends to drift due to large object appearance changes <ref type="figure" target="#fig_1">(Figure 1 (b)</ref>).</p><p>The spatio-temporal tubelet proposal module in our framework combines the still-image object detection and generic object tracking together. It has the discriminative ability from object detectors and the temporal consistency from object trackers. The tubelet proposal module has 3 major steps: 1) image object proposal, 2) object proposal scoring and 3) high-confidence object tracking.</p><p>Step 1. Image object proposal. The general object proposals are generated by the Selective Search (SS) algorithm <ref type="bibr" target="#b33">[34]</ref>. The SS method outputs around 2, 000 object proposals on each video frame. The majority object proposals are negative samples and may not contain objects. We use the ImageNet pre-trained AlexNet <ref type="bibr" target="#b16">[17]</ref> model provided by R-CNN to remove easy negative object proposals where all detection scores of ImageNet detection 200 classes are below a certain threshold. In our experiments, we use −1.1 as threshold and around 6.1% of the region proposals are kept, while the recall at this threshold is 80.49%. The image object proposal process is illustrated in <ref type="figure" target="#fig_3">Figure 2</ref> (a).</p><p>Step 2. Object proposal scoring. Since the VID 30 classes are a subset of DET 200 classes, the detection models trained for the DET task can be used directly for VID classes. Our detector is a GoogLeNet <ref type="bibr" target="#b29">[30]</ref> pre-trained on ImageNet image classification data, and fine-tuned for the DET task. Similar to the R-CNN, for each DET class, an SVM is trained with hard negative mining using the "pool5" features extracted from the model. The 30 SVM models corresponding to VID classes are used here to classify object proposals into background or one of the object classes. The higher the SVM score, the higher the confidence that the box contains an object of that class <ref type="figure" target="#fig_3">(Figure 2 (b)</ref>).</p><p>Step 3. High-confidence proposal tracking. For each object class, we track high-confidence detection proposals bidirectionally in the video clip. The tracker we choose for this task is from <ref type="bibr" target="#b35">[36]</ref>, which in our experiments shows more robust performance to object pose and scale changes. The starting detections of tracking are called "anchors", which are chosen from the most confident box proposals from Step 2. Starting from an anchor, we track backward to the first frame, and track forward to the last frame. Two tracklets  are then concatenated to produce a complete track. As the tracking moves away from the anchors, the tracker may drift to background and other objects, or may not keep up with the scale and pose changes of the target object. Therefore, we early stop the tracking when the tracking confidence is below a threshold (probability of 0.1 in our experiments) to reduce false positive tracklets. After getting a track, a new anchor is selected from the rest detections. Usually, high-confidence detections tend to cluster both spatially and temporally, therefore directly tracking the next most confident detection tends to result in tracklets with large mutual overlaps on the same object. To reduce the redundancy and cover as many objects as possible, we perform a suppression process similar to NMS. Detections from Step 2 that have overlaps with the existing tracks beyond a certain threshold (IOU 0.3 in our experiment) will not be chosen as new anchors. The tracking-suppression process performs iteratively until confidence values of all remaining detections are lower than a threshold (SVM score below 0 in our setting). For each video clip, such tracking process is performed for each of the 30 VID classes.</p><p>With the above three major steps, we can obtain tracks starting from high-confidence anchors for each classes. The produced tracks are tubelet proposals for tubelet classification of later part of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Tubelet classification and rescoring</head><p>After the tubelet proposal module, for each class, we have tubelets with high-confidence anchor detections. A naive approach is to classify each bounding box on the tubelets using the same method as Step 2 before. In our experiment, this baseline approach has only modest performance compared to direct still-image object detection R-CNN. The reason for that is 4-fold.</p><p>1) The overall number of bounding box proposals from tubelets is significantly smaller than those from Selective Search, which might miss some objects and result in lower recall on the test set.</p><p>2) The detector trained for object detection in still images is usually sensitive to small location changes <ref type="figure" target="#fig_3">(Figure 2 (d)</ref>) and a tracked boxes may not have a reasonable detection score even if it has large overlap with the object.</p><p>3) In the tracking process, we performed proposal suppression to reduce redundant tubelets. The tubelets are therefore more sparse compared than image proposals. This suppression may be conflict with conventional NMS. Because in conventional NMS, even a positive box has very low confidences, as long as other boxes with large mutual overlaps have higher confidence, it will be suppressed during NMS and will not affect the overall average precision. The consequence of early suppression is that some lowconfidence positive boxes do not have overlaps with high confidence detections, thus are not suppressed and become false negatives.</p><p>4) The detection score along the tubelet has large variations even on ground truth tubelets <ref type="figure" target="#fig_1">(Figure 1 (a)</ref>). The temporal information should be incorporated to obtain consistent detection scores.</p><p>To handle these problems in tubelet classification, we designed the following steps to augment proposals, increase spatial detection robustness and incorporate temporal consistency into the detection scores.</p><p>Step 4. Tubelet box perturbation and max-pooling. The tubelet box perturbation and max-pooling process is to replace tubelet boxs with boxes of higher confidence. There are two kinds of perturbations in our framework. The first method is to generate new boxes around each tubelet box on each frame by randomly perturbing the boundaries of the tubelet box. That is, we randomly sample coordinates for upper-left and bottom-right corners of a tubelet box. The random offsets for the corners are generated from two uniform distributions:</p><formula xml:id="formula_0">∆x ∼ U (−r · w, r · w), (1) ∆y ∼ U (−r · h, r · h),<label>(2)</label></formula><p>where U is uniform distribution, w and h are width and height of the tubelet box, and r is the sampling ratio hyperparameter. Higher sampling ratio means less confidence on the original box, while lower sampling ratio means more confidence on the tubelet box. We evaluated performances of different sampling configurations (Section 4). The second perturbation method is to replace each tubelet box with original object detections that have overlaps with the tubelet box beyond a threshold. This process is to simulate the conventional NMS process. If the tubelet box is positive box with a low detection score, this process can help bring back some positive boxes to suppress this box. The higher the overlap threshold, the more confidence on the tubelet box. We find this method really effective and different overlap threshold are evaluated in Section 4.</p><p>After the box perturbation step, all augmented boxes and the original tubelet boxes are scored using the same detector in Step 2. For each tubelet box, only the augmented box with the maximum detection score is kept and used to replace the original tubelet box. The max-pooling process is to increase the spatial robustness of detector and utlize the original object detections around the tubelets.</p><p>Step 5. Temporal convolution and re-scoring. Even with the spatial max-pooling process, the detection scores along the same track might still have large variations. This naturally results in performance loss. For example, if tubelet boxes on ajacent frames all have high detection scores, it is very likely that the tublet box on this frame also has high confidence on the same object. The still-image object detection framework does not take temporal consistency into consideration.</p><p>In our framework, we propose a Temporal Convolutional Network (TCN) that uses 1-D serial features including detection scores, tracking scores, anchor offsets and generates temporally dense prediction on every tubelet box.</p><p>The structure of the proposed TCN is shown in <ref type="figure" target="#fig_4">Figure 3</ref>. It is a 4-layer 1-D fully convolution network that outputs temporally dense prediction scores on every tubelet box. For each class, we train a class-specific TCN using the tubelet features as input. The inputs are time series including detection scores, tracking scores and anchor offsets. The output values are probablities whether each tubelet box contains objects of the class. The supervision labels are 1 if the overlap with ground truth is above 0.5, and 0 otherwise.</p><p>The temporal convolution learns to generate classification prediction based on the temporal features within the receptive field. The dense 1-D labels provide richer supervision than single tubelet-level labels. During testing, the continuous classification score instead of the binary decision values. We found that this re-scoring process has consistent improvement on tubelet detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>ImageNet VID. We utilize the ImageNet object detection from video (VID) task dataset to evaluate the overall pipeline and individual component of the proposed framework. The framework is evaluated on the initial release of VID dataset, which consists of three distinct splits. 1) The training set contains 1952 fully-labeled video snippets ranging from 6 frames to 5213 frames per snippet. 2) The validation set contains 281 fully-labeled video snippets ranging from 11 frames to 2898 frame per snippet. 3) The test set contains 458 snippets and the ground truth annotation are not publicly available yet.</p><p>We report all results on the validation set as a common convention for object detection task. YTO dataset. In addition to the ImageNet VID dataset, we also evaluate our proposed framework on the YTO dataset for the object localization task. The YTO dataset contains 10 object classes, which are a subset of the ImageNet VID dataset. Different from the VID dataset which contains full annotations on all video frames, the YTO dataset is weakly annotated, i.e. each video is only ensured to contain one object of the corresponding class, and only very few frames are annotated for each video. The weak annotation makes it infeasible to train the our models on the YTO dataset. However, since the YTO classes are a subset of the VID dataset classes, we can directly apply the trained models on  the YTO dataset for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Parameter settings</head><p>Image object proposal. For image object proposal, we used the "fast" mode in Selective Search <ref type="bibr" target="#b33">[34]</ref>, and resized all input images to width of 500 pixels and mapped the generated box proposals back to original image coordinates. The R-CNN provided AlexNet model is used to remove easy negative box proposals. We used threshold of −1.1 and remove boxes whose detections scores of all DET 200 are below the threshold. This process kept 6.1% of all the proposals (about 96 boxes per image). Tracking. The early stopping tracking confidence is set to probability 0.1. Therefore, if the tracking confidence is below 0.1, the tracklet is terminated. The minimum detection score for a new tracking anchor is 0. If no more detection beyond this threshold, the whole tracking process ends for this class. The track-detection suppression overlap is set to 0.3. For each video snippet, we chose at most 20 anchors for tracking for each class. Tubelet perturbation. For tubelet box perturbation, we denote R(n, r) for random perturbation with perturbation ratio r and n samples per tubelet box, and O(t) for adding original proposals whose overlaps with the tubelet boxes  beyond threshold t. Different combinations of perturbation ratios and sampling numbers are evaluated as shown in <ref type="table">Table 1</ref>   <ref type="table">Table 2</ref>). O(0.5) is chosen for the framework.</p><p>Temporal convolutional network The TCN in our experiments has 4 convolutional layers, the network structure is shown in <ref type="table" target="#tab_2">Table 3</ref>. The network initialization parameter and optimization parameter such as learning rate are manually adjusted on one class and remained unchanged for all 30 classes.</p><p>The network raw detection score, tracking score and absolute anchor offsets (which is normalized by length of the track) are used as input feature for the TCN, without other preprocessing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Results on VID</head><p>Tubelet proposal and baseline performance. After obtaining the tubelet proposals, a straight-forward baseline approach for tubelet scoring is to directly evaluate tubelet boxes with the object detector for still images. The performance of this approach is 37.4% in mean AP. In comparison, the still-image object detection result is 45.3% in mean AP.</p><p>Tubelet perturbation and spatial max-pooling. The performances of different tubelect box perturbation and maxpooling schemes are shown in <ref type="table">Table 1 and Table 2</ref>. From the tables, we can see that in most settings, both the random sampling and adding original proposals improves over the baseline approach. Also, if the perturbation is too large or too small, the performance gain is small. The reasons are: 1) large perturbation may result in replacing the tubelet box with a box too far away from it, and on the other hand, 2) small perturbation may obtain redundant boxes that reduce the effect of spatial max-pooling.</p><p>The best performance of random sampling scheme is 41.7% of R(20, 0.2). For replacing with original propos-Method a ir p la n e a n te lo p e b e a r b ic y c le b ir d b u s c a r c a tt le d o g d o m e s ti c c a t e le p h a n t f o x g ia n t p a n  <ref type="table">Table 4</ref>. Performances of different methods and experimental settings. "s#" stands for different settings. R(n, r) represents random sampling pertubation scheme with perturbation ration of r and n samples per tubelet box. O(t) represents adding original proposals with overlap larger than threshold r.</p><p>als, the best result is 44.5% of O(0.5). It is worth noticing that the tubelet perturbation and max-pooling scheme does not increase the overall boxes of tubelet proposals but replaces original tubelet boxes with nearby ones with the highest confidences.</p><p>We also investigated the complementary properties of the two perturbation schemes. The perturbed tubelets from the best settings of the both schemes (41.7% model from R(20, 0.2) and 44.5% model from O(0.5)) are combined for evaluation. The direct combination doubles the number of tubelets, and the performance increases from 41.7% and 44.5% to 45.2%, which is comparable to still-image object detection result with much fewer proposals on each image (around 1 : 38).</p><p>Temporal convolution. Using the tubelets proposals, we trained a TCN for each of the 30 classes for re-scoring. We use the continuous values of Sigmoid foreground scores as the confidence values for the tubelet boxes in evaluation.</p><p>For the baseline 37.4% model, the performance increases to 39.4% by 2%. On the best single perturbation scheme proposal (O(0.5)), the performance increases from 44.5% to 46.4%. For combined tubelet proposals from two perturbation schemes, a 45.2% model with R(20, 0.2) and O(0.5) increases the performance to 47.4, while a 45.1 model with R(20, 0.1) and O(0.5) increases to 47.5%.</p><p>From the results we can see that our temporal convolution network using detection scores, tracking scores and anchor offsets provides consistent performance improvement (around 2 percents in mean AP) on the tubelet proposals.</p><p>Overall, the best performance on tubelet proposals by our proposed method is 47.5%, 2.2 percents increase from still-image object detection framework with only 1/38 the number of boxes for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative Results on VID</head><p>Tubelet proposal. The tubelet proposal results are shown in <ref type="figure">Figure 5</ref>. <ref type="figure">Figure 5 (a)</ref> shows the positive tubelets obtained from tubelet proposal module, and <ref type="figure">Figure 5</ref> (b) shows the negative samples.</p><p>From the figure we can see, positive samples usually aggregate around objects while still appear sparse compared to dense proposals from Selective Search. The sparsity comes from the track-proposal suppression process performed in tracking to ensure the tubelet covers as many objects as possible.</p><p>With the frame index increases, some tracks will disappear while others may be added, which results from the early stopping for low tracking confidence and new anchor selections.</p><p>As for the negative samples, the tubelet are much fewer (in fact, some videos do not have tubelet proposals for some classes) and isolated. This is because we only start tracking on high-confident anchors. This largely reduces the number of false positives and significantly save inference time in later steps in the framework.</p><p>Temporal convolution. In <ref type="figure" target="#fig_8">Figure 6</ref>, we show some examples of results of temporal convolution. Each plot shows the tubelet scores (detection score, tracking score and anchor offsets) and the output probability scores of the TCN network.</p><p>The detection score shows significant variations across frames. A tubelet box may have significantly low detection score even if adjacent frames have high detection values. However, after temporal convolution, the output probability curve are much more temporally consistent. Compare to detection scores, the probability output of our network conforms better with the ground truth overlaps, which shows  <ref type="figure">Figure 5</ref>. Qualitative results of tubelet proposals. The first three rows are positive bounding boxes of tubelet proposals generated by our framework. The proposal boxes usually aggregate on ground truth objects while keep sparsely allocated to cover as many objects as possible. The last two rows shows some failure cases of tubelet proposals. The first kind of failure cases are false detections (for example, mis-detect zebras as horses). The second kind of failure cases are tracking failures. Tracker drifts to background objects due to large scale changes of target objects while the mis-tracked targets are not confident enough to start new tracking processes. the effectiveness of our re-scoring module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation on YouTube-Objects (YTO) dataset</head><p>In order to show the effectiveness of our proposed framework, we applied the models trained on the VID task directly on the YTO dataset and compared with the state-ofthe-art works in <ref type="table" target="#tab_4">Table 5</ref>. The localization metric CorLoc <ref type="bibr" target="#b6">[7]</ref> is used for evaluation as a convention on YTO. From the table, we can see that our proposed framework outperforms by a large margin. This is because the Ima-geNet datasets (CLS, DET and VID) provide rich supervision for feature learning, and the trained networks have good generalization capability on other datasets. The full framework has around 2% improvement over the baseline approach on the YTO dataset, which is consistent with the results on VID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we propose a complete multi-stage pipeline for object detection in videos. The framework efficiently combines still-image object detection with generic object tracking for tubelet proposal. Their relationship and contributions are extensively investigated and evaluated. Based on tubelet proposals, different perturbation and scoring schemes are evaluated and analyzed. A novel temporal convolutional network is proposed to incorporate temporal consistency and shows consistent performance improvement over still-image detections. Based on this work, a more advanced tubelet-based framework is further developed which won the ILSVRC2015 ImageNet VID challenge with provided data <ref type="bibr" target="#b14">[15]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Challenges in object detection from video. Red boxes are ground truth annotations. (a) Still-image object detection methods have large temporal fluctuations across frames even on ground truth bounding boxes. The fluctuations may result from motion blur, video defocus, part occlusion and bad pose. Information of boxes of the same object on adjacent frames need to be utilized for object detection in video. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Video object detection framework. The proposed video object detection framework contains two major components. 1) The tubelet proposal component: (a), (b) and (c). 2) The tubelet classification and re-scoring component: (d) and (e). (a) In object proposal, class-independent proposals are generated on every frame. (b) An object scoring model outputs a detection score on every proposal. The darker the color, the higher the score. (c) High-confidence proposals are chosen as anchors for bidirectional tracking. (d) Tubelet boxes are perturbed by sampling boxes around them or replacing with original proposals. The spatial max-pooling on keeps the boxes with the highest detection scores for each tubelet box. (e) The time series of detection scores (Red), tracking score (Yellow) and anchor offsets (Green) are the inputs of the proposed temporal convolutional network. The purple line is the output of our network and blue line is the ground truth overlap value (supervision).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Temporal convolutional network (TCN). The TCN is a 1-D convolutional network that operates on tubelet proposals. The inputs are time series including detection scores, tracking scores and anchor offsets. The output values are probablities that whether each tubelet box has overlap with ground truth above 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Performances of different max-pooling schemes. The blue and yellow lines are random sampling 10 and 20 samples per box with different perturbation ratios (bottom). The green line shows the performances of different overlap thresholds (top) for adding orginal proposals. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>and Figure 4. R(20, 0.1) and R(20, 0.2) are chosen for later components. For O(t) schemes, O(0.1) to O(0.9) are evaluated (Figure 4 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results of temporal convolution. The time series of detection scores, tracking scores and absolute values of anchor offsets are the inputs for TCN. The blue line are overlaps with ground truth annotations and purple lines are the output of TCN. The detection scores have large temporal variations while the TCN output has temporal consistency and comply better to ground truth overlaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Temporal convolutional network (TCN) structure.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Localization performances on the YTO dataset</figDesc><table>Method 
aero 
bird 
boat 
car 
cat 
cow 
dog 
horse mbike train 
Avg. 

Prest et al. [27] 
51.7 
17.5 
34.4 
34.7 
22.3 
17.9 
13.5 
26.7 
41.2 
25.0 
28.5 
Joulin et al. [14] 
25.1 
31.2 
27.8 
38.5 
41.2 
28.4 
33.9 
35.6 
23.1 
25.0 
31.0 
Kwak et al. [18] 
56.5 
66.4 
58.0 
76.8 
39.9 
69.3 
50.4 
56.3 
53.0 
31.0 
55.7 
Baseline 
92.4 
68.4 
85.4 
75.8 
77.3 
18.6 
87.2 
87.3 
84.2 
72.8 
74.9 
Ours (TCN:s3) 
94.1 
69.7 
88.2 
79.3 
76.6 
18.6 
89.6 
89.0 
87.3 
75.3 
76.8 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partially supported by SenseTime Group Limited, and the General Research Fund sponsored by the Research Grants Council of Hong Kong (Project Nos. CUHK14206114, CUHK14205615, CUHK14203015, CUHK14207814).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">People-tracking-bydetection and people-detection-by-tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discretecontinuous optimization for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database. CVPR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Localizing Objects While Learning Their Appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Finding action tubes. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-store tracker (muster): a cognitive psychology inspired approach to object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient Image and Video Co-localization with Frank-Wolfe Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>T-Cnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02532</idno>
		<title level="m">Tubelets with convolutional neural networks for object detection from videos</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fully convolutional neural networks for crowd segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4464</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks. NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised Object Discovery and Tracking in Video Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integrating context and occlusion for car detection by hierarchical and-or model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reliable patch trackers: Robust visual tracking by exploiting reliable patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07945</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepID-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast Object Segmentation in Unconstrained Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Occlusion geodesics for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stct: Sequentially training convolutional networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huchuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Visual tracking with fully convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
