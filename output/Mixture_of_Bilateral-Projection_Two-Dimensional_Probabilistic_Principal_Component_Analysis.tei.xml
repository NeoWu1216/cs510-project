<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixture of Bilateral-Projection Two-dimensional Probabilistic Principal Component Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fujiao</forename><surname>Ju</surname></persName>
							<email>jufujiao2013@emails.bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Metropolitan Transportation</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Sun</surname></persName>
							<email>yfsun@bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Metropolitan Transportation</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Gao</surname></persName>
							<email>junbin.gao@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">Discipline of Business Analytics</orgName>
								<orgName type="institution" key="instit1">The University of Sydney Business School</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simeng</forename><surname>Liu</surname></persName>
							<email>smliu@emails.bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Metropolitan Transportation</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongli</forename><surname>Hu</surname></persName>
							<email>huyongli@bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Metropolitan Transportation</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer Science</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mixture of Bilateral-Projection Two-dimensional Probabilistic Principal Component Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The probabilistic principal component analysis (PPCA) is built upon a global linear mapping, with which it is insufficient to model complex data variation. This paper proposes a mixture of bilateral-projection probabilistic principal component analysis model (mixB2DPPCA) on 2D data. With multi-components in the mixture, this model can be seen as a 'soft' cluster algorithm and has capability of modeling data with complex structures. A Bayesian inference scheme has been proposed based on the variational EM (Expectation-Maximization) approach for learning model parameters.</head><p>Experiments on some publicly available databases show that the performance of mixB2DPPCA has been largely improved, resulting in more accurate reconstruction errors and recognition rates than the existing PCA-based algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Principle Component Analysis (PCA) <ref type="bibr" target="#b2">[3]</ref> is one of popular dimensionality reduction methods widely used in image analysis <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>, pattern recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> and machine learning <ref type="bibr" target="#b12">[13]</ref> for data analysis. It can be derived under algebraic framework. However, algebraic models don't have flexibility of providing confidence information of the model when dealing with noisy data. This is due to the absence of an associated probability density or generative model in algebraic framework.</p><p>To compensate the algebraic PCA drawbacks, Tipping and Bishop <ref type="bibr" target="#b18">[19]</ref> firstly proposed a probabilistic PCA model, called PPCA. Under the probabilistic framework, PPCA takes advantage of Bayesian learning and inference by combining the likelihood with appropriate priors. As a result, the observed data are regarded as random variables, generated from a set of latent random variables which follow the Gaussian distribution of zero mean and identity covariance, with additive noises following a Gaussian distribution with zero mean and an isotropic covariance. Under such a probabilistic learning framework, the model parameters in PPCA can be easily solved by the maximum likelihood estimation (MLE). Much progress has been made based on PCA and PPCA in the last couple of decades <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>PPCA and standard PCA methods can be interpreted in many ways, one of which assumes that the observed highdimensional data are generated from their low-dimensional factors through a linear model with the corruption of Gaussian noise. So those algorithms essentially use a linear model for representing the entire data in a low dimensional subspace. It may be insufficient to model data with large variation caused by, for example, pose, expression and lighting in face recognition. Thus the application scope of PPCA and PCA-based methods is necessarily somewhat limited by its global linearity assumption. An alternative improving paradigm is to model the complex manifold with a mixture of local linear PPCA sub-models. Thus the single PCA model could be extended to a mixture of such sub-models.</p><p>A number of 'mixture of PPCA' have been proposed in literature. The first work was done by Ghahramani and Hinton <ref type="bibr" target="#b6">[7]</ref>. They presented an exact Expectation-Maximization (EM) algorithm for fitting the parameters of the mixture of factor analyzers. By constraining the error covariance to be a diagonal matrix whose elements are usually equal, the mixture of factor analyzers became the mixture of PPCA <ref type="bibr" target="#b19">[20]</ref>. <ref type="bibr">Bishop and Tipping [4]</ref> extended the mixture of PP-CA model to achieve a hierarchical mixture model. Su and Dy <ref type="bibr" target="#b16">[17]</ref> introduced an automated hierarchical mixture of P-PCA algorithm, which utilizes the integrated classification likelihood as a criterion for splitting and stopping the addition of hierarchical levels. Kim et al. <ref type="bibr" target="#b11">[12]</ref> proposed a fast and sub-optimal selection method of model order such as the number of mixture components and the number of PCA bases for the PCA mixture model, consisting of a combination of many PCAs. In addition, under the assumption of the Student-t distribution, the related research includes the mixture model of Student-t components <ref type="bibr" target="#b14">[15]</ref>, which actually is a generalized mixture of Gaussian model without considering subspace structures, and more recent work such as the robust subspace mixture model <ref type="bibr" target="#b15">[16]</ref>, in which both the likelihood and the latent variables were supposed to follow the Student-t distribution and the EM algorithm was applied to the model. In 2005, Archambeau <ref type="bibr" target="#b0">[1]</ref> discussed the robust models in the context of finite mixture models, and a similar work for the mixture of the robust Laplacians was presented in <ref type="bibr" target="#b5">[6]</ref>. These mixture models are important as it enables one to model nonlinear relationships by aligning a collection of such local models.</p><p>The aforementioned models are concerned with vectorial data. In order to apply these methods to 2D data, a typical workaround way is to vectorize 2D data. Vectorizing 2D data not only results in very high-dimensional data, causing the problem of the curse of dimensionality <ref type="bibr" target="#b22">[23]</ref>, but also ignores valuable information on the spatial relationship among 2D data. Instead of using vectorization, PCA approaches for two-dimensional data (2DPCA) have been proposed <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>, to generally extract features of 2D data under the assumption of Gaussian noises. Ju et al. <ref type="bibr" target="#b9">[10]</ref> proposed a probabilistic 2DPCA model to deal with outlier noises by using Laplacian distribution. This model benefits outlier detection. Wang et al. <ref type="bibr" target="#b20">[21]</ref> extended the probabilistic 2DPCA to a mixture of local probabilistic 2DPCA models (MP2DPCA). MP2DPCA offers a tempting prospect of being able to model data with complex variation.</p><p>MP2DPCA model regards each row vector of the 2D data as an observed sample and uses all rows to train the mixture model, resulting in mean vectors from the mixture model. This is essentially a unilateral projection based scheme, where only one side multiplication is taken into account. The unilateral scheme usually preserves the correlation information among the row/column vectors of the images and more parameters are needed to well represent an image. To tackle these problems, a bilateral-projection scheme is favored. In this study, our intention is to propose a mixture of bilateral-projection-based probabilistic 2DP-CA (mixB2DPPCA) model. Different from MP2DPCA, we regard each 2D images as observed samples in their natural shape and reduce 2D dimensionality directly. The mixB2DPPCA has two major advantages: 1) The model makes use of structured information of 2D data and can be easily extended for high order tensorial data. All the algorithm derivations remain without major difficulties. 2) mix2DPPCA carries over all the advantages of the mixture of PPCA.</p><p>The remainder of the paper is organized as follows. In Section 2, the mixture of bilateral-projection two-dimensional probabilistic PCA model is introduced. The variational approximation approach for solving the model is presented in Section 3. In Section 4, some experimental results are conducted to evaluate the performance of the proposed model. Finally, conclusions are summarized in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mixture of Bilateral-Projection 2DPPCA</head><p>Model (mixB2DPPCA)</p><p>In this section, we introduce the mixture of bilateralprojection probabilistic 2DPCA model. For the purpose, we introduce several notations. Let X = {X 1 , X 2 , ..., X N } be N independent and identical random samples with values in R p×q . For n = 1, ..., N , we suppose that sample X n is generated independently from a mixture of K underlying components with unknown probabilities π 1 , π 2 , ..., π K ,</p><formula xml:id="formula_0">p(X n |B n ) = K k=1 π k N (X n |L k B (k) n R T k + M k , σ k I, σ k I)<label>(1)</label></formula><p>where M k ∈ R p×q is the mean matrix, π k s satisfy π k &gt; 0 and K k=1 π k = 1, and L k ∈ R p×r and R k ∈ R q×c are the row and column loading matrices with r ≤ p, c ≤ q. Note that M k , L k and R k are associated with each component of mixture model. B</p><p>(k) n ∈ R r×c is the latent variable core of X n associated with k-th matrix-variate Gaussian component <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">Sec 3.3]</ref> with σ 2 k as residual variance. Like <ref type="bibr" target="#b2">[3]</ref>, we introduce a K-dimensional binary random variable z having a 1-of-K representation in which a particular element z k is equal to 1 and all other elements are equal to 0. That is, z k ∈ {0, 1} and K k=1 z k = 1. The distribution of z is defined by</p><formula xml:id="formula_1">p(z k = 1, z k ′ = 0, k ′ = k) := π k ,</formula><p>which can be written as</p><formula xml:id="formula_2">p(z) = K k=1 π z k k .</formula><p>Thus the conditional distribution of X n given a particular value for z n and B (k) n is the matrix-variate Gaussian</p><formula xml:id="formula_3">p(X n |z nk = 1, B (k) n ) = N (X n |L k B (k) n R T k + M k , σ k I, σ k I).</formula><p>Generally we have</p><formula xml:id="formula_4">p(X n |z n , B (k) n ) = K k=1 N (X n |L k B (k) n R T k +M k , σ k I, σ k I) z nk .</formula><p>In this model setting, the parameters are Θ = {π k , M k , L k , R k , σ 2 k }(k = 1, .., K), and the latent variables are z n and B To develop a generative Bayesian model, we define a matrix-variate Gaussian prior p(B (k) n ) over the latent variable with zero-mean unit-covariance, defined as</p><formula xml:id="formula_5">p(B (k) n ) = N (0, I r , I c ) = 1 2π rc 2 ·exp{− 1 2 tr(B (k)T n B (k) n )}.</formula><p>Hence the joint log-likelihood of the observed data set for such a mixture model is:</p><formula xml:id="formula_6">L = N n=1 K k=1 z nk ln{π k p(X n , B (k) n )}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Variational Approximation for mixB2DPPCA Model</head><p>We employ the Expectation Maximization (EM) algorithm to solve for model parameters Θ. To maximize the log-likelihood of mixB2DPPCA, we take the expectation of L with respect to the posterior distribution of both B (k) n and z nk , i.e.,</p><formula xml:id="formula_7">L = N n=1 K k=1 z nk {ln π k − pq 2 ln σ 2 k − 1 2 tr( B (k)T n B (k) n ) − 1 2σ 2 k tr(X n − M k ) T (X n − M k ) + 1 σ 2 tr((X n − M k ) T L k B (k) n R T k ) − 1 2σ 2 tr( B (k)T n L T k L k B (k) n R T k R k ),<label>(2)</label></formula><p>where · denotes the expectation.</p><p>In E-step, we update Q-distributions of all hidden variables B (k) n and z nk with the current fixed parameter values for Θ. In M-step, maximizing the function L with respect to the model parameters Θ, we can obtain 'new' values for these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Variational E-step</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Update the Posterior Distribution of z nk</head><p>Suppose γ nk := z nk and it is actually the posterior probability of k-mixture generating data point X n . By using the same strategy for the mixture Gaussian model <ref type="bibr" target="#b2">[3]</ref>, we can obtain</p><formula xml:id="formula_8">γ nk = π k p(X n |k) p(X n ) ,<label>(3)</label></formula><p>where p(X n |k) is the k-the component, representing the marginal distribution for the observed data X n over the latent variable. In our case, the marginal distribution of X n is obtained by integrating out the latent variable B (k) n :</p><formula xml:id="formula_9">p(X n |k) = p(X n |B (k) n )p(B (k) n )dB (k) n .</formula><p>Different from the vectorial PPCA, we note that the marginal distribution of the observed data X n is in general no longer a matrix-variate Gaussian. Thus it is difficult to work with p(X n |k) directly. Let x n := vec(X n ), now we can work with p(x n |k) instead of p(X n |k). Fortunately, the marginal distribution of x n is a multivariate Gaussian distribution when taking the special matrix-variate Gaussian prior B</p><formula xml:id="formula_10">(k) n ∼ N (0, I r , I c ). Let m k = vec(M k ), we can obtain p(x n |k) ∼ N (m k , C k ),</formula><p>where the observation covariance model is specified by</p><formula xml:id="formula_11">C k = (R k R T k )⊗(L k L T k )+σ 2 k I.</formula><p>We refer readers to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> for more details. Then the denominator in <ref type="formula" target="#formula_8">(3)</ref> becomes</p><formula xml:id="formula_12">p(x n ) = K k=1 π k p(x n |k).</formula><p>After getting γ nk , we update the estimated mean matrices M k 's and mixing proportions π k 's, respectively, by</p><formula xml:id="formula_13">π k = 1 N N n=1 γ nk and M k = N n=1 γ nk X n N n=1 γ nk .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Update the Posterior Distribution of B (k) n</head><p>In computing the posterior distribution of B (k) n , we encounter a difficulty as the posteriori distribution of B</p><formula xml:id="formula_14">(k) n giv- en X n p(B (k) n |X n , L k , R k , σ 2 ) ∝ p(X n |B (k) n , L k , R k , σ 2 )p(B (k) n )</formula><p>is also in general not a matrix-variate Gaussian. To get a tractable posterior in the variational EM, we restrict the approximated variational distribution to be a matrix-variate</p><formula xml:id="formula_15">Gaussian N (B (k) n | Q (k) n , T (k) n , S (k)</formula><p>n ) to approximate the true posterior with the mean Q n can be estimated through the maximization of a single likelihood function. Particularly, the derived formulas for estimating these parameters are given by, see more details in <ref type="bibr" target="#b25">[26]</ref>,</p><formula xml:id="formula_16">T (k) n = cσ 2 k [tr(R T k R k S (k) n )L T k L k + σ 2 k tr(S (k) n )I r ] −1 S (k) n = rσ 2 k [tr(L T k L k T (k) n )R T k R k + σ 2 k tr(T (k) n )I c ] −1 and each Q (k)</formula><p>n needs to satisfy</p><formula xml:id="formula_17">L T k L k Q (k) n R T k R k + σ 2 k Q (k) n = L T k (X n − M k )R k .</formula><p>To solve this we need to make a vectorization on both sides and solve a linear equation</p><formula xml:id="formula_18">(R T k R k ⊗ L T k L k + σ k I ⊗ σ k I)vec(Q (k) n ) = y (k) n (5) with respect to vec(Q (k) n ), where y (k) n = vec(L T k (X n − M k )R k ), then reshape vec(Q (k) n ) back to get Q (k)</formula><p>n . As we assume the approximated posterior distribution of B (k) n is matrix-variate Gaussian, so we can get B</p><formula xml:id="formula_19">(k) n = Q (k)</formula><p>n and the following second-order expectations:</p><formula xml:id="formula_20">B (k)T n B (k) n = Q (k)T n Q (k) n + S (k) n tr(T (k) n )<label>(6)</label></formula><formula xml:id="formula_21">B (k)T n L T k L k B (k) n = Q (k)T n L T k L k Q (k) n + S (k) n tr(T (k) n L T k L k )<label>(7)</label></formula><formula xml:id="formula_22">B (k) n R T k R k B (k)T n = Q (k) n R T k R k Q (k)T n + T (k) n tr(S (k) n R T k R k )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Variational M-step</head><p>In the M-step, we fix all the distributions over the hidden variables and gather all the terms containing parameters L k , R k and σ 2 k in (2) to maximize them respectively. It turns out that:</p><formula xml:id="formula_23">L k =[ N n=1 γ nk (X n − M k )R k B (k) n T ] × [ N n=1 γ nk B (k) n R T k R k B (k)T n ] −1 (9) R k =[ N n=1 γ nk (X n − M k ) T L k B (k) n ] × [ N n=1 γ nk B (k)T n L T k L k B (k) n ] −1<label>(10)</label></formula><p>and</p><formula xml:id="formula_24">σ 2 k = 1 pqN k { N n=1 γ nk tr(X n − M k ) T (X n − M k ) − 2 N n=1 γ nk tr(R k B (k) n T L T k (X n − M k ) + N n=1 γ nk tr( B (k)T n L T k L k B (k) n R T k R k )}<label>(11)</label></formula><p>where N k = n γ nk . The overall variational EM algorithm is to alternate between E-step and M-step. The final variational EM algorithm is summarized in Algorithm 1. Variational E-step:</p><p>• Iterate the mean matrix Q (k) n based on (5) and update the second-order expectations based on (6), <ref type="formula" target="#formula_21">(7)</ref> and <ref type="formula" target="#formula_22">(8)</ref>.</p><p>• Update each γ nk , mixing proportions π k and mean matrices M k based on <ref type="formula" target="#formula_8">(3)</ref> and (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Variational M-step:</p><p>• Maximize objective function L with respect to each elements L k , R k and σ 2 k based on (9), (10) and (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4: end for</head><p>Define the average reconstruction error</p><formula xml:id="formula_25">e(t) = N n=1 X n − X (t) n 2 F N<label>(12)</label></formula><p>where</p><formula xml:id="formula_26">X n = L k ′ B (k ′ ) n R T k ′ + M k ′ with k ′ = arg max k {γ nk } the reconstructed image.</formula><p>Algorithm 1 may terminate either a given maximum iterative number T is achieved or the following condition is satisfied,</p><formula xml:id="formula_27">|e(t) − e(t + 1)| ≤ ǫ<label>(13)</label></formula><p>where ǫ is a given error tolerance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Reduced-Dimensionality Representation for a New Sample</head><p>In order to obtain the reduced-dimensionality representation for a given sample, we should solve for the latent variable cores. From the probabilistic perspective, the posterior mean Q new |X new can be seen as the reduceddimensionality representation, which is a r × c feature matrix and given by solving a linear equation</p><formula xml:id="formula_28">(R T k R k ⊗ L T k L k + σ k I ⊗ σ k I)vec(Q (k) new ) = y (k) new with respect to vec(Q (k) new ), where y (k) new = vec(L T k (X new − M k )R k ),</formula><p>then reshape vec(Q </p><formula xml:id="formula_29">γ new,k = p(X new |k)π k p(X new ) .</formula><p>We find the largest γ new,k (k = 1, ..., K) from which the most appropriate local 2DPPCA model can be identified for the new sample. That is, a natural choice is to assign the new sample to a cluster with the largest posterior probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results and Analysis</head><p>In this section, we conduct several experiments on some public databases to assess the proposed mixB2DPPCA model. These experiments are designed to evaluate the performance of the proposed mix2DPPCA in reconstruction and recognition by comparing with existing models and algorithms.</p><p>The relevant PCA algorithms that can be fairly compared against our proposed mixB2DPPCA are GLRAM (Generalized Low Rank Approximations of Matrices) <ref type="bibr" target="#b24">[25]</ref>, PSOP-CA (Probabilistic Second-Order PCA) <ref type="bibr" target="#b26">[27]</ref>, mixture of PP-CA <ref type="bibr" target="#b19">[20]</ref> with the code from http://www.science.uva. nl/˜jverbeek. Because the zero-noise PSOPCA model and GLRAM have the same stationary point <ref type="bibr" target="#b26">[27]</ref>, we only compare with GLRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Preparation and Experiment Setting</head><p>All of the experiments are conducted on the following four public available datasets:</p><p>• A subset of handwritten digits images from the M-NIST database (http://yann.lecun.com/exdb/ mnist).</p><p>• The Yale face database (http://vision.ucsd.</p><p>edu/content/yale-face-database).</p><p>• The AR face database (http://rvl1.ecn.purdue. edu/aleix/aleix_face_DB.html).</p><p>• The FERET face database (http://www.itl.nist.</p><p>gov/iad/humanid/feret/feret_master.html).</p><p>The subset of handwritten digits images is selected from MNIST database, which contains 1000 digital images with 100 images of each digit. All images are in grayscale and have a uniform size of 28 × 28 pixels.</p><p>The Yale face database contains 15 individuals, with 11 images for each individual. The images were captured under different illumination and expression conditions. The images are all 100 × 100 pixels with 256 grey levels. In the experiments, we randomly select 6 images of each person as the training samples, and use the remaining images to form the testing sample set. All images are scaled to a resolution of 64 × 64 pixels.</p><p>The AR face database contains over 4,000 color images corresponding to 126 subjects. There are variations of facial expressions, illumination conditions, and occlusions (sun glasses and scarf) with each person. Each individual consists of 26 frontal view images taken in two sessions (separated by 2 weeks), where each session has 13 images. <ref type="figure" target="#fig_2">Figure 1</ref> shows the 26 images of one subject. In the experiments, we select 30 subjects (15 man and 15 women), and only use the non-occluded 14 images (i.e., the first seven face images of each row in <ref type="figure" target="#fig_2">Figure 1</ref>). The first seven of each subject are used for training and the last seven for testing. All images are cropped and resized to 50 × 40 pixels.</p><p>FERET database includes 1400 images of 200 different subjects, with 7 images per subject. In the experiments, we select 50 subjects randomly. Five images of each subject are used for training and the remained images are used for testing. All images are cropped and resized to 32 × 32 pixels.</p><p>In experiments, the initial mixing proportions are set to π k = 1/K and the initial loading matrices L k and R k are given randomly. Besides, we choose randomly K samples as mean matrices M k of the mixture gaussian model and set all σ 2 k = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Reconstruction Performance</head><p>In this section, we test reconstruction error of the proposed mixB2DPPCA model <ref type="bibr" target="#b0">(1)</ref>. Applying the proposed model, all digital images can be softly grouped into K clusters, each of which is modelled by a local B2DPPCA. From all the trained γ nk , the most appropriate local B2DPPCA for a given sample can be found. Then we use the most appropriate local B2DPPCA to reconstruct the initial digit image, that is:</p><formula xml:id="formula_30">X n = L k ′ * Q (k ′ ) n * R T k ′ + M k ′ ,</formula><p>where k ′ represents the k ′ -th local B2DPPCA which most appropriate to the sample X n . After obtaining all reconstructed digit images X n , we can using the equation <ref type="bibr" target="#b11">(12)</ref> to compute the average reconstruction error.</p><p>Next we compare the reconstruction error of different algorithms on three databases. In all algorithms, we set the iterative number is T = 50 and the reduced dimension is r = c = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Reconstruction Error on Digit Image Set</head><p>We use the given digital image subset in Section 4.1 as training set. In this phase, we compare the reconstruction error of the training set. <ref type="figure" target="#fig_6">Figure 2</ref> shows the average reconstruction error of the relevant algorithms. From left to right, the component num-  ber is K = 2, K = 5 and K = 10 respectively. Firstly, from these three sub-figures, we can see that the reconstruction error of GLRAM algorithm has no change. This is because GLRAM has no relationship with K. Besides, GLRAM works by iteratively computing the leading eigenvectors of the left and right one-sided sample covariance matrices. Thus GLRAM convergent in five steps and the change of reconstruction error is not obvious in the figure. Secondly, fixing the same number of reduced dimension, the performance of our proposed mixB2DPPCA is better than GLRAM. From the view of compression, decoded images from our algorithm have higher quality for the compression ratio of 49 : 1. It illustrates that mixB2DPPCA can correctly identify data according to clusters. When K becomes larger, the mixB2DPPCA outperform the mixture of PPCA in terms of reconstruction errors.</p><p>The reconstructed images of different methods are shown in <ref type="figure" target="#fig_7">Fig. 3</ref> with K = 10. The first row shows four original images. The second, third and fourth rows are the reconstructed images by GLRAM, mixture of PPCA and mixB2DPPCA, respectively. It can be found that the proposed mixB2DPPCA has better reconstruction outcomes, while the results of other two methods show a little degra-dation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Reconstruction Error on Yale and AR Databases</head><p>In this experiment, we compare the reconstruction error on Yale and AR databases. <ref type="figure" target="#fig_8">Figure 4</ref> shows the average reconstruction error of all the algorithms: (a) on the Yale database and (b) on the AR database. The component number is K = 5 and the reduced dimensionality is (r, c) = <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4)</ref>. It is obvious that the reconstruction error of mixB2DPPCA on testing set has reduced greatly than other algorithms. <ref type="figure" target="#fig_9">Figure 5</ref> shows some reconstructed images of different algorithms on Yale database. The first row is four original images. The last three rows are the corresponding images reconstructed by mixture of PPCA, GLRAM and mixB2DPPCA. It can be shown that the results of our algorithm have better visual effect than that of GLRAM. Besides we can also see that although the face images reconstructed by mixture of PPCA are relatively clear, they don't match the same original images visually. The reconstructed images on AR database are shown in <ref type="figure" target="#fig_10">Figure 6</ref>. The first row shows five original images in the test set and the last three    rows are the reconstructed images from three models. From the reconstruction experiments, we can conclude that mixB2DPPCA generally outperforms global linear 2D-PCA algorithms in terms of reconstruction errors. It demonstrates that the classification of training set in advanced is important for the performance of feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Recognition Performance</head><p>In this section, we compare the recognition performances of GLRAM, mixture of PPCA and mixB2DPCA on Yale, AR and FERET face databases. These algorithms can be used for extracting features of facial images from the training samples, respectively, and then a nearest neighbor classifier (1-NN) is used to find the most-similar face from the training samples for a querying face. In our exper- iments, the distance measure between two sets of feature matrices B n1 and B n2 , is defined as</p><formula xml:id="formula_31">dist = K k=1 B (k) n1 − B (k) n2 F . where B n = [B (1) n , ..., B<label>(K)</label></formula><p>n ] represents the combination of K latent variable cores related with n-th sample 1 . In all algorithms, we set maximum iteration number is 50 and ǫ is 1E-3. We repeat the procedure 10 times, and the mean values and relevant variances are reported in <ref type="table" target="#tab_1">Tables 1 to 3.  Table 1</ref> shows the recognition rates of three feature extraction algorithms: GLRAM, mixture of PPCA and mixB2DPPCA training on Yale database. The mean values and relevant variances are reported for the cases of the reduced dimension (r, c) = (2, 2), <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b5">6)</ref> and <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b7">8)</ref>. For the mixture of PPCA and mixB2DPPCA, we also computed the recognition rates for the different component number K (K = 4, 6, 8), shown in <ref type="table" target="#tab_1">Table 1</ref>    ly, from the table we can see that the recognition rates of the mixture of PPCA and mixB2DPPCA have a little fluc-tuation compared with GLRAM. This may be caused by the uncertainty of probability. Secondly, compared with GLRAM, the mean recognition rates of mixB2DPPCA algorithm have obviously improved. The bold figures are the best results in the comparison. <ref type="table" target="#tab_2">Table 2</ref> shows the recognition rates of the above three algorithms training on AR database. The reduced dimensions are (r, c) = (4, 4), <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b5">6)</ref> and <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b7">8)</ref> and component numbers are K = 6, 8, 10, respectively. From the table we can see that the mean recognition rates of mixB2DPPCA algorithm have better improvement over the other two algorithms. <ref type="table" target="#tab_4">Table 3</ref> shows the recognition rates on FERET database. The reduced dimensions are (r, c) = (4, 4), <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b5">6)</ref>, <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b7">8)</ref> and <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b9">10)</ref>, and the component numbers are K = 6, 8, 10, respectively. In this case, both the mixture of PPCA and the proposed mixB2DPPCA produce slightly larger variances, however the mean recognition rates have risen greatly. GLRAM is relatively more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a mixture of bilateralprojection probabilistic PCA model for feature extraction and dimensionality reduction. Different from the standard PCA which is a global dimension reduction model, this model employs the mixture of matrix-variate Gaussian to model local linear sub-models. All the parameters in the resulting probabilistic model can be estimated through the maximization of the likelihood function. The new model not only makes good use of spatial (structural) information of 2D data but also can softly group data into a given number of clusters. The performance of feature extraction of the proposed method generally outperforms other existing 2D algorithms in terms of reconstruction error and recognition rate. The approach used in this paper can be readily extended to higher order tensorial data and other non-Gaussian noise models can also be integrated into the model such.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>n</head><label></label><figDesc>(n = 1, ..., N ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>0 of size r × r and S (k) n ≻ 0 of size c × c, respectively. For mixB2DPPCA model, it follows as a natural extension of a single 2DPPCA. So the parameters Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Variational EM algorithm for mixB2DPPCA.Initialize: Training set X = {X n } N n=1 ; Initialize all of model parameters Θ and covariance matrices T n = 1, ..., N and k = 1, ..., K. 1: for t = 1 to T do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) back to get Q(k) new . At the same time, we can compute the corresponding γ new,k , i.e., the posterior probability of k-th component generating the new sample, given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :</head><label>1</label><figDesc>Twenty-six face examples of one subject from AR database. The first row is from the first session, and the second row images are from the second session.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Average reconstruction error versus iteration number with the components number K = 2, K = 5 and K = 10 from the left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Original images and reconstructed images: The first row shows four original digital images. The second, third and fourth rows are the reconstructed images by GLRAM, mixture of PPCA and mixB2DPPCA, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Average reconstruction error versus iteration number with the components number K = 5 on Yale database (a) and AR database (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Original and reconstructed images (in the Yale database): The first row is original images. The second, third and fourth rows are the reconstructed images by mixture PPCA, GLRAM and mixB2DPPCA, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Original and reconstructed images (in the AR database): The first row is original images. The second, third and fourth rows are the reconstructed images by GLRAM, mixture PPCA and mixB2DPPCA, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Recognition accuracy of GLRAM, mixture of PPCA and mixB2DPPCA training on the Yale database</figDesc><table>r, c GLRAM 
K = 6 
K = 8 
K = 10 
mixPPCA 
mixB2DPPCA 
mixPPCA 
mixB2DPPCA 
mixPPCA 
mixB2DPPCA 
4 
0.5714 
0.5328±0.0220 
0.6671±0.0333 
0.5595±0.0214 
0.7000±0.0371 0.5752±0.0297 
0.7244±0.0381 
6 
0.6857 
0.6252±0.0242 
0.7867±0.0138 
0.6343±0.0236 
0.8017±0.0291 0.6613±0.0182 
0.7576±0.0366 
8 
0.7190 
0.7004±0.0190 
0.8116±0.0231 
0.7100±0.0246 
0.8211±0.0246 0.7133±0.0222 
0.8357±0.0237 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Recognition accuracy of GLRAM, mixture of PPCA and mixB2DPPCA training on the AR database</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>. First-</figDesc><table>r, c GLRAM 

K = 6 
K = 8 
K = 10 
mixPPCA 
mixB2DPPCA 
mixPPCA 
mixB2DPPCA 
mixPPCA 
mixB2DPPCA 
4 
0.5000 
0.4620±0.0315 
0.6070±0.0427 
0.4690±0.0470 
0.6210±0.0326 0.4840±0.0316 
0.5900±0.0429 
6 
0.5300 
0.5140±0.0206 
0.6733±0.0541 
0.5350±0.0283 
0.6467±0.0343 0.5320±0.0297 
0.6644±0.0328 
8 
0.5400 
0.5440±0.0298 
0.6900±0.0458 
0.5610±0.0159 
0.6945±0.0526 0.5580±0.0187 
0.6770±0.0593 
10 
0.5500 
0.5910±0.0460 
0.6890±0.0455 
0.5720±0.0364 
0.6960±0.0599 0.5970±0.0336 
0.7100±0.0573 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Recognition accuracy of GLRAM, mixture of PPCA and mixB2DPPCA training on the FERET database</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A more accurate way is to use γ n 1 k γ n 2 k to weight the individual distance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic models in noisy environments and their application to a visual prosthesis for the blind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Archambeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Unpublished doctoral dissertation</title>
		<meeting><address><addrLine>Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Université Catholique de Louvain</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust probabilistic projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Delannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning. Information Science and Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable model for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="293" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust L1 principal component analysis and its Bayesian variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="555" to="572" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mixture of the robust L1 distributions and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4830</biblScope>
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The EM algorithm for mixtures of factor analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CRG-TR- 96-1</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face recognition using Laplacianfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Independent component analysis applied to feature extraction from colour and stereo images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="191" to="210" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image outlier detection and feature extraction via L1-norm based 2D probabilistic PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4834" to="4846" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PCA-SIFT: A more distinctive representation for local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="506" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An efficient model order selection for PCA mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Bang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1385" to="1393" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A general framework for increasing the robustness of PCA-based correlation clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific and Statistical Database Management</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="418" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ensemble-based discriminant learning with boosting for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venetsanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Network</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="166" to="178" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust mixture modelling using the t distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="339" to="348" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust subspace mixture models using t-distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ridder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Franc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th British Machine Vision Conference</title>
		<meeting>the 14th British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated hierarchical mixtures of probabilistic principal component analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Applied Multivariate Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Timm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mixtures of probabilistic principal component analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic twodimensional principal component analysis and its mixture model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="541" to="547" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2DPCA with L1-norm for simultaneously robust and sparse modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Network</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matrix-variate factor analysis and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1821" to="1826" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Twodimensional PCA: A new approach to appearance-based face representation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="137" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generalized low rank approximations of matrices. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="167" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matrix-variate factor analysis and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop Data Mining Using Matrix and Tensors</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Matrix-variate and higher-order probabilistic projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="392" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
