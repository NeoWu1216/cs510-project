<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Traffic-Sign Detection and Classification in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">Zhu</forename><surname>Tnlist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dun</forename><surname>Liang Tnlist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhai</forename><forename type="middle">Zhang</forename><surname>Tnlist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
							<email>huang@cse.lehigh.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoli</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimin</forename><forename type="middle">Hu</forename><surname>Tnlist</surname></persName>
							<email>shimin@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Lehigh University Bethlehem</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>PA</region>
									<country>USA, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Traffic-Sign Detection and Classification in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although promising results have been achieved in the areas of traffic-sign detection and classification, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large traffic-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. It provides 100000 images containing 30000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. We call this benchmark Tsinghua-Tencent 100K. Secondly, we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify trafficsigns. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the traffic-signs here. Experimental results show the robustness of our network and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene understanding is the ultimate goal of computer vision; detecting and classifying objects of various sizes in the scene is an important sub-task. Recently, deep learning methods have shown superior performance for many tasks such as image classification and speech recognition. One particular variant of deep neural networks, convolu-1 http://cg.cs.tsinghua.edu.cn/traffic-sign/ tional neural networks (CNNs), have shown their strengths for tasks including image classification, localization and detection. Two benchmarks widely used to evaluate detection performance are PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> and ImageNet ILSVR-C <ref type="bibr" target="#b19">[20]</ref>. In these datasets, target objects typically occupy a large proportion of each image (the bounding box of each object of interest fills on average about 20% of the image). However, for some tasks, objects of interest may only occupy a small fraction of an image, such as traffic-signs in images captured while driving. A typical traffic-sign might be say 80 × 80 pixels, in a 2000 × 2000 pixel image, or just 0.2% of the image. In fact, many tasks require detection and classification of small but significant objects, so it is important to devise and evaluate methods which perform well when the object of interest is not the main, or even a major, scene item.</p><p>Traffic signs may be divided into different categories according to function, and in each category they may be further divided into subclasses with similar generic shape and appearance but different details. This suggests traffic-sign recognition should be carried out as a two-phase task: detection followed by classification. The detection step uses shared information to suggest bounding boxes that may contain traffic-signs in a specific category, while the classification step uses differences to determine which specific kind of sign is present (if any). (We note that the words 'detection' and 'classification' have different meanings in the general object recognition community where, as exemplified by the ImageNet competition, classification means giving an image a label rather than an object, and detection means finding the bounding box of an object in a specific category.)</p><p>Since the launch of the German traffic-sign detection and classification benchmark data <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, various research groups have made progress in both the detection bench-mark(GTSDB) <ref type="bibr" target="#b24">[25]</ref> task and classification benchmark (GT-SRB) <ref type="bibr" target="#b23">[24]</ref> task. Current methods achieve perfect or nearperfect results for both tasks, with 100% recall and precision for detection and 99.67% precision for classification. While it may appear that these are thus solved problems, unfortunately, this benchmark data is not representative of that encountered in real tasks. In the GTSDB detection benchmark task, the algorithms must only detect trafficsigns in one of 4 major categories. In the GTSRB classification benchmark, the traffic-sign occupies most of the image, and the algorithms must only decide which subclass the sign belongs to; furthermore; there are no negative samples disrupting the classification. In real world tasks, the main difficulty when detecting and classifying traffic-signs in an ordinary image is their very small size, often less than 1% of the image. The potential candidate regions are orders of magnitude smaller than in PASCAL VOC and ImageNet ILSVRC. Furthermore, the algorithm must filter out many potential negative cases while retaining true traffic-signs. We have thus created a new, more realistic benchmark, and have also used it to evaluate a combined CNN approach to traffic sign detection and classification.</p><p>The contributions of this paper are as follows.</p><p>• We have created a new, more realistic traffic-sign benchmark. Compared to the widely used detection benchmark GTSDB, our benchmark contains 111 times as many images, at 32 times the image resolution. The traffic-signs in our benchmark cover realworld conditions, with large variations in such aspects as illuminance and weather conditions, also including examples with occlusion. Our benchmark is, unlike previous ones, annotated with a pixel mask for each traffic-sign, as well as giving its bounding box and class. We call this benchmark Tsinghua-Tencent 100K.</p><p>• We have trained two CNNs for detecting traffic signs, and simultaneously detecting and classifying trafficsigns. Evaluation on our benchmark shows the robustness of the two networks.</p><p>The rest of the paper is organized as follows: in Section 2 we discuss related work. Details of our benchmark are given in Section 3, while the architecture of our network is presented in Section 4. We give experimental results in Section 5 and conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Traffic Sign Classification</head><p>Before the widespread adoption of convolutional neural networks, various object detection methods were adapted for traffic-sign classification, e.g. based on SVMs <ref type="bibr" target="#b17">[18]</ref> and  sparse representations <ref type="bibr" target="#b16">[17]</ref>. Recently, convolutional neural network approaches have been shown to outperform such simple classifiers when tested on the GTSRB benchmark. These approaches include using a committee of CNNs <ref type="bibr" target="#b3">[4]</ref>, multi-scale CNNs <ref type="bibr" target="#b21">[22]</ref> and CNNs with a hinge loss function <ref type="bibr" target="#b13">[14]</ref>, the latter achieving a precision rate of 99.65%, better than human performance <ref type="bibr" target="#b24">[25]</ref>. However, as noted earlier, these approaches perform classification on already detected signs, which is impractical in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Detection by CNNs</head><p>After interest in CNNs was initially rekindled by their use in <ref type="bibr" target="#b14">[15]</ref> for image classification, they were quickly adapted to object detection. In OverFeat <ref type="bibr" target="#b20">[21]</ref>, Sermanet et al. observed that convolutional networks are inherently efficient when used in a sliding window fashion, as many computations can be reused in overlapping regions. They demonstrated a network that can determine an object's bounding box together with its class label.</p><p>Another widely used strategy for object detection using CNNs is to first calculate some generic object proposals and perform classification only on these candidates. R-CNN <ref type="bibr" target="#b7">[8]</ref> was the first to use this strategy, but it is very slow for two reasons. Firstly, generating category-independent object proposals is costly. Selective search <ref type="bibr" target="#b28">[29]</ref> takes about 3 s to generate 1000 proposals for the Pascal VOC 2007 images; the more efficient EdgeBoxes approach <ref type="bibr" target="#b29">[30]</ref> still takes about 0.3 s. Secondly, it applies a deep convolutional network to every candidate proposal, which is very inefficient. <ref type="figure">Figure 2</ref>. Chinese traffic-sign classes. Signs in yellow, red and blue boxes are warning, prohibitory and mandatory signs respectively. Each traffic-sign has a unique label. Some signs shown are representative of a family (e.g. speed limit signs for different speeds). Such signs are generically denoted above (e.g. 'pl*'); the unique label is determined by replacing '*' by a specific value (e.g. 'pl40' for a 40 kmh speed limit sign).</p><p>To improve efficiency, the spatial pyramid pooling network (SPP-Net) <ref type="bibr" target="#b9">[10]</ref> calculates a convolutional feature map for the entire image and extracts feature vectors from the shared feature map for each proposal. This speeds up the R-CNN approach about 100 times.</p><p>Girshick et al. later proposed Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>, which uses a softmax layer above the network instead of the SVM classifier used in R-CNN. Ignoring object proposal time, it takes 0.3 s for Fast R-CNN to process each image. To overcome the bottleneck in the object proposal step, in Faster R-CNN <ref type="bibr" target="#b18">[19]</ref>, Ren et al. proposed region proposal networks (RPNs) which use convolutional feature maps to generate object proposals. This allows the object proposal generator to share full-image convolutional features with the detection network, allowing their detection system to achieve a frame rate of 5 fps on a powerful GPU.</p><p>While these works determine object proposals by hand, Szegedy et al. <ref type="bibr" target="#b26">[27]</ref> improved upon a data-driven proposal generation method <ref type="bibr" target="#b5">[6]</ref>, as well as improving the network architecture, to achieve a frame rate of 50 fps in testing, with competitive detection performance.</p><p>However, the performance of all of these object detection networks was evaluated on PASCAL VOC and ILSVRC, where target objects occupy a large proportion of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Benchmark</head><p>We now explain our new benchmark: where we obtained the data, how we annotated it, and what it finally contains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection</head><p>While general image datasets such as ImageNet <ref type="bibr" target="#b4">[5]</ref> and Microsoft COCO <ref type="bibr" target="#b15">[16]</ref> have been generated by downloading Internet images retrieved by search engines using keywords, relatively few Internet users upload real-world images containing traffic-signs as might be seen in the street, and even when they do, the traffic signs are incidental: such images will not be tagged with the names of any signs they contain. Such an approach cannot be used here. Furthermore, to mimic a real world application scenario, images without traffic-signs should be also included in the benchmark, to evaluate if a detector can distinguish real traffic-signs from other similar looking objects. We determined that an ideal way to collect useful images would be to extract data from Tencent Street Views.</p><p>Presently, Tencent Street Views cover about 300 Chinese cities and the road networks linking them. The original panoramas were captured by 6 SLR cameras and then stitched together. Image processing techniques such as exposure adjustment were also used. Images were captured both from vehicles and shoulder-mounted equipment, at intervals of about 10 m. The nature of the images provide two benefits for our benchmark. Firstly, traffic-signs in successive shots are related by a homography. Unlike in GT-SRB <ref type="bibr" target="#b24">[25]</ref>, whose traffic-signs were extracted from a video sequence, leading to many very similar images, the appearances of an instance of a traffic-sign in our benchmark vary significantly. Secondly, an instance of a traffic-sign in successive images helps the participants constructing the benchmark to correctly determine its classes: partially occluded or blurred traffic-signs can be recognized from their occurrences in previous or subsequent shots.</p><p>To create the benchmark images, the top 25% and bottom 25% of each panorama image was cropped off (as unlikely to contain any signs), and the remainder sliced vertically into 4 sub-images. See <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>We chose 10 regions from 5 different cities in China (including both downtown regions and suburbs for each city) and downloaded 100000 panoramas from the Tencent Data Center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Annotation</head><p>The images collected were next annotated by hand. Traffic signs in China follow international patterns, and can be classified into three categories: warnings (mostly yellow triangles with a black boundary and information), prohibitions (mostly white surrounded by a red circle and also possibly having a diagonal bar), and mandatory (mostly blue circles with white information). Other signs exist that resemble traffic-signs but are in fact not; some are illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Such signs are placed in an 'other' class of a particular category. During traffic-sign annotation, we recorded the bounding box, boundary vertices and class label for the sign. To determine the pixel mask for the sign, we use two modes: polygon mode and ellipse mode. In polygon mode, we mark the vertices of the polygon while in ellipse mode we mark arbitrary 'vertices' along the boundary of the ellipse, and we fit the shape automatically using the marked vertices. For a triangle sign we only mark three vertices; for distorted signs we may mark additional vertices for accurate segmentation. Circle signs appear as ellipses, unless occluded, so we mark 5 vertices to which we can fit a ellipse during post-processing. The most complicated cases concern occluded signs. In this case, we mark the bounding box, the polygon boundary and ellipse boundary (if appropriate), and intersect them to find the final mask. We illustrate our annotation pipeline in <ref type="figure">Figure 4</ref>, and show a complicated annotation case in <ref type="figure">Figure 5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dataset Statistics</head><p>Our new benchmark has 100000 cropped images after discarding some of the images only containing background. Of these, 10000 contain 30000 traffic-signs in total. Although our source images cover much of China, an imbalance still exists between different classes of traffic-sign in our benchmark. This is unavoidable: classes such as signs  to warn the driver to be cautious on mountain roads appear rarely. Instances per class are given in <ref type="figure" target="#fig_5">Figure 6</ref>; most instances appear in relatively few classes. The image sizes (in pixels) of the traffic-signs is given in <ref type="figure" target="#fig_6">Figure 7</ref>; note that small traffic-signs are most common.</p><p>In summary, our newly created benchmark provides detailed annotation for each sign: its bounding box, its pixel mask, and its class. The signs fall into many classes, and there are many instances in many of those classes. The images in this benchmark have resolution 2048 × 2048. and cover large variations in illuminance and weather conditions. It will hopefully provide a suitable basis for research into both detecting and classifying small objects. We have used it to train our own CNN for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Neural Network</head><p>We trained two networks in total, one for detection alone, and one for simultaneous detection and classification. They share most of the same structure except for the branches in the last layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture</head><p>In <ref type="bibr" target="#b11">[12]</ref>, Huval et al. evaluate the performance of C-NNs on lane and vehicle detection. They use the OverFeat framework with a bounding box regression step. Their network is fully convolutional, and the last layer is branched into two streams: a pixel layer and a bounding box layer. Each result output by the pixel layer represents the probability of a certain 4 × 4 pixel region in the input image containing a target object. For the bounding box layer, each result represents the distance between that region and the four sides of the predicted bounding box of the target. They evaluated their network on a 1.5 hour highway video. Although their network perfectly detected vehicles (regarding all kinds of vehicles as one class), their network can not be directly adapted to train a multi-class detector for small objects, as needed for our problem. Nevertheless, we build upon their network architecture but make noticeable modifications. Firstly, we make the network branch after layer 6, while in <ref type="bibr" target="#b11">[12]</ref> branching is done after layer 7. During experiment we found this modification makes the network converge faster compared with precious network structure. As noted in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>, deeper networks perform better: more layers bring more capability. If we let the network branch more earlier, although it has the potential to perform better, this increases training time and consumes more GPU memory, so is not cost-efficient. Thus it is a better balance between speed and accuracy to branch the network after layer 6. Another modification is that our network finally branches into three streams rather than the two streams in <ref type="bibr" target="#b11">[12]</ref>. Apart from a bounding box layer and a pixel layer, we added a label layer which can output a classification vector with n elements, where each element is the probability it belongs to a specific class. This allows our network to simultaneously detect and classify traffic signs. Our network architecture is illustrated in <ref type="figure">Figure 8</ref>. More details can be found in our source code. Our implementation uses the Caffe learning framework <ref type="bibr" target="#b12">[13]</ref>. When removing the label layer in the 8th layer, this network can be used as a traffic sign detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>Due to the uneven numbers of examples of different classes of traffic signs, we used a data augmentation technique during training <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>. We simply ignored classes with fewer than 100 instances. This left 45 classes to classify. Classes with between 100 and 1000 instances in the training set were augmented to give them 1000 instances. Other classes that have more than 1000 instances remain unchanged.</p><p>To augment the data, we used the standard template for each class of traffic signs <ref type="bibr" target="#b0">[1]</ref>, rotated it randomly by an amount in the range [−20 • , 20 • ], scaled it randomly to have size in the range <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">200]</ref>, and also added a random but reasonable perspective distortion. We then manually picked images without traffic signs and blended in the transformed template, with additional random noise. <ref type="table" target="#tab_0">Table 1. Network architecture for multi-class model   layer  data  conv1  conv2  con3  conv4  conv5  conv6  conv7  conv8-conv8-conv8bbox  pixel  label  output size  3,480,  96,118, 256,59,  384,29, 384,29,  384,29, 4096,15,  4096,15,  256,15,  128,15,  1000,15  (chan×h×w)  640  158  79  39  39  39  20  20  20  20  20  input size  3,480,  96,59,  256,29, 384,29,  384,29,  384,29,  4096,15,  4096,15, 4096,15,  4096,15,  640  79  39  39  39  39  20  20  20  20  kernel size</ref>  <ref type="figure">Figure 8</ref>. Architecture of our multi-class network. Our network is fully convolutional, and branches after the 6th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In our experimental evaluation of our neural network, both training and testing were done on a Linux PC with an Intel Xeon E5-1620 CPU, two NVIDIA Tesla K20 G-PUs and 32GB memory. For 10000 panoramas containing traffic-signs, we separated them into a training set and a testing set (as explained in the released benchmark), with about 2:1 ratio to give the deep learning methods plenty of training samples. The other 90000 panoramas were included during testing.</p><p>We used the evaluation metrics used for the Microsoft COCO benchmark, and divided the traffic-signs into three categories according to their size: small objects(area &lt; 32 2 pixels), medium objects(32 2 &lt; area &lt; 96 2 ) and large objects (area &gt; 96 2 ). This evaluation scheme can tell the ability of a detector on different sizes of object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Detection</head><p>We now consider how well various object proposal methods work, as well as our detection network, using our benchmark. A common feature of most popular object detection networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9]</ref> is that they rely on generic object proposals. If the target object is missing from the proposal set, later steps are in vain. The sizes of objects of interest in our benchmark are much smaller than in previous benchmarks, and typical object proposal methods do not work well for such small objects, resulting in trafficsign candidates of low quality, as we now show. Selective Search <ref type="bibr" target="#b28">[29]</ref>, Edge Boxes <ref type="bibr" target="#b29">[30]</ref> and Multiscale Combinatorial Grouping(MCG) <ref type="bibr" target="#b1">[2]</ref> are suggested in <ref type="bibr" target="#b10">[11]</ref> to be the most effective object proposal methods. Since MCG is memory intensive and our images are of high resolution, we evaluated the proposal performance for traffic-signs using Selective Search, Edge Boxes and BING <ref type="bibr" target="#b2">[3]</ref> instead. Note that Selective Search and Edge Boxes do not need training data, so we directly evaluated their performance on the test set of our benchmark. We trained BING using the same augmented training set as used for our network. The results are illustrated in <ref type="figure">Figure 9</ref>. The average recall for all 10000 proposals for those three approaches is under 0.7. This indicates that object proposal approaches are not suitable for locating small objects in large images, even when the number of candidate proposals is sufficiently large.</p><p>Instead, treating all traffic-signs as one category, we trained a detection network using our architecture. Our network achieved 84% accuracy and 94% recall at a Jaccard similarity coefficient of 0.5, without carefully tuning its parameters, which significantly outperforms the results obtained by previous objection detection methods. Also, we note that our network performs in essence just as well for each size of objects.</p><p>We also tested our detector on the 90000 panoramas that contained no traffic-signs, and the network perfectly identi- fied them all as only containing background. Further details are provided in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Simultaneous detection and classification</head><p>We now turn to the combined problem of both finding traffic signs and classifying them. We compare the results provided by our network and the state-of-the-art Fast R-CNN method <ref type="bibr" target="#b8">[9]</ref>. (We could not readily make a comparison with Faster R-CNN <ref type="bibr" target="#b18">[19]</ref> or the detector in <ref type="bibr" target="#b27">[28]</ref> as source code is unavailable). We also generate 10000 proposal for each image when we run the test of Fast R-CNN. The results are given in <ref type="figure" target="#fig_1">Figure 10</ref>. This clearly indicates that our approach outperforms R-CNN, especially when traffic signs are small. We also give the accuracy and recall for each category for Jaccard similarity coefficient 0.5 in <ref type="table" target="#tab_2">Table 2</ref>, and the average accuracy and recall for different object sizes in <ref type="table" target="#tab_0">Table 3</ref>; Fast R-CNN has better performance for larger objects. Overall, Fast R-CNN has a recall 0.56 and accuracy 0.50 while our approach has a recall 0.91 and accuracy 0.88.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have created a new benchmark for simultaneously detecting and classifying traffic signs. Compared with previous traffic sign benchmarks, images in this benchmark are more variable, and signs in these images are much smaller. It contains more images than previous benchmarks, and the images have a higher resolution. Furthermore, pixel-wise segmentation of signs is provided. This benchmark provides a new challenge for the traffic sign recognition community. We have trained two networks on this benchmark: one treats all sign classes as a single category and can be regarded as a traffic sign detector. The other network can simultaneously detect and classify traffic signs. Both significantly outperform previous work, and can be used as a baseline for future research. To assist research in this field, we make this benchmark, trained models and source code public available.</p><p>In future, we plan to seek out more traffic signs of the classes that rarely appear in this benchmark. We also plan to accelerate the speed of the process in order to run it on mobile devices in real-time. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>8192×2048 panorama from Tencent Street View before slicing vertically into 4 images. Sky and ground at top and bottom have been cropped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Our benchmark contains 100000 high resolution images in which all traffic-signs are annotated with class label, bounding box, and pixel mask. The images are cut from from Tencent Street Views which contain realistic views traffic-signs in their environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Signs like traffic-signs, but with other meanings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Annotation pipeline. Firstly we locate the traffic-sign and draw its bounding box. Then boundary vertices are marked on the sign's contour to determine the pixel mask. Finally the class label is attached. Sign annotation for a complicated case. We mark the bounding box, polygon boundary and circle boundary, and compute their intersection to give the final segmentation mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Number of instances in each class, for classes with more than 100 instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Number of instances of each size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Object proposal results for traffic-signs for various object location methods, for small, medium and large signs. Simultaneous traffic sign detection and classification results Fast R-CNN and our approach, for small, medium and large signs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Simultaneous detection and classification results for different sizes of traffic signs using Fast R-CNN and our approach. FR: Fast R-CNN recall, FA: Fast R-CNN accuracy, OR: Our method's recall, OA: Our method's accuracy.</figDesc><table>Object size (0,32] 
(32,96] 
(96,400] 
FR 
0.24 
0.74 
0.86 
FA 
0.45 
0.51 
0.55 
OR 
0.87 
0.94 
0.88 
OA 
0.82 
0.91 
0.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Simultaneous detection and classification results for each class using Fast R-CNN and our approach. FR: Fast R-CNN recall, FA: Fast R-CNN accuracy, OR: Our method's recall, OA: Our method's accuracy.</figDesc><table>Class 
i2 
i4 
i5 
il100 
il60 
il80 
io 
ip 
p10 
p11 
p12 
p19 
p23 
p26 
p27 
FR 
0.32 
0.61 0.69 
0.41 
0.8 
0.39 
0.65 0.67 
0.51 
0.44 
0.48 
0.79 
0.70 
0.60 
0.60 
FA 
0.68 
0.62 0.71 
0.52 
0.63 
0.76 
0.51 0.48 
0.54 
0.69 
0.73 
0.67 
0.94 
0.67 
0.67 
OR 
0.82 
0.94 0.95 
0.97 
0.91 
0.94 
0.89 0.92 
0.95 
0.91 
0.89 
0.94 
0.94 
0.93 
0.96 
OA 
0.72 
0.83 0.92 
1.00 
0.91 
0.93 
0.76 0.87 
0.78 
0.89 
0.88 
0.53 
0.87 
0.82 
0.78 
Class 
p3 
p5 
p6 
pg 
ph4 
ph4.5 
ph5 
pl100 pl120 pl20 
pl30 
pl40 
pl5 
pl50 pl60 
FR 
0.40 
0.72 0.54 
0.89 
0.42 
0.83 
0.31 
0.82 
0.57 
0.25 
0.43 
0.48 
0.65 
0.29 
0.42 
FA 
0.62 
0.92 0.66 
0.44 
0.94 
0.75 
0.63 
0.81 
0.91 
0.88 
0.73 
0.85 
0.89 
0.76 
0.86 
OR 
0.91 
0.95 0.87 
0.91 
0.82 
0.88 
0.82 
0.98 
0.98 
0.96 
0.94 
0.96 
0.94 
0.94 
0.93 
OA 
0.80 
0.89 0.87 
0.93 
0.94 
0.88 
0.89 
0.97 
1.00 
0.90 
0.90 
0.89 
0.84 
0.87 
0.93 
Class pl70 
pl80 
pm20 pm30 pm55 
pn 
pne 
po 
pr40 
w13 
w32 
w55 
w57 
w59 
wo 
FR 
0.23 
0.40 
0.53 
0.63 
0.79 
0.59 
0.77 0.29 
0.98 
0.32 0.29 
0.50 
0.56 
0.67 
0.32 
FA 
0.67 
0.76 
0.93 
0.77 
0.57 
0.86 
0.21 0.33 
0.10 
0.36 0.30 
0.70 
0.38 
0.53 
0.16 
OR 
0.93 
0.95 
0.88 
0.91 
0.95 
0.91 
0.93 0.67 
0.98 
0.65 0.71 
0.72 
0.79 
0.82 
0.45 
OA 
0.95 
0.94 
0.91 
0.81 
0.60 
0.92 
0.93 0.84 
0.76 
0.65 0.89 
0.86 
0.95 
0.75 
0.52 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Ralph Martin, Mingming Cheng and the anonymous reviewers for the valuable discussions. We thank Jiaming Lu for his work in the experiment. This work was supported by the Natural Science Foundation of China (Project Number 61120106007,61521002,61373069), Research Grant of Beijing Higher Institution Engineering Research Center, and Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology. Songhai Zhang is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Chinese traffic sign template</title>
		<ptr target="http://www.bjjtgl.gov.cn/jgj/jgbz/index.html" />
		<imprint>
			<biblScope unit="page" from="2015" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marqués</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno>ab- s/1312.2249</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fast R-Cnn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">How good are detection proposals, really? In BMVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An empirical evaluation of deep learning on highway driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pazhayampallil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng-Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mujica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1504.01716</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia, MM &apos;14</title>
		<meeting>the ACM International Conference on Multimedia, MM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with hinge loss trained convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>ab- s/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse-representation-based graph embedding for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1515" to="1524" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Road-sign detection and recognition based on support vector machines. Intelligent Transportation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maldonado-Bascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lafuente-Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gil-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gomez-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lopez-Ferreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="278" />
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>ab- s/1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on</title>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="2809" to="2813" />
		</imprint>
	</monogr>
	<note>Neural Networks (I-JCNN)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: a multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Scalable, high-quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno>abs/1412.1441</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
