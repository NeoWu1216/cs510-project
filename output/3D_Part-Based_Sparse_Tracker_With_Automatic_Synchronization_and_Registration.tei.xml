<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Part-Based Sparse Tracker with Automatic Synchronization and Registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Bibi</surname></persName>
							<email>adel.bibi@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
							<email>tianzhu.zhang@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Part-Based Sparse Tracker with Automatic Synchronization and Registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a part-based sparse tracker in a particle filter framework where both the motion and appearance model are formulated in 3D. The motion model is adaptive and directed according to a simple yet powerful occlusion handling paradigm, which is intrinsically fused in the motion model. Also, since 3D trackers are sensitive to synchronization and registration noise in the RGB and depth streams, we propose automated methods to solve these two issues. Extensive experiments are conducted on a popular RGBD tracking benchmark, which demonstrate that our tracker can achieve superior results, outperforming many other recent and state-of-the-art RGBD trackers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual object tracking is a classical and very popular problem in computer vision with a plethora of applications such as vehicle navigation, human computer interface, human motion analysis, surveillance, and many more. The problem involves estimating the location of an initialized visual target in each frame of a video. Despite numerous object tracking methods that have been proposed in recent years <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref>, most of these trackers suffer a degradation in performance mainly because of several challenges that include illumination changes, motion blur, complex motion, out of plane rotation, and partial or full occlusion, while occlusion is usually the most contributing factor in degrading the majority of trackers, if not all of them.</p><p>Fortunately, there is a recent surge in the availability of affordable and increasingly reliable RGBD sensors that provide image and depth data such as the Microsoft Kinect, Asus Xtion, and PrimeSense. When depth data is available, there exist more visual cues that can help resolve the nuisances of object tracking, especially occlusion. Recently, a relatively large RGBD dataset for visual tracking (100 videos) was compiled and released to the public <ref type="bibr" target="#b26">[27]</ref> in the form of an online competition, where the ground truth object tracks are mostly suppressed. Since then, much deserved attention has been brought towards developing robust RGBD visual trackers. Although only a few of these <ref type="bibr">Figure 1</ref>. Top: Shows the tracking results on the videos "bear front", "face occ 5", and "new ex occ 4" respectively comparing our method, DS-KCF <ref type="bibr" target="#b8">[9]</ref>, and Princeton RGBD tracker <ref type="bibr" target="#b26">[27]</ref>. Bottom: Shows 3D cuboids and their parts for different sequences as proposed in this paper. trackers exist, they easily and with a big margin outperform state-of-art RGB trackers on the same videos. This is clear evidence of how depth information can be useful to visual tracking, especially regarding early and robust detection of occlusion and proper model update, both of which can significantly boost the performance of any tracker.</p><p>Along with providing an RGBD benchmark <ref type="bibr" target="#b26">[27]</ref>, Song et al. proposed an RGBD tracker that performs quite well on the benchmark. They adopt an exhaustive search paradigm coupled with an SVM classifier trained on image/depth HOG and point cloud features, which reduces the tracker's runtime to only 0. <ref type="bibr" target="#b25">26</ref> FPS. An occlusion handling method is also adopted, where the tracked object is assumed to be contained in the plane closest to the camera. This method of handling occlusions makes it difficult to properly update the target's depth throughout the frames.</p><p>Recently, the work in <ref type="bibr" target="#b8">[9]</ref> incorporated depth information into the popular Kernelized Correlation Filter tracker <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref>. This method demonstrates very promising performance with real-time speeds of up to 40 FPS. However, tracking is still done in the 2D image plane, which might not make use fully of the depth information. Also, an occlusion handling method was proposed that is very similar in spirit to <ref type="bibr" target="#b26">[27]</ref>, thus, suffering from similar difficulties.</p><p>Part-based RGB trackers have been prevalent for a while now. They demonstrate very desirable performance as compared to RGB trackers that use only one part, owing to the fact that an object can still be tracked as long as some (not necessarily) all its parts are visible in the next frame. This is most helpful in handling partial occlusions and recovering from complete occlusions. Inspired by this line of work <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref>, we propose a 3D part based-tracker, whereby the parts not only help in representing the target but also support the detection of partial or full occlusion.</p><p>When testing our tracker and other 3D methods on the RGBD Princeton benchmark <ref type="bibr" target="#b26">[27]</ref>, we realized that the provided RGB and depth sequences contain many synchronization and registration errors. A synchronization error occurs when an RGB frame is assigned to an incorrect depth frame in the sequence. This happens because the RGB and depth video streams are usually generated independently and although the frame rate of both cameras is similar, it might fluctuate slightly over time and there might also be dropped frames. This can cause substantial errors when tracking is done in 3D, since the projection to the image plane is affected. For example, even if the 3D tracking results are accurate, their resulting 2D bounding boxes, which are in turn used to evaluate the RGBD tracker, will not be since they are with respect to the depth image plane while the ground truth is in the RGB image plane. On the other hand, a registration error usually occurs due to imprecise calibration between the RGB and depth cameras. This imprecision manifests itself in erroneous assignment of depth values to RGB pixels. For example, background RGB pixels are assigned the target's depth values and vice versa. Due to both types of errors, the tracking performance of methods, which perform representation, sampling, and tracking in 3D, can be significantly affected. In fact, we suspect that this is the reason why the authors of a very recent RGBD tracker <ref type="bibr" target="#b8">[9]</ref> mention that they mandate these errors be rectified (mostly probably manually) before tracking can be performed. In this paper, we suggest a simple yet effective method to automatically alleviate many of these errors that plague 3D tracking and possibly other applications that make use of RGBD stream data.</p><p>Contributions This work makes two main contributions. (i) To the best of our knowledge, we propose the first generic 3D part-based tracker with a very effective part-based oc-clusion handling method with desirable performance. At the time of submission, outranks all other trackers in the online Princteon RGBD benchmark <ref type="bibr" target="#b26">[27]</ref>. (ii) We propose a simple method to synchronize and register RGBD videos for the purpose of single object tracking. We also provide to the vision community the manual synchronization and registration of the videos provided in the benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>RGB Trackers. In general, they can be divided into two main categories: discriminative and generative methods. Discriminative trackers formulate visual object tracking as a binary classification problem that searches for the target location that is the most distinctive from the background. Examples of discriminative trackers include multiple instance learning tracking (MIL) <ref type="bibr" target="#b4">[5]</ref>, ensemble tracking <ref type="bibr" target="#b3">[4]</ref>, support vector tracking <ref type="bibr" target="#b2">[3]</ref>, correlation filter based trackers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref>, multiple experts entropy minimization (MEEM) tracker <ref type="bibr" target="#b31">[32]</ref> and other multi-object based trackers like the tracklet association with identity constraints <ref type="bibr" target="#b27">[28]</ref>. On the other hand, a generative tracker searches for a candidate target that is best represented by its current appearance model. As such, the major contribution of this type of tracker is in developing suitable and versatile representative models that can reliably describe the object even when it undergoes different appearance changes. Examples of generative models include the mean shift <ref type="bibr" target="#b10">[11]</ref>, incremental (IVT) <ref type="bibr" target="#b22">[23]</ref>, fragment-based (Frag) <ref type="bibr" target="#b0">[1]</ref>, L1-min <ref type="bibr" target="#b20">[21]</ref>, multitask (MTT) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref>, low-rank sparse <ref type="bibr" target="#b32">[33]</ref>, exclusive context modelling based tracker <ref type="bibr" target="#b35">[36]</ref>, occlusion detection based structural sparse learning based tracker <ref type="bibr" target="#b36">[37]</ref> and structural sparse tracker <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGBD Trackers.</head><p>As for the RGBD domain, only a limited number of generic methods exist in the literature, owing to the novelty of the problem and the only recent availability of RGBD tracking data. In <ref type="bibr" target="#b26">[27]</ref>, a computationally expensive tracker is proposed that combines the SVM scores of two tracking-by-detection instances: one based on RG-B and depth HOG features and the other is based on point cloud features. This scoring function is used to evaluate a dense sampling of 3D non-overlapping cells, thus, incurring a large computational cost. In our proposed tracker, we avoid this unnecessary computation induced by naive 3D sampling by exploiting the object's part-based structure as well as its previous motion. In doing so, only a very small number of 3D samples need to be evaluated at any given time. Also, the authors of <ref type="bibr" target="#b26">[27]</ref> propose an expensive depth based histogram segmentation method (based on a strict assumption that the object lies in the nearest plane) to detect occlusion, unlike our tracker that infuses the occlusion handling scheme directly in the tracking process without any additional complexity.</p><p>Moreover, the conventional and high-speed KCF tracker Overall pipeline for the proposed method, including the three major modules synchronization, registration, and 3D tracking. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> was adapted to the RGBD domain in <ref type="bibr" target="#b8">[9]</ref>. Similar to <ref type="bibr" target="#b26">[27]</ref>, occlusion is claimed to be handled by a segmentation of the depth histogram followed by a connected component analysis to incorporate spatial features of the object. As mentioned in the paper, the results of <ref type="bibr" target="#b8">[9]</ref> on the Princeton benchmark relied on an apriori synchronization and realigning of the provided RGB and depth sequences. To the best of our knowledge, this process was most probably done manually, since no elaboration was given on the method used. In this paper, we show how this issue can be alleviated automatically. In <ref type="bibr" target="#b16">[17]</ref>, a 2D particle filter was adopted in which the sampling variance of each individual particle changes according to its occlusion state. Despite its good performance on the benchmark, its representation and motion models are both constructed in 2D, unlike our method that natively treats the problem in 3D, from both representation and motion perspectives. In <ref type="bibr" target="#b11">[12]</ref>, authors build an adaptive boosting classifier from a pool of features (color, depth, and grayscale) that is incrementally re-trained from frame-to-frame. Similar to the previous method, this tracker operates solely in 2D representations with no clear method of handling occlusion. Other 3D trackers usually rely on a target-specific 3D object model, which is known apriori <ref type="bibr" target="#b29">[30]</ref>. For example, the trackers of <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref> assume a 3D model of a hand is provided before tracking begins. Other methods like <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b9">10]</ref> are category-specific RGBD trackers, which are mainly used for human detection and tracking.</p><p>In this paper, we propose a 3D particle filter tracker that exploits sparse representation, object structure (parts), as well as, adaptive particle sampling and pruning, all in a unified framework. We also present a simple yet effective method to automatically re-synchronize and re-register RG-B and depth pairs for better tracking performance. Through extensive experiments, we show that our tracker outperforms all the state-of-the-art methods in Princeton RGBD benchmark by ranking first and third on the manually and automatically synchronized and registered data respectively in the online evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Part-Based Sparse 3D Tracker</head><p>In this paper, we propose a 3D part-based particle-filter tracker, where both representation and motion models are constructed entirely in 3D. We also propose a simple yet effective method for handling structural information provided by the object parts and how it is used for both representation and particle pruning. We represent target candidates (particles) and their parts using a linear sparse representation. Moveover, an occlusion handling scheme supported by the part representation is integrated with the motion model so as to adaptively guide/direct how particles are sampled in the next frame. As compared to tracking-by-detection methods that densely sample the 3D space, our strategy produces only a small number of particles needed for 3D tracking. As mentioned earlier, synchronized and registered data is exceptionally important for 3D trackers; therefore, we propose an automatic method to synchronize and register RGB-D sequences and apply it to those in the Princeton RGBD benchmark <ref type="bibr" target="#b26">[27]</ref>. The overall pipeline of our method is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Representation Model</head><p>Generative RGB trackers <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref> and most RGBD trackers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9]</ref> define their target candidates (e.g. particles) to be 2D bounding boxes in the RGB or depth image plane, from which features for representation are extracted. In our formulation, we use a particle filter to sample 3D cuboid candidates. To limit the number of particles to a practical number, we propose an adaptive data-driven sampling approach. Each particle cuboid is divided into a pre-defined number of overlapping parts, each of which is defined also as a 3D cuboid. For convenience, all particles have the same part layout. One of these parts covers 65% volume of the entire particle (cuboid) and is located at its center as seen in <ref type="figure">Figure 1</ref>. This part captures holistic object information, while the other smaller parts capture information from different sides of the object (refer to <ref type="figure">Figure 1</ref>). Clearly, other (possibly hierarchical) part layouts can be used here too. Each part is represented using a m = 13 dimensional feature: ten color names and three 3D shape features, as proposed in <ref type="bibr" target="#b26">[27]</ref>. We model each particle part as a sparse linear combination of a set of dictionary elements. To do this, we adopt a similar approach as in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref>. At the first frame, we collect several observations of the object (and its parts) by sampling multiple cuboids around its ground truth location. We build K = 2 sparsifying dictionaries (using KSVD <ref type="bibr" target="#b1">[2]</ref>, one for each type of feature and for each part. Therefore, the representation of each particle can be described mathematically as follows:</p><formula xml:id="formula_0">min X k K k D k X k − Y k 2 F + λ X k 1,1 ,<label>(1)</label></formula><p>where D k denotes the dictionary corresponding to the k th feature type, such that</p><formula xml:id="formula_1">D k = [D 1k |D 2k |...|D N k |I m k ], whereD ik ∈ R m k ×n k ,</formula><p>where m k and n k are the dimensionality of the k th feature space and the number of atoms in dictionary k of the part i respectively. I m k is an identity matrix that encodes sparse error (e.g. partial occlusion); therefore, D k ∈ R m k ×(N n k +m k ) . For computational reasons, these dictionaries are not updated with time. We concatenate the k th feature type for all the parts of a particle in the observation matrix Y k ∈ R m k ×N , where N is the total number of parts (N = 9 in our experiments). The resulting sparse code matrix is denoted as X k ∈ R (N n k +m k )×N . The optimization in Eq <ref type="formula" target="#formula_0">(1)</ref> is non-smooth but convex. It is the matrix form of the popular Lasso problem. It can be efficiently solved using a number of methods, including Alternating Direction Method of Multipliers (ADMM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Temporal Coherence on Part Structure</head><p>Unlike sampling in 2D where the image plane is dense, most of the 3D point cloud scene is in fact empty. This may result in empty particle parts or completely empty particles. Scoring particles that are partially or completely empty using the objective in Eq <ref type="formula" target="#formula_0">(1)</ref> is not appropriate because the objective is not representative of these instances. That is, if one of the parts has disappeared, one of the two very different scenarios have occurred. Either the part is occluded, or the particle having that part is not representative enough. The first has to be associated with a low cost, while the latter with a high one. In Eq (1), an empty part (one of the columns of Y k is the zero vector) will result in a the trivial solution of Eq (1) for both scenarios. That means the objective will favor parts that are empty; thus, leading the tracker to drift into completely empty regions in 3D space. Similarly, setting the cost too high for empty parts within a cuboid would favor dense regions and will lead to favoring any nearby object, even if it were the occluder.</p><p>To address this issue, we make use of the temporal coherence of the object's part structure, which is modeled using the distribution of 3D points within each part of the current target. For simplicity, each part is described with a single binary value (1 or 0), which depicts whether that part is empty or not. As such, each particle is described using an N -dimensional binary vector, denoted as the part-based structure feature. We expect that this feature changes gradually with time, i.e. many parts tend not to abruptly disappear or re-appear in the next frame. To preserve structural information in between consecutive frames and to determine occluded parts, we use the hamming distance between the binary structure feature of the current target and that of each particle sampled in the next frame. Only those particles with the minimum hamming distance are selected and represented using Eq (1). This strategy also helps prune unlikely particles, thus, substantially reducing the overall computation time needed for representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motion Model</head><p>In this part, we give a detailed explanation of how we use particle filters in our tracker. But first, we give a brief of particle filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Particle Filter</head><p>The particle filter is a Bayesian sequential important sampling technique for estimating a posterior distribution of state variables x t that characterize a dynamic mode. It consists of two major steps, the prediction and the update for re-sampling. At the new instance t, a new observation z t is available and the new probability distribution given all observations and the previous state is given by:</p><formula xml:id="formula_2">p(x t |z 1:t ) = p(z t |x t )p(x t |z 1:t−1 ) p(z t |z 1:t−1 )<label>(2)</label></formula><p>As for the update step, it is based on p(z t |x i t ). The higher the probability is, the higher the weight is. As such, more particles will be sampled from a state that has been observed with a higher probability. It is computed as follows:</p><formula xml:id="formula_3">w i t = w i t−1 p(z t |x t )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Adaptive Directed Sampling</head><p>Owing to the way objects move in 3D and since our particles are modeled as cuboids, we take the state vector x t to represent a 3D rigid body transformation, i.e. x t ∈ R 6 , which is characterized by a 3D rotation and 3D translation. Note that scale could be incorporated in this setup; however, in most cases and unlike the 2D scale, the 3D scale of a target does not change dramatically from its initial value.</p><p>Depending on the state of the object (whether it is occluded or not), the motion model changes accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No occlusion.</head><p>To not sample particles unnecessarily, we do not use a zero-order motion model. Instead, we use 2D optical flow on the current target to the next RGB frame to compute a crude estimate of its new 3D location. Since only a crude estimate is needed, most optical flow methods can be used here. So, for mainly computational reasons, the basic Horn-Schunck optical flow <ref type="bibr" target="#b15">[16]</ref> is sufficient. In fact, we experimented with more sophisticated large-displacement methods (e.g. the work in <ref type="bibr" target="#b7">[8]</ref>), only to find that the tracking performance is only subtly affected. Given the pixel correspondences from optical flow, we can get a crude estimate of the rigid body transformation (rotation R and translation t) of the current target. Then, we sample the particle states using a Gaussian distribution centered at the previous estimate (R, t) and with a diagonal covariance. Since the orientation of a target cuboid does not change much over time (e.g. a human walking on a planar surface), we set the variances of the translation parameters to be much larger than those of the rotation.</p><p>Occlusion. When the target is determined to be in an occlusion state (as we will describe in the next section), optical flow is no longer a valid measure. Therefore, we resort back to a zero-mean motion model with large translational variance, so as to recapture the target when it re-appears.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Occlusion Handling</head><p>We integrate the occlusion handling scheme with the particle filter formulation. As discussed earlier, each sampled particle represents a cuboid, which contains a certain number of 3D points. In case of occlusion, we observe that this number tends to decrease significantly in all particles all at once. By monitoring this change, we can determine if an object is being occluded or not. Since image resolution is inversely proportional to the distance from the camera, the number of points in a cuboid is also inversely proportional with depth. Therefore, we need to compensate for the number of points in depth by computing a depth-normalized</p><formula xml:id="formula_4">measure:t j = z j z 1 2 t j .</formula><p>Here, z j and z 1 are the average depth values in the 3D cuboids in frame j and the first frame respectively. t j andt j are the original and the depth adjusted average number of points in the current 3D cuboid in frame j. In case of an occlusion at frame j, the depthnormalized number of points among all particles will be very low. If the average depth adjusted falls below some threshold compared to the average number of points in the first frame, the object is identified to be in an occlusion state as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Synchronizing and Registering RGBD Images</head><p>There are several videos in Princeton RGBD benchmark <ref type="bibr" target="#b26">[27]</ref> with registration and synchronization issues that can substantially affect 3D tracking performance. We consider this as a fundamental problem that needs to be addressed because it not only affects 3D trackers but many other computer vision applications that involve operations on 3D data. In fact, it is clearly stated in previous work <ref type="bibr" target="#b8">[9]</ref> that the RGB and depth sequences need to be synchronized and realigned properly before tracking can be applied. In the following, we propose a method to solve both problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Synchronization</head><p>As explained earlier, synchronization problems arise in RGBD videos because the RGB and depth streams tend to be acquired independently, with different frame rates, and sometimes frames are dropped. A commonly used method to resolve this issue is to assign a depth image to every RGB image that has the closest time stamp. This is what is done in the benchmark <ref type="bibr" target="#b26">[27]</ref>. Obviously, this issue can also be resolved from a hardware perspective by simply increasing the frame rate of both cameras, thus, reducing the effect of time stamp offset. In this work, we formulate the problem as a matching task to alleviate the synchronization issues.</p><p>We take the RGB sequence as reference. We seek to match a depth image to every image in the RGB sequence. An RGB image is allowed to match to any depth image, whose time stamp is close enough to that of the RGB image. We start with a manually synchronized pair of RGB and depth images in the first frame, denoted as I 1 RGB and I 1 D respectively. Under the assumption that the depth values of a scene change smoothly between consecutive frames, we can compute a large-displacement optical flow <ref type="bibr" target="#b7">[8]</ref> between the current synchronized RGB image I 1 RGB and the next one in the sequence I 2 RGB . Only point correspon-</p><formula xml:id="formula_5">dences {(x 1 i , y 2 i )} p i=1</formula><p>with a large enough flow magnitude are maintained, since moving points give clear indication of when synchronization errors occur. Then, we generate a synthetic depth imageÎ 2 D by transferring over depth values, i.e.Î 2 D (y 2 i ) = I 1 D (x 1 i ). If C is the set of frame indices identifying the depth images that are close in time stamp to I 2 RGB , then we can synchronize I 2 RGB to the depth image I j * D , whose depth values at {y 2 i } p i=1 are closest to those in I 2 D . We formulate this mathematically as follows:</p><formula xml:id="formula_6">j * = argmin j∈C p i=1 Î 2 D (y 2 i ) − I j D (y 2 i ) 2<label>(4)</label></formula><p>This strategy can be applied iteratively (refer to <ref type="figure" target="#fig_3">Fig. 4</ref>) until all the images in the RGB sequence are synchronized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Registration</head><p>Several videos in the Princeton RGBD benchmark <ref type="bibr" target="#b26">[27]</ref> also suffer from incorrect registration between synchronized RGB and depth image pairs. Being able to correctly register these pairs is very important, since this registration defines the correspondences between pixels in the RGB image and those in the depth image, which in turn enable the generation of the 3D point cloud of the scene.</p><p>The usual strategy for registering these image pairs is to stereo calibrate both the depth and RGB cameras together. This calibration can be used to map pixels from one of them to the other. This strategy is used in the RGBD benchmark <ref type="bibr" target="#b26">[27]</ref>; however, in many cases, registration errors do occur and they could be as large as 30-40 pixels. Such an offset can negatively affect any RGBD based tracker, especially one that tracks the object in 3D. This offset is not uniformly random at each pixel, since the source of the error arises from perturbations in the extrinsic calibration parameters, i.e. the rotation and translation that transforms the coordinate system of the RGB camera to that of the depth. By assuming that the perturbation in the rotation is negligible w.r.t. the perturbation in the translation, it can be shown that the per-pixel offset in registration varies with the depth and image location of the pixel (refer to supplementary material). However, this variation is structured. For example, neighboring pixels with similar depth values tend to have very similar offsets. This is why some of the registration errors in the benchmark (corresponding to videos showing a predominant foreground object infront of a far away background) can be easily fixed by simply translating the whole depth image in a single direction. To estimate the registration offsets from one frame to the next, we formulate a structured selection problem as shown in Eq (5). We only require the first RGBD image pair to be correctly registered.</p><formula xml:id="formula_7">min zi∀i 1 p p i (d i 1 − d i 2 T z i ) 2 s.t z i ∈ {0, 1} N ∀i, 1 T z i = 1 ∀i, rank(Z) ≤ r<label>(5)</label></formula><p>Here, d i 1 is the depth of pixel i in the previous RGBD image pair, which is assumed to be correctly registered. We assume that the offset of this pixel in the next frame can be one of N possible offsets. The vector d i 2 ∈ R N contains the depth values in the next depth image corresponding to the N possible offsets. Moreover, the binary vector z i can be viewed as a selection vector that chooses only one of the offset depth values in the next frame for pixel i. The selection vectors Z = [z 1 |...|z p ] for the p pixels (i.e. matched pixels using optical flow) must be constrained to follow the perturbation model described above. Since neighboring pixels with similar depths tend to have the same offset, we only expect a small number of distinct offsets to be selected among the p pixels. We formulate this as a low-rank constraint on the binary matrix Z.</p><p>Computing the global solution of this binary problem cannot be done in polynomial time, so we seek a tradeoff between solution quality and computational efficiency. We observe that when r = 1, any feasible matrix Z is all zeros except for an entire row. We can exhaustively evaluate all N feasible matrices and return the one leading to the smallest objective value. This is equivalent to selecting a single offset for all p pixels. We can exploit this observation to efficiently compute an approximate solution when r &gt; 1. This can be done in two ways. (i) We can pre-cluster the p pixels (e.g. according to their depth values d i 1 and their image locations) and then find the best rank-1 solution for each cluster independently. (ii) We first find the best rank-1 solution and remove all pixels whose offset has been found (i.e. their contribution to the overall objective is zero). Then, we reiterate this process on the remaining pixels at most (r −1) times or until no pixels are left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>To evaluate the performance of our proposed tracker, we conduct extensive experiments on the well-known Prince- ton RGBD benchmark <ref type="bibr" target="#b26">[27]</ref>, comprising a total of 100 video sequences only five of which have publicly available ground truth tracks. The videos contain many challenges including partial and full occlusion, fast motion, out of plane rotation, background clutter, moving camera, and shape deformation and distortion. For comparison, the authors of <ref type="bibr" target="#b26">[27]</ref> provide an online evaluation system that compares a tracker's performance with that of 20 others, 12 of which use depth information while the rest are popular state-of-the-art RGB trackers. The evaluation for the comparisons among trackers was based on the intersection over union criterion (IOU). For the details of the tracking criterion used for evaluation, reader is referred to <ref type="bibr" target="#b26">[27]</ref>. To evaluate the merits of our synchronization and registration method, we compare our tracker on both the RGBD data provided in the benchmark and the same data after both methods are applied. Moreover, we empirically validate the benefits of using multiple parts instead of a holistic representation for tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>All our experiments are done using MATLAB R2014b on a 3.07GHz Intel(R) Xeon(R) with 48GB RAM. The number of parts N = 9, with K = 2 dictionaries learnt for each part. Only a total of 20 particles were used in our method. As for particle sampling, we set the standard deviations of the translation component to {0.05, 0.05, 0.03} for when the object is not in a state of occlusion and {0.35, 0.10, 0.2} when it is. To reduce computational cost, we do not sample the rotation obtained from point correspondences. Moreover, we set the Lasso parameter λ = 0.05 as a decent tradeoff between sparsity and meaningful reconstruction. The target is in an occlusion state, when the depth-normalized average number of points in a frame for all particles falls below 0.2t 1 , wheret 1 is the average number of points inside the cuboid in the first frame.</p><p>As for synchronization, any RGB image is allowed to match to one depth image that is within 5 frames from it. For the registration problem, we use the offset in the previous frame as an initialization for the next one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of Design Choices</head><p>In the following, we show the impact on tracking performance of using the automatically synchronized and registered data and compare it to the provided benchmark data <ref type="bibr" target="#b26">[27]</ref>. In the defense of parts, we see a significant performance improvement when multiple parts are considered.</p><p>Synchronization results. As discussed earlier, around 14% of Princeton RGBD benchmark <ref type="bibr" target="#b26">[27]</ref> videos have synchronization error, while an additional 8% also require reregistration. Since this is very important for 3D based trackers, <ref type="table">Table 1</ref> shows our proposed tracker results on the RGB-D ground truth data provided by <ref type="bibr" target="#b26">[27]</ref> and the same data after our proposed method of synchronization is used. For both the fast motion and occlusion categories, we observe a notable improvement in performance (4% and 5% respectively), since the un-synchronized RGB and depth image pairs in these categories tend to be significantly different. Registration results. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates the problem of registration with a qualitative example. Around 12% of the testing videos provided by <ref type="bibr" target="#b26">[27]</ref> suffer from severe registration errors. <ref type="table">Table 1</ref> shows tracking results on the videos as given by <ref type="bibr" target="#b26">[27]</ref> and on the data registered using our proposed method. Significant improvement in performance is noted in almost all categories Again, this is mainly because 3D based trackers project the points back into the depth frame's reference, while the evaluation ground truth is constructed in the RGB image. Moreover, unregistered data contaminates the representation of the target because color attributes of the background appear in the target and vice versa. In the defense of parts. <ref type="table">Table 1</ref> summarizes the performance of our method when only one holistic part is used. Clearly, adding multiple parts substantially improves performance across all tracking categories. It is evident that parts help provide a more robust representation of the target, as well as, structural information that is important to prune unnecessary and possibly confusing particles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on the RGBD Tracking Benchmark</head><p>In <ref type="table">Table 2</ref>, we summarize the performance of our tracker on both the manually and automatically synchronized and registered data compared to the top-5 closest trackers on a total of 95 videos. We use the same category breakdown as the online benchmark. The list of competing trackers was shortened to show only the best 5 methods (refer to the supplementary material for the entire table). Our part-based sparse tracker ranks first among all other methods on the manually synchronized and registered data while ranking third when the automatically synchronized and registered data is used. In some categories (e.g. Human), it registers an improvement of 7% over the second best tracker. This is attributed to the use of parts and the temporal coherence in their structure, which is a reasonable assumption for humans. Other categories (e.g. Animal and Fast Motion) show the target interacting in very close distance with other objects that have similar cues and/or exhibiting complex motion, thus, restricting tracking performance. Qualitative results. <ref type="figure" target="#fig_5">Figure (6)</ref> shows the tracking results in 3D of sample frames taken from two benchmark videos, namely "new ex occ 4 "and "three people". Both of these videos include examples of full occlusion. For visualization purposes, we only show two of the nine constituent parts, shown as red and blue cuboids. We also backproject these cuboids into the image plane as upright bounding boxes. The yellow cuboids are instances when the object is determined to be in an occlusion state. Notice how our tracker is able to easily recover from this full occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a 3D part-based sparse tracker, which exploits parts to preserve temporal structural information and help in particle pruning. A fast yet powerful method was proposed to embed occlusion detection in the motion model framework. Since 3D trackers are sensitive to synchronization and registration noise, we proposed methods to correct for both, especially since no less than 30% of the videos on the popular RGBD tracking benchmark <ref type="bibr" target="#b26">[27]</ref> suffer from these issues. Extensive experiments demonstrate the positive impact of each module of the proposed tracker. In fact, our tracker currently ranks first on the benchmark, as compared to many state-of-the-art trackers. For future work, we aim to exploit part-to-part spatial and appearance relationships to encode particles and to detect occlusion. Moreover, we plan to develop a strategy to incrementally build and maintain a prototypical 3D model of the target by registering its tracking results with time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overall pipeline for the proposed method, including the three major modules synchronization, registration, and 3D tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Shows the proposed occlusion handling scheme on video "new ex occ 4 ", where the number of points in all the particles decreases significantly in 3D space when the tracked object is occluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(a)&amp;(b) is a previously synchronized RGBD image pair. (c)&amp;(d) is an RGBD pair formed of the next RGB image in the sequence and its corresponding synthetic depth image generated using optical flow. (e) shows a set of possible matches to (c) most similar to (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Pairs (a)&amp;(b) and (c)&amp;(d) denote the registered RGB and depth pairs as provided in the benchmark [27], compared with our method of registration on the video "new student center no occ" at frames 2 and 57 respectively. Similarly, we show pairs (e)&amp;(f) and (g)&amp;(h) for the video "new student center 3 " at frames 2 and 77 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Images (a)&amp;(c) show the tracking results in 3D where the red and blue cuboids are two different parts tracked. The yellow cuboid is an occlusion detection. Images (b)&amp;(d) show the corresponding 2D tracking results. The videos from top to bottom are "new ex occ 4 "and "three people "respectively.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust fragmentsbased tracking using the integral histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR), 2006</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="798" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">K-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1064" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<title level="m">Ensemble tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="261" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="983" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-template scale-adaptive kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2544" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time rgb-d tracking with depth scaling kernelised correlation filters and occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hannuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paiement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bristol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE British Machine Vision conference (BMVC)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting and tracking people using an rgb-d camera via multiple detector fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1076" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="564" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive multi-cue 3D tracking of arbitrary objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">booktitle=IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tracking via robust multi-task multi-view joint sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Society for Optics and Photonics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="319" to="331" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
	<note>Technical symposium east</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Occlusion aware particle filter tracker to handle complex and persistent occlusions. Computer Vision and Image Understanding (CVIU)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O H S Y L</forename><surname>Kourosh Meshgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Under Review</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision Workshops (ECCVW)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="254" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">People tracking in rgb-d data with on-line boosted target models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3844" to="3849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5388" to="5396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust visual tracking using &amp;# x2113; 1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient model-based 3d tracking of hand articulations using kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE British Machine Vision conference (BMVC)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incremental and decremental support vector machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<idno>13:409</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Realtime and robust hand tracking from depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1106" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive appearance modeling for video tracking: Survey and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4334" to="4348" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tracking revisited using rgbd camera: Unified benchmark and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simultaneous clustering and tracklet linking for multi-face tracking in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2856" to="2863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular multiview object tracking with 3d aspect parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meem: Robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="188" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Low-rank sparse learning for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="470" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust visual tracking via multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2042" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust visual tracking via structured multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="383" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust visual tracking via exclusive context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="63" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object tracking by occlusion detection via structured sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Partial occlusion handling for visual tracking via robust part matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1258" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structural sparse tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
