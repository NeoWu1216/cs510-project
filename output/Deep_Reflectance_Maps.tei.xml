<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reflectance Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Rematas</surname></persName>
							<email>krematas@esat.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">MPI Informatics</orgName>
								<orgName type="institution" key="instit3">Efstratios Gavves University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">MPI Informatics</orgName>
								<orgName type="institution" key="instit3">Efstratios Gavves University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
							<email>t.ritschel@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">MPI Informatics</orgName>
								<orgName type="institution" key="instit3">Efstratios Gavves University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<email>mfritz@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">MPI Informatics</orgName>
								<orgName type="institution" key="instit3">Efstratios Gavves University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves@uva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">MPI Informatics</orgName>
								<orgName type="institution" key="instit3">Efstratios Gavves University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Nl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">MPI Informatics</orgName>
								<orgName type="institution" key="instit3">Efstratios Gavves University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Tuytelaars</surname></persName>
							<email>tinne.tuytelaars@esat.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">MPI Informatics</orgName>
								<orgName type="institution" key="instit3">Efstratios Gavves University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leuven</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">MPI Informatics</orgName>
								<orgName type="institution" key="instit3">Efstratios Gavves University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reflectance Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Undoing the image formation process and therefore decomposing appearance into its intrinsic properties is a challenging task due to the under-constrained nature of this inverse problem. While significant progress has been made on inferring shape, materials and illumination from images only, progress in an unconstrained setting is still limited. We propose a convolutional neural architecture to estimate reflectance maps of specular materials in natural lighting conditions. We achieve this in an end-to-end learning formulation that directly predicts a reflectance map from the image itself. We show how to improve estimates by facilitating additional supervision in an indirect scheme that first predicts surface orientation and afterwards predicts the reflectance map by a learning-based sparse data interpolation.</p><p>In order to analyze performance on this difficult task, we propose a new challenge of Specular MAterials on SHapes with complex IllumiNation (SMASHINg) using both synthetic and real images. Furthermore, we show the application of our method to a range of image editing tasks on real images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A classic computer vision task is the decomposition of an image into its intrinsic shape, material and illumination. The physics of image formation are well-understood: the light hits a scene surface with specific orientation and material properties and is reflected to the camera. Factoring an image into its intrinsic properties, however, is very difficult, as the same visual result might be due to many different combinations of intrinsic object properties.</p><p>For the estimation of those properties, a common practice is to assume one or more properties as known or simplified and try to estimate the others. For example, traditional Project: http://homes.cs.washington.edu/˜krematas/ DRM/ approaches to intrinsic images or shape-from-shading assume lambertian materials, or point lights. Furthermore, to simplify the problem, shape is often either assumed to be known in the form of a 3D model, or it is restricted to simple geometry such as spheres.</p><p>In this work, we extract reflectance maps <ref type="bibr" target="#b13">[14]</ref> from images of objects with complex shapes and specular material, under complex natural illumination. A reflectance map holds the orientation-dependent appearance of a fixed material under a fixed illumination. It does not attempt to factor out material and/or illuminant and should not be confused with a reflection map that contains illumination <ref type="bibr" target="#b5">[6]</ref> or with surface reflectance <ref type="bibr" target="#b4">[5]</ref>.</p><p>Under the assumptions of a constant material, no shadows, a distant light source and a distant viewer, the relation of surface orientation and appearance is fully described by the reflectance map. It can represent all illuminants and all materials, in particular specular materials under high-frequency natural illumination. Therefore, besides allowing for a better understanding and analysis of 2D imagery, the ability to estimate reflectance maps lends itself to a broad spectrum of applications, including material transfer, inpainting, augmented reality and a range of image editing methods.</p><p>The input of our system is a 2D image where an object from a known class, (e.g., cars), was segmented <ref type="figure" target="#fig_0">(Fig 1)</ref> and output is a reflectance map. To this end, we propose two different approaches: The first approach directly estimates a reflectance map from the input image using an end-to-end learning framework based on CNNs and upconvolutions. The second approach decomposes the process in two steps, enabling the use of additional supervision in form of object surface normals at training time. For the second approach we first predict per-pixel surface normals, which we use to compute sparse reflectance maps from the visible normals of the objects. Given the sparse reflectance map, we introduce a learned sparse data-interpolation scheme in order to arrive at the final reflectance map. In summary, we make the following five key contributions:</p><p>• First end-to-end approach to infer reflectance maps from a 2D image of complex shapes of specular materials under natural illumination.</p><p>• Dataset based on synthetic images and real photographs that facilitates the study of this task.</p><p>• A CNNs/upconvolutional architecture to learn the complex mapping from the spatial 2D image to the spherical domain.</p><p>• A CNN addressing a data-interpolation task of sparse unstructured data.</p><p>• Demonstration of our approach on a range of real images and a range of image-based editing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Factoring images into their constituting components is an important goal of computer vision. It is inherently hard, as many combinations of factors can result in the same image. Having a decomposition available would help solving several important computer graphics and computer vision problems.</p><p>Factoring Images. Classic intrinsic images factor an image into illuminant and reflectance <ref type="bibr" target="#b2">[3]</ref>. Similarly, shapefrom-shading decomposes into reflectance and shading, eventually leading to an orientation map or even a full 3D shape. Larger-scale acquisition of reflectance <ref type="bibr" target="#b4">[5]</ref> and illumination <ref type="bibr" target="#b5">[6]</ref> have allowed to compute their statistics <ref type="bibr" target="#b6">[7]</ref> helping to better solve inverse and synthesis problems.</p><p>Recently, factoring images has received renewed interest. Lombardi and Nishino <ref type="bibr" target="#b23">[24]</ref> as well as Johnson and Adelson <ref type="bibr" target="#b14">[15]</ref> have studied the relation of shape, reflectance and natural illumination. A key idea in their work is, that under natural illumination, appearance and orientation are in a much more specific relation (as used in Photometric stereo <ref type="bibr" target="#b12">[13]</ref>) than for a single point light, where many similar appearance for totally different orientations can be present. They present different optimization approaches that allow for high-quality estimation of one component if at least one other component is known. In this work, we assume that the object is made of a single material and its object class and its segmentation mask are known. However, we do not aim at factoring out illuminant, reflectance and shape, but keep the combination of reflectance and illuminant and only factor it from the shape. Further factoring the reflectance map produced in our approach into material and illuminant would be complemented by methods such as <ref type="bibr" target="#b23">[24]</ref> or <ref type="bibr" target="#b14">[15]</ref>.</p><p>Baron and Malik <ref type="bibr" target="#b1">[2]</ref> factors shaded images into shape, reflectance and lighting, but only for scalar reflectance, i.e. diffuse albedo and for limited illumination frequencies. In a very different vein, Internet photo collections of diffuse objects can be used to produce a rough 3D shape that serves extracting reflectance maps in a second step <ref type="bibr" target="#b10">[11]</ref>.</p><p>A recent approach by Richter et al. <ref type="bibr" target="#b30">[31]</ref> first estimates a diffuse reflectance map using approximate normals and then refines the normal map using the reflectance map as a guide. Different from our approach, they assume diffuse surfaces to be approximated using 2nd-order spherical harmonics (SH) and learn to refine the normals from the reflectance map using a regression forest. We compare the reflectance maps produced by our approach to reflectance maps using an SH basis which are limited to diffuse materials.</p><p>Computer Graphics. While appearance is considered view-independent in intrinsic images, view-dependent shading is described by reflectance maps <ref type="bibr" target="#b13">[14]</ref>. In computer graphics, reflectance maps are popular and known as lit spheres <ref type="bibr" target="#b32">[33]</ref> or MatCaps <ref type="bibr" target="#b31">[32]</ref>. They are used to capture, transfer and manipulate the orientation-dependent appearance of photorealistic or artistic shading. A special user interface is required, to map surface orientation to appearance at sparse points in an image, from which orientations are interpolated for in-between pixels to fill the lit sphere (e.g. <ref type="bibr" target="#b29">[30]</ref> manually aligned a 3D model with an image to generate lit spheres). Small diffuse objects in a single cluttered image were made to appear specular or transparent using image manipulations with manual intervention <ref type="bibr" target="#b15">[16]</ref>. Our approach shares the simple and effective lit half-sphere parametrization but automates the task of matching orientation and appearance.</p><p>Deep Learning. In recent years convolutional neural networks (CNNs) have shown strong performance across different domains. In particular, the strong models for object recognition <ref type="bibr" target="#b17">[18]</ref> and detection <ref type="bibr" target="#b9">[10]</ref> can be seen as a layer-wise encoder of successively improved features. Based on ideas of encoding-decoding strategies similar to auto-encoders, convolutional decoders have been developed <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b19">20]</ref> to decode condensed representations back to images. This has led to fully convolutional or deconvolutional techniques that have seen wide applicability for tasks where there is a per-pixel prediction target. In <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b11">12]</ref>, this paradigm has been applied to semantic image segmentation. In <ref type="bibr" target="#b0">[1]</ref> image synthesis was proposed given object class, view and view transformations as input and synthesizing segmented new object instances as output. Similarly, <ref type="bibr" target="#b18">[19]</ref> propose the deep convolutional inverse graphics networks with an encoderdecoder architecture, that given an image can synthesize novel views. In contrast, our approach achieves a new mapping to an intrinsic property -the reflectance map.</p><p>Deep lambertian networks <ref type="bibr" target="#b33">[34]</ref> apply deep belief networks to the joint estimation of a reflectance, an orientation map and the direction of a single point light source. They rely on Gaussian Restricted Boltzmann Machines to model the prior of the albedo and the surface normals for inference from a single image. In contrast, we address specular materials under general illumination, but without factoring material and illuminant.</p><p>Another branch of research proposes to use neural networks for depth estimation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>, normal estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref>, intrinsic image decomposition <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> and lightness <ref type="bibr" target="#b27">[28]</ref>. Wang et al. <ref type="bibr" target="#b34">[35]</ref> show that a careful mixture of deep architectures with hand-engineered models allow for accurate surface normal estimation. Observing that normals, depth and segmentations are related tasks, <ref type="bibr" target="#b7">[8]</ref> propose a coarse-to-fine, multi-scale and multi-purpose deep network that jointly optimizes depth and normal estimation and semantic segmentation. Likewise, <ref type="bibr" target="#b20">[21]</ref> apply deep regression using convolutional neural networks for depth and normal estimation, whose output is further refined by a conditional random field. Going one step further, <ref type="bibr" target="#b22">[23]</ref> propose to embed both the unary and the pairwise potentials of a conditional random field in a unified deep network. In contrast, our goal is not normal, but rather reflectance map estimation. In particular, our "direct approach" makes do without any supervision of normal information, while the "indirect approach" has normals as a by-product. In addition, our new challenge dataset captures reflectance maps and normals for the specular case, which are not well represented in prior recordings -in particular as also range sensors have difficulties on specular surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>Motivation We address a challenging inverse problem that is highly underconstrained. Therefore, any solution needs to mediate between evidence from the data and prior expectations -in our case over reflectance maps. In the general settings of specular materials and natural illuminations, modeling prior expectations over reflectance maps -let alone obtaining a parametric representation -seems problematic. This motivated us to follow a data-driven approach in an end-to-end learning framework, where the dependence of reflectance maps on object appearances is learnt from a substantial number of synthesized images of a given object class.</p><p>Overview The goal of our network is the estimation of the reflectance map of an object depicted in a single RGB image <ref type="figure" target="#fig_1">(Fig. 2</ref>). This is equivalent to estimating how a sphere <ref type="bibr" target="#b32">[33]</ref> with the same material as the object would look like from the same camera position and the same illumination. From the estimated reflectance map, we can make the association between surface orientation and appearance. This allows surface manipulation and transfer of materials and illumination between objects or even scenes.</p><p>We propose two approaches to estimate reflectance maps: a direct (Sec. 3.2) and an indirect one (Sec. 3.3). Both have a general RGB image as input and a reflectance map as an output. The indirect method also produces a conjoint per-pixel normal map.</p><p>Both variants are trained from and evaluated on the SMASHINg dataset introduced in detail in Sec. 4.1. For now, we can assume the training data to consists of pairs of 2D RGB images (domain) and reflection maps (range) in the parametrization explained in Sec. 3.1. This section now explains the two alternative approaches in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reflectance Map Representation</head><p>A reflectance map L(ω) ∈ S + → R 3 <ref type="bibr" target="#b13">[14]</ref> is a map from orientations ω in the positive half-sphere S + to the RGB radiance value L leaving that surface to a distant viewer. It combines the effect of illumination and material. For the case of a mirror sphere it captures illumination <ref type="bibr" target="#b5">[6]</ref> but is not limited to it. It also does not capture surface reflectance <ref type="bibr" target="#b4">[5]</ref>, which would be independent of illumination, but joins the two.</p><p>There are multiple ways to parameterize orientation ω. Here Horn <ref type="bibr" target="#b13">[14]</ref> used positional gradients which are suitable for an analytic derivation but less attractive for computation as they are defined on the infinite real line. We instead parameterize the orientation simply by s, t the normalized surface normal's x and y components. Dropping the z coordinate is equivalent to drawing a sphere under orthographic projection with exactly this reflectance map as seen right in <ref type="figure" target="#fig_1">Fig. 2</ref>. Note, that orientations of surfaces in an image only cover the upper half-sphere, so we only need to parameterize a half-sphere, avoiding to deal with spherical functions, e.g. spherical harmonics <ref type="bibr" target="#b30">[31]</ref>, that reduce the maximal frequency, only allowing for diffuse materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Direct approach: End-to-end model for prediction of reflectance maps</head><p>In the direct approach, we learn a mapping between the object image and its reflectance map, following a convolutional-upconvolutional architecture. The full architecture can be seen if figure <ref type="figure" target="#fig_2">Fig. 3</ref>. Starting from a series of convolutional layers, followed by batch normalization, ReLU and pooling layers, the size of the input feature maps is reduced to 1 × 1. After continuing with two fully connected layers, the feature maps are upsampled until the output size is 32 × 32 pixels. In all convolutional layers a stride of 1 is used and padded with zeros such that the output has the same size as the input. The final layer uses an euclidean loss between the RGB values for the predicted and the ground truth reflectance map.</p><p>In a typical CNN regression architecture, there is a spatial correspondence between input and output, e.g. in normal or depth estimation or semantic segmentation. In our case, the network needs to learn how to "encode" the input image so it can correspond to a specific reflectance map. This task is particularly challenging as the model has to learn not only how to place the image pixels to locations in the sphere (change from image to directional domain), but also to impute and interpolate appearance for unobserved normals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Indirect approach: Reflectance maps from inferred normals and sparse interpolation</head><p>The indirect approach proceeds in four steps: i) estimating per-pixel orientation maps from the RGB image. ii) upsampling the orientation map to the full available input image resolution. iii) changing from the image domain into the directional domain, producing a sparse reflectance map. iv) predicting a dense reflectance map from the sparse one.</p><p>The first and fourth step are model by CNN architectures, while the second and third step are prescribed transformations, related to the parametrization of the reflectance map. We will detail each step in the following paragraph. Orientation estimation Our goal in the first step is to predict a surface orientation map from the RGB image. Thanks to our parametrization of the directional domain to coordinates in a flat 2D image of a lit sphere, the task is slightly simpler than finding full orientation. We seek to find the s and t parameters according to our reflectance map parameterization.</p><p>We train a CNN to learn the s, t coordinates. The architecture of the network is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The network is fully convolutional as in <ref type="bibr" target="#b24">[25]</ref> and it consist of a series of convolutional layers followed by ReLU and pooling layers that reduce the spatial extend of the feature maps. After the fully convolutional layers, there is a series of upconvolutional layers that upscale the feature representation to half the original size. Finally, we use two euclidean losses between the prediction and the L 2 normalized ground truth normals. The first one takes into account the x, y, z coordinates of the normals, while the second only the x, y.</p><p>Orientation upsampling The orientations are estimated at a resolution of n = 128 × 128, so the number of appearance samples is in the order of ten-thousands. Most input images however are of much higher resolution with millions of pixels. A full-resolution orientation map is useful for resolving all appearance details in the orientation domain. The appearance of one orientation in the reflectance map can be related to all high-resolution image pixels (millions). Also intended applications performing shape manipulation in the 2D image (cf. Sec. 5.2) will benefit from a refined map. To produce a high-resolution orientation map, we used joint bilateral upsampling <ref type="bibr" target="#b16">[17]</ref> as also done in range image <ref type="bibr" target="#b3">[4]</ref>. Once we have an estimation of the object's normals, they can be mapped to a sphere and associated with appearance.</p><p>Change-of-domain We now reconstruct a sparse reflectance map from the orientation map and the input image. This is a prescribed mapping transformation: The pairs of appearance L i and orientation ω i in every pixel are unstructured samples of the continuous reflectance map function L(ω) we seek to recover. Our goal now is to map these samples from the image to the directional domain, constituting the reflectance map. The most straightforward solution is to perform scattered data interpolation, such as</p><formula xml:id="formula_0">L(ω) = ( n i=1 w( ω, ω i )) −1 n i=1 w( ω, ω i )L i , (1) where w(x) = exp(−(σ cos −1 (x)) 2 ) is an RBF kernel.</formula><p>In practice however, the orientation estimates are noisy and the requirements of a global reflectance map (infinite illumination, orthographic view, no shadows) are never fully met, asking for a more robust estimate. We found darkening due to shadows to be the largest issue in practice. Therefore, we perform a max operation over all samples closer than a threshold ǫ = cos(5 • ) instead of an average, as in</p><formula xml:id="formula_1">L(ω) = max{w( ω, ω i )L i }, w(x) = 1 if x &gt; ǫ 0 otherwise.</formula><p>If one orientation is observed under different amounts of shadow, only the one that is not in shadow will contribute -which is the intended effect. Still, the map resulting from this step is sparse due to normals that were not observed in the image as seen in <ref type="figure" target="#fig_4">Fig. 5 (left)</ref>. This requires imputing and interpolating the sparse data in order to arrive at a dense estimate.</p><p>SparseNet: (Sparse-to-dense) Learning-based approach to sparse data interpolation The result of the previous step is a sparse reflectance map. It is noisy due to errors from incorrect normal estimation and has missing information at orientations that were not observed in the image. Note, that the latter is not a limitation of the normal estimation, but even occurs for ground truth surface orientations: If an orientation is not present, its appearance remains unknown. A simple solution is to use Eq. 1 which already provides a dense output. We propose a learning-based approach to predict a dense reflectance maps from a sparse and noisy one. Accordingly, the network is trained on pairs of sparse and dense reflectance maps. The sparse ones are created using the first three steps explained (orientation from CNN, upsampling, Change-of-domain) on synthetic data where the target reflectance map is known by rendering a sphere.</p><p>The employed CNN architecture is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Input is the sparse reflectance map and output the dense one. We use the output of the convolutional layers as additional cue. After each upconvolution layer, we concatenate its output with the feature map from the respective convolution layer. Again an L 2 loss between the output and the dense reference reflectance map is used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The SMASHINg Challenge</head><p>We propose the Specular MAterials on SHapes with complex IllumiNation (SMASHINg) challenge. It includes a dataset (Sec. 4.1) of real as well as synthetic images, groundtruth reflectance maps and normals (where available), results from different methods to reconstruct and a set of metrics (Sec. 4.2) that we propose to evaluate and compare performance. At the time of publication we will make the data, baselines, our methods as well as the performance metrics publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>Our dataset combines synthetic images (Sec. 4.1.1), photographs (Sec. 4.1.2) and images from the web (Sec. 4.1.3) of cars. All images are segmented into foreground and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Synthetic images</head><p>Synthetic images are produced with random i) views, ii) 3D shapes, iii) materials, iv) illumination and v) exposure <ref type="figure">(Fig. 6)</ref> . The view is sampled from a random position around the object, looking at the center of the object with a FOV of 40 • . The 140 3D shapes come from the free 3D Warehouse repository, indexed by Shapenet <ref type="bibr" target="#b21">[22]</ref>. For each sample the object orientation around the y axis is randomized. Illumination is provided by 40 free HDR environment maps. The exposure is sampled over the "key" parameter of Reinhard et al.'s photographic tone mapper <ref type="bibr" target="#b28">[29]</ref> between 0.4 and 0.6. For materials, the MERL BRDF database <ref type="bibr" target="#b25">[26]</ref> containing 100 materials is used. Overall 60 k sample images from that space are generated. We define a training-test split so that no shape, material or illumination is shared between the training and test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Illumination</head><p>BRDF 3D shape <ref type="figure">Figure 6</ref>. Our dataset comprises synthetic images with random view, 3D shape, material, illumination and exposure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Photographs</head><p>As real test images, we have recorded photos of six toy cars that were completely painted with a single car lacquer, placed in four different lighting conditions and photographed from five different views, resulting a total of 120 images. For the corresponding ground truth reflectance maps, we placed in the same locations spheres painted with the same material. A qualitative evaluation, including examples of such real images is found in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Internet Images</head><p>In order to provide an even more challenging test set, we collect an additional 32 car images from Internet search.</p><p>Here we do not have access to groundtruth normals or reflectance maps, but the test provides a realistic test case for imaged-based editing methods. Again, we have manually segmented out the body of the car. This allows the study of single material normal and reflectance map prediction 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Methods and Metrics</head><formula xml:id="formula_2">i) ii) iii) iv) v) vi)</formula><p>We include six different methods to reconstruct reflectance maps: i) ground truth, ii) our direct , iii) our indirect approach, iv) an approach that follows our indirect one, but does not use a CNN for sparse interpolation but an RBF reconstruction as described in Eq. 1 (RBF), v) spherical harmonics (SH) where project the ground truth reflectance map to the SH domain, vi) an indirect approach where the estimated normals are replaced by ground truth normals. We employ two different metrics to assess the quality of reflectance map estimation. The first is plain L 2 error between all defined pixels of the reflectance map in RGB and the second the SSIM structural difference <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our proposed end-to-end direct approach to reflectance maps on the new SMASHINg Challenge and compare it to the indirect approach in its different variants. We start with a quantitative evaluation (Sec. 5.1) followed by qualitative results in (Sec. 5.2) including a range of imagebased editing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Results</head><p>Setup. Our quantitative results are summarized in Tbl. 1. We provide results for our Direct method that learns to predict reflectance maps directly from the image in an endto-end scheme, as well as several variants of our Indirect approach that utilizes intermediate result facilitated by supervision through normals at training time. The variants of the indirect scheme are based on our normal estimate, but differ in their second stage that has to perform a type of data interpolation to arrive at a dense reflectance map, given the sparse estimate. For such interpolation scheme, we investigate the proposed learning-based approach Indirect (SparseNet) as well as using radial basis function interpolation Indirect (RBF). Furthermore, we provide best case analysis by using ground-truth normals in the indirect approach Indirect (GT Normals) (only possible for synthetic data) and computing a diffuse version of the ground-truth by means of spherical harmonics GT (SH). The latter gives an upper bound on the result that could be achieved by methods relying on a diffuse material assumption. Reflectance Map Analysis. Overall, we observe consistency among the two investigated metrics in how they rank approaches. We obtain accurate estimations for the synthetic challenge set for our direct as well as the best indirect methods. The quantitative findings are underpinned by the visual results, e.g. showing the predicted reflectance maps in <ref type="figure" target="#fig_5">Fig. 7</ref>. The performance on the real images is generally lower with the error roughly increasing by one order of magnitude. Yet, the reconstruction still preserve rich specular structures and give a truthful reconstruction of the represented material.</p><p>In more detail, we observe that the best direct and indirect approach perform similar on the synthetic data, although direct did not use the normal information during training. For the real examples, this form of additional supervision seems to pay off more and even the simpler interpolations scheme RBF achieves best results in the considered metrics. Closer inspection of the results clearly shows limitations of image-based metrics. While the RBF-based technique yields a low error, it frequently fails to generate well localized highlight features on the reflectance map (see also illustration in Sec. 4.2). We encourage the reader to visit the supplementary material, where a detailed visual comparison for all methods is provided.</p><p>The ground-truth baselines give further insights into improvements over prior diffuse material assumptions and the future potential of the method. The GT (SH) baseline shows that our best methods improve over a best-case diffuse estimate with a large margin on in the DSSIM metric -highlight-ing the importance of considering more general reflectance maps. The error metric is again affected by the aforementioned issues. The Indirect (GT Normals) illustrates a best case analysis of the indirect approach where we provide ground-truth normals. The results show that there is potential to double the performance by having better normal estimation in the first stage. Normal Analysis. Tbl. 2 quantifies the error in the normal estimate by the first stage of our indirect approach. This experiment is facilitated by the synthetic data where normals are available by the rendering pipeline. L 2 corresponds to a network using the euclidean loss on the x, y, z components of the normals, while dual uses the two losses described in Sec. 3.3. Up refers to a network trained on upsampled normals. Both, the dual loss and joint upsampling improve the estimation of normals. Despite providing more data to the down-stream computation, the employed upsampling procedure does not decrease -but rather slightly increase the accuracy of the normals. While this analysis is conducted on synthetic data, we found that our models predict very convincing normal estimation even in the most challenging scenario that we consider, e.g. <ref type="figure" target="#fig_5">Fig. 7</ref> and <ref type="figure" target="#fig_7">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative Results</head><p>Automatically extracting reflectance maps -together with the normal information we get as a by-product -facilitate a range of image-based editing applications, such as material acquisition, material transfer and shape manipulation. In the following, we present several example applications. The supplementary material contains images and videos that complement our following presentation.</p><p>Reflectance Map and Normal Estimation. Typical results of estimated reflectance maps are presented in <ref type="figure" target="#fig_5">Fig. 7</ref>, also showing the quality of the predicted normals. The first row shows two examples on synthetic images, the second and third row show examples on real images and the last row shows examples of web images (no reference reflectance map is available here). Notice how the overall appearance, reflecting the interplay between material and the complex illumination, is captured by our estimates. In most examples, highlights are reproduced and even a schematic structure of the environment can be seen in the case of very specular materials.</p><p>Material Acquisition for Virtual Objects. <ref type="figure" target="#fig_6">Fig. 8</ref> shows synthesize image (column 2-5) that we have rendered from 3D models using the reflectance map automatically acquired from the images in column 1. Here, we use ambient occlusion <ref type="bibr" target="#b38">[39]</ref> to produce virtual shadows. This application shows how material representations can be acquired from real objects and transferred to a virtual object. Notice how the virtual objects match in material, specularity and illumination to the source image on the left.</p><p>Appearance Transfer. In order to transfer appearance between objects of a scene, we estimate reflectance maps for each object independently, swapped the maps, and then use the estimated normals to re-render the objects using a normal lookup from the new map. To preserve details such as shadows and textures, we first re-synthesize each object with its original reflectance map, save the per-pixel difference in LAB color space, re-synthesize with the swapped reflectance map and add back the difference in LAB. An example is shown in <ref type="figure" target="#fig_7">Fig. 9</ref>. Despite the uncontrolled conditions, we achieve photorealistic transfer of the appearancemaking it hard to distinguish source from target.</p><p>Shape Manipulation. As we estimate reflectance maps and surface normals, this enables various manipulation and re-synthesis approaches that work in the directional or normal domain. Here, the surface orientation is changed, e.g. using a painting interface and new appearance for the new orientation can be sampled from the reflectance map. Again, we save and restore the delta of the original reflectance map value and the re-synthesized one to keep details and shadows. An example is shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. The final result gives a strong sense of 3D structure while maintaining an overall consistent appearance w.r.t. material and scene illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>While our approach addresses a more general setting than previous methods, we still make certain assumptions and observe limitations: i) Up to now, all our experiments were conducted on cars and we assume that the car is segmented or in uniform background. Yet, our approach is learningbased and should -in principle -adapt to other classes in particular given dedicated training data. ii) We assume that the object is made out of a single material and up to now we cannot handle multiple parts or textures. iii) We assume distant illumination and therefore light interaction of close by objects or support surfaces (e.g. road) cannot be accurately handled by our model. iv) Due to our target representation of reflectance maps, the illumination is "baked in" and surface reflectance and illumination cannot be edited separately. v) Our quantitative evaluations are limited due to the absence of reliable (e.g. perceptual) metrics of reflectance maps.   . Appearance transfer application: Images on the diagonal are the original input. Off-diagonal images have the appearance of the input in its column combined with the input shape of its row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented an approach to estimate reflectance maps from images of complex shapes with specular materials under complex natural illumination. Our study is facilitated by a new benchmark of synthetic, real and web images of in- <ref type="figure" target="#fig_0">Figure 10</ref>. Shape manipulation application. A user has drawn to manipulate the normal map extracted from our indirect approach. The reflectance map and the new normal map can be used to simulate the new shape's appearance.</p><p>creasing difficulty, that we will make available to the public. Our approach features the first mapping using end-to-end learning from image to the directional domain as well as an application of neural networks to learning-based sparse data interpolation. We show how to incorporate additional supervision by normal information that increase accuracy as well as results in normal estimations as a byproduct. Our results show truthful reflectance maps in all three investigated scenarios and we demonstrate the applicability on several image-based editing tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Top: Input 2D image with three cars of unknown shape and material under unknown natural illumination. Right: Our automatically extracted reflectance map and the reference. Bottom: Transfer of reflectance maps between the objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our approach, that comprises two variants: A direct one and an indirect one extracting surface orientations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of the direct approach. The yellow boxes represent the filters' size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of the normal step of our indirect approach. The middle elements correspond to the fully convolutional filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Architecture of the reflectance map step of our indirect approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Results of different variants and steps of our approach (left to right). Input image, GT RM, RM result of the direct approach, RM result of the indirect approach, the sparse RM input produced in the indirect variant, and the normals produced by the indirect variant as well. Each result is annotated to come from the synthetic, photographed or Internet part of our database. For the Internet-based part, no reference RM is available. Please see the supplemental material for exhaustive results in this form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Transfer of reflectance maps from real photographs (1st col.) to virtual objects (other col.'s) of the same and other shape. The supplemental video shows animations of those figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9</head><label>9</label><figDesc>Figure 9. Appearance transfer application: Images on the diagonal are the original input. Off-diagonal images have the appearance of the input in its column combined with the input shape of its row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Results for the different methods defined in Sec.</figDesc><table>4.2. 
Synthetic 
Real 
Method 
MSE DSSIM MSE DSSIM 

Direct 
.0019 .0209 .0120 .0976 
Indirect (SparseNet) 
.0018 .0180 .0143 .0991 
Indirect (RBF) 
.0038 .0250 .0116 .0814 

Indirect (GT Normals) 
.0008 .0111 
-
-
GT (SH) 
.0044 .0301 .0114 .0914 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Normals estimation of indirect approach on synthetic data.</figDesc><table>Mean Median RMSE 

L2 
14.3 
9.1 
20.6 
Dual 
13.4 
8.2 
19.8 
Dual up 13.3 
8.2 
19.9 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the Internet Images we used networks that were trained on synthetic data from segmented meshes to contain only the body.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work is supported by the IWT SBO project PARIS and the FWO project "Representations and algorithms for the captation, visualization and manipulation of moving 3D objects, subjects and scenes".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recovering intrinsic scene characteristics from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Vis. Sys</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A noiseaware filter for real-time depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Buisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. M2SFA</title>
		<meeting>M2SFA</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Koenderink. Reflectance and texture of real-world surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans, Graph</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistics of real-world illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relighting objects from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bekaer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Example-based photometric stereo: Shape reconstruction with general, varying BRDFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Calculating the reflectance map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Sjoberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">App. Opt</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shape estimation in natural illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-based material editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint bilateral upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIG-GRAPH)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>NIPS. 2012. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mansinghka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint embeddings of shapes and images via cnn image purification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reflectance and illumination recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A datadriven reflectance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Direct intrinsics: Learning albedo-shading decomposition by convolutional regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Narihira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning lightness from human judgement on relative reflectance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Narihira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Photographic tone reproduction for digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferwerda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagebased synthesis and re-synthesis of viewpoints guided by 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative shape from shading in uncalibrated illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Right Hemisphere. ZBruhs MatCap</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The lit sphere: A model for capturing NPR shading from art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><forename type="middle">J</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics interface</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6445</idno>
		<title level="m">Deep lambertian networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Sign., Sys. and Comp</title>
		<meeting>Sign., Sys. and Comp</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning data-driven reflectance priors for intrinsic image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An ambient light illumination model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kronin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EGSR</title>
		<meeting>EGSR</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
