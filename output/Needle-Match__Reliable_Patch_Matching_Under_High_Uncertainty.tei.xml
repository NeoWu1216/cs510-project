<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Needle-Match: Reliable Patch Matching under High Uncertainty</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Lotan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">ISRAEL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">ISRAEL</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Needle-Match: Reliable Patch Matching under High Uncertainty</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reliable patch-matching forms the basis for many algorithms (super-resolution, denoising, inpainting, etc.)  However, when the image quality deteriorates (by noise, blur or geometric distortions), the reliability of patch-matching deteriorates as well. Matched patches in the degraded image, do not necessarily imply similarity of the underlying patches in the (unknown) high-quality image. This restricts the applicability of patch-based methods. In this paper we present a patch representation called "Needle", which consists of small multi-scale versions of the patch and its immediate surrounding region. While the patch at the finest image scale is severely degraded, the degradation decreases dramatically in coarser needle scales, revealing reliable information for matching. We show that the Needle is robust to many types of image degradations, leads to matches faithful to the underlying high-quality patches, and to improvement in existing patch-based methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Finding similar patches is a fundamental component in many computer vision algorithms. These include super resolution <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">14]</ref>, texture synthesis <ref type="bibr" target="#b10">[11]</ref>, image completion <ref type="bibr" target="#b16">[16]</ref>, image retargetting <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">26]</ref>, and many more. However, when the image quality undergoes degradation (noise, blur, geometric deformations, etc.), the reliability of patchmatching decreases dramatically. Finding two "similar" patchesp ≈q in the degraded image, may no longer imply that their underlying (hidden) clean patches p and q are similar. This however, is the very basic assumption in most patch-based methods. For example, unconstrained matching of noisy patches (globally in the noisy image, or in a database of clean images), was shown to suffer from severe overfit of the noise <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b34">34]</ref>. Similarly, severe motion blur or geometric distortions harm the ability to find good matches (correspondences) across frames, hence optical flow estimation algorithms tend to fail under such degradations.</p><p>Partial solutions have been proposed for this problem, which offer only a limited remedy. For example, denoising algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref> typically restrict the search space to a Funded in part by the Israel Science Foundation (Grant 931/14). small image region around each patch, or use larger patches which are less prone to noise-overfit. This, however, restricts the ability to recover fine textures. Patch matching under severe blur was addressed in <ref type="bibr" target="#b2">[3]</ref>, by pre-processing the image patches to a normalized frequency range. It addresses only isotropic blur, and does not generalize to other blur types (motion blur, non-parametric blur).</p><p>This paper presents a general framework for reliable patch matching under severe image degradations. The "Needle", a new patch descriptor, comprises of small patches obtained from multiple image scales, at the same relative image coordinates <ref type="figure" target="#fig_0">(Fig. 1a)</ref>. While the patch at the finest image scale is severely degraded, hence cannot be reliably matched, the degradation decreases dramatically in coarser Needle scales, revealing reliable information for matching ( <ref type="figure" target="#fig_0">Fig. 1(c,d,e)</ref>). We show that if N eedle(p) ≈ N eedle(q) for two distorted patchesp andq, then with high likelihood p ≈ q (the hidden high-quality patches). Our paper has two main contributions: (i) A new patch descriptor which is resilient to many types of image degradations, (ii) Using NeedleMatch in patch-based algorithms (e.g., denoising, correspondence-estimation) leads to a substantial improvement under severe image degradations.</p><p>The term "Needle" was coined in <ref type="bibr" target="#b35">[35]</ref>, but for a different purpose. Each noisy patch was matched against coarser patches inside its own needle, looking for its cleanest version among its own needle patches. In contrast, we employ the Needle as a 'descriptor', which is matched against other needle-descriptors, to find good matches elsewhere in the image (or in other images). Contrary to <ref type="bibr" target="#b35">[35]</ref>, we do not rely on cross-scale recurrence of patches; our coarser-scale needle patches need not resemble the fine-scale patch <ref type="figure" target="#fig_0">(Fig. 1b)</ref>.</p><p>Our Needle is different than other image descriptors (e.g., SURF, SIFT, Geometric-Blur), as its aim is to represent well the unknown high-quality patch signal hidden under severe degradation. A dense multiscale descriptor was also developed by <ref type="bibr" target="#b15">[15]</ref>, based on SIFT computed at multiple scales. Their aim was different -scale-invariant matching. Being a gradient-based descriptor, their decriptor is likely to fail in the presence of severe image degradation.</p><p>Incorporating information from multiple scales is commonly used in coarse-to-fine algorithms (optical flow <ref type="bibr" target="#b20">[20]</ref>, stereo <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b33">33]</ref>, etc.) However, these methods use multiscale information sequentially (coarse to fine). Such sequential approaches fail under severe degradations (see Sec. 5.2.2), since their final optimization stage is misguided by the last-visited but most-degraded finest scales. In contrast, The Needle employs information from all scales simultaneously. Some multiscale cost aggregation methods also employ simultaneous multiscale information (e.g., <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b32">32]</ref>). However, these are special purpose algorithms designed to solve a specific application. In contrast, NeedleMatch can be plugged into a wide variety of existing patch-based algorithms, simply by replacing their patch-based similarity with Needle-based similarity, thus making them robust to a wide variety of image degradations <ref type="figure" target="#fig_1">(Fig 2)</ref>. The rest of the paper is organized as follows. Sec. 2 presents the Needle construction. Sec. 3 analyzes the source of its resilience to severe degradations. Sec. 4 analyzes why the Needle is able to accurately lock onto fine details despite using large contextual information at coarser scales. Sec. 5 shows the superiority of the Needle matching over regular patch matching in various patch-based applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">"Needle" -A Multiscale Patch Descriptor</head><p>Let I be a high-quality image, andĨ be its degraded version.p ∈Ĩ is a degraded version of the patch p ∈ I. When seeking a "similar" patchq ∈Ĩ, we wish to find a patch which is similar in its underlying (unknown) high-quality signal, i.e., q ≈ p. The difficulty in finding reliable matches stems from the fact that the "distortion component" in a degraded patch is often larger than its"signal component" p (i.e, ||p − p||&gt;||p||). This leads to an overfit of the distortion component instead of the signal, resulting in bad patch matches. In other words, a good fit ofq top in imageĨ does not necessarily imply a good fit of q to p in image I.</p><p>We compute a Needle-descriptor for each image patch, such that if N eedle(p) ≈ N eedle(q), then with high likelihood p ≈ q. To build the descriptor, the imageĨ is downscaled to generate multiscale images {Ĩ s } (we used a bicu-bic downscaling kernel). N eedle(p) = {p s } is obtained by stacking all the patchesp s ∈Ĩ s which reside at the same relative image coordinates asp; i.e., if (x, y) are the center coordinates ofp inĨ, then (sx, sy) are the center coordinates of the patchp s inĨ s , at sub-pixel accuracy <ref type="figure" target="#fig_0">(Fig. 1a)</ref>.</p><p>In order to match image patches, a Nearest-Neighbor (NN) search is conducted in the needle space. For two corrupted patchesp andq to match, their corresponding needles {p s ℓ } L ℓ=0 and {q s ℓ } L ℓ=0 must match (have low ℓ 2 distance). We refer to this process as NeedleMatch. Note thatp need not necessarily be of same size as its needle patches {p s ℓ }. For example, a 5 × 5 patchp at the original scale may be represented using a multi-scale needle of 3 × 3 patches. In our experiments we typically used needles of 3 × 3 multi-scale patches, obtained from 8 scales s ℓ = 0.75 ℓ (ℓ = 0, 1, . . . , 7).</p><p>Three Needle properties lead to reliable patch matching: (a) High-quality signal is revealed at coarse Needle scales: While the patch at the finest scale of a degraded image is severely distorted, the degradation decreases dramatically in coarser scales. Additive Gaussian noise, general image blur and general geometric distortions -all decrease linearly with scale, revealing high-quality patches at coarser needle scales ( <ref type="figure" target="#fig_0">Fig. 1.c,d,e)</ref>. These claims are derived analytically in Sec. 3. Note that the different types of degradations in <ref type="figure" target="#fig_0">Fig. 1</ref> reveal similar signals in their coarser scales. This is a strong property of the Needle, as NeedleMatch need not assume a specific degradation model for its search. In fact, NeedleMatch can be used to match across images with different types of degradation (e.g., matching patches in a noisy image to patches in a blurry image).</p><p>(b) Context without penalty: NeedleMatch benefits from local context information, without suffering from the inherent limitation of matching large rigid regions. Wider and wider image regions around the patch are represented by increasingly coarser scales of its needle. This provides reliable context information for matching. However, one would expect that this should harm the ability of the Needle to find accurate well-localized matches in textured regions (e.g., tree leaves, bushes, hair, etc.) Surprisingly, this is not the case. NeedleMatch provides very good matches in such regions (e.g., see the tree leaves in <ref type="figure" target="#fig_7">Fig. 6h</ref>). Sec. 4 explains and analyzes this surprising phenomenon. We show that NeedleMatch gives rise to patch matches which only roughly share the same local context. It permits non-rigid deformations of the surrounding region, while enforcing high match accuracy of the center patch. Such deforming contexts cannot be exploited by matching large rigid patches, as these are known to lack good matches <ref type="bibr" target="#b18">[18]</ref>. (c) Simultaneous use of all scales: The needle does not commit to any specific scale, but rather uses all scales simultaneously. This has several benefits: (i) Different image regions may benefit from using different scales, depending on the amount of details they contain. (ii) We do not know a-priori the degree of blur/noise/distortion, hence do not know ahead of time which scale is most informative. (iii) Coarse needle levels provide high-quality signal information and rough context information, thus guaranteeing high match reliability. Fine needle levels, on the other hand, allow to match the fine details of the signal (e.g., texture, edges), thus guaranteeing high localization accuracy.</p><p>These theoretical benefits of the needle are derived analytically in Sec. 3 and 4, and are supported by the quantitative and qualitative experiments in Sec. 5. We further show the superiority of NeedleMatch over: (i) Patch matching at the original scale alone (of either small or large patches), and (ii) superiority over sequential coarse-to-fine patch matching. In particular, under severe degradation, NeedleMatch provides more reliable correspondences than advanced coarse-to-fine optical-flow methods <ref type="bibr" target="#b28">[28]</ref> (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reliable Signal is Revealed at Coarse Scales</head><p>We next show that signal uncertainty, which is high in fine scales of the degraded image, reduces dramatically in coarser scales, revealing high quality signal. This is what provides NeedleMatch with its robustness. (a) Noise Uncertainty Reduces with Scale: Letp = p + n be a noisy patch. Letq = q +n be a 'Nearest-Neighbor' (NN) ofp. When the noise n in the patch is larger than its signal p (e.g., in uniform patches p), the patchwise Signalto-Noise Ratio <ref type="bibr" target="#b25">[25]</ref>: "PatchSNR"= var(p) var(n) is low. This leads to overfit of the noise instead of the signal (See <ref type="bibr" target="#b25">[25]</ref>).</p><p>While the noise severely corrupts the original image scale, the noise level drops significantly at coarser image scales. For additive Gaussian noise:Ĩ=I + N (0, σ 2 ), the noise in an α-times coarser image will be distributed like <ref type="bibr" target="#b35">[35]</ref>: n (α) ∼ N (0, σ α 2 ), i.e., α-times smaller. Thus,</p><formula xml:id="formula_0">P atchSN R (α) (p) ≥ var(p)</formula><p>var(n (α) ) = α · P atchSN R (p), revealing more reliable signal for matching <ref type="figure" target="#fig_0">(Fig. 1c)</ref>. The average PatchSNR of the needle is thus much higher. This significantly lowers the chance to overfit noise, leading to reliable patch matching (see Figs. 1c, 2c). (b) Blur Uncertainty Reduces with Scale: LetĨ = I * b be a blurry image obtained by convolving a sharp image I with a severe blur b. The difficulty in finding reliable matches stems from the fact that the blur b eliminates much of the high frequency information of I, leaving patches inĨ with very few details for reliable matching.</p><p>When the blurry imageĨ is scaled down by a factor of α&gt;1, its blur also decreases by a factor of α. For example, an edge smeared over 10 pixels inĨ would appear smeared over only 5 pixels in its half-scaled version (Ĩ↓ 2 )(x, y)=Ĩ(2x, 2y). More formally <ref type="bibr" target="#b23">[23]</ref>, for any scaling factor α: <ref type="figure">αy)</ref> is α-times narrower in x and in y, and α 2 -times taller than the original blur b(x, y), thus maintaining an integral of 1. Since lim α→∞ (α 2 · b ↓ α (x, y)) = δ(x, y), significant downscaling of a blurry image will result in a sharp image with no blur (an observation also used in Blind-Deblurring <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b29">29]</ref>). Thus, blur uncertainty reduces with needle scales <ref type="figure" target="#fig_0">(Fig. 1d)</ref>, resolving match ambiguities. (c) Geometric Distortion Reduces with Scale: While noise and blur introduce uncertainty in pixel intensities, we next analyze uncertainty in pixel locations, namely geometric uncertainty. Such uncertainty can be caused, e.g., by atmospheric turbulence, imaging through smoke, underwater photography, etc. These distortions presents a challenging task for patch matching.</p><formula xml:id="formula_1">Ĩ ↓ α (x, y) = I ↓ α (x, y) * (α 2 · b ↓ α (x, y)). Note that α 2 · b ↓ α (x, y) = α 2 · b (αx,</formula><p>However, a pixel dislocated by ∆ in the original scale, is dislocated by only 1 α ∆ in an α-times coarser image. Thus, the geometric uncertainty reduces with needle scales, leading to reliable signal-fit <ref type="figure" target="#fig_0">(Fig. 1e)</ref>. For our empirical evaluations (Sec. 5.1), we used a simplistic model to geometrically distort images, by randomly displacing each pixel using a 2D-Gaussian distribution. A pixel p ∈ I, located at (x, y), is randomly displaced to (x+∆ x , y+∆ y ) where ∆ x , ∆ y ∼N (0, σ). This process results in severely degraded images (e.g., Figs. 1e,6i, for σ=4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Context without Penalty</head><p>We saw that cleaner/sharper signals emerge in coarser needle-scales, resulting in Needles with much higher SNR than their degraded source patches. But why should the Needle, which captures information from a large image region (substantially larger than the patch), be able to find good and well-localized patch matches of fine structures? After all, even if two patches p and q are good matches, their surrounding contexts P and Q may be quite different (see <ref type="figure" target="#fig_2">Fig. 3a</ref>). This section shows that NeedleMatch can match patches well, despite some context variations. In fact, contexts usually help improve the matches.</p><p>Context information has been shown to significantly im-  prove the performance of detection and recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">31]</ref>, super-resolution <ref type="bibr" target="#b30">[30]</ref>, and is the guiding principle behind the fast PatchMatch method of <ref type="bibr" target="#b0">[1]</ref>. NeedleMatch benefits from this property. When certainty is low (e.g., under degradation), the context information guides the search for NNs to more likely image regions. For example, when searching for a leave patch (e.g., the small red patch in <ref type="figure" target="#fig_2">Fig. 3a</ref>), we are more likely to find reliable matches for it among other leave patches, than among building patches. However, two similar leave patches may have quite different contexts, despite sharing similar appearance properties (see two larger orange regions in <ref type="figure" target="#fig_2">Fig. 3a)</ref>. This prohibits the ability to match the two small leave patches via matching their larger context regions. We next show that larger context regions, which share similar appearance properties, but differ at finer image scales, are very similar in coarser Needle scales. Thus, coarse needle scales guide the search to reliable matches in similar, but not identical contexts. This phenomenon is explained and analyzed next.</p><p>Let p and q be two corresponding patches of size m × m, whose surrounding M × M regions P and Q (their "contexts") deform with respect to each other <ref type="figure" target="#fig_2">(Fig. 3a)</ref>. Since P and Q share similar appearance properties (similar colors/intensities), we can describe one of them as a non-rigid deformation of the other. The arrows (u, v) in <ref type="figure" target="#fig_2">Fig. 3b</ref> illustrate a deformation between the contexts P and Q (namely, how far a pixel from P has to move in order to find a pixel in Q with similar color). Let's assume that N eedle(p) = {p s ℓ } L ℓ=1 and N eedle(q) = {q s ℓ } L ℓ=1 are composed of patches of size m×m (where m≪M , e.g., m=3 and M =30). Let's further assume that the coarsest needle-patches p s L and q s L captures information from their entire surrounding contexts P and Q, respectively. Namely, P ↓ α = p s L and Q↓ α = q s L , where α=1/s L . We analyze the case where p and q are clean image patches that match well (i.e., if placed on top of each other, the two patches will be well aligned, with very similar colors). However, their surrounding context regions P and Q are not well aligned. We assume that the contexts do not change drastically, but rather deform gradually with respect to each other (an assumption which holds for the vast majority of the patches that do not lie near occlusion boundaries). In other words, we allow for increasing deformation (larger "relative misalignments" (u, v) between P and Q) as a function of the distance from their centers <ref type="figure" target="#fig_2">(Fig. 3b)</ref>. We make the simplifying assumption that the magnitudes of these "relative misalignments" are proportional to their distance from the region center (0, 0):</p><formula xml:id="formula_2">||(u(x, y), v(x, y))|| = t · ||(x, y)||<label>(1)</label></formula><p>for some scalar t. Note that this assumption holds not only for gradual non-rigid deformations, but also for contexts deforming by global linear parametric transformations (e.g., zoom, rotation, affine deformation). We next show that although the average misalignment is large for the contexts P and Q (Claim 1), it is small for the corresponding Needles of p and q (Claim 2), leading to good matches. </p><formula xml:id="formula_3">AvgM isalignment(P, Q) = M t √ 6<label>(2)</label></formula><p>Proof: For simplicity, we perform the derivation in the continuum. The average misalignment per pixel between P and Q is: AvgSquareM isalignment(P, Q) =</p><formula xml:id="formula_4">= 1 M 2 M/2 −M/2 M/2 −M/2 (u(x, y) 2 + v(x, y) 2 ) dx dy = 1 M 2 M/2 −M/2 M/2 −M/2 t 2 (x 2 + y 2 ) dx dy = M 2 t 2 6</formula><p>Therefore, AvgM isalignment(P, Q) = M t √ 6 Large contexts P and Q have large misalignment. E.g., for M =25 (25×25 regions) and t=0.2 (e.g., zoom by 0.8), the average pixel misalignment between P and Q is ≈2 pixels. This results in a large grayscale difference, especially in textured regions, hence a bad match between P and Q.</p><p>In contrast, when the needles of two matching patches p and q are compared, N eedle(p)={p s ℓ } L ℓ=1 and N eedle(q)={q s ℓ } L ℓ=1 , their average misalignment will be significantly smaller.</p><p>The misalignment between corresponding needle-patches at coarser scales will still grow linearly with distance from the patch centers, with the same scalar t. When two image regions are scaled-down by a factor of α, their relative misalignments become α-times smaller. In our case: ||(u↓ α (x, y), v↓ α (x, y))||= 1 α ||(u(αx, αy), v(αx, αy))|| = 1 α t||(αx, αy)||=t||(x, y)||. Hence, the scalar t remains the same in all scales. Nevertheless, the resulting average misalignment per needle-pixel will be much smaller, since all its patches are small (m×m). This is shown next: Proof: AvgSquareM isalign(N eedle(p), N eedle(q)) =</p><formula xml:id="formula_5">= 1 L Σ L ℓ=1 AvgSquareM isalignment(p s ℓ , q s ℓ ) = 1 L Σ L ℓ=1    1 m 2 m/2 −m/2 m/2 −m/2 (t 2 (x 2 + y 2 ) dx dy    = 1 L Σ L ℓ=1 m 2 t 2 6 = m 2 t 2 6 ⇒ AvgM isalign(Needle(p), Needle(q)) = mt √ 6</formula><p>The expression for the average misalignment per needlepixel is very similar to that per context-pixel ( mt (1) holds for the entire context P ). It only depends on the size m of the needle patches (which is typically small), and on the rate t of non-rigid distortion. For example, if the needle patches are of size 3 × 3 (m=3), and t=0.2 (same as above), the average misalignment error per pixel between N eedle(p) and N eedle(q) will be ≈0.25 pixel -an order of magnitude smaller than the average misalignment between their contexts P and Q ! The small misalignment leads to high similarity between the two needles. E.g., for textured regions with gradient magnitude of ∼10 grayscales per pixel, a rough calculation (assuming linearized brightness constancy) yields RM SE(N eedle(p), N eedle(q)) ≈ ||grad|| · ||(u, v)|| = 10 · 0.25 = 2.5 grayscales per pixel. The small error indicates a good match between the patches p and q. In contrast, the RMSE between their contexts is large: RM SE(P, Q) ≈ ||grad|| · ||(u, v)|| = 10 · 2 = 20.</p><p>Note that two random needles of non-matching patches will not have small misalignments. This is because the basic assumptions (that the central patches are well aligned, and that the misalignments between their surrounding contexts grow slowly with distance from the centers), do not hold. <ref type="figure" target="#fig_7">Figs. 4 and 6h</ref> to find other matching tree-leave patches. Similar tree leaves share roughly the same local context, up to mild non-rigid deformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In summary, NeedleMatch benefits from local context information, without suffering from the inherent limitation of large region matching (with the exception of patches near occlusion boundaries). This is what allows patches on treeleave in</head><p>Note that the assumption of Eq. (1), holds not only for gradual non-rigid deformations, but also for linear parametric transformations (e.g., rotation, zoom, affine deformations). The ability of NeedleMatch to find good matches under such transformations is illustrated in <ref type="figure">Fig. 4</ref>, and is further contrasted with matching large rigid patches. A 'reference' image ( <ref type="figure">Fig. 4a)</ref> was zoomed by 0.75 (t =0.25=1-0.75, for all image patches 1 ), to generate a 'target' image. Each patch in the reference image searched for its best match (1st NN) in the target image, once using NeedleMatch and once using large 25×25 rigid patches (the spatial support covered by the needles). New images were reconstructed <ref type="figure">(Fig. 4b,c)</ref>, by replacing each pixel in the 'reference' image with its corresponding pixel in the 'target' image, according to the computed NNs. Needle-NNs find good matches, even in highly textured regions (e.g., tree leaves), whereas large patches do not, resulting in a blurry unwarped image (please zoom in). This experiment was repeated for a rotation by 15 • (t≈0.26 for all patches), yielding similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>A wide variety of algorithms rely on patch similarity to solve computer vision tasks. These include image denoising <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>, image deblurring <ref type="bibr" target="#b23">[23]</ref>, super resolution <ref type="bibr" target="#b14">[14]</ref>, optical flow estimation <ref type="bibr" target="#b20">[20]</ref>, inpainting [6] and more. The success of these algorithms relies on the assumption that when we search for "similar patches" (in the same image, or in other images), we find other patches with the same signal. This assumption, however, is not true when the image (signal) undergoes degradations. In such cases, patch-based similarity often leads to patches with similar degradations, instead of similar signal, resulting in high reconstruction errors in the above algorithms. NeedleMatch, on the other hand, provides more accurate "signal-fit" in the presence of degradations. Thus, replacing the patch-based similarity with needle-based similarity leads to improved results 2 .</p><p>We first compare the ability of regular patch matching vs. NeedleMatch to find good NNs (patches with similar underlying signal). Sec. 5.1 presents such quantitative evaluation on hundreds of images, undergoing different degradation types. We then show that replacing patch-based similar-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Empirical Evaluation: "Signal Fit" Accuracy</head><p>We compare the match-quality of patch matching vs. NeedleMatch under 3 types of degradation (noise, blur, geometric distortion). Given a degraded 5×5 patchp, we search for a similar patchq=NN(p,Ĩ) among all other patches in the degraded imageĨ (omitting self matches). The "signalfit" between the degraded patchesp andq is assessed by:</p><p>SignalMatchError(p,q) = ||p − q|| , where p and q denote the ground-truth high quality patches underlyingp andq in the ground-truth clean image I.</p><p>The graphs in <ref type="figure" target="#fig_5">Fig. 5</ref> report SignalMatchError, averaged over all 5×5 patches from hundreds of degraded images (degradations applied to the DB of <ref type="bibr" target="#b21">[21]</ref> and <ref type="bibr" target="#b29">[29]</ref>). Statistics were collected at varying degradation levels: (i) Noisy images were generated with varying Gaussian noise levels (σ N =0,..,75). (ii) Blurry images were generated via convolution with Gaussian blurs of varying width (σ B =0,..,15), as well as with the 8 non-parametric blur kernels of <ref type="bibr" target="#b29">[29]</ref>. (iii) Geometrically deformed images were generated by randomly displacing pixels with 2D-Gaussian perturbations (σ G =0,.., <ref type="bibr" target="#b8">9)</ref>. The NN search used KDtree <ref type="bibr" target="#b13">[13]</ref>, applied once in the space of 5 × 5 degraded patches (q =NN(p,Ĩ)), and once in the space of 3 × 3 × 8 needles emerging from degraded patches (q =NN needle (p,Ĩ)). The graph indicates that for patches with no degradation (σ N =σ B =σ G =0), regular patch matching is better than NeedleMatch (due to better matches near occluding boundaries). However, already  at small levels of degradations, there is a dramatic improvement in "signal-fit" for NeedleMatch over regular patch matching (in all 3 degradation types). <ref type="figure" target="#fig_7">Fig. 6</ref> is a visualization of the signal-fit errors of <ref type="figure" target="#fig_5">Fig. 5</ref>. It shows image reconstructions based on the NNs found. The NNs were searched in the corrupted images (e.g., <ref type="figure" target="#fig_7">Fig. 6a</ref> for noise), but the reconstructed images were built from the underlying clean patches <ref type="figure" target="#fig_7">(Fig. 6b)</ref>. When the "signalfit" is good, the reconstructed images should resemble the ground-truth images (as in <ref type="figure" target="#fig_7">Fig. 6d)</ref>; but when the "signalfit" is poor, the reconstructed images are of low quality <ref type="figure" target="#fig_7">(Fig. 6c)</ref>, even though they were constructed from clean patches. NeedleMatch provides much higher signal-fit reliability than regular patch-matching, even in highly textured regions (e.g., the hat in <ref type="figure" target="#fig_7">Fig. 6d</ref>, the tree leaves in <ref type="figure" target="#fig_7">Fig. 6h)</ref>. We next show a few example applications of NeedleMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Example Applications</head><p>We examined a variety of patch-based reconstruction methods, once using their regular patch-based similarity, and once using our Needle-based similarity. The latter significantly improves the results in the presence of degradation. <ref type="figure" target="#fig_1">Figs. 2 and 7</ref> show visual results for a variety of such applications, whereas Secs. 5.2.1 and 5.2.2 provide a more extensive empirical evaluation for two of these applications. <ref type="figure" target="#fig_1">Fig. 2a</ref> shows results of applying the Single-Image Super-Resolution (SR) method of <ref type="bibr" target="#b14">[14]</ref> to a low resolution image with mild Gaussian noise (σ=20). The goal in SR is to reconstruct new high frequency details which are invisible in the low-res image (beyond the Nyquist limit). However, SR with regular patch-matching tries to increase not only the details of the signal, but also the noise <ref type="bibr" target="#b27">[27]</ref>. NN-search for noisy uniform patches tries to fit their noise (as best possible) with signal-details elsewhere in the image <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b34">34]</ref>. Replacing the patch-matching in <ref type="bibr" target="#b14">[14]</ref> with NeedleMatch, results in a super-resolved image with reconstructed high-frequency signal details, while discarding the noise (please zoom in on <ref type="figure" target="#fig_1">Fig. 2a</ref>). This is due to the ability of NeedleMatch to fit the signal but not the noise.</p><p>Even in image denoising, where the images are inherently assumed to be corrupted, employing NeedleMatch improves the results <ref type="figure" target="#fig_1">(Fig. 2c)</ref>. We replaced the patch-based similarity of BM3D <ref type="bibr" target="#b6">[7]</ref> with Needle-based similarity. The improvement is mostly visible in the fine textures (please zoom-in on <ref type="figure" target="#fig_1">Fig.2c)</ref>. More details and an extensive quantitative comparison are provided in Sec. 5.2.1 and <ref type="table">Table.</ref> 1. <ref type="figure" target="#fig_1">Fig. 2b</ref> shows "removal" of geometric distortion caused by heat turbulence. Every patch in the turbulent frame was replaced by the median of its k-NNs, searched in other turbulent frames in the same video. Patch-NNs preserve the geometric distortion, while Needle-NNs match the signal, allowing recovery of straight lines (e.g., see line above license plate, or at the bottom of the car -please zoom in).</p><p>Even in correspondence estimation, NeedleMatch outperforms advanced coarse-to-fine optical flow methods in the presence severe degradations <ref type="figure" target="#fig_8">(Fig. 7</ref> and Sec. 5.2.2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">BM3D Denoising -Patches vs. Needles</head><p>As observed in <ref type="bibr" target="#b34">[34]</ref> and in Sec. 5.1, unrestricted patch matching suffers from severe overfit of noise. Patch-based denoising methods (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>) avoid this problem by restricting their search region to a local area around each patch (e.g., 30×30). A small surrounding region contains many other patches with a similar (hidden) signal, while limiting the risk to overfit the noise <ref type="bibr" target="#b34">[34]</ref>. However, this restricted search region also restricts their ability to find good matches for textured patches, resulting in blurry details. We ran BM3D on hundreds of natural images from the dataset of <ref type="bibr" target="#b21">[21]</ref>, contaminated by Gaussian noise at varying levels (σ=15, ..., 75). We applied the BM3D code of <ref type="bibr" target="#b17">[17]</ref>, once with patch-based similarity, and once with Needlebased similarity (We used 3×3×8-needles and 8×8-patches for σ = 15, 35, and 5×5×8-needles and 12×12-patches for σ = 55, 75). As expected, applying BM3D to the entire image using its regular patch-based similarity, leads to worse results (see <ref type="table">Table.</ref> 1 -BM3D local vs. global). This deterioration is due to the noise-fitting problem of patch matching in unrestricted domains. Global NeedleMatch, on the other hand, avoids noise fitting, leading to an overall improvement, especially at high noise-levels (0.42-0.81dB -see Table. 1). Using a larger search space without being prone to noise-fitting, offers better reconstruction in textured regions (see <ref type="figure" target="#fig_1">Fig. 2c</ref>). We further ran the experiment also on the 20 images of BM3D benchmark for σ=15, 35, 55, obtaining an average improvement of 0.25, 0.36, 0.66dB on color images, and 0.18, 0.3, 0.51dB on the BW images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Correspondence Estimation under Severe Blur</head><p>Coarse-to-fine optical flow (such as Lucas&amp;Kanade) can be regarded as a sophisticated coarse-to-fine patch matching between two images. We therefore compared the robustness of NeedleMatch vs. coarse-to-fine optical flow, under severe blur. We compared to 2 optical flow methods -Lu-cas&amp;Kanade (L&amp;K) <ref type="bibr" target="#b20">[20]</ref> and Black&amp;Anandan (B&amp;A) <ref type="bibr" target="#b1">[2]</ref> (using the advanced implementations of <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b28">28]</ref>, respectively). We generated a database of hundreds of pairs of degraded images, by applying large transformations (relative zoom of 0.75 and relative rotation of 15 • ) to the images of <ref type="bibr" target="#b29">[29]</ref>, and then severely blurring them (one image from each pair was blurred vertically, while its transformed ver- sion was blurred horizontally). The width of the horizontal and vertical blur kernels varied from zero-width (no blur) to 14 pixels wide (severe blur). The graphs in <ref type="figure" target="#fig_8">Fig. 7b</ref> show the resulting errors (RMSE) between the computed displacements, and the ground-truth displacements. Optical flow estimation is superior to NeedleMatch under small blur. However, under severe blur, NeedleMatch provides more accurate correspondences, even though it uses no smoothness constraint (each patch independently seeks its 1 st Needle-NN). Deep 3×3×12 Needles were used for correspondence estimation, since the goal is to find a single best accurate correspondence (not mupltiple NNs). <ref type="figure" target="#fig_8">Fig. 7a</ref> shows results on a real-video. NeedleMatch produces much better correspondences on the blurry fan. Incorporating smoothness into NeedleMatch is likely to improve its results. Although using multiple scales, optical-flow methods do so sequentially. Their accuracy deteriorates at finer scales, as fine-scale patches are very blurry, misguiding the final search. In contrast, NeedleMatch uses information from all scales simultaneously, not committing to any specific scale.</p><p>Note that matching patches at a fixed predetermined coarse scale will not achieve Needle performance. Different image regions benefit from different scales, depending on their patchwise Signal-to-Degradation Ratio (Patch-SDR). E.g., texture patches have higher Patch-SDR than uniform patches, thus benefit from finer scales in denoising and SR <ref type="figure" target="#fig_1">(Fig. 2)</ref>. The degradation may also vary across the image: patches on the blurry fan <ref type="figure" target="#fig_8">(Fig. 7a)</ref> benefit from correspondences at coarse scales, whereas patches on the sharp static background benefit from finer scales. NeedleMatch inherently handles this ambiguity: coarse needle levels guarantee high match reliability, while fine needle levels guarantee high localization accuracy, when such details are available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Needle Descriptor. Needles of degraded patches (e.g., N eedle(p), N eedle(q)) reveal high-quality signal at their coarser scales. These can be used to reliably guide the matching of degraded patches (p andq) at the original scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Replacing 'patch-based similarity' with 'Needle-based similarity' improves a variety of patch-based applications. (a) SR; (b) Turbulence removal (See Sec. 5.2 for details); (c) BM3D denoising. Please zoom in to see details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Similar patches with different contexts. (a) The patches p and q are similar but their surrounding contexts P and Q are not. (b) "Relative misalignments" between the two contexts increase with distance from their centers. Claim 1 [Context Misalignment:] Under the assumption of Eq. (1), the average misalignment per-pixel between two context regions of size M × M is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Claim 2 [Figure 4 :</head><label>24</label><figDesc>Needle Misalignment:] Under the assumption of Eq. (1), the average misalignment per needle-pixel between the two Needles of patch-size m × m is: AvgM isalignment(Needle(p), Needle(q)) Matching Needles vs. Matching Large Patches. A sharp 'reference' image (a) was zoomed by 0.75 to obtain a 'target' image. Reference patches searched for their 1st NN in the target image. Reconstructed images (b and c) were generated using the grayscale of the center pixel of each NN. Needle-NNs find good matches even in textured regions, whereas 25 × 25 patches do not (Please zoom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>important distinction that m ≪ M . It is interesting to note that the average misalignment per needle-pixel does not depend on the number L of levels (patches) along the needle, nor on the size M of the spatial context P covered by the needle (as long as Eq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Reliability of "signal fit" in degraded images. Matches are searched for corrupted patches (q = N N (p)), but errors ||p − q|| are measured between the true underlying signals. (Red for Patch-NNs; Blue for Needle-NNs). Already for small levels of degradation, NeedleMatch shows a dramatic improvement over regular patch-matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Non-parametric blur kernels ity with Needle-based similarity in a variety of patch-based applications, leads to improved results (Sec. 5.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Visual example of "signal-fit" errors in degraded images (Patch-NNs vs. Needle-NNs). NNs are searched in the corrupted image (upper-left),but the reconstructed image is built from the underlying high-quality patches in the groundtruth image (upper-right). Patch-NNs lead to bad "signal-fit", hence to poor reconstruction (bottom-left), even though built from high quality patches. Needle-NNs provide much better "signal-fit", hence much better reconstruction (bottom-right) (Please zoom-in). Please note that this is NOT an application; only a visualization of the error shown in the graphs ofFig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Needle-correspondences vs. Coarse-to-fine Optical-Flow. (a) A fast rotating fan induces severe motion blur. NeedleMatch provides better frame-to-frame correspondences on the blurry fan. (b) Errors averaged on hundreds of image pairs with varying blurs (see text). Optical flow is better for small blurs, while NeedleMatch is superior under severe blurs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>BM3D Denoising results. Average PSNR results for hundreds of contaminated images from<ref type="bibr" target="#b21">[21]</ref>. NeedleMatch leads to an overall PSNR improvement, especially at high noise-levels. (see Sec. 5.2.1 for more details)</figDesc><table>σ BM3D local 

(regular) 

BM3D 
global 

BM3D 
needle 

Improvement 
(over BM3D-regular) 

15 
31.04 
26.91 
31.19 
+0.15 
35 
27.34 
26.17 
27.57 
+0.24 
55 
24.96 
24.83 
25.38 
+0.42 
75 
24.13 
23.67 
24.94 
+0.81 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that zoom around the origin (0, 0) by a scale factor α, can be rewritten as zoom by α around any other point in the image (e.g., any patch center), plus a global image shift. The same holds also for image rotation.<ref type="bibr" target="#b1">2</ref> Average runtimes per 321×481 image (on a single CPU): When using KDtree-based search -5.2s for matching patches, and 16s for the needles. When using an efficient PatchMatch-like implementation<ref type="bibr" target="#b0">[1]</ref> for finding NNs, runtimes decrease to 0.8s for patches, and 1.5s for Needles.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Goldman. PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for the robust estimation of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1993-05" />
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local signal equalization for correspondence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="1881" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A review of image denoising algorithms, with a new one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buadess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling and Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="490" to="530" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The patch transform and its applications to image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Butman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised patch-based context from millions of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno>1010. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CMU Tech Report</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context as supervisory signal: Discovering objects with predictable context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="362" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Examplebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An algorithm for finding best matches in logarithmic expected time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematics Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="1977-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Tokyo,Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On sifts and their scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mayzels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistics of patch offsets for image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 12th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Analysis and Implementation of the BM3D Image Denoising Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="175" to="213" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Patch complexity, finite pixel correlations and optimal denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 12th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cost aggregation strategy with bilateral filter based on multi-scale nonlinear structure tensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="958" to="965" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 7th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc. 1</publisher>
			<date type="published" when="1981" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time dense and accurate parallel optical flow using cuda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marzat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dumortier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ducrot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG)</title>
		<meeting>the 17th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG)<address><addrLine>Plzen, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blind deblurring using internal patch recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cost aggregation and occlusion handling with WLS in stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1431" to="1442" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining the power of internal and external denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Photography (ICCP)</title>
		<meeting><address><addrLine>Cambridge, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Super-resolving noisy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Edge-based blur kernel estimation using patch priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Super-resolution from internet-scale scene matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contextual priming for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="191" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-scale cost aggregation for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1590" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-based local stereo matching using orthogonal integral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lafruit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. for Video Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1073" to="1079" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The internal statistics of a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Separating signal from noise using patch recurrence across scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
