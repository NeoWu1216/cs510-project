<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Going Deeper into First-Person Activity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghuang</forename><surname>Ma</surname></persName>
							<email>minghuam@andrew.cmu.eduhaoqif@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
							<email>kkitani@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Going Deeper into First-Person Activity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We bring together ideas from recent work on feature design for egocentric action recognition under one framework by exploring the use of deep convolutional neural networks (CNN). Recent work has shown that features such as hand appearance, object attributes, local hand motion and camera ego-motion are important for characterizing first-person actions. To integrate these ideas under one framework, we propose a twin stream network architecture, where one stream analyzes appearance information and the other stream analyzes motion information. Our appearance stream encodes prior knowledge of the egocentric paradigm by explicitly training the network to segment hands and localize objects. By visualizing certain neuron activation of our network, we show that our proposed architecture naturally learns features that capture object attributes and hand-object configurations. Our extensive experiments on benchmark egocentric action datasets show that our deep architecture enables recognition rates that significantly outperform state-of-the-art techniques -an average 6.6% increase in accuracy over all datasets. Furthermore, by learning to recognize objects, actions and activities jointly, the performance of individual recognition tasks also increase by 30% (actions) and 14% (objects). We also include the results of extensive ablative analysis to highlight the importance of network design decisions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently there has been a renewed interest in the use of first-person point-of-view cameras to better understand human activity. In order to accurately recognize first-person activities, recent work in first-person activity understanding has highlighted the importance of taking into consideration both appearance and motion information. Since the majority of actions are centered around hand-object interactions in the first-person sensing scenario, it is important to capture appearance corresponding to such features as hand regions, grasp shape, object type or object attributes. Capturing motion information such as local hand movements and global  <ref type="figure">Figure 1</ref>: Approach overview. Our framework integrates both appearance and motion information. The appearance stream captures hand configurations and object attributes to recognize objects. The motion stream captures objects motion and head movement to recognize actions. The two streams are also learned jointly to recognize activities. head motion, is another important visual cue as the temporal motion signature can be used to differentiate between complementary actions such as take and put or periodic actions such as the cut with knife action. It is also critical to reason about appearance and motion jointly. It has been shown in both third-person <ref type="bibr" target="#b10">[11]</ref> and first-person activity analysis <ref type="bibr" target="#b19">[20]</ref> that these two streams of activity information, appearance and motion, should be analyzed jointly to obtain best performance.</p><p>Based on these insights, we propose a deep learning architecture designed specifically for egocentric video, that integrates both action appearance and motion within a single model 1. More specifically, our proposed network has a two stream architecture composed of an appearancebased CNN that works on localized object of interest image frames and a motion-based CNN that uses stacked optical flow fields as input. Using the terminology of <ref type="bibr" target="#b4">[5]</ref>, we use late fusion with a fully-connected top layer to formulate a multi-task prediction network over actions, objects and activities. The term action describes motions such as put, scoop or spread. The term object refers to item such as bread, spoon or cup. The term activity is used to represent an action-object pair such as take milk container.</p><p>The appearance-based stream is customized for ego-centric video analysis by explicitly training a hand segmentation network to enable an attention-based mechanism to focus on certain regions of the image near the hand. The appearance-based stream is also trained with object images cropped based on hand location to identify objects of manipulation. In this way, the appearance-based stream is enabled to encode such features such as hand-object configurations and object attributes. The motion-based stream is a generalized CNN that takes as input a stack of optical-flow motion fields. This stream is trained to differentiate between action labels such as put, take, close, scoop and spread. Instead of compensating for camera ego-motion as a pre-processing step, we allow the network to automatically discover which motion patterns (camera, object or hand motion) are most useful for discriminating between action types. Results show that the network automatically learns to differentiate between different motion types.</p><p>We train the appearance stream and motion stream jointly as a multi-task learning problem. Our experiments show that by learning the parameters of our proposed network jointly, we are able to outperform state-of-the-art techniques by over 6.6% on the task of egocentric activity recognition without the use of gaze information, and in addition improve the accuracy of each sub-task (30% for action recognition and 14% object recognition).</p><p>Perhaps more importantly, the trained network also helps to better understand and to reconfirm the value of key features needed to discriminate between various egocentric activities. We include visualizations of neuron activations and show that the network has learned intuitive features such as hand-object configurations, object attributes and hand motion signatures isolated from global motion.</p><p>Contributions: (1) we formulate a deep learning architecture customized for ego-centric vision; (2) we obtain state-of-the-art performance propelling the field towards higher performance; (3) we provide ablative analysis of design choices to help understand how each component contributes to performance; and (4) visualization and analysis of the resulting network to understand what is being learned at the intermediate layers of the network. The related work is summarized as follows. Human Activity Recognition: Traditionally, in videobased human activity understanding research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref>, many approaches make use of local visual features like HOG <ref type="bibr" target="#b16">[17]</ref>, HOF <ref type="bibr" target="#b16">[17]</ref> and MBH <ref type="bibr" target="#b33">[34]</ref> to encode appearance information. These features are typically extracted from spatiotemporal keypoints <ref type="bibr" target="#b15">[16]</ref> but can also be extracted over dense trajectories <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>, which can improve recognition performance. Most recently, it has been shown that the visual feature representation can be learned automatically using a deep convolutional neural network for image understanding tasks <ref type="bibr" target="#b14">[15]</ref>. In the realm of action recognition, Si-monyan and Zisserman <ref type="bibr" target="#b29">[30]</ref> proposed a two-stream network to capture spatial appearance on still images and temporal motion between frames. Ji et al. <ref type="bibr" target="#b11">[12]</ref> used 3D convolutions to extract both spatial and temporal features using a one stream network. Wang et al. <ref type="bibr" target="#b35">[36]</ref> further develops trajectory-pooled deep-convolutional descriptor (TDD) to incorporate both specially designed features and deeplearned features to achieve state-of-the-art results. First-Person Video Analysis: In a similar fashion to thirdperson activity analysis, the first-person vision community has also explored various types of visual features for representing human activity. Kitani et al. <ref type="bibr" target="#b13">[14]</ref> used optical flowbased global motion descriptors to discover ego-action in sports videos. Spriggs et al. <ref type="bibr" target="#b31">[32]</ref> performed activity segmentation GIST descriptors. Pirsiavash et al. <ref type="bibr" target="#b26">[27]</ref> developed a composition of HOG features to model object and hand appearance during an activity. Bambach et al. <ref type="bibr" target="#b1">[2]</ref> used hand regions to understand activity. Fathi et al. proposed mid-level motion features and gaze for recognizing ego-centric activities in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. To encode first-person videos using those features, the most prevalent representations are BoW and improved Fisher Vector <ref type="bibr" target="#b24">[25]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, Li et al.</p><p>performed a systemic evaluation of features and provided a list of best practices of combining different cues to achieve state-of-the-art results for activity recognition. Similar to third-person vision activity recognition research, there has also been a number of attempts to use CNN for understanding activities in first-person videos. Ryoo et al. <ref type="bibr" target="#b28">[29]</ref> develops a new pooled feature representation and shows superior performance using CNN as a appearance feature extractor. Poleg et al. <ref type="bibr" target="#b27">[28]</ref> proposes to use temporal convolutions over optical flow motion fields to index first-person videos. However, a framework to integrate the success of ego-centric features and the power of CNNs is still missing due to challenges of feature diversity and limited training data. In this paper, we aim to design such a framework to address the problem of ego-centric activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Egocentric Activity Deep Network</head><p>We describe our proposed deep network architecture for recognizing activity labels from short video clips taken by an egocentric camera. As we have argued above, the manifestation of an activity can be decomposed into observed appearance (hand and objects) and observed motion (local hand movement and user ego-motion). Based on this decomposition, we develop two base networks: (1) ObjectNet takes a single image as input to determine the appearance features of the activity and is trained using object labels;</p><p>(2) ActionNet takes a sequence of optical flow fields to determine the motion features of the activity and is trained using action labels. Taking the output of both of these networks, we use a late fusion step to concatenate the output of the two networks and uses the joint representation to pre-   Hand segmentation network is first trained using images and binary hand masks. Localization network is then fine-tuned from hand segmentation network using images and object location heatmaps synthesized from object locations. dict three outputs, namely, action, object and activity. More formally, given a short video sequence of N image frames I = {I 1 , . . . , I N }, our network predicts three output labels: {y object , y action , y activity }. The architecture of the entire network is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">ObjectNet: Recognizing Objects from Appearance</head><p>As shown in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref>, recognizing objects in videos is an important aspect of ego-centric activity understanding. We aim to predict the object label y object in this section. To do so, we are particularly interested in the object being interacted with or manipulated -the object of interest. However, detecting all objects accurately in the scene is difficult. It also provides limited information about the interested object. Our proposed model will first localize and then recognize the object of interest.</p><p>Although we can assume that the object of interest is often located at the center of the subject's reachable region, it is not always present at the center of the camera image due to head motion. Instead, we observe that the object of interest most frequently appears in the vicinity of hands. A similar observation was also made in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref>. Besides hand location, hand pose and shape is also important to estimate the manipulation points as shown in <ref type="bibr" target="#b18">[19]</ref>. We therefore seek to segment the hands out of the image and use hand appearance to predict the location of the object of interest. We first train a pixel-to-pixel hand segmentation network using raw images and binary hand masks. This network will output a hand probability map. To predict object of interest location using this hand representation, a naive approach is to build a regression model on top. For instance, we can train another CNN or a regressor using features from the hand segmentation network. However, our experiments with this approach achieve low performance due to limited training data. The prediction tends to favor the image center as it is where the object of interest occurs most frequently. Our final pipeline is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. After training the hand segmentation network, we fine-tune a localization network to predict a pixel-level object occurrence probability map. Inspired by previous work in pose estimation <ref type="bibr" target="#b25">[26]</ref>, we synthesize a heatmap by placing a 2D Gaussian distribution at the location of the object of interest. We use this heatmap as ground-truth and use per-pixel Euclidean loss to train the network. To transfer the hand representation learned from the segmentation network, we initialize the localization network with the weights from the segmentation network and then fine-tune the localization network with the new loss layer. The details of the segmentation and localization CNNs are listed as follows.</p><p>(1) Hand segmentation network: For training data, we can either use annotated ground-truth hand masks or output of pixel-level hand detectors like <ref type="bibr" target="#b17">[18]</ref>. For the network architecture, we use a low resolution FCN32-s as in <ref type="bibr" target="#b20">[21]</ref> as it is a relatively smaller model and converges faster. The loss function for the segmentation network is the sum of (b) Ground-truth hand masks which can be annotated manually or generated using hand detectors such as <ref type="bibr" target="#b17">[18]</ref>. (c) Synthesized location heat-maps by placing a Gaussian bump at the object location.</p><p>per-pixel two-class softmax losses.</p><p>(2) Object localization network: For training data, we first manually annotate object of interest locations in the training images of the hand segmentation network. We then synthesize the location heatmaps using a Gaussian distribution as discussed above. Examples of training data are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. We use the same FCN32-s network architecture and replace the softmax layer with a per-pixel Euclidean loss layer.</p><p>To this extent, we have trained an object localization network that outputs a per-pixel occurrence probability map of the object of interest. To generate the final object region proposals, we first run the localization network on input image sequence I = {I 1 , I 2 , . . . , I N } and generate probability maps of object locations H = {H 1 , H 2 , . . . , H N }. We then threshold each probability map and use the centroid of the largest blob as the predicted center of the object. We then crop the object out of the raw image at the predicted center using a fixed-size bounding box. We fix the crop size and ignore the scale difference by observing that the object of interest is always within the reachable distance of the subject. In this way, we generate a sequence of cropped object region images O = {O 1 , O 2 , . . . , O N } as the input of the object recognition CNN. The localization result is stable on a per-frame basis, hence there is no temporal smoothness adopted.</p><p>With the cropped image sequence of objects of interest {O i }, we then train the object CNN using the model of CNN-M-2048 <ref type="bibr" target="#b2">[3]</ref> to recognize the objects. We choose this network architecture due to its high performance on Ima-geNet image classification. Since a better architecture of base network is out of the scope of this work, we use this network as our base network in this paper unless otherwise mentioned. We adapt it to different tasks (e.g. action recognition) with minimum modifications in this paper. For object recognition, we train the network using {(O i , y object }) pairs as training data and softmax as the loss function. At testing time, we run the network on the cropped object image O i to predict object class scores. We then calculate the mean score of all frames in a sequence for each activity class and select the activity label with largest mean score as the final predicted label of object.</p><p>Up until now, we have trained a localization network to localize the object of interest by explicitly incorporating hand appearance. Using the cropped images of the localized object of interested, we have trained an object recognition network to predict the object label y object . We will show later that this recognition pipeline also captures important appearance cues such as object attributes and hand appearance. We now move forward to the motion stream of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">ActionNet: Recognizing Actions from Motion</head><p>In this section, our target is to predict the action label y action from motion. Unlike straightforward appearance cues like hands and objects discussed in previous section, motion features in ego-centric videos are more complex because the head motion might cancel the object and hand motion. Although Wang et al. <ref type="bibr" target="#b34">[35]</ref> shows that compensation of camera motion improves accuracy in traditional action recognition tasks, for ego-centric videos, background motion is often a good estimation of head motion and thus an important cue for recognizing actions <ref type="bibr" target="#b19">[20]</ref>. Instead of decoupling foreground (object and hand) motion and background (camera) motion and calculating features separately, we aim to use CNN to capture different local motion features and temporal features together implicitly.</p><p>In order to train a CNN network with motion input, we follow <ref type="bibr" target="#b29">[30]</ref> to use optical flow images to represent motion information. In particular, given a video sequence of N frames I = {I 1 , I 2 , ..., I N } and corresponding action label y action , we first calculate optical flow of each two consecutive frames and encode the horizontal and vertical flow separately in U = {U 1 , U 2 , ..., U N −1 } and V = {V 1 , V 2 , ..., V N −1 }. To incorporate temporal information, we use a fixed length of L frames and stack corresponding optical flow images together as input samples of the network noted as</p><formula xml:id="formula_0">X = {X 1 , ..., X N −L+1 } where X i = {U i , V i , ..., U i+L−1 , V i+L−1 }.</formula><p>With motion represented in optical flow images, we train the motion CNN using {(X i , y action )} pairs as training data and softmax as the loss function. At testing time, we run the network on input motion data X i to predict the scores for each action class. We then average the scores for all frames in the sequence and pick the action class with maximum average score as the predicted label of the action. With the learned representation of objects and actions, we now move to the next section for activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fusion Layer: Recognizing Activity</head><p>In this section, we seek to recognize the activity label y activity given the representations learned from the two network streams in previous sections. A natural approach is to use the two networks as feature extractors and training a classifier using activity labels. However, this approach ignores the co-relation between actions, objects and activities. For instance, if we are confident that the action is stir from repeated circular motion, it is highly probable that the object is tea or coffee. In the other way, if we know the object is tea or coffee, the probability that the action is cut or fold should be very low. Based on this intuition, we fuse the action and object networks together as one network by concatenating the second last fully connected layers from the two networks and add another fully connected layer on top. We then add another loss layer for activity on top. The final fused network therefore has three weighted losses: action, object and activity loss. Then weighted sum of three losses is calculated as the overall loss. We can set the weights empirically by the relative importance of three tasks and train one network to learn activity, action and object simultaneously. The loss function for the final network can be formulated as L network = w 1 · L action + w 2 · L object + w 3 · L activity .</p><p>To train the fused network, we transfer the weights of the trained motion CNN and object CNN and fine-tune it to recognize the activity. Specifically, given a video sequence of N frames I = {I 1 , I 2 , ..., I N }, we follow section 2.1 to localize the objects of interest and get a sequence of object images O = {O 1 , O 2 , ..., O N }. We follow section 2.2 to calculate optical flow image pairs {U, V} and stack them using a fixed length of L frames into X = {X 1 , ..., X N −L+1 } where X i = (U i , V i ). At training time, for each optical flow blob X i , we randomly pick a object image O j where i ≤ j &lt; i + L and form the training data pair (X i , O j , y action , y object , y activity ). This is also a way to augment the training data to avoid over-fitting. At testing time, we pick the center object image frame such that j = (2i + L)/2 as the annotated boundary of an activity sequence is loose. We run the network on all data pairs to predict the scores for activity. We then average the scores and use the activity class with maximum average score as the predicted activity label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We briefly introduce the datasets in Section 3.1 and describe the details for training networks in Section 3.2. We then present experimental results for the three tasks of object recognition (Section 3.3), action recognition (Section 3.4) and activity recognition (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We run experiments on three public datasets: GTEA, GTEA gaze (Gaze) and GTEA gaze+ (Gaze+) as these datasets were collected using a head-mount camera and most of the activities involve hand-object interactions. The annotation label for each activity contains a verb (action) and a set of nouns (object). We perform all our experiments using leave-one-subject-out cross-validation. We also report results on fixed-splits following previous work. GTEA: This dataset <ref type="bibr" target="#b8">[9]</ref> contains 7 types of activities performed by 4 different subjects. There are 71 activity categories and 525 instances in the original labels. We report comparative results on two subsets used in previous work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9]</ref>: 71 classes and 61 classes. Gaze: This dataset <ref type="bibr" target="#b5">[6]</ref> contains 17 sequences performed by 14 different subjects. There are 40 activity categories and 331 instances in the original labels. We report results on two subsets used in previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6]</ref>: 40 classes and 25 classes. Gaze+: This dataset <ref type="bibr" target="#b5">[6]</ref> contains 7 types of activities performed by 10 different subjects. We report results on a 44 classes subset with 1958 action instances following <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Training</head><p>For network architecture, we use FCN32-s <ref type="bibr" target="#b20">[21]</ref> for hand segmentation and object localization. We use CNN-M-2048 <ref type="bibr" target="#b2">[3]</ref> for action and object recognition. Due to the limited sizes of the three public datasets, we adopt the finetuning <ref type="bibr" target="#b22">[23]</ref> approach to initialize our networks. Specifically, we use available pre-trained models from three large-scale datasets: UCF101 <ref type="bibr" target="#b30">[31]</ref>, Pascal-Context <ref type="bibr" target="#b21">[22]</ref> and ImageNet <ref type="bibr" target="#b3">[4]</ref> for motion, hand segmentation and object CNN respectively.</p><p>Data augmentation. To further address the problem of limited data, we apply data augmentation <ref type="bibr" target="#b14">[15]</ref> to improve generalization of CNN networks. Crop: All of our network inputs are resized to K × C × 256 × 256, where K is batch size, C is input channels. We randomly crop them to K × C × 224 × 224 at training time. Flip: We randomly mirror input images horizontally. For optical flow frames (U i , V i ), we mirror them to (255 − U i , V i ). Replication: We also replicate training data by repeating minority classes to match with majority classes at training time.</p><p>Training details. We use a modified version of Caffe <ref type="bibr" target="#b12">[13]</ref> and Nvidia Titan X GPU to train our networks. We use stochastic gradient descent with momentum as our optimization method. We use a fixed learning rate of γ = 1e−8 for fine-tuning hand segmentation and object localization CNNs, γ = 5e − 4 for motion CNN and γ = 1e − 4 for object CNN. For joint training, we lower the learning rate of two sub-networks by a factor of 10. We use batch sizes of 16, 128, 180 for localization, object and motion CNNs respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ObjectNet Performance</head><p>We evaluate the localization network and object recognition network of the ObjectNet stream. Localizing object of interest. As described in Section 2.1 and illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, we first train a hand segmen- tation to learn the bottom layers of the object localization network. The intuition behind this training procedure is to purposefully bias the object localization network to use the hands as evidence to infer an object bounding box. We first train a hand segmentation network using the model of <ref type="bibr" target="#b20">[21]</ref> to capture hand appearance information. We then swap out the top layer for segmentation with a top layer optimized for object localization (i.e., fine-tune the network to repurpose it for object localization). The network has an input size of K × 3 × 256 × 256 where K is the batch size. After five convolutional (conv1 − conv5) layers of 2 × 2 pooling operations, the input image dimension is down-sampled to 1/32 of the original size. The final deconvolution layer upsamples it back to the original size of K ×2×256×256. We use raw images and hand masks provided with GTEA and Gaze as training data for the hand segmentation network. Since Gaze+ is not annotated with hand masks, we use <ref type="bibr" target="#b17">[18]</ref> to detect hands and use the result to train the network. Once the segmentation network is trained, we use manually annotated training images of object locations to re-purpose the the network for localization. Instead of using raw object locations (exact center position of the object), we place a Gaussian bump at the center position to create a heat-map representation as described in Section 2.1. <ref type="figure" target="#fig_5">Figure 5</ref> shows qualitative results of the localization network. The localization network successfully predicts the key object of interest out of other irrelevant objects in the scene. Notice that the result is strongly tied to the hand as the network is pre-train for hand segmentation. The results also show that the model can deal seamlessly with different hand configurations like one-hand or two-hand scenarios.</p><p>Recognizing object of interest. The localized object images are used to train the object CNN. <ref type="table" target="#tab_1">Table 1</ref> compares the performance of our proposed methods with <ref type="bibr" target="#b4">[5]</ref>. Our proposed method dramatically outperforms <ref type="bibr" target="#b4">[5]</ref> by 14%. As seen in <ref type="table" target="#tab_1">Table 1</ref> the boost in performance can be attributed to improved localization through the use of hand segmentation-based pre-training.</p><p>We visualize the activations of the object recognition network and present two important findings: (1) Hands are important for object recognition: Although the localization network is targeted for object of interest, the cropped image also contains a large portion of hands. We visualize the activations of the conv5 layer and find that the 50 th   neuron unit responds particularly strongly to training images with large hand regions as shown in <ref type="figure" target="#fig_6">Figure 6</ref>. We further test the network with test images shown in <ref type="figure" target="#fig_6">Figure 6</ref>. We observe that the strongest activations overlap with hand regions. We therefore conclude that the object recognition network is learning appearance features from hand regions to help recognize objects. When there is no hand in the scene, the localization network will predict no interacting object. Since some of the iterating objects as tea bags and utensils are small, it is extremely challenging to locate them using an traditional object detector. The hands, their shape and their motion can act as a type of proxy small objects.</p><p>(2) Object attributes are important for object recognition: <ref type="figure" target="#fig_7">Figure 7</ref> shows examples of a particular neuron unit responding to particular object attributes like color, texture and shape. In <ref type="figure" target="#fig_7">Figure 7b</ref>, we observe that this specific neuron is activated when it observes round objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">ActionNet performance</head><p>We first evaluate the ActionNet to recognize actions. In our experiments, we crop and resize input images to 256 × 256 and calculate optical flow using OpenCV GPU implementation of <ref type="bibr" target="#b36">[37]</ref>. We clip and normalize the flow values from [−20, 20] to [0, 255]. We found empirically that L = 10 optical flow frames generates good performance. <ref type="table" target="#tab_3">Table 2</ref> compares our proposed method with the baseline in <ref type="bibr" target="#b4">[5]</ref>. While our motion network significantly improves the average recognition accuracy, we are also interested in understanding what the network is learning. Our visual-    ization shows two important discoveries: (1) our motion network automatically identifies foreground (objects and hands) motion out of complex background (camera) motion (2) our motion network automatically encodes temporal motion patterns.</p><p>(1) Camera motion compensation is important for action recognition: As summarized in <ref type="bibr" target="#b19">[20]</ref>, motion compensation is important for ego-centric action understanding as it provides more reliable foreground motion features. Through visualization, we discover that the network is automatically learning to identify foreground objects and hands. <ref type="figure" target="#fig_8">Figure  8</ref> shows top 4 training sequences that activate a particular neuron unit most strongly in the conv5 layer. All these sequences have the same action verb put despite the diversity in camera egomotion. This shows that the network automatically learns to ignore background camera motion for( this neuron. We further test the network with a few test sequences of put actions. The results (in <ref type="figure" target="#fig_9">Figure 9</ref>) agree with our observation in the following aspects: (1) Activation of the same unit is very strong on all these test put actions compared with other actions; (2) The strongest activation location coincides roughly with foreground objects and hands location in <ref type="figure" target="#fig_9">Figure 9e</ref>.</p><formula xml:id="formula_1">a)(b) (c) (d) (e) (f)</formula><p>(2) Temporal motion patterns are important for action recognition: While instantaneous motion is an important cue for action recognition, it is also crucial to integrate temporal motion information as shown in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. <ref type="figure" target="#fig_8">Figure 8</ref> shows that the neuron unit is able to capture the movement of subjects during image sequences. We perform another experiment by reversing the order of the input optical flow images to observe how this neuron responds. <ref type="figure" target="#fig_9">Figure  9f</ref> shows the activation maps of the same neuron unit with respect to reversed optical flow frames. The weak responses on suggests that the temporal ordering has been encoded in this neuron. This is reasonable, as actions such as put and take can only be differentiated be preserving temporal ordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Activity recognition</head><p>We finally evaluate our framework for the task of activity recognition. In this experiment, we concatenate the two fully connected layers from the ActionNet and Object-Net and add another fully connected layer on top. Then we fine-tune the two streams together with optical flow images, cropped object images and three weighted losses for three tasks. We compare our results with the state-of-the-art method from Li et al. <ref type="bibr" target="#b19">[20]</ref> in <ref type="table">Table 3</ref>. We also report results using the two-stream networks from Simonyan and Zisserman <ref type="bibr" target="#b29">[30]</ref> without decomposing activity labels. The confusion matrices are shown in <ref type="figure">Figure 10</ref>. Our proposed method   significantly improves the state-of-the-art performance on all datasets. We conclude that this is due to better representations of action and object from the base motion and appearance streams in our framework. We further analyze two main findings from our experiments.</p><p>(1) Joint training is effective: Instead of fixing Action-Net and ObjectNet, and only training stacked layers on top, we jointly train all three networks using three losses as discussed in Section 2.3. This avoids over-fitting the newly added top layers and leads to a joint representation of activities with actions and objects. In our experiments, we set w action = 0.2, w object = 0.2 and w activity = 1.0. We set the activity loss weight higher for faster convergence of activity recognition. We also compare joint training with SVM fusion of two networks in <ref type="table">Table 3</ref>. Joint training boosts the performance consistently by 27% over all datasets.</p><p>(2) Object localization is crucial: We seek to understand the importance of localizing objects by training a network using cropped object images and activity labels. We compare three networks for activity recognition with best results reported in <ref type="bibr" target="#b19">[20]</ref>: (1) motion-cnn (temporal-cnn) using optical flow images and activity labels (2) spatial-cnn using raw images and activity labels (3) object-cnn using cropped object images and activity labels. The performance is lower than <ref type="bibr" target="#b19">[20]</ref> on three networks as shown in <ref type="table">Table 3</ref> because we are not using any motion or temporal information. However, the performance of object-cnn is surprisingly close, only 9.6% lower (25.5% lower with motion-cnn, 20.6% lower with spatial-cnn) on average. We conclude that localizing the key object of interest is crucial for egocentric activity understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We have developed a twin stream CNN network architecture to integrate features that characterize ego-centric activities. We demonstrated how our proposed network jointly learns to recognize actions, objects and activities. We evaluated our model on three public datasets and it significantly outperformed the state-of-the-art methods. We further analyzed what the networks were learning. Our visualizations showed that the networks learned important cues like hand appearance, object attribute, local hand motion and global ego-motion as designed. We believe this will help advance the field of ego-centric activity analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Framework architecture for action, object and activity recognition. Hand segmentation network is first trained to capture hand appearance. It is then fine-tuned to a localization network to localize object of interest. Object CNN and motion CNN are then trained separately to recognize objects and actions. Finally, the two networks are fine-tuned jointly with a triplet loss function to recognize objects, actions and activities. This proposed network beats all baseline models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Pipeline for localization network training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Training data examples for localization CNN. (a) Raw video images with annotated object locations (in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Object localization using Hand Information. Visualization of object location probability map (red) and object bounding box (green). (a: GTEA, b: Gaze+)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>(a) Top 5 training images with strongest activations from the 50 th neuron unit in the conv5 layer. (b) 5 test images (top row) and 13×13 activations (bottom row) of the same unit. The visualization shows that this unit responds strongly to hand regions. The object network is capturing hand appearance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Neuron activation in the conv5 layer for test images. Neuron responding to: (a) transparent bottle, (b) edges of container, (c) cups, (d) white round shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Top 4 training sequences with strongest activations for the 346 th neuron unit in conv5 layer. (a) Start/end image frames, (b) Start/end optical flow images, (c) Average optical flow for each sequence. From top to bottom, groundtruth activity labels are put cupPlateBowl, put knife, put cupPlateBowl and put lettuce container.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>4 test sequences and activations of the 346 th neuron unit in conv5 layer. (a) Start/end image frames, (b) Start/end optical flow images, (c) Average optical flow, (d) 13 × 13 activation maps of the neuron unit using the optical flow sequence, (e) Overlay of activation map on the end image frame, (f) 13 × 13 activation maps of the neuron unit using reversed optical flow sequence. From top to bottom, ground-truth activity labels are put milk container, put milk container, put cupPlateBowl, put tomato cupPlate-Bowl.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3 :Figure 10 :</head><label>310</label><figDesc>Quantitative results for activity recognition. (a) Best results reported from Li et al.<ref type="bibr" target="#b19">[20]</ref>. (b) Two-stream CNN<ref type="bibr" target="#b29">[30]</ref> results with single streams, SVM-fusion and joint training. (c) Results from our proposed methods with localized object only, SVM-fusion and joint training. Our proposed joint training model significantly outperforms the two baseline approaches on all datasets. Note that even the network trained using only cropped object images (object-cnn) achieves very promising results thanks to our localization network. ( * : fixed split, O: object, M: motion, E: egocentric, H: hand, G: gaze). Confusion matrices of our proposed method for activity recognition. Improvement on the Gaze dataset is lower due to low video quality and inefficient data. (best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Average object recognition accuracy. Proposed 
method performs 14% better than the baseline. Joint 
training of motion and object networks improves accuracy 
across all datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average action recognition accuracy. Proposed 
method performs 30% better than the baseline. Joint 
training of motion and object networks improves accuracy 
across all datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was funded in part by a grant from the Pennsylvania Department of Health's Commonwealth Universal Research Enhancement Program and CREST, JST.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bambach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="314" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Action recognition by learning midlevel motion features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling actions through state changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2579" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference On</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Observing humanobject interactions: Using spatial and functional compatibility for recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast unsupervised ego-action learning for first-person sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3241" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pixel-level hand detection in egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to predict gaze in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3216" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compact cnn for indexing egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Applications of Computer Vision WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV- TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal segmentation and activity classification from first-person sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Spriggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops, 2009. CVPR Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
