<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Sparse Light Field Coding Reveals about Scene Structure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Johannsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonin</forename><surname>Sulc</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Goldluecke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Sparse Light Field Coding Reveals about Scene Structure</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel method for depth estimation in light fields which employs a specifically designed sparse decomposition to leverage the depth-orientation relationship on its epipolar plane images. The proposed method learns the structure of the central view and uses this information to construct a light field dictionary for which groups of atoms correspond to unique disparities. This dictionary is then used to code a sparse representation of the light field. Analyzing the coefficients of this representation with respect to the disparities of their corresponding atoms yields an accurate and robust estimate of depth. In addition, if the light field has multiple depth layers, such as for reflective or transparent surfaces, statistical analysis of the coefficients can be employed to infer the respective depth of the superimposed layers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the scope of this work, light fields are dense collections of views of a scene with view point shifting parallel to the image plane. Image plane coordinates are pairs (x, y), view point coordinates are pairs (s, t), so a light field is four-dimensional. Let us look at the structure of such a light field and its epipolar plane images (EPIs), which are slices in the (x, s) and (y, t) planes, see figure 1. One can immediately observe that the data exhibits a large amount of redundancy, as patches in any of the views reappear slightly shifted in multiple neighbouring views. For a light field of a Lambertian scene, the amount of shift from one view to the next depends linearly on the disparity of the patch, which yields the well-known orientation-depth relationship on epipolar plane images <ref type="bibr" target="#b1">[2]</ref>.</p><p>This inherently sparse structure of the light field has been exploited in several lines of research. A natural application is light field compression <ref type="bibr" target="#b14">[15]</ref> and compressive sensing <ref type="bibr" target="#b16">[17]</ref>, where the redundancy is used to generate an efficient light field coding scheme or reduce the amount of data one has to record to capture one. Indeed, the key idea is that   <ref type="bibr">(EPIs)</ref>. The picture shows the center view of a light field parametrized by image coordinates x and y. On the bottom and right, the epipolar plane images for the white lines in the center view are shown, where s and t describe varying view point coordinates. As the camera moves, 3D scene points trace straight lines on the EPIs, whose slope is inversely proportional to the distance of the point <ref type="bibr" target="#b1">[2]</ref>. Thus, orientation on the EPI is related to local depth.</p><p>if one knows the depth (and thus disparity) for all of the points in any of the 2D views, one can perfectly reconstruct the light field except for occlusions. The latter work <ref type="bibr" target="#b16">[17]</ref> also employs patch dictionaries to learn the 4D structure and improve reconstruction.</p><p>Just like traditional 2D image patch dictionaries <ref type="bibr" target="#b6">[7]</ref>, 4D light field dictionaries can also be used for regularizing inverse problems. In particular, these have been employed for light field denoising and deconvolution, inpainting, and super-resolution <ref type="bibr" target="#b13">[14]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, they solve similar problems by modeling light field patches as Gaussian random variables conditioned on disparity to construct a GMM prior. Similarly to our work, they generate patches synthetically based on disparity, but not to create dictionaries. In contrast, the papers which also explicitly employ light field dictionaries <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref> learn structure directly on 4D light field patches. While this allows sparse coding and construction of priors, it can not be expected that the orientation-depth relationship on the EPIs is preserved in the atoms.</p><p>However, the correspondence between depth and orientation is exactly what has been leveraged in a lot of work on lightfield-based depth estimation, e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. We will discuss this line of research in depth in the next section, as it is most closely related to our work.</p><p>Contributions. In this work, for the first time, we unify the idea of orientation-based depth reconstruction with sparse light field coding based on generating a depthbased dictionary. In contrast to previous work, we first learn a lower-dimensional dictionary on the center view only. Then, the base atoms of the center view dictionary are "lifted" into the 4D light field domain based on a generative model such that the resulting light field atoms have a unique, known disparity. It turns out that with a simple averaging strategy, the sparse coding coefficients for the lifted dictionary already allow to compute a reliable per-pixel estimate of disparity for Lambertian surfaces. However, in contrast to orientation analysis using the structure tensor of EPIs <ref type="bibr" target="#b29">[30]</ref>, disparities can be much larger than one pixel.</p><p>Moreover, statistical analysis of the coefficients reveals whether the light field contains multiple depth layers caused by transparent or reflective surfaces. The disparity of these layers can be reconstructed, substantially surpassing existing state-of-the-art <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref> for multi-orientation estimation in accuracy and robustness, in particular on real-world light fields from plenoptic cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There has been a substantial amount of recent work and great progress in depth estimation for Lambertian surfaces in light fields. The pioneering work which introduced epipolar volumes is <ref type="bibr" target="#b1">[2]</ref>, where they analyze slopes of lines by line fitting to estimate disparity. Based on these ideas, <ref type="bibr" target="#b4">[5]</ref> perform shearing of the epipolar volume to subsequently extract the lines with the smallest color variations. In <ref type="bibr" target="#b12">[13]</ref>, they refine the idea of line extraction and obtain very accurate results on extremely large scale light fields. The first order structure tensor is used in <ref type="bibr" target="#b29">[30]</ref> to compute orientation on the EPIs and exploit the orientation-depth relationship. They also propose a variational framework to optimize results with respect to occlusion constraints. In <ref type="bibr" target="#b23">[24]</ref>, they calculate depth by shearing EPIs and measuring defocus responses. The work <ref type="bibr" target="#b10">[11]</ref> employs the phase-shift theorem to match sub-aperture images in order to deal with the narrow baseline of light field cameras. The idea of a scaledepth space is pursued in <ref type="bibr" target="#b25">[26]</ref> to find the best disparity. A data term based on active wavefront sampling is considered in <ref type="bibr" target="#b9">[10]</ref> within a variational stereo framework. None of the above methods employ sparse coding of the light field for the purpose of depth reconstruction. However, in <ref type="bibr" target="#b8">[9]</ref> they use the idea of redundancy of sub-aperture views and used sparsity of the RPCA as new matching term. Likewise, <ref type="bibr" target="#b17">[18]</ref> employ sparsity ideas to model light field patches as Gaussian random variables conditioned on its disparity value. They construct a patch prior and can estimate disparity by finding the nearest PCA subspace. <ref type="figure">Figure 2</ref>. Dictionary lifting. Left: 2D epipolar plane patches are generated from line shaped atoms on the center view. The trained patches are extrapolated by slightly shifting the base patch according to the chosen disparity when moving from layer to layer (yellow). Afterwards, the final patch (red) is extracted from the region where all data is valid. Right: illustration of the same procedure for crosshair-shaped base patches. Please note that the 3D visualization is insufficient to show all aspects of what happens, as the underlying data is 4D. Thus, the values at the intersection of the two EPI slices do not match outside of the blue base patch area.</p><p>Much fewer work has been pursued on the topic of multilayer light fields, which appear in the context of reflective or transparent surfaces, but also to some extent in the case of specular reflections. Again based on the idea of orientation estimation, <ref type="bibr" target="#b28">[29]</ref> use the second order structure tensor to estimate disparity values for superimposed patterns. The work is again extended in <ref type="bibr" target="#b11">[12]</ref> with an improvement to handle different contributions from horizontal and vertical EPIs for a slightly better accuracy. It can also separate the two layers from each other. While both methods work well on rendered and gantry datasets, it turns out in section 6 that compared to ours, they are very sensitive to noise and calibration inaccuracies. Similarly, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref> optimize for two overlaid matching models for an epipolar volume using graph cuts or semi-global matching, respectively. In <ref type="bibr" target="#b24">[25]</ref>, they describe depth estimation for glossy surfaces with light field cameras. Light sources are estimated in order to separate the diffuse and specular part of signal. Again, none of these works explore the relationships between disparity estimation and sparse coding, which will be our focus in the remainder of the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sparse light field coding</head><p>We first briefly review ideas and notation for sparse coding and dictionary learning, and afterwards specialize to our scenario of light field coding.</p><p>The central idea is to represent a signal as a linear combination of elements from a dictionary. Consider a set of n patches x 1 , . . . , x n written as m-dimensional vectors x i ∈ R m , and a patch dictionary D ∈ R m×k where k is the number of elements, usually with k ≫ n. The problem of l 1 -sparse coding or Lasso <ref type="bibr" target="#b19">[20]</ref> is to find <ref type="figure">Figure 3</ref>. Visualization of a subset of the atoms of a light field patch dictionary with lines as base atoms. Each light field atom represents a 2D 5 × 5 epipolar plane image patch generated from a specific center view atom and using an individual disparity value. Every row of patches corresponds to one center view base atom, while each column of patches corresponds to a distinct disparity. A total of 320 light field atoms is visible, corresponding to 64 disparities and 5 different center view atoms.</p><formula xml:id="formula_0">argmin αi∈R k 1 2 x i − Dα i 2 2 + λ α i 1 , ∀1 ≤ i ≤ n,<label>(1)</label></formula><p>where λ is a regularization parameter. The columns of D represent the dictionary elements to approximate the input signal and are called atoms. To ensure comparability between the different atoms and input patches both are normalised, i.e. rescaled to a zero-mean and constant variance.</p><p>A real sparse solution would be obtained for the regulariser α 0 -i.e. the number of non-zero elements -which is not convex. Although there is no analytical link between the l 1 and l 0 norm, the l 1 norm is widely used instead and gives sparse solutions similar to the l 0 norm. The problem of dictionary learning is closely related to (1), but instead of optimizing over α given a dictionary D, the task is to find a dictionary D which is optimal in terms of representing the signal with a sparse α <ref type="bibr" target="#b15">[16]</ref>.</p><p>Creating light field dictionaries. When considering the problem of sparse coding for light fields, one needs to decide for the shape of the patches x i and thus the dictionaries' atoms. Remember that the key idea of the paper is to encode light field dictionaries in such a way that their sparse coding coefficients α yield information for disparity estimation. For this, we leverage the fact that if disparity is constant, the center view of a light field completely determines all other views. With this in mind, we create light field dictionary atoms according to a generative model. They are generated from atoms which are learned by training a standard image dictionary on the center view. Each center view dictionary atom generates a large number of light field dictionary atoms, one for each discrete disparity value under consideration. In particular, each light field atom which is generated corresponds to a unique disparity value. In the remainder of this section, we will formalize this process and discuss several possibilities to choose the patch shape of the base dictionary.</p><p>1D base patches, 2D EPI patches. The most straightforward way to create light field atoms is to lift a center view dictionary made up of lines. We train the base dictionary on all horizontal and vertical patches of a fixed constant length p of the center view. Every 1D-atom of the base dictionary can then be lifted to atoms for an epipolar plane image as follows. Consider a shift in view point parallel to the line, and assume the points on the line have constant disparity. Since for fixed disparity, there is a linear relationship between view point and image plane coordinates of the projection, all pixels in the base patch will shift by the same amount. To simplify notation, we assume disparity units are chosen such that when shifting to the next view, the pixel shift is exactly equal to the disparity value. We repeat this shifting for every view point coordinate, and thus generate a 2D patch where one coordinate is along the line in the image plane, the other coordinate is along the line of view points parallel to it, see <ref type="figure">figure 2</ref>.</p><p>In effect, this describes a generative model which creates an EPI patch from a 1D image patch, similar to <ref type="bibr" target="#b11">[12]</ref>, but without the need to model occlusion. To obtain the final light field dictionary atom, we cut out the valid area which lies exactly above the base line. To generate the complete light field dictionary, we repeat this for every base atom and every disparity label d = 1, . . . , L. Note that disparities itself need not be integer, each integer label corresponds to an actual disparity value λ d ∈ R. <ref type="figure">Figure 3</ref> shows an exemplary 2D patch dictionary created for a light field. We use the same dictionary to solve the Lasso 1 for both horizontal and vertical EPI patches, and all color channels.</p><p>In the following, we refer to this light field dictionary type as the 2D dictionary. It is light-weight, simple and efficient to compute. As a drawback, there is no inherent correlation between the horizontal and vertical EPI patches corresponding to a pixel, while of course disparity should be the same for both. Thus, we also consider two alternatives.</p><p>Crosshair base patch, 2×2D EPI patches. The first alternative is designed to enforce disparity consistency between horizontal and vertical EPIs. Here, the base patch has the shape of a crosshair of width and height p. For simplicity of implementation, the center pixel is duplicated and the horizontal and vertical lines lifted separately as above to create horizontal and vertical EPI patches. Thus, a single light field atom consists of a pair of orthogonal 2D patches in EPI space with consistent disparity, see <ref type="figure">figure 2</ref>.</p><p>The resulting Lasso problems (1) are higher-dimensional and thus computationally more expensive, however, only one has to be solved for horizontal and vertical EPI together. Thus, the different contributions do not have to be made consistent in the later optimization pass, which is conceptually more satisfying. However, as we will later see in the results, this approach is of higher accuracy for smooth areas but has more problems at occlusions, especially if the occlusion boundary is close to being vertical or horizontal. We will refer to this type of light field dictionary as cross.</p><p>Square base patch, 4D EPI patches. The most ambitious implementation employs complete 4D light field patches as atoms. Each base center view atom is a p × p square, which is lifted from the center view into every other <ref type="bibr">Figure 4</ref>. Different coding coefficient distributions. The graphs illustrate typical distributions for the sparse coding coefficients a d (x) as grey bars over the disparity range. Top: this distribution is likely to have only a single mode, we can see that a single normal distribution is a good fit. Bottom: distribution with two modes and a two-component GMM fitted to it using the EM-algorithm <ref type="bibr" target="#b20">[21]</ref>.</p><p>of the light field views to create the 4D light field atom. As we will also see later on, this type of light field dictionary, referred to as 4D, yields a high accuracy but is computationally very expensive. It also has a tendency to smear edges because of an inherently built-in spatial smoothing. In our experience, it is often advantageous to keep the point-wise results potentially more noisy but also more precise, and leave regularization to a global optimization step which is actually designed for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sparse coding for disparity estimation</head><p>As explained in section 3, estimating depth from epipolar plane images is equivalent to estimating the slope of the linear structures, as in the Lambertian case each line corresponds to the projection of a single 3D point. The non-Lambertian case will be discussed in the next section. Since each dictionary atom by construction corresponds to a well-defined slope, depth estimation from sparse dictionary learning comes down to analysing the result of the Lasso (1) for each pixel in the center view. To simplify notation, we first fix one pixel and drop indices related to the pixel position.</p><p>For each different disparity label d, we collect the responses of all coefficients α i in (1) which correspond to atoms of disparity d in a vector A d . As our dictionary is grayscale, if we consider color light fields, we need to solve several instances of (1) per pixel, one for each channel. Additional instances are required if horizontal and vertical EPIs are analyzed separately and not e.g. with the crosshair dictionary. In the cases of multiple instances, we just add all responses from all problem instances and atoms of disparity d to the vector A d as separate elements.</p><p>From the vector A d of responses for disparity d, we now compute a single number a d ∈ R by taking the sum of the absolute values in A d . Doing this for every pixel yields the final response function a : Ω ∋ x → <ref type="figure" target="#fig_1">(a 1 (x), . . . , a d (x)</ref>, . . . , a L (x)) ∈ R L (2) which returns a real number a d (x) for every pixel x and disparity label d. According to our dictionary model, the number will be larger the more likely it is that the pixel has a disparity of d. We now analyze the distribution of the coefficients over d for every pixel in order to obtain the final disparity estimate.</p><p>Lambertian Case. In the case of a Lambertian surface, for every pixel the positive values of a d (x) will be clustered around the correct disparity value, see <ref type="figure">figure 4</ref>, graph on top. Thus, we fit a Gaussian to the data at each pixel by computing mean and standard deviation as</p><formula xml:id="formula_1">µ(x) = L d=1 d a d (x), σ 2 (x) = L d=1 a d (x)(d − µ(x)) 2 ,<label>(3)</label></formula><p>respectively. Although the disparity values are quantized when creating the dictionary, this strategy achieves a pointwise sub-label accurate estimate, as oriented patterns of slope between e.g. d and d + 1 will be a weighted mixture of the atoms corresponding to the discrete labels.</p><p>In cases that an estimate in a pixel is completely inaccurate, we will likely have a very high standard deviation. Also, in a textureless region, the response from all atoms will be close or equal to zero. Thus, we inpaint unreliable estimates and perform an overall smoothing by solving the L 1 -inpainting problem with weighted second order total generalized variation (TGV) <ref type="bibr" target="#b2">[3]</ref> argmin u:Ω→ <ref type="bibr">[1,L]</ref> </p><formula xml:id="formula_2">λ TGV g (u) + m 2 u − µ 2 2<label>(4)</label></formula><p>to obtain the final disparity map u. The point-wise regularizer weight g is adapted to the edges in the center view I, and defined in a standard fashion as</p><formula xml:id="formula_3">g(x) = exp(−K ∇I(x) 2 ),<label>(5)</label></formula><p>where we set K = 5. The inpainting mask m is zero whenever the variance σ 2 (x) is larger than one-fourth its maximum value, and one otherwise. By design, the method sketched above assumed Lambertian surfaces, since it assumes the presence of a single orientation in every pixel. In the following section, we will generalize this to a second disparity layer to handle more difficult materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Twin peaks: two disparity layers</head><p>In <ref type="bibr" target="#b28">[29]</ref>, it is discussed how flat reflecting or transparent surfaces give rise to layered epipolar plane images, which consist of two differently oriented superimposed patterns, see <ref type="figure" target="#fig_2">figure 6</ref> for an illustration. In this section, we show how to analyze this situation with the help of the sparse coding coefficients obtained using the generated dictionary. In contrast to previous work, we show how to compute an accurate estimate for the regions where the two-layer model applies. Furthermore, we establish an optimization framework to obtain disparity estimates for both layers which far surpass previous work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref> in quality.</p><p>Computing the mask for the two-layer model. In a first step, we need to detect the region T ⊂ Ω in the center view where the two-layer model actually applies. Let a l x be the sparse coding coefficients of pixel x for disparity labels l = 1, . . . , L computed with the method described in section 4.</p><p>If pixel x belongs to a reflective or transparent surface, the surrounding regions on the epipolar plane images should exhibit two superimposed orientations. Thus, the question of whether x belongs to T is a question to the distribution of the coefficients a l</p><p>x . If x ∈ T , then we should see two distinct peaks corresponding to the two different disparities of the two layers, otherwise we should see only a single peak. <ref type="figure">Figure 4</ref> shows a few examples taken from our light fields.</p><p>In order to assess whether there are two layers, we perform different statistical tests. First, we perform a pixel-wise fit of a two-component Gaussian Mixture Model (GMM) using a GPU implementation of the EMalgorithm <ref type="bibr" target="#b20">[21]</ref> which runs in parallel on all the pixels. We use a fixed number of fifty iterations, where the means are initialized with µ − = 0 and µ + = L + 1, respectively, and both with standard deviation L/4. Let µ 1 ≤ µ 2 be the estimated means of the mixture components.</p><p>We now construct a data term ρ : Ω → R for a global binary segmentation. First, the data term should be negative where we have a preference for the one-layer model. A good test whether a distribution has only a single mode is to check whether γ 2 − κ ≤ <ref type="bibr">5 6</ref> , where γ is the skewness and κ the kurtosis of the distribution <ref type="bibr" target="#b21">[22]</ref>. Formulas to compute these are formed similar to µ and σ in (3). Second, ρ should be positive in case of a preference for the twolayer model. There is a preference for this if the initial estimates µ 1 and µ 2 are both valid and substantially different from each other.</p><p>We thus define the data term ρ to strike a balance between these two indications,</p><formula xml:id="formula_4">ρ := −1 {γ 2 −κ≤ 5 6 } + τ 1 {1≤µ1,µ2≤L} (µ 2 − µ 1 ). (6)</formula><p>Above, the notation 1 S denotes the characteristic function of the set S, and τ &gt; 0 is a constant, which we set at τ = 1 throughout the experiments. A special case occurs if l a l x = 0, which happens when the region around x is completely devoid of texture. In this case, a valid estimate is not possible, and we flag x to belong to a region I ⊂ Ω which will later be inpainted during optimization.</p><p>First, we compute the final mask T denoting the region flagged for the two-layer model by solving the binary seg-  <ref type="bibr" target="#b28">[29]</ref>. The EPI corresponding to the white line is depicted below. Parts of the surface are non-Lambertian and show a reflection, the corresponding regions of the EPI in turn exhibit two superimposed oriented patterns. mentation problem with weighted length regularity</p><formula xml:id="formula_5">argmin t:Ω→{0,1} Ω g ∇t(x) 2 + ρ(x)t(x) dx .<label>(7)</label></formula><p>for its characteristic function t = 1 T . We achieve global optimality with relaxation to functions taking values in [0, 1], optimization via the primal-dual algorithm <ref type="bibr" target="#b3">[4]</ref>, and subsequent thresholding, The point-wise regularizer weight g is defined as in <ref type="formula" target="#formula_3">(5)</ref>. An example two-layer mask as a result of this optimization can be observed in <ref type="figure">figure 5</ref>. Note that such a detection was not reliably possible with any of the previous methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Optimizing disparities for the two-layer model. In the region Ω \ T , one can compute disparities with the method described in the preceding section. Within T , however, two disparity maps v, u need to be extracted from the coefficient distributions. To facilitate this, we introduce the notion of a separating disparity label s(p) assigned to each pixel. We demand v ≤ s ≤ u. The key idea is that the coefficients below s should explain u, the coefficients above s should explain v. We allow fractional separation in the sense that in case of s(p) lying in between the integers l and l +1, then the coefficient a l x is split, and a fraction s(p) − l is assigned to the upper layer u, the rest to the lower layer v.</p><p>Let us formalize this idea a bit: for every pixel, we extend a l x to a function A x (l) over the continuous interval l ∈ (0, L] by setting A x (l) := A lAx(l) dl, <ref type="bibr" target="#b7">(8)</ref> respectively. Of course, this is just notation, in practice, the integrals can be computed by simple summation, correctly taking care of a possibly split coefficient.</p><p>In an ideal world, all observed data would be explained perfectly with the dictionary. In practice, however, we often have regions where the estimate is noisy or invalid due to  <ref type="figure">Figure 5</ref>. Accuracy evaluation of two-layer disparity reconstructions. We compare our method to <ref type="bibr" target="#b28">[29]</ref> on their ray-traced Tiger dataset (top) with a reflective plane, and a data set captured with a gantry (bottom), for which ground truth was acquired with a laser scanner <ref type="bibr" target="#b28">[29]</ref>. Note that in contrast to <ref type="bibr" target="#b28">[29]</ref>, our algorithm detects the reflection mask automatically (2nd column from left), while the results in <ref type="bibr" target="#b28">[29]</ref> were obtained for the ground truth mask. For both methods, disparity maps are shown after smoothing with an L 2 -data term using (4). The disparity maps are visually much better, confirmed by a better mean squared disparity error (MSE). We also experimentally verified that the error remains similar if we remove a subset of the views and thus increase maximum disparity to up to four pixels.  lack of texture. Thus, our algorithm consists of finding a local optimum of the functional <ref type="formula">(9)</ref> for the three unknowns. Above, R is a regularizer for the disparity maps u and v, we use total generalized variation <ref type="bibr" target="#b2">[3]</ref> with a point-wise weight g to account for image edges, defined as before. The functions m u and m v are masks depending on the coefficients and the estimated separator s. For some pixels, all coefficients above or below s might be zero, i.e. no information is available about u or v, respectively. In this case, we set the respective mask value and thus data term weight to zero, effectively performing inpainting. For all other pixels, the mask is set to one.</p><formula xml:id="formula_6">E(u, v, s) = λR(u, v)+ T mu(u−ūs) 2 +mv(v −vs) 2 dx</formula><p>The optimization can only be performed up to a local minimum. We initialize u, v with the corresponding estimates µ 2 and µ 1 from the GMM, s as the mean of u, v, masks as defined above, and iterate the following steps:</p><p>1. Keep v, s fixed and minimize E for u, which is an instance of TGV-smoothing and inpainting (4).</p><p>2. Keep u, s fixed and minimize E for v, the same TGVsmoothing and inpainting (4).</p><p>3. Keep u, v fixed, ignore the masks, and compute a new estimate for s by optimizing E point-wise. Then update the masks as described above.</p><p>In our experiments, this scheme converges in about ten iterations to a steady state. Example results for the different steps can be observed in <ref type="figure">figure 5</ref>, several more final results are referenced in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Evaluation</head><p>We conduct experiments with a Matlab implementation of our method, with a solver for the Lasso (1) from the SPAMS toolbox <ref type="bibr" target="#b15">[16]</ref>. For a thorough evaluation, we have several data sets available of varying origin. The first type of data is ray-traced. Here, we use several data sets from the HCI light field benchmark <ref type="bibr" target="#b30">[31]</ref>, which has mostly Lambertian scenes with some mild specular reflections. On these, we only evaluate our method up to section 4, without the sophisticated scheme for two-layered light fields. As example light fields with multiple layers, we use one rendered light field with ground truth from <ref type="bibr" target="#b28">[29]</ref>, as well as one gantry light field with ground truth also from the benchmark <ref type="bibr" target="#b30">[31]</ref>. To our knowledge, these are the only multi-layer light fields with ground truth available. of the methods, we also compare qualitatively on a data set captured with a Lytro Illum plenoptic camera <ref type="bibr" target="#b18">[19]</ref>. We first verify on the synthetic benchmark <ref type="bibr" target="#b30">[31]</ref> how different parameters and choices of dictionary for our method influence the quality of results. Graphs of the results can be seen in <ref type="figure" target="#fig_6">figure 9</ref>, we now proceed with a detailed discussion.</p><p>Influence of the dictionary size. First, we verify the influence of the dictionary size on the quality of the results. Two different factors have to be distinguished when in comes to the size of the dictionary. On the one hand, the number of disparity levels used, and on the other hand the number of trained atoms for the center view. Somewhat remarkably, the influence of the size of the dictionary on the quality of the estimation is diminutive. For both the number of disparity levels as well as the number of base atoms, it turns out that the results do not improve as long as the values are chosen above a certain threshold. In our experience, a good tradeoff between accuracy and run-time is to discretize disparity space such that the difference between two subsequent labels equals about one-third of a pixel. Finer disparity levels do not improve the results, and using fewer labels implies that the method becomes faster. For the number of center view atoms, four times the number of pixels in each patch seems to be a good rule of thumb. Further increasing the disparity resolution and the overall dictionary size gives close to no profit in case of Lambertian scenes, and we thus do not include a figure with detailed numbers.</p><p>Influence of patch shape.</p><p>In theory, the patch shapes cross and 4D have certain advantages. The shape cross enforces a coherent estimate over both epipolar plane image directions, while 4D in addition enforces some spatial coherence. However, this does not reflect in performance, the best results are actually attained by 2D, which returns coefficients for 2D patches for both directions separately. A reason could be that the more complex atoms are also less flexible to adapt to occlusions. In general, except for 4D patches at higher values of λ, coding the individual color channels separately yields better quality than using aggregated grayscale information. Note that while the point-wise results for the different patch shapes can differ greatly, the results are much closer after inpainting and smoothing with (4). Influence of the sparsity parameter λ. For the raw point-wise estimates, the sparsity norm weight λ has a sweet spot at around λ = 0.3. Again, after inpainting and smoothing with (4), this sweet spot is not visible anymore and the results of our method are quite robust with respect to different choices of λ. Thus, as the runtime decreases with larger values of λ, we suggest a value of around λ = 0.8 for general purpose depth estimation.</p><p>Accuracy under Lambertian assumption. <ref type="table">Table 6</ref> shows the mean squared disparity error for evaluation on the complete HCI light field database <ref type="bibr" target="#b28">[29]</ref>. Results for competing methods were taken from the benchmark evaluation in <ref type="bibr" target="#b28">[29]</ref>. As the light fields compared here are mostly Lambertian, we only employ our basic method described in section 4. The method EPI C refers to <ref type="bibr" target="#b7">[8]</ref> which enforces consistent depth labeling at occlusions. The globally optimal labeling scheme <ref type="bibr" target="#b29">[30]</ref>, which constructs a cost volume from structure tensor orientation estimates for horizontal and vertical epipolar plane images, is denoted EPI G. The multi view stereo methods ST S and ST G compute a data term based on point-wise color consistency of all views. ST S takes the point-wise optimum and performs simple smoothing, while ST G performs global optimization of a continuous multi-label problem, respectively. See <ref type="bibr" target="#b28">[29]</ref> for details on the methods. <ref type="figure" target="#fig_1">Figure 10</ref>. Disparity estimation for two superimposed layers using Lytro data. Our method can reliably estimate a mask for the superimposed region, as well as generate reasonably accurate disparity maps for both surface and reflection. The challenge imposed by this dataset can be appreciated by comparing to the raw estimates from the second order structure tensor <ref type="bibr" target="#b11">[12]</ref> below, which do not turn out to be useful despite our best efforts at applying regularization (not shown). Contrast-enhanced for better visibility.</p><p>We can see that our method performs on par with the above methods, achieving second lowest MSE. Note that all of these are not occlusion-aware, which is a major drawback -indeed, more recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref> outperform these results. However, in contrast to the other methods, we are also capable of estimating two depth layers at the same time. This is what sets us truly apart from previous techniques, and where we make a big leap in quality.</p><p>Accuracy on reflective and transparent surfaces. For light fields with two layers, we compare to the method <ref type="bibr" target="#b28">[29]</ref>, which is based on decomposing the two orientations using superimposed pattern analysis <ref type="bibr" target="#b0">[1]</ref>. We compute accuracy for both light fields where we have ground truth available, see <ref type="figure">figure 5</ref>, and note that we achieve both qualitatively as well as quantitatively far superior results. In addition, our method computes a robust segmentation into regions with and without multiple layers. Note that both light fields are of very high quality (rendered and from a gantry, respectively), so to test our limits we move to data from a plenoptic camera.</p><p>Plenoptic camera data. In order to process the data captured with the Lytro Illum, we employ the Lightfield Toolbox provided by <ref type="bibr" target="#b5">[6]</ref> to construct an epipolar volume. This creates challenging data, quite noisy and with nonlinear distortions from calibration inaccuracies, as can be observed in <ref type="figure" target="#fig_1">figure 1</ref>. We then compare our full algorithm <ref type="figure" target="#fig_1">Figure 11</ref>. Layer seperation of a real world light field. Given the disparity estimates from our method for both layers and the mask for the two-layer region, see <ref type="figure" target="#fig_1">figure 10</ref>, we can seperate the two layers of the light field using the method described in <ref type="bibr" target="#b11">[12]</ref>. Disparity maps obtained with <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref> are not accurate enough to perform this decomposition. Contrast-enhanced for better visibility.</p><p>with the second-order structure tensor approach <ref type="bibr" target="#b11">[12]</ref>, see <ref type="figure" target="#fig_1">figure 10</ref>. While the previous method <ref type="bibr" target="#b11">[12]</ref>, which is already an improvement over <ref type="bibr" target="#b28">[29]</ref>, works reasonably well on highquality data as in <ref type="figure">figure 5</ref>, it completely breaks down on the Lytro data set and does not yield any useful estimate. In contrast, our method is still capable of estimating reflection masks as well as the disparities of the separate layers robustly and with visually convincing accuracy. Using the algorithm from <ref type="bibr" target="#b11">[12]</ref>, we can thus proceed with performing a separation of the two light field layers, see figure 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we present a novel approach for depth estimation from light fields. The key idea is to build a dictionary for sparse light field coding such that the disparity for every atom is known. For this, we first learn the structure of the center view using dictionary learning, and afterwards, lift the trained patches to the higher dimensional epipolar space using shifting proportional to disparity. The method supports different shapes of base patches, which capture different aspects of spatial coherence within the views and among epipolar plane images. Using the generated light field atoms, we then employ the Lasso (1) in order to compute sparse coding coefficients. Accumulating these with respect to the different disparities of the atoms allows to infer the depth of individual pixels of the center view, provided the light field is Lambertian.</p><p>Using statistical analysis, we are also able to detect regions where the Lambertian assumption is violated, and the light field is composed of different superimposed disparity layers. Experiments demonstrate that our method far surpasses previous work for multi-layered disparity estimation in robustness and accuracy. For purely Lambertian scenes, however, our method performs only on par with earlier methods which are not occlusion-aware. This is to be expected, as only disparity across the complete patch is considered, which decreases accuracy at object boundaries. We will remedy this in an upcoming work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the ERC Starting Grant "Light Field Imaging and Analysis" (LIA 336978, FP7-2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Epipolar plane images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Light field with two disparity layers. The top image shows a close-up of the center view of the tiger dataset, see figure 5, courtesy of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>⌈l⌉ x , where ⌈l⌉ denotes rounding up, seefigure 7. Then, we define the lower expectation valuev s (p) and upper expectation valueū s (p) as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Separation, upper and lower expectations. The coordinate s(x) separates the label range into an upper and lower part and thus the two disparity layers. The upper and lower estimates us(p) andvs(p) are computed as expectation values over the red and blue distributions, respectively. Note that s(x) is allowed to be fractional -if it lies between two disparity labels, it cuts a bar from the chart in two.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>MSE for different types of patches and regularization depending on the sparsity parameter λ. Solid lines show the raw point-wise estimates, while dashed lines show results after TGV-L 2 regularisation and inpainting using 4. See text in section 3 for a description of the different light field patches used, and section 6 for a detailed discussion of the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>They are of very high quality and present only a moderate challenge, so to test the limits</figDesc><table>lightfield 

EPI C EPI G ST S ST G Ours 
buddha 
0.55 
0.62 
0.78 
0.90 
0.57 
buddha2 
0.87 
0.89 
1.05 
0.68 
1.08 
horses 
2.21 
2.67 
1.85 
1.00 
3.26 
medieval 
1.10 
1.24 
0.91 
0.76 
0.84 
monasRoom 
0.82 
0.93 
1.05 
0.79 
0.65 
papillon 
2.52 
2.48 
2.92 
3.65 
1.85 
stillLife 
2.61 
3.37 
4.23 
4.04 
2.95 
couple 
0.16 
0.19 
0.24 
0.30 
0.42 
cube 
0.82 
0.87 
0.51 
0.56 
0.51 
maria 
0.10 
0.11 
0.11 
0.11 
0.11 
pyramide 
0.38 
0.39 
0.42 
0.42 
0.48 
statue 
0.29 
0.35 
0.21 
0.21 
0.62 
average 
1.04 
1.18 
1.19 
1.12 
1.11 

Figure 8. Comparison of different methods for disparity estima-
tion. Datasets are from the HCI light field benchmark [31]. The 
numbers show mean squared disparity error for the method de-
scribed in section 4, which assumes Lambertian surfaces. Best 
results in each row are bold-faced. We achieve the best result on 
three of the data sets, and are second place on average. See text in 
section 6 for a description of the competing methods. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of superimposed oriented patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stuke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muehlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3690" to="3700" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Epipolar-plane image analysis: An approach to determining structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marimont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="55" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Total generalized variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kunisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A first-order primal-dual algorithm for convex problems with applications to imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="145" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Extracting layers and analyzing their specular properties using epipolar-plane-image analysis. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="51" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decoding, calibration and rectification for lenselet-based plenoptic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dansereau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1027" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The variational structure of disparity and regularization of 4D light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shape from light field meets robust PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational shape from light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Energy Minimization Methods for Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="66" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate depth map estimation from a lenslet light field camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Variational separation of light field layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision, Modelling and Visualization (VMV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scene reconstruction from high spatio-angular resolution light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image patch modeling in a light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data compression for light field rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="343" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compressive light field photography using overcomplete dictionaries and optimized projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marwah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Light field denoising, light field superresolution and stereo camera based refocussing using a GMM light field patch prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Note: thesis led to commercial light field camera, see also www</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<idno>lytro.com. 7</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Digital Light Field Photography</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Computer Vision: Models Learning and Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prince</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sharp inequalities between skewness and kurtosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szekely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="297" to="299" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-Based Rendering for Scenes with Reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Depth from combining defocus and correspondence using lightfield cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Depth estimation for glossy surfaces with light-field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014 Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="533" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Light field scale-depth space transform for dense depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tosic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Berkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stereo Matching with Linear Superposition of Layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="290" to="301" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Occlusion-aware depth estimation using light-field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3487" to="3495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reconstructing reflective and transparent surfaces from epipolar plane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (Proc. GCPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Variational light field analysis for disparity estimation and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="606" to="619" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Datasets and benchmarks for densely sampled 4D light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision, Modelling and Visualization (VMV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
