<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Online Smooth Predictors for Realtime Camera Planning using Recurrent Decision Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen˚hoang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">niversity of British Columbia</orgName>
								<orgName type="department" key="dep2">Disney Research</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Le :</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">niversity of British Columbia</orgName>
								<orgName type="department" key="dep2">Disney Research</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
							<email>carr@disneyresearch.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">niversity of British Columbia</orgName>
								<orgName type="department" key="dep2">Disney Research</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
							<email>yyue@caltech.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">niversity of British Columbia</orgName>
								<orgName type="department" key="dep2">Disney Research</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Littleů</surname></persName>
							<email>little@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">niversity of British Columbia</orgName>
								<orgName type="department" key="dep2">Disney Research</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Online Smooth Predictors for Realtime Camera Planning using Recurrent Decision Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of online prediction for realtime camera planning, where the goal is to predict smooth trajectories that correctly track and frame objects of interest (e.g., players in a basketball game). The conventional approach for training predictors does not directly consider temporal consistency, and often produces undesirable jitter. Although post-hoc smoothing (e.g., via a Kalman filter) can mitigate this issue to some degree, it is not ideal due to overly stringent modeling assumptions (e.g., Gaussian noise). We propose a recurrent decision tree framework that can directly incorporate temporal consistency into a data-driven predictor, as well as a learning algorithm that can efficiently learn such temporally smooth models. Our approach does not require any post-processing, making online smooth predictions much easier to generate when the noise model is unknown. We apply our approach to sports broadcasting: given noisy player detections, we learn where the camera should look based on human demonstrations. Our experiments exhibit significant improvements over conventional baselines and showcase the practicality of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We investigate the problem of determining where a camera should look when broadcasting a team sporting event, such as basketball or soccer (see <ref type="figure">Fig. 1</ref>). Realtime camera planning shares many similarities with online object tracking: in both cases, the algorithm must constantly revise an estimated target position as new evidence is acquired. Noise and other ambiguities can cause non-ideal jittery trajectories, which in camera planning lead to unaesthetic results (see <ref type="figure" target="#fig_0">Fig. 2</ref>). In contrast to object tracking, smoothness is of paramount importance in camera control: fluid movements that maintain adequate framing are preferable to erratic motions that constantly pursue perfect composition.</p><p>Non-parametric or model-free estimation methods, such as random forests <ref type="bibr" target="#b9">[10]</ref>, are very popular because they can learn (almost) arbitrary predictors directly from training <ref type="bibr">Figure 1</ref>. Camera Planning. The goal is to predict the pan angle for a broadcast camera based on noisy player detections. Consider two planning algorithms (the blue and red curves) which both make the same mistake at time A but recover to a good framing by C (the ideal camera trajectory is shown in black). The blue solution quickly corrects by time B using a jerky motion, whereas the red curve conducts a gradual correction. Although the red curve has a larger discrepancy with the ideal motion curve, its velocity characteristics are most similar to the ideal motion path. data. When applied to smooth trajectory prediction, the estimator is often learned within a time-independent paradigm, with temporal regularization integrated afterwards as a postprocessing stage (e.g., via a Kalman filter) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>. One major limitation of this two-stage approach for camera planning is that the smoothing is done in a context-independent way, which can lead to uninformed tradeoffs between accuracy and smoothness (see <ref type="figure">Fig. 1</ref>).</p><p>In this paper, we propose a recurrent decision tree framework that can make predictions conditioned on its own previous predictions, which allows it to learn temporal patterns within the data (in addition to any direct feature-based relationships). However, this recursive formulation (similar to reinforcement learning <ref type="bibr" target="#b30">[31]</ref>) makes the learning problem much more challenging compared to the time-independent approach. We develop a learning algorithm based on the "search and learn" (SEARN) approach <ref type="bibr" target="#b11">[12]</ref> to efficiently converge to a stable recurrent model.</p><p>We applied our approach to autonomous camera control in sports, where the goal is to generate smooth camera motion that imitates a human expert. We provide both quantitative and qualitative evidence showing our approach significantly outperforms several strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Sequential Supervised Learning Sequential supervised learning is broadly applied in many domains, including natural language processing tasks such as part-of-speech tagging <ref type="bibr" target="#b8">[9]</ref> and computational biology tasks such as protein alignment <ref type="bibr" target="#b31">[32]</ref>. Unlike standard supervised learning, sequential supervised learning operates on sequences: each training example px i , y i q is a sequence of features x i " x i,1 , x i,2 , . . . , x i,Ti and a corresponding sequence of labels y i " y i,1 , y i,2 , . . . , y i,Ti . The learned predictor outputs a sequence of labels for an input sequence of features.</p><p>Our setting is distinguished from conventional sequential learning because we must learn an online predictor. In other words, conventional sequential learning typically assumes access to all of x before predicting y <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>. Our setting is further distinguished from most previous sequential prediction work by aiming to learn model-free or non-parametric predictors. For instance, the bulk of previous work utilize linear predictors and thus require a wellspecified feature representation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>One approach for utilizing arbitrary predictors is via a sliding window <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>, in which any supervised learning method can be used to learn the sliding window predictor. However, if the predictor is defined recurrently (i.e., it depends on its previous predictions), then it is not obvious what values to use for previous predictions when generating supervised training examples. One way to address this issue is via "stacked" sequential learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, which is essentially the two-stage approach of first training a nonrecurrent predictor, and then employing recurrent smoothing (e.g., a hidden Markov model or Kalman filter).</p><p>Our approach instead directly trains a predictor to make good predictions given previous predictions, which leads to a "chicken and egg" problem of how to define a training set that depends on the predictions of the model to be trained. We employ a learning reduction approach, based on SEARN <ref type="bibr" target="#b11">[12]</ref>, that iteratively constructs a sequence of supervised training sets such that the resulting sequence of predictors efficiently converges to a stable recurrent predictor. Other learning reduction approaches for sequential learning include DAgger <ref type="bibr" target="#b28">[29]</ref>, which can be more efficient in the number of iterations needed to converge, but with each iteration being more computationally expensive.</p><p>Camera Planning Algorithms for determining where a camera should look have been investigated for a variety of scenarios from scripted cooking shows and college lectures to team sports <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>. It is widely accepted that a smoothly moving camera is critical for generating aesthetic video <ref type="bibr" target="#b15">[16]</ref>. One approach is post-processing to regulate the sequential predictions, which leads to a trade-off between smoothness and responsiveness <ref type="bibr" target="#b5">[6]</ref>. Alternatively, offline batch processes <ref type="bibr" target="#b18">[19]</ref> can be used if there is not an online re-  <ref type="bibr" target="#b5">[6]</ref> includes a sudden direction change (green dashed box) and jitter. (b) Post-processing independent predictions with a Kalman filter reduces jitter but causes over shooting (black dashed box). quirement. Significant filtering constrains the motion of the camera, making it unable to track fast moving objects <ref type="bibr" target="#b25">[26]</ref>. Other approaches can offer more refined control but first require users to select the main features of interest <ref type="bibr" target="#b16">[17]</ref>.</p><p>Camera Motion Models Camera motion is well-studied in the context of stabilization. Liu et al. <ref type="bibr" target="#b25">[26]</ref> used a low-pass filter to create constant angular velocity motions for video stabilization. The rotation component was filtered by linearizing the rotation space using derivatives with respect to time to obtain angular velocities <ref type="bibr" target="#b24">[25]</ref>. The method was extended to polynomial eigen-trajectories for subspace video stabilization <ref type="bibr" target="#b26">[27]</ref>. In contrast, we are interested in the problem of camera planning. In other words, rather than trying to reconstruct a stable camera trajectory from existing trajectories, our goal is to plan a brand new trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Setup</head><p>Let d x denote a distribution of input sequences: x " xx 1 , . . . , x T y " d x . In camera planning, x can be a sequence of (noisy) detected player locations from stationary cameras. For clarity, we assume each sequence has the same length T , but in general T can vary and/or be unknown.</p><p>Let Π denote a class of policies that our learner is considering. Given a stream of inputs x " xx 1 , . . . , x T y, each π P Π generates a stream of outputs y " xy 1 , . . . , y T y. In camera planning, y can be a sequence of pan angles of the broadcast camera. Section 4 describes our recurrent decision tree framework that instantiates Π in our experiments.</p><p>Operationally, each π is a streaming predictor that takes a state s t " tx t , . . . x t´τ , y t´1 , . . . , y t´τ u composed of the recent inputs and predictions, and generates a new prediction y t " πps t q. Note that the next state s t`1 depends only on the new input x t`1 , the current state s t , and the current prediction y t . Hence, for any input sequence x, π equivalently generates a state sequence s " xs 1 , . . . , s T y. We also abuse notation to say that y " πpxq denotes the sequence of predictions y generated by streaming x into π (with the construction of the state sequence s being implicit).</p><p>For any policy π P Π, let d π t denote the distribution of states at time t if π is executed for the first t´1 time steps (d π t is defined exactly by d x and π). Furthermore, let d π " 1 T ř T t"1 d π t be the average distribution of states if we follow π for all T steps. The goal of the learner is to find a policyπ P Π that minimizes the imitation loss under its own induced distribution of states:</p><p>π " argmin πPΠ E s"dπ rℓps, π, π˚qs .</p><p>(1)</p><p>Since our goal is to learn a policy π that both imitates the (human) expert π˚and is also smooth, we decompose our loss function into precision and smoothness components:</p><p>ℓps, π, π˚q " ℓ˚ps, π, π˚q`ωℓ R ps, πq,</p><p>where ℓ˚measures how well πpsq agrees with π˚psq, and ℓ R measures how smooth the prediction of πpsq is relative to the current state s, with ω ě 0 controlling the trade-off between the two. The precision error ℓ˚is typically the squared deviation: ℓ˚ps, π, π˚q " }π˚psq´πpsq} 2 .</p><p>A standard way to instantiate the smoothness error ℓ R is via the squared deviation of the velocity: ℓ R ps, πq " }vpsq´vpπpsqq} 2 , where vpsq denotes the velocity in state s, which can be computed from the information encoded in s. One can thus interpret the smoothness error as encouraging the curvature of the predicted trajectories to be low.</p><p>We assume the agnostic setting, where the minimizer of (1) does not necessarily achieve 0 loss (i.e., we cannot perfectly imitate the human expert). In practice, we approximate the expectation in (1) with a finite sample (e.g., a training set of basketball gameplay sequences), and also solve (1) via alternating minimization: collect training data according to currentπ, and then train a newπ using that training data (see Section 5 for more details).</p><p>Discussion and Interpretation. By utilizing a recurrent policy class Π, such as our recurrent decision tree framework (see Section 4), the learner is able to reason about inherent temporal relationships. For instance, suppose the learner incorrectly predicted a previous value (see <ref type="figure">Fig. 1</ref>). Then the learner could make a drastic correction (blue line) to minimize the discrepancy as quickly as possible, which might result in unaesthetic high-frequency camera motion. Instead, a more gradual correction (red line) may better trade off between instantaneous error and smooth motion.</p><p>Estimating the bestπ is challenging due to its dependence on its own previous predictions. Most notably, this recurrent relationship makes it nontrivial to extract a set of independent training examples to use with conventional supervised learning. This issue is formally highlighted in the learning objective (1), where the distribution that the loss is evaluated over is induced by the policy under consideration. In other words, the distribution d π that the loss is evaluated on in (1) depends on the π being considered, which leads to complicated learning problem since conventional supervised learning assumes that distribution is fixed.</p><p>One straightforward approach is to approximate d π in (1) using the distribution of states, d π˚, that the human expert π˚induces on d x , and then select theπ P Π to minimize:</p><p>π " argmin πPΠ E s"d π˚r ℓps, π, π˚qs .</p><p>(</p><p>Note that <ref type="formula" target="#formula_1">(3)</ref> is easy to evaluate since the expectation is over d π˚a nd does not depend on the policy π under consideration. <ref type="bibr" target="#b0">1</ref> However, as soon as the behavior ofπ deviates from π˚, then the distribution of states experienced byπ will differ from π˚, and thus optimizing <ref type="formula" target="#formula_1">(3)</ref> will not be aligned with optimizing (1). <ref type="bibr" target="#b1">2</ref> We show in Section 5 how to address this distribution mismatch problem via an alternating minimization approach that efficiently converges to a stable solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Recurrent Decision Tree Framework</head><p>Decision trees are amongst the best performing learning approaches, and are popular whenever a non-parametric predictor is desirable <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>. We propose a recurrent extension, where the prediction at the leaf node is not necessarily constant, but rather is a (smooth) function of both static leaf node prediction and previous predictions from the tree. For simplicity, we present our framework using a single decision tree, although our approach extends trivially to ensemble methods such as random forests <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>A decision tree specifies a partitioning of the input space (i.e, the space of all possible states s). Let D " tps m , ymqu M m"1 denote a training set of state/target pairs. Conventional regression tree learning aims to learn a partitioning such that each leaf node, denoted by node, makes a constant prediction to minimize the squared loss:</p><formula xml:id="formula_2">y node " argmin y ÿ ps,y˚qPDnode py˚´yq 2 ,<label>(4)</label></formula><p>where D node denotes the training data from D that has partitioned into the leaf node. The solution to (4) is:</p><formula xml:id="formula_3">y node " 1 sizepD node q ÿ ps,y˚qPDnode y˚(5)</formula><p>One typically trains via greedy top-down induction <ref type="bibr" target="#b9">[10]</ref>, which is an iterative process that repeatly chooses a leaf node to be split based on a binary query of the input state s. The above formulation (4) can be viewed as the simplest version of our recurrent decision tree framework. In particular, the decision tree is allowed to branch on the input state s, which, as discussed in Section 3, includes the previous predictions ty´1, . . . , y´τ u. Thus, decision trees that minimize (4) form a valid recurrent policy class, and can be used to instantiate Π. In the following, we will describe a more general class of recurrent decision trees that enforce more explicit smoothness requirements.</p><p>Note that recurrent neural networks (RNNs) <ref type="bibr" target="#b1">[2]</ref> impose a slightly different notion of "recurrent" than our approach. Whereas our approach is only recurrently defined with respect to the previous predictions of our approach, RNNs are recurrently defined with respect to the previous hidden unit activations and (or) previous predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Jointly Capturing Dynamics and Control</head><p>Let f π py´1, . . . , y´τ q denote an autoregressor of the temporal dynamics of π over the distribution of input sequences d x , while ignoring the exogenous inputs x. In other words, at time step t, f π predicts the behavior y t " πps t q given only y t´1 , . . . , y t´τ . Typically, f π is selected from a class of autoregressors F (e.g., smooth autoregressors). For our experiments, we use regularized linear autoregressors as F , although one could also use more complex functions (e.g., based on Kalman filters). See the supplemental material for more details on linear autoregressors.</p><p>We now present a policy class Π of recurrent decision trees π that make smoothed predictions by regularizing to be close to its autoregressor f π . For any tree/policy π, each leaf node is associated with a "control" or "target" signal y node such that the predictionŷ given input state s is:</p><formula xml:id="formula_4">y " πpsq " argmin y py´ȳ nodepsq q 2`λ py´f π psqq 2 ,<label>(6)</label></formula><p>where nodepsq denotes the leaf node of the decision tree that s branches to, and λ ě 0 trades off betweenŷ matching the target signalȳ nodepsq versus the smooth autoregressor f π psq. The closed-form solution to (6) is:</p><formula xml:id="formula_5">ypsq "ȳ nodepsq`λ f π psq 1`λ .<label>(7)</label></formula><p>Compared to just predictingŷ "ȳ nodepsq , the prediction in (6) varies much more smoothly with s, sinceŷ is now an interpolation between the target signalȳ nodepsq and smooth extrapolation f π psq from previous predictions. Training requires estimating both the autoregressor f π and the decision tree of target signalsȳ node . In practice, given training data D, we perform greedy training by first estimating f π on D (which ignores the exogenous inputs x), 3 and then estimating the decision tree of target signals to "course correct" f π . Given a fixed f π and decision tree structure, one can setȳ node as:</p><formula xml:id="formula_6">y node " argmin y ÿ ps,y˚qPDnode py˚´ŷps|yqq 2 ,<label>(8)</label></formula><p>forŷps|yq defined as in <ref type="formula" target="#formula_5">(7)</ref> with y "ȳ nodepsq . Similar to (5), we can write the closed-form solution of (8) as:</p><p>ynode " 1 sizepDnodeq ÿ ps,y˚qPD node`p 1`λqy˚´λfπpsq˘. <ref type="bibr" target="#b8">(9)</ref> Note that when λ " 0, then the autoregressor has no influence, and (9) reduces to <ref type="bibr" target="#b4">(5)</ref>. Note also that <ref type="formula" target="#formula_6">(8)</ref> is a simplified setting that only looks at imitation loss ℓ˚, but not smoothness loss ℓ R . We refer to the supplemental material for more details regarding our full training procedure. One can interpret our recurrent decision tree framework as holistically integrating model-based temporal dynamics f with model-free controlȳ node . The target signalȳ node can thus focus on generating "course corrections" of the smooth temporal dynamics imposed by f . In practice, the target signalȳ node can also be set using an ensemble such as random forests <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>, or regression trees that predict piecewise-linear variables in the leaf nodes <ref type="bibr" target="#b14">[15]</ref>. In this way, our framework is completely complementary with previous work on learning smoother regression tree predictors.</p><p>Extensions. One could also define the predictions y as velocity rather than absolute coordinates, which coupled with the current state s, will encode the absolute coordinates. Such an approach would be desirable if one wants to do perform autoregressor regularization in the velocity domain.</p><p>Another extension is to replace the L2 autoregression penalty in <ref type="bibr" target="#b7">(8)</ref> with an L1 penalty. With an L1 penalty, small target signals would not deviate the prediction from the autoregressor, thus making the temporal curvature potentially piecewise linear (which may be desirable). However, the non-smooth nature of L1 regularization would make the prediction more sensitive to the choice of λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Iterative Training Procedure</head><p>In general, it can be difficult to exactly solve (1) due to the circular dependency between the distribution of states and the policy under consideration. One meta-learning approach is to alternate between estimating d π over a fixed π, and optimizing π over a fixed d π . At a high level, this can be described as:</p><p>1. Start with some initial policyπ 0 2. At the i-th iteration, use the previously estimated policiesπ 0 , . . . ,π i´1 to construct a new distribution d i 3. Estimate the best policyπ i via:</p><formula xml:id="formula_7">π i " argmin πPΠ E s"di rℓ i ps, π, π˚qs .<label>(10)</label></formula><p>Note that the loss function ℓ i need not be the original loss function, but simply needs to converge to it. 4. Repeat from Step 2 with i Ð i`1 until some termination condition is met.</p><p>One typically initializes using the human expertπ 0 " π˚, which we have demonstrations for in the training set (i.e., π˚is memorized on the training set). The estimation problem in (10) can be solved via standard supervised learning (see the supplemental material for how we solve (10) for our recurrent decision tree framework). It remains to decide how to construct a stable and converging sequence of distributions d i . For instance, it is known that simply choosing d i " dπ i´1 does not guarantee stable behavior <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>We build upon the SEARN approach <ref type="bibr" target="#b11">[12]</ref> to iteratively construct a stable sequence of distributions d i for training eachπ i . Algorithm 1 describes our approach, which is a meta-learning approach that can be applied to other policy classes beyond our recurrent decision tree framework (i.e., by using a different implementation of Learn). Note also that we approximate each state distribution d i using a finite empirical sampleS i -i.e.,S i is constructed from the given input sequences X and the predictions iteration i,Ỹ i . Algorithm 1 is an instance of the alternating procedure described above. Given a state distribution d i , the training problem is a straightforward supervised learning problem (Line 7). Note that the iteration-specific loss ℓ i is simply the original loss ℓ using a (potentially) modified imitation target Yi that converges to the original Y (Line 6).</p><p>The key innovation of SEARN <ref type="bibr" target="#b11">[12]</ref> is that the new distribution d i should be induced by an exponentially decaying interpolation between everyπ 0 , . . . ,π i´1 . In practice, we found it more convenient to interpolate the trajectorieŝ Y 0 , . . . ,Ŷ i´1 (Line 10), which leads to a set of trajectories Y i that can be combined with the input X to form an empirical sample of statesS i to approximate d i (Line 5). Because of this interpolation, the sequence of distributions d i will converge in a stable way (see <ref type="bibr" target="#b11">[12]</ref> for a rigorous analysis of the convergence behavior). This stable behavior is especially important considering our greedy training procedure for our recurrent decision tree framework: if the resultingπ i has a very different behavior from the training distribution d i , then fπ i will not be a good autoregressor ofπ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Choosing β. A straightforward choice of interpolation parameter is to set β to a small constant (close to 0) to ensure that the new distribution d i stays close to the previous distribution d i´1 , which is the original approach of SEARN <ref type="bibr" target="#b11">[12]</ref>. One drawback of this conservative approach is slower convergence rate, especially when the learner needs to quickly move away from a bad policyπ i .</p><p>We can also adaptively select β based on relative empirical loss of learned policies (Line 9 of Algorithm 1). Let</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Iterative Training Procedure</head><p>Input: input sequences X Input: expert demonstrations Y for X Input: autoregression function class F Input: history time horizon τ for generating states //see Section 3 Input: βp. . .q //distribution drift step size 1: InitializeS0 Ð state sequences defined by Y and X 2: π0 Ð LearnpS0, Yq. 3: InitializeỸ1 Ð tπ0pxq|x P Xu //initialize exploration 4: for i " 1, . . . , N do 5:Si Ð state sequences defined byỸi and X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Yi Ð expert feedback on eachs PSi //see Section 5.1 7:πi Ð LearnpSi, Yi q //minimizing (10) 8:Ŷi Ð tπipxq|x P Xu //roll-outπi 9:βi Ð βperrorpŶiq, errorpỸiqq //see Section 5.1 10:Ỹi`1 ÐβiŶi`p1´βiqỸi //distribution interpolation 11: end for 12: returnπ P tπ1, . . . ,πN u with best validation performance errorpŶ i q and errorpỸ i q denote the loss (2) of rolledout trajectoriesŶ i ,Ỹ i , respectively, with respect to ground truth Y. We can then set β as:</p><formula xml:id="formula_8">β i " errorpỸ i q errorpŶ i q`errorpỸ i q .<label>(11)</label></formula><p>This adaptive selection of β encourages the learner to disregard bad policies when interpolating, thus allowing fast convergence to a good policy.</p><p>Choosing Y˚. Intuitively, whenever the current policyπ i makes a mistake, the expert feedback y˚should recommend smooth corrections (e.g., follow the red line in <ref type="figure">Figure 1</ref>). In many cases, it suffices to simply use the original expert trajectories Yi Ð Y. However, whenỸ i differs substantially from Y, especially during early iterations, it can be beneficial to use "smoothed" expert feedback Yi in place of Y. Specifically, each step along Yi is computed to be a gradual 1-step correction ofỸ i towards Y, which will gradually converge to Y in later iterations. It is not strictly necessary, as our recurrent decision tree framework can offset non-smooth corrections by enforcing a larger regularization parameter λ (trading higher smoothness for lower imitation accuracy, cf. 8). Generally however, given the same smoothness weight λ, smoothed expert feedback leads to more stable learning. One simple way to provide smoothed Yi is to reduce the distance ratio py˚´yq{pỹ´yq by a small decaying rate: y˚" y`e´ηpỹ´yq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluate our approach automated broadcasting for basketball and soccer (see <ref type="figure" target="#fig_1">Fig. 3)</ref>. A high-school basketball match was recorded using two cameras: one near the ceiling for player detection, and one at ground level for broadcasting (operated by a human expert). The videos were synchronized at 60fps. 'Timeouts' were manually removed, resulting in 32 minutes of 'in-play' data divided into roughly 50 segments (each about 40 seconds long), with two held out for validation and testing.</p><p>A semi-professional soccer match was recorded using three cameras: two near the flood lights for player detection, and a robotic PTZ located at mid-field remotely operated by a human expert. The videos were synchronized at 60 fps. About 91 minutes was used for training, and two 2 minute sequences were held out for validation and testing.</p><p>Features The ground locations of players were determined from 3D geometric primitives which best justified the background subtraction results <ref type="bibr" target="#b3">[4]</ref>. Each ground position was projected to a spherical coordinate system centered and aligned with the broadcast camera. Because the number of detections varies due to clutter and occlusions, a fixed length feature vector was constructed using spatial frequency counts. The surface of the sphere was quantized at three resolutions (1ˆ2, 1ˆ4, and 1ˆ8) resulting in a 14 dimensional feature vector x t <ref type="bibr" target="#b5">[6]</ref>.</p><p>Labels Pan/tilt/zoom parameters are estimated for each frame of the broadcast video by matching detected SIFT key points to a collection of manually calibrated reference frames in a similar fashion to <ref type="bibr" target="#b19">[20]</ref>. The homography between the current frame and the best match in the database of reference images is estimated, from which the camera's pan-tilt-zoom settings are extracted. Because the tilt and zoom of the broadcast camera do not vary significantly over the dataset, our experiments only focus on building an estimator for online prediction of pan angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Baselines</head><p>Savitzky-Golay. <ref type="bibr" target="#b5">[6]</ref> learns a predictor using a random forest trained using only current player locations. A Savitzky-Golay (SG) filter smooths the predictions, but induces a de-lay. Our implementation of this method augments the current player locations with previous player locations. This modification makes the instantaneous predictions more reliable, as the predictor has more temporal information.</p><p>Kalman Filter. We replace the Savitzky-Golay filter with a Kalman filter employing a constant velocity process model. Parameters were determined through validation (see supplemental material).</p><p>Dual Kalman Filter. A dual Kalman filter <ref type="bibr" target="#b20">[21]</ref> simultaneously estimates the unknown state of the system, as well as its process matrix. Similar to our formulation, we assume the system adheres to an autoregressive model. This method then applies two Kalman filters in parallel: one to estimate the coefficients of the autoregressive function, and a second to estimate the trajectory of the camera, based on the current estimate of the autoregressive model. Again, parameters were tuned through validation.</p><p>Conditional Regression Forests. Conditional regression forests (CRFs) <ref type="bibr" target="#b10">[11]</ref> split the training data into multiple subsets. We tested various splitting methods based on camera position and velocity, such as dividing the data into 4, 8 and 16 subsets of pan angle. We also tried both disjoint sets and joint sets with different overlap ratios. We report the best result from 8 subsets with 50% overlap. The output of the CRF is further smoothed by a SG filter.</p><p>Filter Forests. Filter forests (FF) <ref type="bibr" target="#b14">[15]</ref> is an efficient discriminative approach for predicting continuous variables given a signal. FF can learn the optimal filtering kernels to smooth temporal signals. Our implementation includes some adaptations, such as limited candidate window sizes, to improve the performance on our datasets. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the benchmark performance evaluated for both basketball and soccer. We evaluate using joint loss <ref type="bibr" target="#b1">(2)</ref> with ω " 500. The precision and smoothness losses are plotted separately to illustrate their relative contributions to the joint loss. For both settings, we see that our approach achieves the best performance, with the performance gap being especially pronounced in basketball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Benchmark Experiments</head><p>We note that the soccer setting is significantly more challenging than basketball, and no method performs particularly well for soccer. One possible explanation is that soccer camera planning using only player detections is unreliable due to players not following the ball (unlike in basketball). A visual inspection of the generated videos also suggests that lack of ball tracking in the input signal x is a significant limitation in the soccer setting.</p><p>We also observe that our approach achieves very low smoothness loss, despite not utilizing a post-processing smoothing step (see <ref type="table">Table 1</ref> for a summary description of   <ref type="bibr" target="#b1">(2)</ref>. Our method achieves the lowest overall loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Noise the different approaches). For instance, in the basketball setting, our approach actually achieves the smoothest performance. In fact, for the basketball setting, our approach dominates all baselines in both imitation loss and smoothness loss. The KF and FF baselines tend to achieve competitive smoothness loss, but can suffer substantial imitation loss. For soccer, the SG and CRF baselines achieve lower imitation loss, suggesting that they might be better able to track the action. However, they suffer from substantial jitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Visual Inspection</head><p>Figs. 5 and 6 show a visualization of the predictions. From this visual inspection, we can verify qualitatively that our method achieves the best balance of precision and smoothness. Our predicted trajectory remains close to the human operator's trajectory and has less jitter than the other methods. Even with post-smoothing, SG and CRF exhibit significant jitter. KF struggles between jitter and over shooting when the noise is not Gaussian. Surprisingly, dual KF performance is worse than KF, which is again presumably because the noise is not Gaussian, and errors in the process estimation propagate to the state estimation (see supplemental material). FF is very competitive in the basketball dataset, but its performance suffers from large jitter in the more challenging soccer dataset. <ref type="table">Table 1</ref> summarizes qualitative properties of all the methods. SG and CRF assume the noise only has highfrequency components. As a result, they struggle with lowfrequency noise. KF and dual KF rely on a Gaussian noise assumption, which is not reasonable on our datasets.</p><p>Both our method and FF <ref type="bibr" target="#b14">[15]</ref> learn control and dynamics  <ref type="bibr" target="#b10">[11]</ref>, (e) Filter forests <ref type="bibr" target="#b14">[15]</ref>, (f) Our method. The blue line is from human operators and the red line is from predictions. Note our method does not need any post-processing. aspects from the data. Neither requires post-processing and both are able to generate a zero delay response. In camera planning, our method has three advantages over FF. First, our method has fewer parameters so that it requires less data to train. Second, our model can be trained much faster than FF because FF has to solve a large linear equation in each split. Third, the experimental results show that our method is more stable in both datasets. <ref type="figure">Fig. 7</ref> provides a visual comparison of our framework using a random forest of 100 trees and varying influence of the autoregressor, λ. When λ " 0, the autoregressor has no influence, and so the smoothness of the prediction depends solely on the smoothness of decision tree ensemble and the size of the history window, τ . Since decision trees are non- <ref type="figure">Figure 7</ref>. Comparing varying λ for basketball data. λ " 300 is a good trade-off between smoothness and accuracy. Very small λ allows more accurate but noisier predictions, and very large λ leads to smoother but less accurate predictions.</p><p>parametric, one could in principle learn a smooth predictor given sufficient training examples and enough trees, but the data and computational costs would be immense. As λ increases, the autoregressor causes the predictions to be increasingly smooth. Recall that the learner (Algorithm 1) always seeks to find the predictor within the policy class with the lowest loss, and so it can adaptively trade off between smoothness and accuracy depending on the input signal x (rather than rely on post-processing). As λ becomes very large, the policy class becomes restricted to extremely smooth predictors. In practice, λ can be set via a user preference study or validation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">User Preference Study</head><p>We also conducted a user preference study to complement our benchmark experiments. <ref type="figure">Fig. 8</ref> shows our user study interface. Videos were generated by warping the video captured by the human operator. We evaluated our approach against the five baseline algorithms for both basketball and soccer. In each trial, participants were instructed to choose the video that was more appealing (our method was randomly assigned the left or right view). <ref type="table" target="#tab_1">Table 2</ref> shows the results. For basketball, our method is significantly preferred over the other methods based on a two-tailed binomial test (p ă 0.001). For soccer, none of the methods performed particularly well (see Section 6.2), making it challenging for users to judge which method generated better videos. Note that there is still a sizable preference gap compared to the human expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>Although our approach achieved good results for basketball, the results for soccer were much poorer. It is likely that we require more expressive inputs in order to learn good policy, such as tracking the ball in addition to the players. <ref type="figure">Figure 8</ref>. User study screenshot. Users were asked which video was more pleasant to watch, and to consider both composition and smoothness in their assessment. The human demonstrations in the soccer dataset were almost entirely piece-wise linear. In other words, the human expert is almost always directing the camera in a straight line with very sharp course corrections. As such, it may be that we require L1 regularization to an autoregressor that prefers zero acceleration in order to better capture the temporal dynamics of camera planning in soccer.</p><p>We chose decision trees due to their non-parametric or model-free properties as well as ease of training. Using other complex predictors, such as deep neural nets or Gaussian processes, could potentially also work well.</p><p>Finally, our approach only addresses the planning problem, and cannot be directly applied to physical camera control without a control model. It would be interesting to jointly learn to both planning and physical control <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Summary</head><p>We have introduced the problem of smooth online imitation learning, where the goal is to learn a predictor to smoothly imitate a human expert given a stream of input signals. We have also presented a recurrent nonparametric model class for generating smooth predictions, which jointly integrates a model-free control signal with a model-based autoregressor. In order to guarantee stability of training, we extended previous work on iterative learning of dynamical models to our setting. We applied our approach to camera control in sports, where we demonstrated significant improvements over several strong baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Jitter and Overshooting Artifact. (a) A plan generated by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Features and Labels. (a) player detections, (b) pan/tilt/zoom parameters, and (c) spherical quantization scheme for generating features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Prediction loss. (a) Basketball; (b) Soccer. Loss measured by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Comparison on basketball data. (a) Method of [6], (b) Kalman filter, (c) Dual Kalman filter , (d) Conditional regression forests [11], (e) Filter forests [15], (f) Our method. The blue line is from human operators and the red line is from predictions. Note our method does not need any post-processing. Comparison on soccer data.. (a) Method of [6], (b) Kalman filter, (c) Dual Kalman filter , (d) Conditional regression forests</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>User study results. For basketball, our method is significantly preferred over all baselines. For soccer, all methods performed poorly, and users did not have a strong preference. There is still a sizable preference gap compared to expert human.</figDesc><table>Basketball 
Soccer 
Comparison 
win / loss 
win / loss 

vs SG 
22 / 3 
14 / 11 
vs KF 
23 / 2 
12 / 13 
vs dual KF 
25 / 0 
24 / 1 
vs CRF 
24 / 1 
12 / 13 
vs FF 
23 / 2 
14 / 11 
vs human 
1 / 24 
4 / 21 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In practice, optimizing (3) reduces to a standard supervised learning scenario. One simply collects the decisions made by the human expert πt o use as a static training set, and then choose theπ P Π that agrees most with π˚on the states induced by π˚.<ref type="bibr" target="#b1">2</ref> One simplified interpretation is that training on (3) does not teach the predictor how to recover from its own mistakes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Intuitively, if we train fπ on D, then the state distribution of D should be similar to the state distribution that will be induced by the resulting trained π. We address this point further in Section 5.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hidden markov support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular object detection using 3D geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autonomous camera systems: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mimicking human camera operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Apprenticeship learning for helicopter control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM (CACM)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="97" to="105" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stacked sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJ-CAI)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time facial feature detection using conditional regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Machine learning for sequential data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural, Syntactic, and Statistical Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="15" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Filter forests for learning data-dependent convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pattacini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The cameraman operating my virtual camera is artificial: Can the machine be as good as a human ?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Gaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Langseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Griwodz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive zoom and panning from live panoramic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Gaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Langseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ljødal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gurdjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Charvillat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Griwodz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network and Operating System Support on Digital Audio and Video Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extremely randomized trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Auto-directed video stabilization with robust l1 optimal camera paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using line and ellipse features for rectification of broadcast hockey video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canadian Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Kalman filtering and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting regions of interest in dynamic scenes with camera motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A decision tree framework for spatiotemporal sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">General construction of time-domain filters for orientation data. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="128" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contentpreserving warps for 3d video stabilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Subspace video stabilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Randomized trees for human pose detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coupling a dynamic linear model with random forest regression to estimate engine wear</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schimert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wineland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Prognostics and Health Management Society</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Support vector training of protein alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Elber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pillardy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="867" to="880" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
