<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeLay: Robust Spatial Layout Estimation for Cluttered Indoor Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeLay: Robust Spatial Layout Estimation for Cluttered Indoor Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of estimating the spatial layout of an indoor scene from a monocular RGB image, modeled as the projection of a 3D cuboid. Existing solutions to this problem often rely strongly on hand-engineered features and vanishing point detection, which are prone to failure in the presence of clutter. In this paper, we present a method that uses a fully convolutional neural network (FCNN) in conjunction with a novel optimization framework for generating layout estimates. We demonstrate that our method is robust in the presence of clutter and handles a wide range of highly challenging scenes. We evaluate our method on two standard benchmarks and show that it achieves state of the art results, outperforming previous methods by a wide margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider the task of estimating the spatial layout of a cluttered indoor scene (say, a messy classroom). Our goal is to delineate the boundaries of the walls, ground, and ceiling, as depicted in <ref type="figure" target="#fig_1">Fig. 1</ref>. These bounding surfaces are an important source of information. For instance, objects in the scene usually rest on the ground plane. Many objects, like furniture, are also usually aligned with the walls. As a consequence, these support surfaces are valuable for a wide range of tasks such as indoor navigation, object detection, and augmented reality. However, inferring the layout, particularly in the presence of a large amount of clutter, is a challenging task. Indoor scenes have a high degree of intraclass variance, and critical information required for inferring the layout, such as room corners, is often occluded and must be inferred indirectly.</p><p>There are works which approach the same problem given either depth information (e.g. an RGBD frame) or a sequence of monocular images from which depth can be inferred. For our work, we restrict the input to the most general case: a single RGB image. Given this image, our * indicates equal contribution. <ref type="figure" target="#fig_1">Figure 1</ref>: An overview of our layout estimation pipeline. Each heat map corresponds to one of the five layout labels shown in the final output. They are color coded correspondingly. framework outputs the following: i) a dense per-pixel labeling of the input image (as shown in <ref type="figure" target="#fig_1">Fig. 1)</ref>, and ii) a set of corners that allows the layout to be approximated as the projection of a box. The classes in the dense labeling are drawn from the following set: {Left Wall, Front Wall, Right Wall, Ceiling, Ground}. The parameterization of the scene as a box is described in further detail in Sec. <ref type="bibr">3.2.</ref> Prior approaches to this problem usually follow a twostage process. First, a series of layout hypotheses are generated. Next, these are ranked to arrive at the final layout. The first stage is usually accomplished by detecting three orthogonal vanishing points in the scene, often guided by low-level features such as edges. For instance, the influential work by Hedau et al. <ref type="bibr" target="#b5">[6]</ref> generates layout candidates by inferring vanishing points and then ranking them using a structured SVM. Unfortunately, this first stage is highly susceptible to clutter and often fails to produce a sufficiently accurate hypothesis. While subsequent works have proposed improvements to the second stage of this process (i.e., ranking the layouts), they are undermined by the fragility of the candidate generation.</p><p>Our method is motivated by the recent advances in semantic segmentation using fully convolutional neural networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23]</ref>, since one can consider layout estimation to be a special case of this problem. That said, constraints that are unique to layout estimation prevent a direct application of the existing general purpose semantic segmentation methods. For instance, the three potential wall classes do not possess any characteristic appearance. Multiple sub-objects may be contained within their boundaries, so color-consistency assumptions made by CRF methods are not valid. Furthermore, there is an inherent ambiguity with the semantic layout labels (described in further detail in Sec. <ref type="bibr">3.4.5)</ref>. This is in contrast to traditional semantic segmentation problems where the labels are uniquely defined.</p><p>Our contributions are as follows:</p><p>• We demonstrate that fully convolutional neural networks can be effectively trained for generating a belief map over our layout semantic classes.</p><p>• The FCNN output alone is insufficient as it does not enforce geometric constraints and priors. We present a framework that uses the FCNN output to produce geometrically consistent results by optimizing over the space of plausible layouts.</p><p>Our approach is robust even when faced with a high degree of clutter. We demonstrate state of the art results on two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The problem, as stated in Sec. 1, was introduced by Hedau et al. in <ref type="bibr" target="#b5">[6]</ref>. Their method first estimates three orthogonal vanishing points by clustering line segments in the scene. These are then used for generating candidate box layouts that are ranked using a structured regressor. Unlike our approach, this method requires the clutter to be explicitly modeled. Earlier work in this area by Stella et al. <ref type="bibr" target="#b18">[19]</ref> approached this problem by grouping edges into lines and quadrilaterals and finally depth ordered planes.</p><p>In <ref type="bibr" target="#b19">[20]</ref>, Wang et al. model cluttered scenes using latent variables, eliminating the need for labeled clutter. Extending upon this work in <ref type="bibr" target="#b16">[17]</ref>, Schwing et al. improve the efficiency of learning and inference by demonstrating the decomposition of higher-order potentials. In a subsequent work <ref type="bibr" target="#b15">[16]</ref>, Schwing et al. propose a branch and bound based method for jointly inferring both the layout and the objects present the scene. While they demonstrate that their method is guaranteed to retrieve the global optimum of the joint problem, their approach is not robust to occlusions.</p><p>Pero et al., in <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b12">[13]</ref>, investigate generative approaches for solving layout estimation. Inference is performed using Markov Chain Monte Carlo sampling. By incorporating the geometry of the objects in the scene, their method achieves competitive performance.</p><p>A number of works consider a restricted or special variant of this problem. For instance, Liu et al. <ref type="bibr" target="#b9">[10]</ref> generate the room layout given the floor plan. Similarly, <ref type="bibr" target="#b6">[7]</ref> assumes that multiple images of the scene are available, allowing them to recover structure from motion. This 3D information is then incorporated into an MRF-based inference framework. In <ref type="bibr" target="#b0">[1]</ref>, Chao et al. restrict themselves to scenes containing people. They use the people detected in the scene to reason about support surfaces and combine it with the vanishing point based approach of <ref type="bibr" target="#b5">[6]</ref> to arrive at the room layout.</p><p>More recently, Mallya and Lazebnik <ref type="bibr" target="#b11">[12]</ref> used FCNN for the task, similar to ours. However, while we use an FCNN for directly predicting per-pixel semantic labels, their method uses it solely for generating an intermediate feature they refer to as "informative edges". These informative edges are then integrated into a more conventional pipeline, where layout hypotheses are generated and ranked using a method similar to the one used in <ref type="bibr" target="#b5">[6]</ref>. Their results do not improve significantly upon those achieved by Schwing et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given an RGB image I with dimensions w × h, our framework produces two outputs:</p><p>1. L, a w × h single channel image that maps each pixel in the input image, I ij , to a label in the output image L ij ∈ {Left, Front, Right, Ceiling, Ground}.</p><p>2. The box layout parameters, as described in Sec. 3.2. <ref type="figure">Figure 2</ref>: Given a w × h × 3 input RGB image (shown on the left), our neural network outputs a w × h × 5 belief map, where each of the 5 slices can be interpreted as a classification map for a specific label. For instance, the slice shown on the right corresponds to the ground plane label.</p><p>The pipeline is described broadly in <ref type="figure" target="#fig_1">Fig. 1</ref>. The estimation of L begins by feeding I into the fully convolutional neural network described in Sec. 3.3. The normalized output of this network is a w × h × 5 multidimensional array T which can be interpreted as:</p><formula xml:id="formula_0">T (k) = Pr (L ij = k | I) ∀k ∈ {1, ..., 5}<label>(1)</label></formula><p>where T (k) is the k th channel of the multidimensional array T. This "belief map" is then used as the input to our optimization framework which searches for the maximum likelihood layout estimate that fits our box parameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modeling the Layout</head><p>Most works in this area <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>, including ours, assume that the room conforms to the so-called "Manhattan world assumption" <ref type="bibr" target="#b2">[3]</ref> that is based on the observation that man-made constructs tend to be composed of orthogonal and parallel planes. This naturally leads to representing indoor scenes by cuboids. The layout of an indoor scene in an image is then the projection of a cuboid.</p><p>Hedau <ref type="bibr" target="#b5">[6]</ref> and Wang <ref type="bibr" target="#b19">[20]</ref> describe how such a cuboid can be modeled using rays emanating from mutually orthogonal vanishing points. The projection of such a cuboid can be modeled using four rays and a vanishing point <ref type="bibr" target="#b19">[20]</ref>, as described in <ref type="figure" target="#fig_0">Fig. 3</ref>. Our parameterization of this model is τ = (l 1 , l 2 , l 3 , l 4 , v), where l i is the equation of the i th line and v is the vanishing point.</p><p>Given τ , we can partition an image into polygonal regions as follows:</p><p>• The intersections of the lines l i give us four vertices p i . The polygon described by these four vertices corresponds to one of the walls.</p><p>• The intersections of the rays starting at v passing through p i with the bounds of the image give us four more vertices, e i . We can now describe four additional polygons defined by (p i , e i , e i+1 , p i+1 ) (where the index additions are modulo 4). These correspond to two additional walls, the ceiling, and the ground.</p><p>The vertices p i and e i may lie outside the bounds of the image, in which case the corresponding polygons are either clipped or absent entirely. We also define a deterministic labeling for these polygons. The top and bottom polygon are free from ambiguity and always labeled as "ceiling" and "ground". The polygons corresponding to the walls are labeled left to right as (left, front, right). If only two walls are visible, they are always labeled (left, right). If only a single wall is visible, it is labeled as "front".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Belief Map Estimation via FCNN</head><p>Deep convolutional neural networks (CNN) have achieved state-of-the-art performance for various vision tasks like image classification and object detection. Recently, they have been adapted for the task of semantic segmentation with great success. Nearly all top methods on the PASCAL VOC segmentation challenge <ref type="bibr" target="#b3">[4]</ref> are now based on CNNs. The hierarchical and convolutional nature of these networks is particularly well suited for layout segmentation. Global context cues and low-level features learned from the data can be fused in a pipeline that is trained from end-to-end.</p><p>Our CNN uses the architecture proposed by Chen et al. in <ref type="bibr" target="#b1">[2]</ref>, which is a variant commonly referred to as a fully convolutional neural network (FCNN). Most common CNN architectures used for image classification, such as AlexNet <ref type="bibr" target="#b8">[9]</ref> and its variants, incorporate fully-connected terminal layers that accept fixed-sized inputs and produce</p><formula xml:id="formula_1">A B C D E F G H I J K L M N O P Q R S T U V W X</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Input Image Convolution Layer</head><p>Max Pooling Layer Interpolated Output <ref type="figure">Figure 4</ref>: The fully convolutional network architecture used for our layout estimation. It is based on the "LargeFOV" variant described in <ref type="bibr" target="#b1">[2]</ref>, which in turn is based on the VGG16 architecture proposed by Simonyan and Zisserman in <ref type="bibr" target="#b17">[18]</ref> . Each convolution layer depicted in this figure is followed by a rectified linear unit (ReLU), excluding the final one (layer W ). During training, dropout regularization is applied to layers U and V .</p><p>non-spatial outputs. In <ref type="bibr" target="#b10">[11]</ref>, Long et al. observe that these fully-connected layers can be viewed as convolutions with kernels that cover their entire input regions. Thus, they can be re-cast into convolutional layers which perform slidingwindow style dense predictions. This conversion also removes the fixed-size constraint previously imposed by the fully connected layers, thereby allowing the new networks to operate on images of arbitrary dimensions.</p><p>One caveat here is that the initial network produces dense classification maps at a lower resolution than the original image. For instance, the network proposed by <ref type="bibr">Long et al.</ref> produces an output that is subsampled by a factor of 32. They compensate for this by learning an upsampling filter, implemented as a deconvolutional layer in the network. In contrast, our network produces an output that is subsampled by only a factor of 8. As a result, simple bilinear interpolation can be used to efficiently upsample the classification map.</p><p>We finetune a model pretrained on the PASCAL VOC 2012 dataset. The weights for the 21-way PASCAL VOC classifier layer (corresponding to layer W shown in <ref type="figure">Fig. 4</ref>) are discarded and replaced with a randomly initialized 5way classifier layer. Since our belief map before interpolation is subsampled by a factor of 8, the ground truth labels are similarly subsampled. The loss function is then formulated as the sum of cross entropy terms for each spatial position in this subsampled output. The network is trained using stochastic gradient descent with momentum for 8000 iterations. Chen et al. describe an efficient method for performing convolution with "holes" [2] -a technique adopted from the wavelet community. We use their implementation of this algorithm within the Caffe framework <ref type="bibr" target="#b7">[8]</ref> to train our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Refinement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">The Problem</head><p>Given the CNN output T, a straightforward way to obtain a labeling is to simply pick the label with the highest score for each pixel:</p><formula xml:id="formula_2">L ij = arg max k T (k) ij ∀i ∈ [1, ..., w], j ∈ [1, ..., h] (2)</formula><p>However, note that there are no guarantees that this layout will be consistent with the model described in Sec. 3.2. Indeed, the wall/ground/ceiling intersections inL are always "wavy" curves (rather than straight lines), and often contain multiple disjoint connected components per label. This is because our CNN does not enforce any smoothness or geometric constraints. A common solution used in general semantic segmentation is to refine the output using a CRF. These usually use the CNN output as the unary potentials and define a pairwise potential over color intensities <ref type="bibr" target="#b1">[2]</ref>. However, we found these CRF based methods to perform poorly in the presence of clutter, where they tend to segment along the clutter boundaries that occlude the true wall/ground/ceiling intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Overview of our Approach</head><p>Given the neural network output T, we want to obtain the refined box layout, τ * , and the corresponding label-map, L * . Let f be a function that maps a layout (parametrized as described in Sec. 3.2) to a label-map. Then, we have:</p><formula xml:id="formula_3">L * = f (τ * )<label>(3)</label></formula><p>For any given layout, we define the following scoring metric:</p><formula xml:id="formula_4">S (L = f (τ ) | T) = 1 wh i,j T (Lij ) ij<label>(4)</label></formula><p>We now pose the refinement process as the following optimization problem:</p><formula xml:id="formula_5">τ * = arg max τ S (f (τ ) | T)<label>(5)</label></formula><p>This involves ten degrees of freedom: two for each of the four lines, and two for the vanishing point. While searching over the entire space of layouts is intractable, we can initialize the search very close to the solution usingL. Furthermore, we can use geometric priors to aggressively prune the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Preprocessing</head><p>We useL (as defined in Eq. 2) for initialization and determining which planes are not visible in the scene. An issue here is thatL may contain spurious regions. In particular, it often includes spurious front wall regions (not surprisingly, given the ambiguity described in Sec. 3.4.5). Furthermore, there may be multiple disjoint components per label. We address the multiple disjoint components by pruning all but the largest connected component for each label. The presence of potentially spurious regions is addressed by considering two candidates in parallel: one with these regions pruned, and another with them preserved. The candidate with the highest score is selected at the end of the optimization. In practice, we found that it is sufficient to restrict this pruning to just the front wall. The "holes" in labeling created by pruning are filled in using a k-nearest neighbor classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Initialization</head><p>Given a preprocessedL, we can produce an initial estimate of the four lines l i in τ by detecting the wall/ceiling/ground boundaries. We do this by considering each relevant pair of labels (say, ground and front wall) and treating it as a binary classification problem. The corresponding line is then obtained using logistic regression. Given an input image, I, so far we have described how to obtain:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5">Optimization</head><p>• A "belief map" T using our neural network • A scoring function S that can be used for comparing layouts</p><p>• An initial layout estimate τ 0</p><p>To obtain the final layout, τ * , we use an iterative refinement process. Our optimization algorithm, described in algorithm 1, is reminiscent of coordinate ascent. It greedily optimizes each parameter in τ sequentially, and repeats until no further improvements in the score can be obtained.</p><p>Sampling vanishing points: We start with the vanishing point, v ∈ τ , since our initialization only provides us estimates for the four lines. While we could have usedL to provide an initial estimate for v, we found that directly using grid search works better in practice. The feasible region for v is the polygon described by the vertices (p 1 , p 2 , p 3 , p 4 ) as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. We evenly sample a grid within this region to generate candidates for v. For each candidate vanishing point, we compute the score using S and update our parameters if the score improves.</p><p>Sampling lines: Next, each line, l i ∈ τ , is sequentially optimized. The search space for each line is the local neighborhood around the current estimate. Let (x 1 , y 1 ) and (x 2 , y 2 ) be the intersections of l i with the image bounds. We evenly sample two sets of points centered about (x 1 , y 1 ) and (x 2 , y 2 ) along the image boundary. Our search space for l i is then the cartesian product of these two sets.</p><p>Handling label ambiguity: There is an inherent ambiguity in the semantic layout labels as demonstrated in <ref type="figure">Fig. 5</ref>. Indeed, our network often has trouble emitting a consistent label when faced with such a scenario. In such cases, the probability is split between the labels "front" and either "left" or "right" as the case may be. Our existing scoring function does not take this issue into account. Therefore, for this special case, we formulate a modified scoring function as follows:Ŝ = max (S(L ′ ), S(L))</p><p>where L ′ is the labeling obtained by replacing all occurrences of the label "front" with either "left" or "right". This allows our optimization algorithm to commit to a label without being unfairly penalized. Note that this modified scoring function is only used when our optimizer is considering a "two-wall" layout scenario as determined during initialization. For a single-wall or three-wall case, the ambiguity issue does not apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We train our network on the Large-scale Scene Understanding Challenge (LSUN) room layout dataset <ref type="bibr" target="#b20">[21]</ref>, a diverse collection of indoor scenes with layouts that can be approximated as boxes. It consists of 4000 training, 394 <ref type="figure">Figure 5</ref>: There is an inherent ambiguity in semantically labeling layouts. Two plausible labelings are shown above. A human may reasonably label the wall behind the bed as "front", whereas a labeling that enforces consistency based on left-to-right ordering may classify it as "right". validation, and 1000 testing images. While these images have a wide range of resolutions, we rescale them anisotropically to 321 × 321 pixels using bicubic interpolation. The ground truth images are relabeled to be consistent with the ordering described in Sec 3.2.</p><p>We also test on the dataset published by Hedau et al. <ref type="bibr" target="#b5">[6]</ref>. This consists of 209 training and 104 testing images. We do not use the training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Accuracy</head><p>We evaluate our performance by measuring two standard metrics:</p><p>1. The pixelwise accuracy between the layout and the ground truth, averaged across all images.</p><p>2. The corner error. This is the error in the position of the visible vertices p i and e i (as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>), normalized by the image diagonal and averaged across all images.</p><p>We use the LSUN room layout challenge toolkit scripts to evaluate these. The toolkit addresses the labeling ambiguity problem by treating it as a bipartite matching problem, solved using the Hungarian algorithm, that maximizes the consistency of the estimated labels with the ground truth. Our performance on both datasets are summarized in Tables 1 and 2. Our approach outperforms all prior methods and achieves state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Efficiency</head><p>The CNN used in our implementation can process 8 frames per second on an Nvidia Titan X. For optimization, we use a step size of 4 pixels for sampling lines and a grid of 200 vanishing points. With these parameters, the optimization procedure takes approximately 30 seconds per frame. The current single-threaded implementation is not tuned  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Qualitative Analysis</head><p>We analyze the qualitative performance of our layout estimator on a collection of scenes sampled from the LSUN validation set. We split our analysis into two broad themes: i) scenarios where our estimator performs well and demonstrates unique strengths of our approach, and ii) scenarios that demonstrate potential weaknesses of our framework and provide insight into future avenues for improvement. <ref type="figure">Fig. 6</ref> shows a collection of scenes where our estimator produces layouts that closely match the human-annotated ground truths. <ref type="figure">Fig. 6a</ref> shows the robustness of our estimator to a high degree of clutter. The ground plane's intersections with the walls are completely occluded by the table. The decorative fixture near the ceiling not only occludes the top corner but also includes multiple strong edges that can be easily confused for wall/ceiling intersections. Despite these challenges, our framework produces a highly accurate estimate. <ref type="figure">Fig. 6f</ref> shows a scene with illumination variation and nearly uniform wall, ground, and ceiling colors with almost no discernible edges at their intersections. Such scenes are <ref type="figure">Figure 6</ref>: Our results on the LSUN validation set. The first row shows the input images. The second row depictsL, the most probable label per pixel before optimization. The third row comprises our final layout estimate L * . The fourth row is the ground truth, while the fifth is our estimate superimposed on the input image. A detailed analysis of these images is provided in Sec. 5.</p><formula xml:id="formula_7">(a) (b) (c) (d) (e) (f)</formula><p>particularly challenging for methods that rely on low-level features and color consistency. However, our method is able to recover the layout almost perfectly. <ref type="figure">Fig. 6e</ref> shows an example where the Manhattan world assumption is violated. Our estimate, however, degrades gracefully. Arguably, it is no less valid than the provided ground truth image, which also attempts to fit a boxy layout to the non-conforming scene. <ref type="figure">Fig. 6d and 6c</ref> show the effectiveness of our optimization procedure, and demonstrate that simply trusting the CNN output is insufficient. Directly using the estimateL produces garbled results with inconsistencies like multiple disjoint components for a single label and oddly shaped boundaries. However, our optimizer is able to successfully recover the layout. It also shows the labeling ambiguity issue we described in Sec. 3.4.5. Observe that the CNN's confidence is split over the front and right wall classes in the ambiguous region. However, our modified scoring function is able to successfully handle this case.</p><p>In <ref type="figure">Fig. 7</ref>, we explore some of the scenarios where our estimator fails to produce results that agree with the human annotations. <ref type="figure">Fig. 7a</ref> is an interesting case where our estimate predicts a left wall that is absent from the ground truth. A closer observation of the image reveals that there is indeed a left wall present (this scene also violates the Manhattan assumption). <ref type="figure">Fig. 7c</ref> shows a scenario where our CNN  <ref type="figure">Figure 7</ref>: A set of challenging cases where our estimator's results do not match the human-annotated ground truths. The first column is the input image, the second isL, the third is our final layout estimate L * , while the fourth is the ground truth. produces a reasonably accurate output, but our optimizer incorrectly prunes the front wall as a spurious region. Occasionally, we encounter cases where the CNN predictions differ so drastically from the ground truth that the optimizer fails to provide any improvements. Such a case is shown in <ref type="figure">Fig. 7d</ref> where the presence of a strong color change in the wall causes our network to consider it as a separate wall. In <ref type="figure">Fig. 7b</ref>, we demonstrate a "semantic failure". The lower half of the scene is dominated by a bed. It is geometrically consistent with the notion of a ground plane, but not semantically.</p><p>Observing the results above, a few patterns emerge that lend themselves to future improvements. For instance, the CNN output suggests that the Manhattan world assumption is not strictly necessary. The issue with the bed in <ref type="figure">Fig. 7b</ref> illustrates the importance of incorporating broader semantics into the room layout estimation problem. A promising approach here would be to train a CNN for performing joint segmentation of both layout and object classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we presented a framework for estimating layouts for indoor scenes from a single monocular image. We demonstrated that a fully convolutional neural network can be adapted to estimate layout labels directly from RGB images. However, as our results show, this output alone is insufficient as the neural network does not enforce geometric consistency. To address this issue, we presented a novel optimization framework that refines the neural network output to produce valid layouts. Our method is robust to clutter and works on a wide range of challenging scenes, achieving state-of-the-art results on two leading room layout datasets and outperforming prior methods by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The layout parametrized using four lines, (l 1 , l 2 , l 3 , l 4 ), and a vanishing point, v, as described in Sec. 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Layout Optimization Input: T // The output of our CNN (l 1 , l 2 , l 3 , l 4 ) // Initialization Output: Layout τ * = (l 1 , l 2 , l 3 , l 4 , v) repeat foreach Candidate vanishing point p do evaluate S (τ = (l 1 , l 2 , l 3 , l 4 , p) | T) if Score Improved then v := p end end foreach i ∈ (1...4) do foreach Candidate line l do evaluate S (τ = (l 1 , ...l..., l 4 , v) | T)if Score Improved then l i := l end end end until Score did not improve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Further examples that demonstrate our estimator's performance on the LSUN validation set. Column 2 is our estimate, while column 3 is the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Performance on the LSUN [21] dataset</figDesc><table>for performance, and significant improvements should be 
achievable (for instance, by parallelizing the sampling loops 
and utilizing SIMD operations). 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Layout estimation of highly cluttered indoor scenes using geometric and semantic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Processing-ICIAP 2013</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="489" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The manhattan world assumption: Regularities in scene statistics which enable bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="845" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1288" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Computer vision</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Surface layout estimation using multiple segmentation methods and 3d reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hödlmoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rent3d: Floor-plan priors for monocular layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3413" to="3421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning informative edge maps for indoor scene layout prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding bayesian rooms using composite 3d object models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bowdish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kermgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian geometric modeling of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Pero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bowdish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kermgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Manhattan junction catalogue for spatial reasoning of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Box in the box: Joint 3d layout and object reasoning from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient structured prediction for 3d indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Inferring spatial layout from a single image via depth-ordered grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative learning with latent variables for cluttered indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Largescale scene understanding challenge: Room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scene parsing by integrating function, geometry and appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3119" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno>abs/1502.03240</idno>
		<title level="m">Conditional random fields as recurrent neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
