<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongye</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
							<email>yaoweiwang@bit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Electronics</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
							<email>tjhuang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The growing explosion in the use of surveillance cameras in public security highlights the importance of vehicle search from a large-scale image or video database. However, compared with person re-identification or face recognition, vehicle search problem has long been neglected by researchers in vision community. This paper focuses on an interesting but challenging problem, vehicle re-identification (a.k.a precise vehicle search). We propose a Deep Relative Distance Learning (DRDL) method which exploits a two-branch deep convolutional network to project raw vehicle images into an Euclidean space where distance can be directly used to measure the similarity of arbitrary two vehicles. To further facilitate the future research on this problem, we also present a carefully-organized largescale image database "VehicleID", which includes multiple images of the same vehicle captured by different realworld cameras in a city. We evaluate our DRDL method on our VehicleID dataset and another recently-released vehicle model classification dataset "CompCars" in three sets of experiments: vehicle re-identification, vehicle model verification and vehicle retrieval. Experimental results show that our method can achieve promising results and outperforms several state-of-the-art approaches. ). stalled for license plate capturing, thus, plate recognition performance drops dramatically on images/video data captured by these cameras. Furthermore, license plates are often occluded, removed, or even faked in a large number of previous security events. Therefore, vision-based vehicle re-identification has a great practical value in realworld surveillance applications. Specifically, vehicle reidentification is the problem of identifying the same vehicle across different surveillance camera views. <ref type="figure">Fig. 1</ref> gives a straightforward description of it.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays, there is an explosive growing requirement of vehicle search and re-identification (Re-ID) from largescale surveillance image and video database in public security systems. License plate naturally is an unique ID of a vehicle, and license plate recognition has already been used widely in transportation management applications. Unfortunately, we can not identify a vehicle simply by its plate in some cases. First, most surveillance cameras are not in- <ref type="bibr">Figure 1</ref>. Given multiple candidates as the gallery set, the vehicle re-identification task is to find the matched one for each probe image. Notice that the illumination and viewpoint in different cameras can be varied a lot and different vehicles could be quite similar if they are of the same model.</p><p>Though the problem of vehicle re-identification has already been discussed for many years, most of the existed works rely on a various of different sensors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. To our knowledge, there is no previous attempt on the vehicle re-identification task purely by vehicle's visual appearance yet and the primary reason could be the lack of highquality and large-scale vehicle Re-ID datasets. Existed vehicle datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref> are usually designed for vehicle at-tributes recognition(e.g. color, type, make, and model). In this paper, we present a new vehicle re-identification dataset named "VehicleID", which is collected from multiple realworld surveillance cameras and includes over 200,000 images of about 26,000 vehicles. All images are attached with id numbers indicating their true identities(according to the vehicle's license plate). In addition, nearly 90,000 images of 10,319 vehicles in this dataset have been labeled with the vehicle model information. Thus, it can also be used for fine-grained vehicle model recognition.</p><p>Another potential reason may be that compared with the classic person re-identification problem, vehicle reidentification could be more challenging as too many (usually thousands) vehicles of one same model have similar visual appearance. It is really difficult even for humans to tell the difference between vehicles of the same model without using their license plates. Nevertheless, there are some special marks that can be used to identify a vehicle from others, such as some customized painting, favorite decorations, or even scratches etc. (as illustrating in <ref type="figure" target="#fig_0">Fig. 2</ref>). Therefore, vehicle re-identification algorithm should be able to capture both the inter-class and intra-class difference efficiently. Deep feature has been proved more effective and robust for recognition task. Inspired by one of the state-ofthe-art method in person re-identification <ref type="bibr" target="#b3">[4]</ref>, we propose a Deep Relative Distance Learning (DRDL) model to address the vehicle re-identification problem.</p><p>DRDL is an end-to-end framework <ref type="figure" target="#fig_1">(Fig. 3</ref>) specifically designed for vehicle re-identification. It aims to learn a deep convolutional network that can project raw vehicle images into an Euclidean space where the L2 distance can thus be used directly to measure the similarity of arbitrary two vehicles. The basic idea of DRDL is to minimize the distances of the same vehicle images and maximize those of other vehicles. Therefore, a coupled cluster loss function and a mixed difference network structure are introduced in DRDL framework. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the input of DRDL are two image sets: one positive set (images of the same vehicle identity) and one negative set (images of other vehicles). The coupled cluster loss is to pull the positive images closer and push those negative ones far away. While the mixed difference network structure will benefit the mapping model with more explicit model information. Namely, deep feature and the distance metric are learned simultaneously in an unified DRDL framework. The experimental results show that our method can achieve promising results and outperforms several state-of-the-art approaches.</p><p>Rest of the paper is organized as follows: Related works are reviewed in section 2. In section 3, we discuss our coupled clusters loss function and a unified deep network structure specifically designed for vehicle re-identification. Section 4 gives an detailed description of our dataset "Vehi-cleID" including how we collect and organize the raw im-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most previous object identification research targets at either person or human face. Both of them have long been popular topics in computer vision communities and can be described as an unified problem: given a probe image and multiple candidates as the gallery, we need to decide which one in gallery is the same object of the probe image. However, there is not much work on vehicle re-identification before even though vehicle is at least of equal importance as person and human face in real-world applications. The most closely related problems which targets at vehicle include vehicle model classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> and vehicle model verification <ref type="bibr" target="#b20">[21]</ref>. But being different from our task, all those methods can only reach the vehicle model level instead of identifying whether two vehicles are exactly the same one. Thus, person re-identification is actually the most closely related problem of ours.</p><p>Existed approaches of person re-identification mainly rely on handcrafted features like color or texture histograms and then try to model the transformation of person's ap-pearance across different cameras <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. Zhao et al. <ref type="bibr" target="#b23">[24]</ref> proposed to make use of mid-level features from automatically discovered hierarchical patch cluster trees for viewinvariant and discriminative feature extraction.</p><p>Recently, deep convolutional network is also introduced into person re-identification problem. Yi et al. <ref type="bibr" target="#b21">[22]</ref> applied a "siamese" deep network which has a symmetric structure with two sub-network to learn pair-wise similarity. Ahmed et al. formulate the person re-identification as a binary classification problem <ref type="bibr" target="#b0">[1]</ref> and solve it with a specifically designed deep neural network. The two input images are first fed into two convolutional layers to extract high level features and then mixed together by a difference measurement layer together with several other fully-connected layers. The last layer in this network is a softmax function to yield the final estimate of whether the input images are of the same person.</p><p>The most similar model of our proposed method is the triplet loss deep convolutional network. Connecting deep convolutional network for feature extraction and a special triplet loss has achieved state-of-the-art performance in both person re-identification and face recognition problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. It is assumed that samples of the same identity should be closer from each other compared to samples of different identities. By optimizing the specifically designed triplet loss function, the network will gradually learn a harmonic embedding of each input image in Euclidean space that tends to maximize the relative distance between the matched pair and the mismatched pair. But generating all possible triplets would result in numerous triplets and most of them are too easy to distinguish that would not make any contribution to the loss convergence in training phase. Either offline or online selection in small batch-size data need to be done in advance.</p><p>In this paper, we first present a large-scale dataset that contains not only a large number of vehicles captured by real-world surveillance cameras but also multiple images of each vehicle that were captured across different time or cameras. Each vehicle image is attached with an unique id by its license plate. To the best of our knowledge, there are no similar large-scale datasets before which includes multiple images captured by multiple different cameras for each vehicle. We call this dataset as "VehicleID". Then, we propose an end-to-end framework DRDL that are suited for both vehicle retrieval and vehicle re-identification tasks. Notice that two different vehicles(with different license plates) could be almost the same regarding their appearance if they belong to the same model. We aim to capture both the inter-model difference and intra-model difference between different vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Relative Distance Learning</head><p>Technically, there are mainly two core components we need to consider for a re-identification problem: a method for feature extraction and a distance metric to compare features across different images. Previous research on reidentification usually focuses either on designing better handcrafted features or building a more comprehensive metric model. But it is mostly empirically determined about which method for feature extraction and which method for distance metric. Can we have an unified model to accomplish both the two tasks? We believe deep convolutional network could be a good answer. In fact, all deep learning approaches for re-identification we mentioned in section 2 are able to extract features and measure the difference at the same time: for each image feed into the network, we get either its embedding in Euclidean space or the similarity estimation with other images directly. The feature extraction component and distance metric component are both contained inside the network. To utilize the advantage of deep convolutional network in vehicle re-identification problem and inspired by the recently proposed triplet loss <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref>, we propose an enhanced model in this paper named Deep Relative Distance Learning(DRDL). Different with other existed frameworks, we designed a new loss function to accelerate the training convergence and add an extra branch to measure the instance difference between different vehicles while of the same model. Here for concreteness, we first briefly review the triplet loss and then discuss the details of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Triplet Loss</head><p>In a standard triplet loss network, the inputs are a batch of triplet units {&lt; x a , x p , x n i &gt;} where x a and x p belong to the same identity while x a and x n belong to the different identities. Let f (x) denote the network's feature representation of image x. For a training triplet &lt; x a , x p , x n &gt;, the ideal feature representation of them should satisfy the following constraint:</p><formula xml:id="formula_0">∥f (x a ) − f (x p )∥ + α ≤ ∥f (x a ) − f (x n )∥ (1) or equally ∥f (x a ) − f (x p )∥ 2 + α ≤ ∥f (x a ) − f (x n )∥ 2<label>(2)</label></formula><p>where α is a predefined constant parameter representing the minimum margin between matched and mismatched pairs. In addition, to avoid the loss function easily exceeding 0, all image features are constrained in a d-dimensional hypersphere ∥f (x)∥ 2 2 = 1. This normalization step is also performed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. <ref type="figure" target="#fig_2">Fig. 4</ref> visualizes Eq.(2) in a more intuitive way. Thus, the loss function can be defined as However, there exists some special cases that the triplet loss may judge falsely when processing randomly selected triplet units. Given 3 samples that two of them belong to the same identity and the other belongs to a different one, there are two different ways of building triplets as the network's input data. <ref type="figure" target="#fig_3">Fig. 5</ref> shows both of them. In the left case, the triplet loss can easily detect the abnormal distance relationship since the intra-class distance is larger than inter-class distance. But the right case is a bit different. The triplet loss is 0 since the distance between anchor and positive point is indeed smaller than the distance between anchor and negative point. Thus, the network will just neglects this triplet during backward propagation. Moreover, since the backward propagation of triplet loss is actually pulling positive point toward anchor point and pushing negative point toward the opposite direction of anchor point, the loss function is quite sensitive to the selection of anchor point. This means improper anchors can result in great interference in the training stage and lead to a slow convergence. We need a lot more proper triplets to correct it.</p><formula xml:id="formula_1">L = N ∑ max{∥f (x a )−f (x p )∥ 2 2 +α−∥f (x a )−f (x n )∥ 2 2 , 0}<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Coupled Clusters Loss</head><p>In order to make the training phase more stable and accelerate the convergence speed, we propose a new loss function to replace the triplet loss here: coupled clusters loss(CCL). We also use a deep convolutional network to extract features for each image here. But the triplet input is replaced by two different image sets: one positive set and one negative set. The former set X p = {x p 1 , · · · , x p N p } contains N p images of the same identity and the other one X n = {x n 1 , · · · , x n N n } contains N n images of other different identities. It is assumed that samples belong to the same identity should locate around a common center point in the d-dimensional Euclidean space. Thus, samples in the positive set should form a cluster together and samples in the negative set should stay relatively far away. <ref type="figure" target="#fig_6">Fig. 7</ref> illustrates the ideal situation.</p><p>We first estimate the center point as the mean value of all positive samples</p><formula xml:id="formula_2">c p = 1 N p N p ∑ i f (x p i )<label>(4)</label></formula><p>The relative distance relationship is reflected as</p><formula xml:id="formula_3">∥f (x p i ) − c p ∥ 2 2 + α ≤ ∥f (x n j ) − c p ∥ 2 2 ∀1 ≤ i ≤ N p and 1 ≤ j ≤ N n<label>(5)</label></formula><p>The coupled clusters loss function is defined as </p><formula xml:id="formula_4">L(W, X p , X n ) = ∑ N p i 1 2 max{0, ∥f (x p i ) − c p ∥ 2 2 + α − ∥f (x n * ) − c p ∥ 2 2 } (6) where x n * is the nearest negative sample to the center point. If ∥f (x p i ) − c p ∥ 2 2 + α − ∥f (x n * ) − c p ∥ 2 2 ≤ 0,</formula><formula xml:id="formula_5">∂L ∂f (x p i ) = f (x p i ) − c p<label>(7)</label></formula><p>The partial derivative of the nearest negative sample is</p><formula xml:id="formula_6">∂L ∂f (x n * ) = c p − f (x n * )<label>(8)</label></formula><p>The main idea behind Eq.(5) is absolutely the same as the triplet constraint in Eq.(2) that intra-class distance should be smaller than the inter-class distance. But the way we express it is quite different:</p><p>• Distances are measured between samples and a cluster center rather than any randomly selected anchor samples;</p><p>• The coupled clusters loss function is defined over multiple samples instead of three.</p><p>The first requirement ensures the distances we get and the direction the samples will be moved to(the loss function's partial derivative of each input sample) in backward propagation step are more reliable than the original triplet loss since the randomly selected anchor is replaced by the cluster center. Then, the specifically designed loss function guarantees all positive samples which are not close enough to the center will move closer(samples which are already close enough will be neglected). The selection of the nearest negative sample x n * will further prevent the relative distance relationship Eq.(5) being too easily satisfied compared with a randomly selected negative reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mixed Difference Network Structure</head><p>There is a small but quite important difference between identifying a specific vehicle and person. In theory, any two  persons could not be exactly the same regarding their visual appearance but two vehicles running on road could be if they belong to the same vehicle model(for instance, they both are Audi A4L 2009). But in real-world scenes, it is still possible to distinguish two vehicles of the same model if some special markers exist which are showed previously in <ref type="figure" target="#fig_0">Fig. 2</ref>. To deal with this case, the distance measurement between a probe image and multiple candidate vehicles should include two kind of differences: whether they belong to the same vehicle model and whether they are the same vehicle. Since existing models for person re-identification did not really consider this, we propose a new network structure to better measure these two difference here.</p><p>The base network structure used in our experiments is VGG CNN M 1024 <ref type="bibr" target="#b1">[2]</ref>. It contains 5 convolutional layers and 2 fully-connected layers. The dimension of the network's last fully-connected layer "fc7" is 1024.</p><p>But since the single branch network structure is not capable of extracting both the vehicle model information and the instance difference between two candidates of the same vehicle model both, we extend the single branch network to a two branches network. <ref type="figure" target="#fig_5">Fig. 6</ref> illustrates the detailed structure.</p><p>Notice that the last fully connected layer "fc8" is a mixed feature of both the vehicle's model information and the feature representation learned from single triplet loss or our coupled clusters loss. The idea behind is quite simple: two vehicle images are definitely different vehicles if they are of different vehicle model and in the other case, i.e. they belong to the same vehicle model, an extra instance difference measurement is needed(The dimension of "fc8" is set to 1024 in accordance with the output dimension of standard VGG CNN M 1024 network to eliminate the influence of feature dimensional difference when performing evaluation experiments). "fc7 2" in the mixed difference network is just the same as the output feature of a standard VGG CNN M 1024 network while "fc8" is an enhanced one suitable for both inter-model difference and intra-model difference metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training the Network</head><p>All the networks in our experiments are trained with the widely-used deep learning framework "Caffe" <ref type="bibr" target="#b7">[8]</ref>. Training data consist of multiple positive and negative image sets and the corresponding labels(i.e. ID and vehicle model). Specifically in our experiments, our networks are all fine tuned on VGG CNN M 1024 which is pre-trained with the ImageNet dataset in ILSVRC-2012 <ref type="bibr" target="#b14">[15]</ref>. We use a momentum of µ = 0.9 and weight decay λ = 2 × 10 −4 . The sizes of both the positive and negative sets are set to 5(images). Batch-size is set to 15 which means we need to feed 15 * (5 + 5) = 150 images in each training iteration. We start with a base learning rate of η (0) = 0.01 and then drops by repeatedly multiply 0.7 after every 8000 batch iterations: η (i) = η (0) 0.7 ⌊i/8000⌋ . Loss weights of the upper branch(softmax), lower branch(CCL) and the final one(CCL) are 0.5, 0.5, 1.0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">VehicleID Dataset</head><p>As mentioned in section 1, the identity information is not available in "CompCars" dataset <ref type="bibr" target="#b20">[21]</ref> and we aim to go further than the existed vehicle model recognition task, a more suitable large-scale vehicle dataset named "VehicleID" 1 for re-identification task has been carefully collected and organized by us. The "VehicleID" dataset contains data captured during daytime by multiple real-world surveillance cameras distributed in a small city in China. Similar to existed person re-identification datasets, each vehicle ever appeared includes more than one images. Thus, this dataset could be well suitable for vehicle search related tasks.</p><p>In addition, we carefully labeled 10319 vehicles(90196 images in total) of their vehicle model information. But different from the "CompCars" dataset <ref type="bibr" target="#b20">[21]</ref>, our dataset does not targets at fine-grained vehicle model classification task since the model distribution is usually quite imbalanced in real-world scenarios. In "VehicleID" dataset, only 250 most commonly appeared vehicle models are included(like "MINI-cooper", "Audi A6L" and "BWM 1 Series"). <ref type="figure" target="#fig_8">Fig. 9</ref> shows the exact numbers of each vehicle model. As we can see, the most commonly seen vehicle models like "Buick Excelle", "Chevrolet Cruze" and "Volkswagen Lavida" all have more than 200 different vehicles(2000+ images) each, while the most seldom seen vehicle models like "C-Quatre", "Toyota Prado" and "Subaru Forester" have only 1 vehicle of each model.</p><p>The "VehicleID" dataset is captured by multiple nonoverlapping surveillance cameras and there are 221763 images of 26267 vehicles in total(8.44 images/vehicle in average). Besides, the vehicle in each image is either captured from the front or the back(viewpoints information is not labeled). <ref type="figure" target="#fig_7">Fig. 8</ref>   Considering the total number of testing data is still too large compared with ordinary testing data for person reidentification(316 pedestrians in VIPeR dataset <ref type="bibr" target="#b6">[7]</ref>, 50 in iLIDS dataset <ref type="bibr" target="#b19">[20]</ref>), we further extract three subsets(i.e. small, medium and large) ordered by their size from the original testing data for our vehicle retrieval and vehicle re-identification tasks. The quantity distribution of "Vehi-cleID" is demonstrated in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Two datasets including "CompCars" <ref type="bibr" target="#b20">[21]</ref> and our "Ve-hicleID" dataset are used to evaluate our method. Both of them have a large collection of different vehicle pictures but originally designed for different tasks.</p><p>"CompCars" is a recently released large-scale and comprehensive image database, much larger both in scale and diversity compared with other existed vehicle image datasets. There are 214, 345 images(collected from the Internet) of 1, 716 car models in total and the entire dataset has been split into three subsets. Following the pipeline of car model verification <ref type="bibr" target="#b20">[21]</ref>, we use the Part-I subset which contains 431 car models with a total of 30955 images capturing the entire car for model training and evaluate our method in vehicle verification task on the Part-III subset which contains 22236 images of 1145 models. Notice that the Part-II subset which contains a list of matched or mismatched pair data is not being utilized when training our model because our network is not designed for pair-wise similarity learning.</p><p>We use VGG CNN M 2048 <ref type="bibr" target="#b1">[2]</ref> and its mixed difference version in section 3.3 as the feature extractor in all our experiments. Theoretically, other convolutional networks like GoogleNet <ref type="bibr" target="#b18">[19]</ref>, VGG ILSVRC 19 layers <ref type="bibr" target="#b16">[17]</ref> can also be embedded in our framework. To make a comprehensive evaluation of our proposed model, we designed 3 different experiments all together: vehicle model verification, vehicle retrieval and vehicle re-identification. The detailed description and the final results are in the following three subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Vehicle Model Verification</head><p>Since vision based vehicle re-identification problem is never deeply explored before and most vehicle search techniques are based on analyzing vehicle model information.</p><p>We first perform the vehicle model verification task following the pipeline of face verification on "CompCars" dataset to give an overview of our proposed method. This task can be described as: given two vehicle images, we need to verify whether they belong to the same vehicle model. Notice that neither our method nor the standard triplet loss network is designed for vehicle model verification or classification tasks. We did not really expect our model to have a comparable result compared with other mature solutions from face verification problem at first.</p><p>Three other methods are introduced to perform the comparison experiments. The experimental results of the first two methods, "Deep Feature+SVM or Joint Bayesian", are referred from Yang's paper <ref type="bibr" target="#b20">[21]</ref>. They first utilize a deep convolutional network to train a vehicle model classification model on Part-I data of "CompCars". Then, Joint Bayesian <ref type="bibr" target="#b2">[3]</ref> or SVM is applied to train a verification model on Part-II data with the classification network in step 1 as a feature extractor. The third algorithm, "VGG CNN M 1024" network with triplet loss function is trained with Part-I data of "CompCars" and the corresponding vehicle model labels. Part-II data is not being used since pair-wise data is not suitable for the network's input.</p><p>The training process of our method is quite similar to the triplet loss network except that the vehicle make information is also introduced to assist the model training and the standard "VGG CNN M 1024" is replaced by its mixed difference version. To describe it more specifically, the first branch of the mixed difference network aims to learn the vehicle make information of the input images and the second branch aims to learn the difference between different vehicle model images. The final mixed feature is an enhanced  <ref type="table" target="#tab_2">Table 3</ref> presents the final verification accuracy of the above methods. In this task, the "GoogleNet+Joint Bayesian" framework achieves the best performance on all three testing datasets and our approach ranked the second place overall. The "VGG CNN M 1024+Triplet Loss" got no results because its loss function failed to converge during the training phase. Maybe it is just not that easy to form perfect clusters of various different samples simply distinguished by their vehicle model information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Vehicle Retrieval</head><p>Another closely related problem of vehicle search is the object retrieval task, we evaluate the performance of our proposed DRDL model following the widely used protocol in object retrieval, mean average precision(MAP). We designed this experiment to measure how much improvement each module in our framework brings. Thus, only three methods are included in this part: "VGG+Triplet Loss", "VGG+Coupled Clusters Loss" and "Mixed Difference VGG+Coupled Clusters Loss". Moreover, as smallscale dataset may affect the deep model's final test accuracy and we aim to fully evaluate the potential power of different models, the entire training data(110178 images in total) in "VehicleID" is being used for model training. In test phase, suppose we have N i images for vehicle model i, we put max{6, N i − 1} images into the gallery set and the rest into the probe set. After extracting the normalized features using the trained deep convolutional network, the difference between arbitrary two vehicle images is measured directly by their L2 distance. <ref type="table" target="#tab_3">Table 4</ref> illustrates the final results.</p><p>Since the training data is absolutely sufficient now, it would not be a difficult problem any more for the standard triplet loss to converge. In all three testing datasets, the mean average precision keeps growing significantly after applying our proposed coupled clusters loss function and the mixed difference network structure compared with the original triplet loss framework. It strongly proves the significant effects of our proposed approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Vehicle Re-identification</head><p>In this part, we adopted the widely used cumulative match curve(CMC) approach <ref type="bibr" target="#b5">[6]</ref> in person re-identification problem and performed the vehicle re-identification experiments on "VehicleID" dataset. For each test dataset split in <ref type="table">Table 2</ref>, we randomly select one image of each vehicle and put it into the gallery set. Other images are all probe queries. The detailed information of the gallery set and the probe set in each test subset is in <ref type="table" target="#tab_4">Table 5</ref>. Following the common method when evaluating model predict accuracy, we repeat it 10 times in testing phase to get the final CMC curve.  The detailed match rate from top-1 to top-50 of the various models evaluated on the small-scale test data(gallery size is 800) is illustrated in <ref type="figure" target="#fig_9">Fig 10.</ref> Considering the number of vehicle models in "CompCars" are far more larger than our "VehicleID" dataset and "VGG CNN M 1024" is a relative small network for multi-class classification, we also trained a more powerful network, i.e. "GoogleNet", on both datasets. From the results, we can see that when using the same learning model(like the Joint Bayesian), "GoogleNet" beats "VGG CNN M 1024" about 2% in top-1 matching rate and owns significant advantages from top-5 to top-50(6% − 7%). On the other hand, though the various vehicle model information the "CompCars" dataset has, the image type difference(web-nature images and surveillancenature images) greatly affected the overall predict accuracy. "GoogleNet" trained on "VehicleID" beats the one trained on "CompCars" about 3%. The standard "CNN+Triplet Loss" framework works well in this experiment, outperforms all other models except ours. After applying our proposed coupled clusters loss and the mixed difference network structure, the match rate further increased about 4% − 10% along the CMC curve. <ref type="table" target="#tab_5">Table 6</ref> illustrates the top-1 and top-5 match rate of the best three models on all three test data splits, which reveal the significant advantages of our method again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we propose a Deep Relative Distance Learning (DRDL) model to solve an important but not well explored problem: vehicle re-identification. We exploits a two-branch deep convolutional network to map vehicle images into an Euclidean space thus, L2 distance can be directly used for similarity estimation. Compared with existed methods, the specifically designed coupled clusters loss function and the mixed difference network structure achieves a high predict accuracy. Experimental results demonstrate that DRDL achieves promising results and outperforms several state-of-the-art approaches. Although the methods is proposed for vehicle Re-ID, it could work well on vehicle model verification and vehicle retrieval tasks either. For the lack of vehicle re-identification datasets, we present a carefully-labeled large-scale dataset named "VehicleID", which includes multiple images of the same vehicle captured by different real-world cameras. With the nearly 200,000 images of 26,267 vehicles and wellorganized identity label, the dataset could easily be used for future research on vehicle re-identification or fine-grained vehicle model recognition tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Special marks which can be used for identification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Framework of our model for vehicle re-identification. The deep neural network aims to map the original vehicle images into an Euclidean space that the images of the same vehicle tend to form a cluster while other images tend to locate relatively far away. ages, the total number of vehicles and extra vehicle model annotations on part of this dataset. The evaluation protocols and experimental results are presented in section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Triplet loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Two different cases when building triplets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the partial derivative of both the positive and negative samples are 0. Otherwise the partial derivative of the positive samples are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Mixed difference network structure based on VGG CNN M 1024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Coupled clusters loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>VehicleID Dataset. Each vehicle have at least 2 images in our dataset. Illumination and viewpoint could both varied a lot in different images. A large number of different vehicles are of the same mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Vehicle numbers of 250 models. ages of 13133 vehicles and 42638 images have been labeled with vehicle model information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>CMC on VehicleID Dataset(gallery size=800).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>demonstrates some examples. To better assist different vehicle-related experiments, this dataset is split into two parts for model training and testing. The first part contains 110178 images of 13134 vehicles and 47558 images have been labeled with vehicle model information. The second part contains 111585 im-1 Available at http://pkuml.com/resources/pku-vehicleid.html</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Data Split For Training And Testing</figDesc><table>Image number 
Training Testing 
With model label 
47558 
42638 
Without model label 
67540 
68947 
All 
110178 111585 

Table 2. Test Data Split 

Small Medium Large 
Number of vehicles 
800 
1600 
2400 
Number of images 
7332 
12995 
20038 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Predict Accuracy of Vehicle Verification Task</figDesc><table>Accuracy 
Easy Medium Hard 
GoogleNet+SVM[21] 
0.700 
0.690 
0.659 
GoogleNet+Joint Bayesian[21] 0.833 
0.824 
0.761 
VGG+Triplet Loss 
/ 
/ 
/ 
Mixed Diff+CCL(Ours) 
0.833 
0.788 
0.703 

feature representation of the input image. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc>MAP of Vehicle Retrieval Task</figDesc><table>MAP 
Small Medium Large 
VGG+Triplet Loss[4] 
0.444 
0.391 
0.373 
VGG+CCL(Ours) 
0.492 
0.448 
0.386 
Mixed Diff+CCL(Ours) 0.546 
0.481 
0.455 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Gallery and Probe Split for Vehicle ReID Task Number of images Small Medium Large</figDesc><table>Gallery size 
800 
1600 
2400 
Probe size 
6532 
11395 
17638 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 6 .</head><label>6</label><figDesc>Match Rate of Vehicle ReID Task</figDesc><table>Match Rate 
Small Medium Large 
VGG+Triplet Loss[4] 
top 1 

0.404 
0.354 
0.319 
VGG+CCL(Ours) 
0.436 
0.370 
0.329 
Mixed Diff+CCL(Ours) 
0.490 
0.428 
0.382 
VGG+Triplet Loss[4] 
top 5 

0.617 
0.546 
0.503 
VGG+CCL(Ours) 
0.642 
0.571 
0.533 
Mixed Diff+CCL(Ours) 
0.735 
0.668 
0.616 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="566" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</title>
		<meeting>IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2008</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Arterial travel time estimation based on vehicle re-identification using wireless magnetic sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kavaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varaiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="586" to="606" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3610" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vehicle re-identification with dynamic time windows for vehicle passage time estimation. Intelligent Transportation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jointly optimizing 3d model fitting and fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="466" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Car make and model recognition using 3d curve alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2014 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="285" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03832</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vehicle reidentification using multidetector fusion. Intelligent Transportation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Arr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Ritchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="155" to="164" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person reidentification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep metric learning for practical person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.4979</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Threedimensional deformable-model-based localization and recognition of road vehicles. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
