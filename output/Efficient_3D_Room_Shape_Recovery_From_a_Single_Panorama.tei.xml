<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient 3D Room Shape Recovery from a Single Panorama</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<email>yanghao14@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
							<email>huizhang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient 3D Room Shape Recovery from a Single Panorama</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method to recover the shape of a 3D room from a full-view indoor panorama. Our algorithm can automatically infer a 3D shape from a collection of partially oriented superpixel facets and line segments. The core part of the algorithm is a constraint graph, which includes lines and superpixels as vertices, and encodes their geometric relations as edges. A novel approach is proposed to perform 3D reconstruction based on the constraint graph by solving all the geometric constraints as constrained linear least-squares. The selected constraints used for reconstruction are identified using an occlusion detection method with a Markov random field. Experiments show that our method can recover room shapes that can not be addressed by previous approaches. Our method is also efficient, that is, the inference time for each panorama is less than 1 minute.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A 360˝full-view indoor panorama is shown in <ref type="figure" target="#fig_0">Fig.  1a</ref>. We intend to recover the 3D room shape from this panorama. Several methods are available to solve this problem, either by adopting the Indoor World model, which consists of a single floor, a single ceiling, and vertical walls <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>, or by estimating a cuboid shape that fits the room layout <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref>  <ref type="bibr" target="#b0">1</ref> . Intrinsically, most of these approaches work in a discretized manner, that is, the results are selected from a set of candidates based on certain scoring functions. The generation rules of the candidates limit the scope of these algorithms. For example, the corridor of the room in <ref type="figure" target="#fig_0">Fig. 1a</ref> cannot be modeled either as part of a cuboid or within an Indoor World because the ceiling of the corridor is lower than that of the room.</p><p>A method is proposed to address this problem in a geometric manner. We extract lines and superpixels from the panorama and estimate their orientation information under <ref type="bibr" target="#b0">1</ref> Most of these methods deal with normal photographs, however, modifying these methods to cope with panoramas is a straightforward process. Manhattan world assumption, as shown in Figs. 1b and 1c. A constraint graph, which includes all the lines and superpixels as vertices is then constructed; and geometric relations are encoded among them. A 3D reconstruction is performed in an iterative manner, which can solve constraints as constrained linear least squares (CLLS). We propose an occlusion detection method using a Markov random field (MRF) to select plausible constraints for reconstruction. The reconstructed lines and superpixel facets of the room layout from Figs. 1b and 1c are shown in <ref type="figure" target="#fig_0">Fig. 1e</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Works</head><p>Interest in single-view reconstruction (SVR) problems has been constant. Methods in this domain can be roughly divided into two groups: geometric and semantic approaches. Most geometric approaches rely on lines. Lee et al. <ref type="bibr" target="#b9">[10]</ref> recovered indoor layout by computing orientation maps (OMs) from lines. Xue et al. <ref type="bibr" target="#b18">[19]</ref> reconstructed symmetric objects by detecting symmetric lines. Xiao et al. <ref type="bibr" target="#b17">[18]</ref> recognized cuboids from single photographs based on both the appearance of corners and edges as well as their geometric relations. Ramalingam <ref type="bibr" target="#b12">[13]</ref> proposed a method that lifted 2D lines to 3D by identifying true line intersections in space.</p><p>For semantic approaches, Hoiem et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> estimated geometric context label on each image pixel. Delage et al. <ref type="bibr" target="#b2">[3]</ref> inferred room layout via floor-wall boundary estimation. Gupta et al. <ref type="bibr" target="#b4">[5]</ref> proposed blocks world to predict 3D arrangement by explaining their volumetric and physical relations. Numerous studies have been presented in recent years by assuming room shape as a single box aligned with Manhattan direction. Hedau et al. <ref type="bibr" target="#b5">[6]</ref> utilized structured learning to improve prediction accuracy. The inferences of both indoor objects and the box-like room layout have been continuously improved thereafter in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14]</ref> due to enhanced object presentation, novel features or more efficient inference techniques. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> recently stressed the limitation of a narrow view angle imposed by standard camera; they illustrated the advantage of panoramic images over normal photographs, where additional contexts could be considered. Cabral <ref type="bibr" target="#b0">[1]</ref> took multiple indoor panoramas as input and utilized structural cues from single image for floorplan reconstruction.</p><p>Our work focuses on 3D room shape recovery. The algorithm is related to <ref type="bibr" target="#b12">[13]</ref>, in which a constraint graph is proposed, its vertices are fully orientated lines, and its edges are intersections/incidences between lines. In the present study, we additionally consider other entities, including lines with unknown orientation, and planar superpixels with varying degrees of freedom (DOF). The constraints are also extended to include connections between two superpixels or between a line and a superpixel. Hence, the graph is enhanced to be capable of representing a complex 3D shape including line segments and planar facets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>The main contributions of our work are as follows.</p><p>• An expressive constraint graph is designed to encode the spatial configurations of line segments and superpixel facets in a uniform manner. • An iterative algorithm is proposed to solve the constraint graph as CLLS to reconstruct both line segments and superpixel facets together. • We propose a method to identify occluding lines using an MRF, which helps select plausible constraints for the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preprocessing</head><p>Our input is a panorama that covers a 360˝horizontal field of view represented in an equirectangular projection. Under this projection, one-to-one correspondence occurs between a panorama pixel and a 3D view direction 2 ; therefore, we use the term angle distance to measure the distance between two pixels by computing the angle of their directions, and use angle length to measure the length of a pixel sequence (like a line segment or a superpixel boundary) by accumulating the vector angles between adjacent pixels.</p><p>An approach similar to that in <ref type="bibr" target="#b19">[20]</ref> is used to detect lines, estimate Manhattan vanishing points, and identify the spatial directions of lines from the panorama. The vanishing point direction which is the most vertical in space is denoted as the vertical direction of the scene. A panorama version of graph cut 3 is utilized to over segment the panorama into superpixels. Orientations of the superpixels are restricted according to the following three priors:</p><p>Wall prior. We assume regions near the horizon to be parts of the walls. In particular, superpixels whose angle distances to horizon ă θ tiny are assigned to be vertical in space with θ tiny as a threshold; that is, their surface planes must be parallel with the vertical direction. <ref type="figure" target="#fig_1">Fig. 2a</ref> illustrates the wall prior assignment for the superpixels generated from <ref type="figure" target="#fig_0">Fig. 1a</ref>. has a known plane normal, it can only slide within its viewing cone; for example, the normal of a horizontal (floor/ceiling) superpixel is fixed to be vertical. By contrast, a superpixel with DOF=2 is restricted to be parallel with a known direction; for example, the plane of a vertical (wall) superpixel must be parallel with the vertical direction. No orientation restriction is imposed on superpixels with DOF=3. Second row: Lines are simpler; those with DOF=1 can only slide on two fixed rays with a fixed orientation, whereas those with DOF=2 are free. Note that the projection of the contours of superpixels and the endpoints of lines must remain unchanged in the panorama.</p><p>Vanishing point prior. We also assume that the superpixels with Manhattan vanishing points should face the corresponding Manhattan direction; therefore, their plane normals are assigned as indicated in <ref type="figure" target="#fig_1">Fig. 2b</ref>.</p><p>Geometric context prior. The geometric context (GC) extraction <ref type="bibr" target="#b6">[7]</ref> on panorama follows the same strategy with <ref type="bibr" target="#b19">[20]</ref>. We calculate the average label scores in each superpixel, and use the label with the maximum score to orient the superpixel. Only superpixels that have high scores in two labels are considered: ground and wall. The ground superpixels are restricted to be horizontal whereas the wall superpixels are all vertical. We do not utilize the wall facing information provided by GC. And only bottom half of the panorama is assigned by this prior as recommended in <ref type="bibr" target="#b19">[20]</ref>. The assignment is illustrated in <ref type="figure" target="#fig_1">Fig. 2c</ref>.</p><p>Finally, all these three assignments are merged together as shown in <ref type="figure" target="#fig_1">Fig. 2d</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Constraint Graph</head><p>A graph G " pV, Eq is constructed by encoding all lines and superpixels as vertices V. Two types of lines and three types of superpixels are considered to correspond to different degrees of freedom (DOF) imposed in preprocessing, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We consider two types of constraints: the constraints of connectivity E con and the constraints of coplanarity E cop . E con exhibits four types of connection be- tween vertices: 1) connections between adjacent superpixels that share the same boundaries, 2) intersections of lines with different orientations, 3) collinearity of lines with the same orientation and 4) connections between adjacent lines and superpixels. By contrast E cop is only concerned with superpixel facets that may lie on the same plane. <ref type="figure" target="#fig_3">Fig. 4</ref> shows an example that illustrates part of a constraint graph.</p><p>The following subsections are organized as follows. First in Sec. 3.1, we explain how we parameterize each kind of vertices to encode their types and orientation restrictions. Then in Sec. 3.2, a novel iterative approach is proposed to perform reconstruction by solving the constraints as CLLS. In Sec. 3.3, an occlusion detection algorithm is provided. Finally in Sec. 3.4, we explain how to build a constraint graph based on the occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vertex Parameterization</head><p>Our objective is to predict the depths of all the visible surfaces to viewpoint. By assuming the surface planarity within each superpixel, the task is simplified to inferring the plane equations of the superpixels.</p><p>The number of parameters of each superpixel should correspond to its DOF; therefore, various parameters are designed for superpixels with different DOFs. We denote x i as the vector that encodes the unknown parameters of vertex i P V and use C i to represent the set of known values of i. Let df i be the DOF of i. The proposed superpixel parameterization is presented in <ref type="table" target="#tab_1">Table 1</ref>, where for each superpixel i (1) If df i " 1, then let n i be the unit normal vector of its plane and C i " tn i u. Define x i " p1{d i q, where d i represents the distance from the plane to the viewpoint.</p><formula xml:id="formula_0">df i x i C i P i 1 p1{d i q tn i u rn i s 2 pa i , b i q ⊺ tu i u » - 1 0 0 1 uix uiz´u iy uiz fi fl 3 pa i , b i , c i q ⊺ H I 3ˆ3</formula><p>(2) If df i " 2, then C i " tu i u, where u i is the unit direction vector that the surface plane must be parallel with.</p><formula xml:id="formula_1">Define x i " pa i , b i q ⊺ , which corresponds to the two param- eters in pa i , b i , c i q of the plane equation a i x`b i y`c i z " 1.</formula><p>(3) If df i " 3, then C i is empty, because no restriction is imposed regarding its orientation. x i is directly defined as pa i , b i , c i q ⊺ which corresponds to the plane parameters.</p><p>Numerous parameterization methods are undoubtedly available for these superpixels. For example, we can simply define</p><formula xml:id="formula_2">x i " pd i q if df i " 1, or define x i " pθ i , d i q ⊺ if df i " 2,</formula><p>where θ i represents the angle of rotation along direction u i and d i is the distance of its plane to the viewpoint. We decide to parameterize superpixels as shown in <ref type="table" target="#tab_1">Table 1</ref> because a linear transformation is available from the proposed superpixel parameter x i to its plane coefficients π i " pa i , b i , c i q ⊺ ; this is required to solve constraints efficiently in Sec. 3.2. These transformation matrices P i are presented in the fourth column of <ref type="table" target="#tab_1">Table 1</ref>. All these matrices satisfy</p><formula xml:id="formula_3">π i " pa i , b i , c i q ⊺ " P i x i .<label>(1)</label></formula><p>For line segments, we propose using their supporting planes to encode their spatial arrangement. The supporting plane of a line is defined by two requirements: 1) it must contain the line in space, and 2) it must be orthogonal to the plane that passes through the viewpoint and that contains the line. For example, the supporting plane of line pP 1i , P 2i q in <ref type="figure">Fig. 5</ref> should always contains the corresponding 3D line and should be orthogonal to the plane that contains rays Ý ÝÝ Ñ OP 1i and Ý ÝÝ Ñ OP 2i . A one-to-one correspondence exists between a lifted line in 3D and its supporting plane, and they share the same DOF value. Hence, we can regard lines as degenerated forms of superpixels and use the same method presented in <ref type="table" target="#tab_1">Table 1</ref> to parameterize them.</p><p>In particular, as shown in <ref type="figure">Fig. 5</ref>, for line i " pP 1i , P 2i q with DOF=1, n i in <ref type="table" target="#tab_1">Table 1</ref> denotes the normal of its supporting plane. For a line i with DOF=2, u i in <ref type="table" target="#tab_1">Table 1</ref> represents the normal of the plane that contains line i and passes through viewpoint O. </p><formula xml:id="formula_4">DOF=1 O P 1i P 2i n i n i DOF=2 O P 1i P 2i u i u i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Constraint Solving</head><p>In this section, we propose an algorithm to solve all the constraints in G as CLLS, including E con and E cop .</p><p>All types of connection constraints under E con share the same objective: to equalize the depths of two vertices on certain connection points. As shown in <ref type="figure">Fig. 6</ref>, if the constraint is a connection between two adjacent superpixels or between a line and a superpixel, then its connection points correspond to their common pixels on the boundary or on the line in the panorama. If the constraint is an intersection or a collinear relation between two lines, then its connection point corresponds to the point shared by these two (extended) lines in the panorama.</p><p>Let S ij be the set of connection points of a constraint pi, jq P E con , and π i " pa i , b i , c i q π j " pa j , b j , c j q be the two (supporting) plane coefficients of vertices i and j. Assume that t P S ij is a unit vector representing the spatial direction that corresponds to a connection point. Then, the depth of vertex i on direction t, d i ptq, which is actually the depth of its (supporting) plane π i , can be calculated as d i ptq " 1{t ⊺ π i or as d i ptq " 1{ pt ⊺ P i x i q after substituting π i with P i x i using Equation <ref type="bibr" target="#b0">1</ref>.</p><p>To simplify the formulations, we denote vector x as the concatenation of all the parameter vectors x i . Let N be the length of x. Then we can construct a (0,1)-matrix V i , whose size is df iˆN and satisfies x i " V i x, by mapping the position of each parameter in x i from its corresponding position in x. Therefore, the plane coefficients of vertex i can be written as π i " P i V i x, and the depth of vertex i on direction t can be represented as d i ptq " 1{ pK i ptqxq with</p><formula xml:id="formula_5">K i ptq " t ⊺ P i V i .</formula><p>We utilize the squared sum energy to equalize the depths d i ptq and d j ptq of the two vertices on each connection direction t of all the constraints in E con , which is formulated as follows:</p><formula xml:id="formula_6">E con pxq " ÿ pi,jqPEcon w con ij ÿ tPSij }d i ptq´d j ptq} 2 " ÿ pi,jqPEcon w con ij ÿ tPSij › › › › pK i ptq´K j ptqq x x ⊺ K ⊺ i ptqK j ptqx › › › › 2 .<label>(2)</label></formula><p>The constraints of coplanarity are quantified by directly measuring the difference between plane coefficients π i and π j of the related two superpixel vertices i and j as follows:</p><formula xml:id="formula_7">E cop pxq " ÿ pi,jqPEcop w cop ij }π i´πj } 2 " ÿ pi,jqPEcop w cop ij }pP i V i´Pj V j q x} 2 .<label>(3)</label></formula><p>w con ij and w cop ij are the weights given to constraints on pi, jq. Directly minimizing E con pxq`αE cop pxq (α is a weight) is intractable due to the non-linearity of E con . Instead, we use an iterative approach which solves an approximated CLLS in each iteration to converge to the solution.</p><p>The approach is described in Algorithm 1. In each iteration: it first freezes the non-linear component φ k ij ptq as a constant and computes an approximated solution x k by addressing the CLLS problem described in Line a, where an inequality is applied to avoid obtaining trivial results. Then, it updates the frozen component φ k ij ptq to φ k`1 ij ptq based on the current solution x k , as is formulated in Line b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Solving Constraints with CLLS</head><p>k Ð 0; φ 0 ij ptq Ð 1, @t P S ij @pi, jq P E con ; repeat a x k Ð the result of a CLLS problem: min x E k con pxq`αE cop pxq s.t. K ti,ju ptqx ě 1.0, @t P S ij , pi, jq P E con with E k con pxq " ÿ pi,jqPEcon</p><formula xml:id="formula_8">w con ij ÿ tPSij › › › › › pK i ptq´K j ptqq x φ k ij ptq › › › › › 2 b φ k`1 ij ptq Ð x k ⊺ K ⊺ i ptqK j ptqx k ,</formula><p>@t P S ij @pi, jq P E con ; k Ð k`1 ; until convergence _k " maximum iterations; return x k ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Occlusion Identification</head><p>Identifying constraints E con and E cop remains challenging because of occlusions. Room occlusions are likely to be covered by detected line segments, and tend to follow the rule of coherence. Using <ref type="figure" target="#fig_5">Fig. 7</ref> for example, if any occlusion exists on l i , then the sidings of the occlusions will likely remain coherent along l i . Therefore, we detect occlusions by labeling lines. Only oriented lines are considered for simplicity. To each line l i with DOF=1, we assign label y i " py l i , y r i q. y l i , y r i P t0, 1u are two flags that encode the occlusion status on the two sides of l i . We use y occluded behind (" 0). In particular, label y i " p1, 1q indicates that no occlusion exists on l i , y i " p1, 0q indicates the left occludes the right, whereas y i " p0, 1q indicates that the right occludes the left. Finally, y i " p0, 0q suggests a dangling line that does not connect to the surfaces.</p><p>Three kinds of evidence are used in occlusion detection: 1) orientation violations between lines and superpixels, 2) T-junctions formed by lines, and 3) the coherence between collinear lines. Based on the preceding discussion, we construct an MRF to infer the y i of each l i by minimizing the following objective:</p><formula xml:id="formula_9">y˚" arg min y ÿ liPV 1 L E unary i py i q`ÿ pli,lj qPE colinear ,li,lj PV 1 L E binary ij py i , y j q (4) with E unary i py i q " E ov i py i q`E tj i py i q`τ py i q,<label>(5)</label></formula><p>where V 1 L is the set of lines with DOF=1. The unary cost E unary i on each line l i consists of three terms: the orientation violation cost E ov i , the T-junction cost E tj i , and a label prior τ py i q to punish disconnections. E binary ij is the coherence cost imposed on collinear lines.</p><p>Orientation Violation Cost Let r i be the spatial orientation of line l i , vp l i and vp r i be two closest vanishing points to l i that satisfy two conditions: 1) it lies on the {left, right} side of l i , and 2) its spatial direction is orthogonal to r i . Then, we sweep l i toward vp tl,ru i to an angle θ and form two spherical quadrilaterals that represent the neighborhoods of line l i in the panorama, denoted as Ω tl,ru i pθq. <ref type="figure" target="#fig_7">Fig. 8</ref> presents an example wherein two lines sweep toward the same vanishing point. </p><p>where θ mid is a threshold. wppq is the pixel weight used to rectify panorama distortion. conflictpp, l i q is 1 if the given orientation of the superpixel with p conflicts with the direction of l i , and 0 otherwise. A conflict occurs if the superpixel has DOF=1 and its plane normal coincides with the direction of l i . E ov i is then defined as follows, where c ov is a weight:</p><formula xml:id="formula_11">E ov i py i q " c ov˜y l i N l i 2 maxtN r i 2 , 1u`y r i N r i 2 maxtN l i 2 , 1u¸.<label>(7)</label></formula><p>T-junction Cost T-junction is a commonly used evidence for occlusion recognition <ref type="bibr" target="#b12">[13]</ref>. For example, in <ref type="figure" target="#fig_7">Fig. 8</ref>  </p><p>Coherence Cost A binary term E binary ij is imposed on each pair of collinear lines l i and l j to encourage the coherence of labels, which is formulated as</p><formula xml:id="formula_13">E binary ij py i , y j q " c binary˜# ✶py l i ‰ y l j q`✶py r i ‰ y r j q if l i Ò l j ✶py l i ‰ y r j q`✶py r i ‰ y l j q if l i Ö l j¸,<label>(9)</label></formula><p>with ✶p¨q as the boolean to 0-1 conversion, Ò, Ö denotes whether the two line directions are equal or opposite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Graph Construction</head><p>Equation 4 is solved using the convex max product <ref type="bibr" target="#b11">[12]</ref>. The resulting labels are used to build graph constraints.</p><p>A superpixel pair pi, jq that shares a same boundary is collected into E con and E cop if its boundary is not covered by any occluding line. Connection points S ij are set as the turning points on the boundary. Let L ij be the angle length of the boundary, then w con ij " L ij {}S ij }. w cop ij " L ij if there are no lines lying on the boundary; otherwise w cop ij " 0.1L ij because non-occluding lines always suggest foldings.</p><p>(a) Annotation (b) Reconstruction from annotation <ref type="figure">Figure 9</ref>: (a) shows the manually labeled face orientations (in red, green and blue) and occlusions (in white) of the panorama in <ref type="figure" target="#fig_0">Fig.  1a. (b)</ref> displays the directly reconstructed 3D model based on the annotation.</p><p>A line and a superpixel pi, jq that are adjacent in the panorama are collected into E con if the superpixel lies on the front side of the line. Connection points S ij are set as the endpoints of the shared segment. w con ij " L ij {}S ij } with L ij as the angle length of the shared segment.</p><p>Recognizing constraints between lines pi, jq is a slightly complex process. We consider a pair of lines with different orientations as intersecting if the angle distance between the line segments is ă θ tiny . Collinearity is considered for lines with the same orientation that are separated by an angle ă θ large . Then, we connect the nearest points of these two lines and check whether the connection is intercepted by any occluding line. If any of i, j is an occluding line, then we place them into E con only if 1) the other line lies on the front side of the occluding line, and 2) their connection is not intercepted. If neither i, j is an occluding line, then pi, jq is collected only if their connection is not intercepted. The S ij of the two lines is set as the common pixel shared by these two (extended) lines, and w con ij " maxtJ ij , 1u, J ij is the junction score proposed by <ref type="bibr" target="#b12">[13]</ref>.</p><p>Finally, we apply searches on the graph from vertices with DOF=1 to find the largest determinable subgraph for reconstruction 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We collected 88 indoor panoramas for evaluations with manual annotation of vps, room layout faces, face orientations and occluding lines (as is shown in <ref type="figure">Fig. 9a</ref>). In experiments, thresholds θ ttiny,mid,largeu are set as 2˝, 5˝, 15˝. In Algorithm 1, the maximum iteration number is set to be 5, the weight α is set as 10´6. In the MRF terms, the label prior τ py i q in Equation 5 is defined as Each panorama is projected onto 12 overlapping perspective images to extract GC. Five GC labels are considered: floor, ceiling, and {front, left, right} wall. We also project the surface normals of our reconstructed superpixels and the ground-truth face normals in the same manner. Each pixel is labeled in the aforementioned labels according to its normal direction in the perspective image. The normals of the superpixels that are not reconstructed in our algorithm are computed by averaging their adjacent superpixels. <ref type="figure" target="#fig_0">Fig.  10</ref> illustrates examples of surface labels predicted by various methods.</p><formula xml:id="formula_14">$ ' &amp; ' % 0 if y i " p1, 1q 5 if y i " p0,</formula><p>We use the per-pixel classification error (i.e., the percentage of pixels that have been wrongly classified) to quantify the predictions in each perspective image. Three types of evaluation are performed: 1) consider all pixels, 2) only considers pixels with GC clutter ă 0.7, 3) only considers pixels with GC clutter ă 0.3. The average error ratios are reported in <ref type="table" target="#tab_3">Table 2</ref>, which shows that our result outperforms GC in all three cases. We also show results of our algorithm that do not use GC prior in the bottom row of <ref type="table" target="#tab_3">Table 2</ref>, the errors rise but are still lower than those of GC.</p><p>Depth Distribution Second, we compare 3D models using their depth maps. A 3D model is directly reconstructed from the ground-truth annotation for each panorama, its depth map is used as the reference (as shown in <ref type="figure" target="#fig_0">Fig. 11a</ref>). Evaluations are performed by comparing depth maps with the reference using two metrics: L2 distance which is formulated as }d a´db } 2 , and cosine distance which is computed by 1´d ⊺ a d b . d˚is a normalized vector that comprises all weighted (by wppq) depths of pixels in depth map.</p><p>Our method is compared with the cuboid of best fit (COBF), which is used to represent the performance upper bound of aligned-box-methods. We enumerate a discretized parameter space of cuboids that are aligned with the Manhattan direction and select the one that best fits the reference (measured by L2 distance) as the COBF of the panorama.</p><p>Results are presented in <ref type="table" target="#tab_4">Table 3</ref>, showing that our method (second row) outperforms COBF (first row). Indirectly, this also suggests the advantage of the proposed approach in the aspect of room shape estimation over the algorithms that estimate room shapes by fitting cuboids aligned with Manhattan direction.</p><p>Results of the proposed method that assumes no occlu-(a) Reference (b) Proposed (c) COBF <ref type="figure" target="#fig_0">Figure 11</ref>: Depth maps of <ref type="figure" target="#fig_0">Fig. 1a. (a)</ref> The reference is set as the depth map from the model which is reconstructed directly from annotation, the corresponding model of (a) is shown in <ref type="figure">Fig. 9b</ref>.  sions are reported in the third row, and the results of that utilizing the ground-truth occlusions are shown in the fourth row. It could be observed that assuming no occlusions raises prediction errors. However, using the ground-truth occlusions does not bring meaningful improvement. We also report the average errors of a uniform sphere (centered at viewpoint) in the bottom row for comparison.   Reconstruction Coverage Algorithmically, our method does not reconstruct all the superpixels and the lines; hence we evaluate the coverage of the reconstruction. The statistics is shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. On average, 81.9% of the vertices are reconstructed, and 97.5% of the superpixel areas are recovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Evaluation</head><p>Some of the results are presented in <ref type="figure" target="#fig_0">Figs. 13 and 14</ref>, which illustrate that our algorithm can reconstruct room shapes with occlusions and even some non-Manhattan parts. Note that in these cases lots of false occlusions are identified by <ref type="figure" target="#fig_0">Figure 13</ref>: Reconstruction results. In each case, the leftmost two images show the input panorama, the extracted lines and the superpixels. Orientation restrictions of the lines and the superpixels are visualized in the same manner with <ref type="figure" target="#fig_0">Fig.1</ref>. Detected occlusions are rendered on the second images of the leftmost columns as black jagged lines, wherein the jagged side of each line represents the side of the front surface. The rest display reconstructed 3D models, the lines are all in black, the superpixels are colored by surface normals in the second columns, and are textured with the input panoramas in the rest. Face culling is applied to better illustrate the inside shape in each view.</p><p>the proposed occlusion detection. However, the recovered 3D models show the robustness of our reconstruction algorithm against the false occlusions. <ref type="figure" target="#fig_0">Fig. 14</ref> shows two failed cases, where our algorithm is misled by wrongly oriented superpixels.</p><p>We invoke code of <ref type="bibr" target="#b5">[6]</ref> to extract GC and use CVX to solve the CLLS, the rest is implemented in C++. On a PC with Intel Core i5 CPU (3.1/3.3GHz), the time cost of occlusion identification is approximately 3 seconds per panorama, and solving the CLLS costs less than 5 seconds per iteration. The total time cost of inference (without preprocessing stage) is within 1 minute for each panorama in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this study a fully automatic method is proposed to reconstruct 3D room shape from a single indoor panorama. The method performs reconstruction based on partially oriented lines and superpixels; it identifies occlusions by labeling lines using an MRF. Geometric relations between lines and superpixels, such as connections and coplanarity, are identified using these labels. A constraint graph is then built; and the constraints are solved as CLLS by a novel iterative algorithm.</p><p>Experiments show that the proposed method outperforms geometric context in surface label inference; the pro- <ref type="figure" target="#fig_0">Figure 14</ref>: Two failed cases. In the first case, the reconstruction is misled by the false horizontal superpixels that cover the sofa in the room. In the second case, some wall superpixels are incorrectly identified as parts of the floor, which leads to a false illusion of doors open on the wall. posed method is also advantageous over algorithms that model room layouts as aligned cuboids. The 3D reconstruction results illustrate that the proposed method can recover room shapes that include occlusions and even non-Manhattan parts from a single indoor panorama.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Input indoor panorama, with detected lines shown in (b). Red, green, and blue indicate Manhattan directions that are assigned on lines in the preprocessing stage. (c) Superpixels generated by over segmentation. Pure colors (except white) indicate that the surface normals are restricted to certain Manhattan directions, striped colors suggest that the surface plane should be parallel with a Manhattan direction in 3D space. In particular, red regions in (c) represent horizontal faces such as floors or ceilings, whereas the striped red regions represent vertical faces such as walls. White indicates that no orientation restriction is imposed. (d) Automatically recovered layout depths and (e) reconstructed 3D lines (in black) and superpixel facets (all textured) from different views. Face culling is applied to better illustrate the inside model in each view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Orientation priors forFig. 1a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Five types of vertices in the constraint graph. Superpixels are categorized into three types with varying DOF, lines are categorized into two types. First row: A superpixel with DOF=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Constraint graph. The graph encodes lines and superpixels as vertices, and their relations as edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Supporting planes for lines. Connection points are shown in red. Each connection point in the panorama represents a view direction in space, along this direction the depths of two related vertices should be equal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>tl,ru i to indicate whether the superpixels on the {left, right} side of l i are in front (" 1) or are Coherence of occlusions. (a) Segmented superpixels near line li. (b) and (c) illustrate possible occluding statuses of adjacent superpixels, where F indicates surface in front and B denotes surface occluded behind. The cases shown in (b) are relatively more plausible than those in (c) based on logic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>defined as the weighted number of nearby pixels that cause violations on the {left, right} side of l i :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Neighborhoods of lines. The right neighborhood of line li, denoted as Ω r i pθq, is generated by sweeping li toward its right vanishing point vp r i to angle θ. The left neighborhood Ω l j pθq of line lj is similarly defined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>line l k forms a T-junction on line l i . We detect T-junctions by collecting all pairs of lines with different orientations, which form T-structures, and their distances are ă θ tiny . Define M number of T-junctions on the {left, right} side of line l i , and the T-junction cost is given by E tj i py i q " c tj`p 1´y l i qM l i`p 1´y r i qM r i˘.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>wppq in Equation 6 is formulated as sinp py H πq, where H is the height of the panorama. The coefficient c ov in Equation 7 is set to be 100, c tj in Equation 8 is set as 1 and c binary in Equation 9 is set as 5.Quantitative EvaluationSurface Label First, the proposed method is evaluated in the aspect of surface label inference. Perspective conversion to compare surface labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(b) The depth map of the proposed reconstruction. (c) The depth map of the corresponding COBF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Coverage of our reconstruction. The first graph shows the ratios of recovered vertices; the second graph shows ratios of superpixel areas reconstructed in each panorama.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>4 .</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Parameterizing vertices.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Surface label classification error (%).</figDesc><table>metric 
cosine dist. L2 dist. 
COBF 
5.23 
28.48 
proposed 
4.27 
27.02 
proposed(no occl) 
4.53 
27.85 
proposed(gt occl) 
4.23 
27.09 
sphere 
7.54 
37.23 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Depth distribution error (ˆ10´2).</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In our implementation, the correspondence is formulated as t " pcosφsinθ, cosφcosθ, sinφq ⊺ where φ and θ are the latitude and longitude of the point on the panorama and t is its corresponding spatial direction. Under the equirectangular projection, we define φ " πpy{h0 .5q, θ " 2πx{w where px, yq are the coordinates of the 2D point in the panorama, and pw, hq is the size of the panorama.<ref type="bibr" target="#b2">3</ref> We refer to the details in the supplementary material because they are similar to<ref type="bibr" target="#b3">[4]</ref>. A similar approach is also utilized by<ref type="bibr" target="#b0">[1]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">If any conflict occurs, the results of the geometric context prior is applied. No conflict will ever occur between the wall prior and the vanishing point prior.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Details are deferred to the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Piecewise planar and compact floorplan reconstruction from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding indoor scenes using 3d geometric phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A dynamic bayesian network model for autonomous 3d reconstruction from a single indoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2418" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blocks world revisited: Image understanding using qualitative geometry and mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="482" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1849" to="1856" />
		</imprint>
	</monogr>
	<note>Computer vision</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Thinking inside the box: Using appearance models and context based on room geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="224" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1288" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convex max-product algorithms for continuous mrfs with applications to protein folding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lifting 3d manhattan lines from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Manhattan junction catalogue for spatial reasoning of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient structured prediction for 3d indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2815" to="2822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Box in the box: Joint 3d layout and object reasoning from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative learning with latent variables for cluttered indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="435" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Localizing 3d cuboids in single-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="755" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Symmetric piecewise planar object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2577" to="2584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Panocontext: A whole-room 3d context model for panoramic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="668" to="686" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
