<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Cue Zero-Shot Learning with Strong Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max-Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max-Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max-Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max-Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Cue Zero-Shot Learning with Strong Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scaling up visual category recognition to large numbers of classes remains challenging. A promising research direction is zero-shot learning, which does not require any training data to recognize new classes, but rather relies on some form of auxiliary information describing the new classes. Ultimately, this may allow to use textbook knowledge that humans employ to learn about new classes by transferring knowledge from classes they know well. The most successful zero-shot learning approaches currently require a particular type of auxiliary information -namely attribute annotations performed by humans -that is not readily available for most classes. Our goal is to circumvent this bottleneck by substituting such annotations by extracting multiple pieces of information from multiple unstructured text sources readily available on the web. To compensate for the weaker form of auxiliary information, we incorporate stronger supervision in the form of semantic part annotations on the classes from which we transfer knowledge. We achieve our goal by a joint embedding framework that maps multiple text parts as well as multiple semantic parts into a common space. Our results consistently and significantly improve on the state-of-the-art in zero-short recognition and retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The acquisition of visual concepts in humans and machines is still very different. It is hypothesized that early concept acquisition in children mostly follows a learning by example approach where visual concepts are directly grounded in sensory information and linked across modalities <ref type="bibr" target="#b39">[40]</ref>. However, this alone does not explain the diverse visual knowledge of an adult. A lot of our knowledge is preserved and conveyed via text and nowadays online resources. This enables humans to recognize objects without ever having seen a single instance of that object. Therefore the knowledge of the class is no longer solely grounded in sensory information, but rather transferred from prior experiences to new classes. A practical example includes field  <ref type="figure">Figure 1</ref>: We propose to jointly embed multiple language representations and multiple semantic visual parts for finegrained zero-shot recognition. Our novel Noun-Attribute-Difference (NAD) representations are based on differences and distances between vectors in the word2vec space.</p><p>guides that describe different animal species via a range of categorizations and part descriptions <ref type="bibr" target="#b44">[45]</ref> and allow to recognize an animal without expert knowledge. Recent work on zero-shot learning for visual recognition aims at equipping computer vision systems to recognize novel classes without a single training example. The required "knowledge" for the recognition task is transferred via auxiliary information of different types. The most successful techniques utilize human annotations of attributes for each class. This is a particular type of auxiliary information that is not readily available in large quantities. This limitation hampers the progress of large-scale zero-shot learning. We argue that ultimately zero-shot learning techniques should leverage the same "auxiliary information" in terms of text books and online articles that humans use, as those are readily available in large quantities.</p><p>Realizing that such sources might be more noisy and difficult to leverage, we propose to combine them and to additionally use existing strong supervision for the visual information. In particular, on the fine grained recognition task that we investigate, detailed semantic part annotations are available and on other datasets such as Pascal 3D <ref type="bibr" target="#b47">[48]</ref> detailed annotations of key-points are available. Such information provides strong visual supervision and has shown to greatly improve recognition accuracy <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b34">35]</ref>. Our goal is to compensate the loss in performance by using weakerbut more broadly available -auxiliary language information with a stronger visual supervision for classes that we are transferring from.</p><p>Following the multi-modal embeddings paradigm for zero-shot learning <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>, we build a new framework that uses strong visual supervision in an embedding formulation that is flexible enough to accommodate a wide range of textual sources. Our contributions are as follows.</p><p>(1) We propose to adapt Deep Fragment Embeddings <ref type="bibr" target="#b22">[23]</ref> used for language generation to zero-shot learning facilitating a joint embedding of multiple language cues and visual information into a joint space. Our framework supports and integrates a wide range of textual and visual sources. <ref type="bibr" target="#b1">(2)</ref> We propose a novel language embedding method leveraging unstructured text as well as attributes without requiring any human annotation. (3) We use strong supervision in terms of semantic part annotations to compensate for weaker but more broadly available auxiliary language information. We improve the state-of-the-art for fine-grained zero-shot learning, both using unsupervised text sources as auxiliary information and supervised attribute annotations if available. <ref type="bibr" target="#b3">(4)</ref> We show that the use of stronger visual annotations during training allows to improve zero-shot performance without requiring the same strong supervision during recognition.</p><p>The rest of the paper is organized as follows. Sec 2 summarizes related work, Sec 3 details our multi-modal embedding framework for zero-shot learning with strong supervision, Sec 4 presents our motivations for using visual parts as strong supervision, Sec 5 details existing and proposed text embedding methods for language parts, Sec 6 presents our experiments, and Sec 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work handles the challenging zero-shot problem <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17]</ref> of the lack of labeled training data. Since the training and test classes are disjoint, traditional supervised learning methods which require per-image class labels cannot be directly applied. Therefore, side information that models the relationship between classes is required.</p><p>Attributes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26]</ref> relate different classes through well-known, shared and human-interpretable traits. They are often collected manually <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b10">11]</ref> and have shown promising results for image classification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1]</ref>. On the other hand, the attribute collection through human annotations becomes a costly process for fine-grained data collections <ref type="bibr" target="#b44">[45]</ref> where often only subtle visual differences between the objects exist. Therefore one needs a large number of attributes some of which can only be recognized and discriminated by field experts. This greatly increases the cost of annotations. Side information can also be collected automatically <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19]</ref> from a large text corpora such as wikipedia. Word2vec <ref type="bibr" target="#b31">[32]</ref> learns a word's representation based on the word occurrence statistics, BoW <ref type="bibr" target="#b18">[19]</ref> uses a pre-defined vocabulary to build word histograms. Label embedding methods <ref type="bibr" target="#b4">[5]</ref> have been shown effective in modeling latent relationships between classes. For optimizing a multi-class classification objective through label embeddings, WSABIE <ref type="bibr" target="#b45">[46]</ref> uses images and corresponding labels to learn a label embedding. For zero-shot learning, DeViSE <ref type="bibr" target="#b15">[16]</ref> employs a ranking based bi-linear label embedding objective with image and distributed text representations as input/output embeddings. Similarly, ALE <ref type="bibr" target="#b0">[1]</ref> employs an approximate ranking objective that uses images and class-based attributes. ConSe <ref type="bibr" target="#b33">[34]</ref> uses the probabilities of a softmax-output layer to weigh the semantic vectors of all the classes. <ref type="bibr" target="#b1">[2]</ref> evaluates class-based vector representations built on fine-grained datasets for the zero-shot setting. Similar embedding principles, often combined with recurrent neural networks <ref type="bibr" target="#b19">[20]</ref> or a dependency parser <ref type="bibr" target="#b7">[8]</ref>, have recently been applied to image-to-text retrieval <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>, language generation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10]</ref>, and question answering about images <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref>. Our work follows the latest research in joint modeling of language and vision features by formulating an embedding of visual and textual representations in a joint space. In contrast to prior work, our approach accommodates and effectively integrates a wide range of textual representations and uses strong supervision in the form of semantic parts that remain optional at test time. In other words, we combine the advantages of two frameworks, i.e. joint image-text embeddings for zero-shot learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and sentence generation through pairwise similarity between visual and textual fragments <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, within a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Zero-Shot Multi-Cue Embeddings</head><p>Following the state of the art zero-shot classification approaches in visual recognition <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>, we cast image classification as learning a compatibility function between images and their textual descriptions. The best known results have been obtained using human attribute descriptions <ref type="bibr" target="#b1">[2]</ref>, which limits the applicability of zero shot approaches due to the necessity of human intervention. Consequently, there is a desire to replace such human input and transition to an unsupervised setting that only leverages data readily available, e.g. from online text sources. Yet, prior evaluation <ref type="bibr" target="#b1">[2]</ref> of such unsupervised approaches, which use data automatically extracted from large text corpora, have shown a significant drop in accuracy. While resources like wikipedia most likely contain more information on a target class than a few human-annotated attributes, it has not yet been possible to leverage them to their fullest.</p><p>To better leverage readily available textual sources in an unsupervised setting, we argue for holistic embedding techniques that combine multiple and diverse language representations together in order to capture the content of rich textual sources in multiple textual parts allowing for a better transfer of knowledge to unknown classes. Additionally, we suggest a stronger supervision on the visual side, e.g. in terms of semantic part annotations, that extracts visual information from the known classes. In the following, we will present our embedding formulation that achieves both objectives.</p><p>We map semantic visual parts and language parts into a common embedding space by combining the compatibility learning framework based on embeddings <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref> and the Deep Fragment embeddings (DeFrag <ref type="bibr" target="#b22">[23]</ref>) objective in a single framework for zero-shot learning.</p><p>Objective. We define a zero-shot prediction function that for given visual input (x), chooses the corresponding class (y) with the maximum compatibility score:</p><formula xml:id="formula_0">f (x) = arg max y F (x, y).<label>(1)</label></formula><p>The compatibility function F is defined over the language and visual parts as follows:</p><formula xml:id="formula_1">F (x, y) = 1 |g x ||g y | i∈gx j∈gy max(0, v T i s j )<label>(2)</label></formula><p>where g x is a set of visual parts for the image x and g y is a set of language parts describing class y. We define our multi-cue language and visual part embeddings as follows:</p><formula xml:id="formula_2">s j = f m W language m l m + b language v i = W visual [CN N θc (I b )] + b visual (3)</formula><p>l m is a token from a language modality m (we use human annotated class-attributes, word2vec, and BoW as language cues in our experiments), and all W language m are the encoders for each modality that embed the language information into a joint space. f (.) is the Rectified Linear Unit (ReLU) which computes x ← max(0, x). CN N (I b ) denotes a part descriptor extracted from the bounding box I b surrounding the image part annotation b using deep convolutional neural networks. The extracted descriptor is subsequently embed-ded into the space of visual parts via the encoder W visual . The max is truncated at 0 because the scores that are greater than 0 are considered as correct assignments.</p><p>Finally, our objective function takes the form:</p><formula xml:id="formula_3">C(θ) = C P (θ) + α θ 2 2<label>(4)</label></formula><p>with θ = {W language , W visual } being the parameters of the framework and the constraints are defined as:</p><formula xml:id="formula_4">F (x n , y) + ∆ ≤ F (x n , y n ), ∀y ∈ Y<label>(5)</label></formula><p>where (x n , y n ) denotes corresponding image-class pairs available during training. Intuitively, we optimize for a compatibility function that scores higher by at least a margin of ∆ for true image-class pairs. The part alignment objective (C P ) in Eq 4 enforces a language part to have a high score if that language part is relevant to the image:</p><formula xml:id="formula_5">C P (θ) = i j max(0, 1 − y ij v T i s j )<label>(6)</label></formula><p>In practice, we solve Eq. 6 via y ij := sign(v T i s j ), a heuristic for Multiple Instance Learning <ref type="bibr" target="#b2">[3]</ref> as it offers an efficient alternative to direct optimization.</p><p>Optimization. The objective function (Eq. 4) is optimized with Stochastic Gradient Descent (SGD) with mini-batches of 100, momentum 0.9, and 20 epochs through the data. We learn the word vectors l m and part descriptors CN N (I b ) once and keep them fixed during the entire optimization procedure. We validate the margin ∆, the learning rate and the dimensionality of the embedding space based on the accuracy on a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Semantic Visual Parts</head><p>Using parts for visual recognition has a long and successful history for general object recognition including <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13]</ref>. The notion of semantic parts plays a central role in domains such as human pose estimation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>, action recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b6">7]</ref> and face detection <ref type="bibr" target="#b54">[55]</ref>. For finegrained classification <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b43">44]</ref> where several object parts are shared across categories, discriminative parts are important for good performance <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>. Also, CNNs have been shown to implicitly <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> model discriminative parts of objects and images.</p><p>Based on the success of using parts in various forms for object recognition we hypothesize that using strong supervision in the form of (semantic) part annotations should help fine-grained zero-shot learning. Also intuitively, an object class can be determined given the visual parts that it is composed of, e.g. a large sea bird with black feet and curved beak is a black footed albatross. Therefore, using strong supervision in the form of part annotations we seek to mitigate the loss of accuracy by using weaker auxiliary information in the form of unsupervised language representations. Note, that in this work we only rely on having the part positions annotated but do not use any other information such as part name or part type. In fact, the objective of our embedding method is formulated so that it does not require such oneto-one correspondence between textual and visual parts.</p><p>More specifically, in this work, we use a pre-trained deep convolutional network (CNN) to extract multiple semantic visual parts from 19 bounding boxes surrounding different image part annotations, i.e. the whole image, head, body, full object, and 15 part locations annotated by fine-grained object experts <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Language Parts</head><p>Zero-shot learning approaches have been struggling to carry over the success from human attribute annotations to less explicit but readily available descriptions like wikipedia articles. In order to advance the transfer of class knowledge, we study a wide range of language part representations that all can be accommodated by our embedding approach. We investigate traditional human attribute annotations, two established word vector extraction methods, word2vec and BoW, as well as propose two novel methods as an improvement of these two, NAD and MBoW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Prior Representations of Attributes and Text</head><p>Attributes <ref type="bibr" target="#b25">[26]</ref> are distinguishing properties of objects, e.g. curved beak, eats planktons, lives in water, etc. that are easily recognized by humans and interpreted by computers. Attributes are typically obtained through a two-step manual annotation process. The first step is building a set of distinguishing properties that are related to a specific class while the second step is about rating the presence/absence or the strength of every attribute for each class. In the context of fine-grained data collections, as most of the properties are common across categories, the number of distinguishing visual properties required is large which increases the annotation cost. We refer to them in our experiment as the supervised scenario and aim to develop solutions for the unsupervised scenario where such human annotations are not necessary anymore.</p><p>Word2vec <ref type="bibr" target="#b31">[32]</ref> maps frequently occurring words in a document to a vector space. It is a two-layer neural network that learns to predict a set of target words from a set of context words within a context window. Word2vec summarizes a document and converts it into a vector. In our case, one class, e.g. black footed albatross, is one document and therefore can be represented as a vector. Word2vec has been previously shown <ref type="bibr" target="#b1">[2]</ref> to be effective for image classification and even fine-grained visual recognition. We use existing <ref type="bibr" target="#b1">[2]</ref> fine-grained class-word2vec vectors for direct comparison of their and our frameworks.</p><p>Bag-of-Words is constructed as a per-class histogram of frequently occurring words. We use wikipedia documents that corresponds to the class of interest. The vocabulary of frequently occurring words is defined by counting the number of frequently repeating words inside the entire document that contains all the classes. The least and most frequently occurring words are eliminated from the vocabulary due to their irrelevance or redundancy. We use the BoW vectors of <ref type="bibr" target="#b1">[2]</ref> for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NAD: Noun-Attribute-Differences</head><p>Parallel to our multiple visual parts argument, we aim to exploit semantic relationships of different words to derive multiple language parts. Word2vec (Sec 5.1) builds a vectorial representation of each word that belongs to a learned vocabulary. The word2vec vector space is constructed with the aim to capture semantics and as a result word2vec captures several semantic regularities <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28]</ref> which can be measured by doing simple arithmetic operations in this vector space.</p><p>In our novel word2vec extensions, we exploit the additive property of word2vec vectors in the context of finegrained zero-shot learning. A concrete example for this property is as follows <ref type="bibr" target="#b32">[33]</ref>. When we subtract the vector of man from the vector of king and add the vector of woman, the resulting vector is closest to the vector of queen. Our fine-grained image classification task requires finding subtle differences between two words describing two different bird species. In the following we assume that we have a list of attributes that name properties of different bird species. Instead of asking for human judgement on how related a certain class, i.e. black footed albatross, and a certain attribute, curved beak, we want to automatically determine this similarity using the vector differences of words in word2vec space. We propose three Noun-Attribute-Difference (NAD) variants to capture relevant language information (sketched in <ref type="figure">Fig 1)</ref>.</p><p>The first version leads to a single language part, the second version consists of a constant number of parts per each class, and the third version leads to a variable number of language parts for each class. In the following formulations, we define a set of classes C ∈ {c 1 , .., c n } with n being the number of classes and a set of attributes A ∈ {a 1 , .., a m } with m being the total number of attributes. Moreover, w2v(.) defines the vectorial representation of a word in the word2vec space. Accordingly, wC(.) is the word2vec of a class and wA(.) is the word2vec of an attribute.</p><p>NAD1. In this version, we aim to build a vector that represents the similarity of class words and attribute words in the semantic word2vec space. We define NAD1 as follows:</p><formula xml:id="formula_6">NAD1(c i , j) = wC(c i ) − wA(a j ) , ∀a j ∈ A (7)</formula><p>The NAD1 of a particular class is defined as the magnitude of the distance between the word2vec of a class and the word2vec of each attribute for all the classes. As the number of attributes is fixed, there is a single NAD1 vector that is associated with each class. In other words, NAD1 corresponds to a single language part, i.e. LP=1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAD2.</head><p>As an alternative to using all attributes and all class names (NAD1), we aim to eliminate attributes that are not relevant for a particular class. Classically this is determined by human (expert) annotations which we want to avoid. Instead we argue that this human annotation effort can be eliminated by considering the similarity of class and attribute words in the word2vec space.</p><p>Based on the magnitude of the distance in the word2vec space, we define the set of attributes that are relevant for a class as follows.</p><formula xml:id="formula_7">B(c i ) = {a j |a j ∈ A top−n (c i )} where A top−n (c i )</formula><p>is a set of attributes that are the top − n nearest neighbors in word2vec space to class c i . Accordingly, our second NAD version is formulated as follows:</p><formula xml:id="formula_8">NAD2(c i ) = {wC(c i ) − wA(a j )|a j ∈ B(c i )} (8)</formula><p>The NAD2 leads to the same number of language parts for each class. However, as for each class the most similar top − n attributes are highly likely to be different, the set of attributes that are used in NAD2 is naturally not the same for each class. We select LP = {5, 10, 25, 50, 75, 100} and build six different sets of NAD2 representations.</p><p>NAD3. For the definition of the final alternative, we additionally assume that we know which attributes are present for which class even though we do not know how important an attribute is for any class. NAD3 is defined as follows:</p><formula xml:id="formula_9">NAD3(c i ) = {wC(c i ) − wA(a j )|a j ∈ A(c i )} (9)</formula><p>where A(c i ) is the set of attributes associated to class c i . In the experiments below A(c i ) is obtained by thresholding the continuous attribute strengths which is known to introduce errors <ref type="bibr" target="#b38">[39]</ref>. It is important to note that only NAD3 requires set A(c i ) and that the other two NAD variants only require the list of attributes that is relevant to all classes and use the similarities of attributes and classes in word2vec space to automatically generate language parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">MBoW: Multiple Bag-of-Words</head><p>Similar to the NAD in Sec 5.3, we build multiple language parts associated for each class as an extension to the BoW method. We use wikipedia articles that corresponds to each class as our text corpus. We build three different versions of multiple bag-of-words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MBoW1</head><p>. As a baseline, we extract a single BoW histogram from the entire wikipedia article of each class. This leads to one language part per class.</p><p>MBoW2. Here, we divide the wikipedia articles of each class into a constant number of paragraphs. This number is selected from the set P = {2, 3, 4, 5}. As the wikipedia article of each class has different length, the MBoW2 vectors that correspond to classes with shorter articles will get sparser with the increasing number of P .</p><p>MBoW3. As wikipedia articles have a structural organization of their own, in this version of multi-bag-of-words, we use this wikipedia structure. We divide the articles into different subject-separated partitions. As different articles have different number of sections, MBoW3 leads to a variable number of vectors for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In our experimental evaluation we use the fine-grained Caltech UCSD Birds-2011 (CUB) <ref type="bibr" target="#b44">[45]</ref> dataset that contains 200 classes of different North-American bird species populated with ≈60 images each. Each class is also annotated with 312 visual attributes. In the zero-shot setting, 150 classes are used for training and the other 50 classes for testing. For parameter validation, we also use a zero-shot setting within the 150 classes of the training set i.e. we use 100 classes for training and the rest for validation.</p><p>We extract image features from the activations of the fully connected layers of a deep CNN. We re-size each image to 224×224 and feed into the network which was pre-trained following the model architecture of the 16-layer VGG network [41] 1 . As multiple visual parts, we use image features extracted from the annotated part locations of the images. For this, we crop the image on the overlapping bounding boxes with the size 50×50 2 that we draw around that particular part location (Sec. 4), resize each bounding box to 224×224 and follow the rest of the pipeline.</p><p>As supervised language parts (Sec. 5), we use humanannotated per-class attributes with continuous values that measure the strength of the attribute for each class. As unsupervised language parts we automatically extract word2vec <ref type="bibr" target="#b31">[32]</ref> from the entire 13.02.2014 wikipedia dump and Bag-of-Words from the wikipedia articles that correspond to our 200 object classes. For NAD, i.e. our novel Noun-Attribute-Differences, we take the word2vec vectors of 200 classes and 312 attributes. For MBoW, we use the same vocabulary as before and extract BoW histograms from different parts of wikipedia articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Effect of Learning Method</head><p>As a baseline for our evaluation, we employ Structured Joint Embedding (SJE) <ref type="bibr" target="#b1">[2]</ref> which learns a bilinear compatibility function between an image and its class embedding.  SJE obtains the current state-of-the-art for zero-shot learning on CUB. We re-evaluate SJE using our 4K-dim VGG-CNN <ref type="bibr" target="#b40">[41]</ref> as input embedding <ref type="bibr" target="#b2">3</ref> . We use attributes as supervised output embeddings, word2vec and bag-of-words as unsupervised output embeddings. On the other hand, our joint part embedding framework learns two compatibility functions parameterized by W language and W visual with an integrated non-linearity computation. Tab. 1 compares SJE and our joint embedding using the standard average per-class Top-1 image classification accuracy on previously unseen classes. Using a single visual part per image, our joint embedding performs worse in the supervised setting (attributes) but slightly better than SJE in the unsupervised setting. Namely, joint part embeddings achieve 25.0% for word2vec while SJE obtains 24.2% and 21.8% for Bagof-Words whereas SJE obtains 20.0% accuracy. Here, the language parts are extracted from wikipedia without using any human annotation. This result is important as we aim to increase the zero-shot learning performance on the CUB dataset for this unsupervised setting. The following section exploits our flexible framework to incorporate both strong visual supervision as well as multiple language parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Strong Supervision by Part Annotations</head><p>Apart from using a non-linear embedding objective, our joint part embedding benefits from using multiple visual or language parts. We extract 19 parts from each image that correspond to the whole image, head, body and full bounding box <ref type="bibr" target="#b50">[51]</ref>, bounding boxes drawn around 15 part loca-tions whose annotations are available within the dataset. We evaluate the effect of parts in the following way: (1) training and testing with a single part, (2) training with multiple parts and testing with a single part, and (3) training and testing with multiple parts.</p><p>Zero-Shot Image Classification. For zero-shot image classification, we calculate the mean per-class Top-1 accuracy obtained on unseen classes. In other words, we consider the prediction as positive only if the predicted class label matches the correct class label for that image. We average the predictions on a per-class basis. The results are presented in <ref type="table" target="#tab_1">Table 2</ref>. For attributes, using multiple visual parts at training time already improves the accuracy from 43.3% to 47.0%, improving the state-of-the-art. On the other hand, using multiple visual parts also at test time achieves 56.5% accuracy, further improving the supervised state-of-the-art on this dataset. For Bag-of-Words, using multiple visual parts improves the accuracy 26.0%. For word2vec, multiple visual parts achieves an impressive 32.1% accuracy which becomes the new state-of-the-art obtained without using human supervision on the language side. These results support our intuition that using strong supervision of semantic visual parts leads to more discriminative image representations and thus is helpful for zero-shot fine-grained image classification.</p><p>Zero-Shot Image Retrieval. For zero-shot image retrieval, we use two popular evaluation methods: the average recall at position 1, 5 and 10 (R@1,R@5 and R@10) on the ranked list of labels predicted for each image and the mean area under the Precision-Recall curve (mAUC). We present our results in Tab 3. The state-of-the-art <ref type="bibr" target="#b3">[4]</ref> retrieval accuracy reported on unseen classes without human supervision on the CUB dataset is 13.0% mAUC. Using VP=1 both BoW (mAUC of 16.2%) and word2vec (22.8%) outperform the state-of-the-art. Using V P = 19 further improves performance for BoW (22%) and word2vec (30.7%). Using supervised text annotation, i.e. attributes, the unseen class mAUC increases to 46.6%. These results indicate that strong visual supervision helps both image retrieval and classification in a zero-shot learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Using Multiple Language Parts</head><p>We now explore the effects of using multiple language parts and to associate them to multiple visual parts extracted using strong supervision.</p><p>NAD. We evaluate the three proposed noun-attribute distance (NAD) variants (Sec 5.3) as language parts. NAD1 measures noun-attribute distances between all classes and all attributes and results in a single language part (LP=1). For NAD2, the noun-attribute distances are computed between all classes and top 5-100 most discriminative attributes. Thus, it corresponds to LP=5-100. NAD3, mea-   suring noun-attribute distances between all classes and all the relevant attributes for that class. Therefore, NAD3 uses different number (≈ 150) of language parts for each class.</p><p>We present our results with NAD on <ref type="figure" target="#fig_1">Fig 2.</ref> NAD1 and NAD3 both do not obtain impressive results. In the case of NAD1 this can be explained with the fact that it only contains a single language part. For NAD3 we suspect that this is due to the fact that there is a large imbalance in the number of descriptive attributes for each class. NAD2 on the other hand obtains promising results. In fact using 50 language parts (NAD2 LP=50) obtains 33.9% (see also Tab. 4) that improves over the previous unsupervised state-of-theart using word2vec alone.</p><p>MBoW. As an alternative multiple language parts setting to NAD, we use MBoW (Sec 5.3) also extracted three different ways. For MBoW1, we construct the BoW using the entire wikipedia article for a bird which results in a single language part (LP=1). For MBoW2, same number of multiple language parts (LP=2-5) are extracted by partitioning the wikipedia articles into 2,3,4 or 5 parts. For MBoW3, we extract variable number of language parts (≈ 4) based on wikipedia's own content-grouped paragraphs.</p><p>We present our results on    multiple language parts help only if they are supported with multiple visual parts. Another interesting configuration is using multiple visual parts during training and, at test time, evaluating the multiple language parts in these cases with or without strong visual annotation. In the former case, again, there are two configurations,i.e. with a single language part and with multiple language parts. With a single language part we obtain 26.8% accuracy which is already higher than 25.0% with a single visual part at training time. On the other hand, with multiple language parts we achieve an impressive 30.5% accuracy. This shows that multiple language parts indeed help even if they are supported by strong visual supervision only at training time. If we use multiple visual parts also at test time, we further improve our results to 33.9%, establishing a new state-of-the-art when using unsupervised text embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Multi-Cue Language Embeddings</head><p>We finally explore combinations of different language parts in our joint part embeddings framework. For single language part setting (LP=1), we combine word2vec and BoW. For training, we use multiple visual parts (VP=19), whereas for testing we either use multiple visual parts (VP=19) or single visual part (VP=1). The results are presented in <ref type="table" target="#tab_6">Table 5</ref>. Combining word2vec with BoW using VP=1 for testing leads to 33.2% accuracy improving both word2vec (32.1%) and BoW (26.0%). Additionally, the same combination with VP=19 for testing leads to 34.7% accuracy which again improves word2vec (32.1%) and BoW (26.0%) on the same setting. These results are consistent and encouraging because they provide a large improvement over the state-of-the-art (24.2%, <ref type="table">Table 1</ref>) and reduces the gap between the state-of-the-art obtained through human annotation (50.2%, <ref type="table">Table 1</ref>).</p><p>For the setting with multiple language parts, we use the best performing NAD2 with VP=50. This method measures the similarity of word2vec vectors between class and attribute names with the most relevant (top50) attributes to each class. Using a single visual part for testing leads to 30.5% accuracy whereas using multiple visual parts obtains 33.9% accuracy. Compared to the single-part word2vec (32.2%) and BoW (26.0%), this is a significant improvement which indicates that combining multiple language parts also help. Moreover, word2vec contains latent relationships between class and attribute names which are released when these nouns are considered relative to each other.</p><p>Finally, the last row of <ref type="table" target="#tab_6">Table 5</ref> shows that the combination of NAD2 (33.9%) and BoW (26.0%) leads to 34.3% accuracy which is again higher than NAD2 and BoW alone. This indicates that our approach can exploit the complementarity of the NAD2 and BoW representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>For the challenging problems of zero-shot fine-grained classification and retrieval, we have presented a formulation that allows to integrate diverse class descriptions and detailed part annotations and consequently improves significantly on the state-of-the-art on both tasks in a range of experimental conditions. In particular, we have demonstrated how to compensate for the loss of accuracy by using weaker auxiliary information with detailed visual part level annotations. Our approach facilitates a joint embedding of multiple language parts and visual information into a joint space. With strong visual supervision and humanannotated attributes we improve the state-of-the-art on the CUB dataset to 56.5% (from 50.2%) in the supervised setting. In addition, we show how to use multiple language sources and extract diverse auxiliary information from unlabeled text corpora, i.e. word2vec and BoW. We build multiple parts on the language side, i.e. NAD and MBoW and thereby improve the state-of-the-art also in the unsupervised setting to 33.9% (from 24.2%). Finally, we combine different unsupervised text embeddings and further improve the results for the unsupervised setting to 34.7%.</p><p>As a conclusion, we propose several extensions for finegrained zero-shot learning. First, using multiple visual parts when available, i.e. training or test time, rather than using a single visual part leads to a significant boost in performance. Second, these multiple visual parts can be supported with multiple language parts for further improvements. Third, word2vec space indeed contains some latent information and distance between class and attribute names can eliminate the costly human annotation of class-attribute associations. Following these practices, we improve the fine-grained zero-shot state-of-the-art on CUB for both supervised and unsupervised text embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Effect of multiple language parts. NAD uses nounattribute distance as a measure of similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 3 .Figure 3 :</head><label>33</label><figDesc>MBoW1 as well as the different versions of MBoW2 obtain reasonable performance even though staying below the best performance Effect of multiple language parts. MBoW uses multiple parts from wikipedia articles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Multiple visual parts (VP) for classification. VP 
are extracted from the annotations that are provided with the 
dataset. (Top-1 avg per-class top-1 acc on unseen classes.) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Multiple visual parts (VP) for retrieval. VP are extracted from the annotations that are provided with the dataset. We 
measure recall at 1,5,10 (R@1,5,10) and mean AUC on unseen classes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Summary of our results with single or multiple visual and language parts. We improve over the state-ofthe-art with unsupervised embeddings significantly. achieved by NAD2. The results of MBoW3 are not that impressive. As this representation is based on the wikipedia article structure itself and as mentioned before the length of the paragraphs is variable and thus the histogram based representation might be not reliable enough.Summary of results. We investigate the effects of using a single visual part for training + test and using multiple visual parts either only on training or both on training + test. The results are summarized in Tab 4. In our framework, we can use a single visual part in combination with either a single language part or multiple language parts. The former configuration leads to 25.0% accuracy whereas the latter obtains a lower accuracy of 23.6%. This indicates that</figDesc><table>Unsupervised 

Test 
LP W2V BoW NAD2 VP=1 VP=19 

1 

26.8 
32.1 
22.6 
26.0 
33.2 
34.7 

50 

30.5 
33.9 
31.0 
32.1 
30.0 
34.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Combining different number of language parts (LP). word2vec: class-based vectors extracted from wikipedia. BoW: histogram of word occurrences per wikipedia article of a class. NAD2: Using noun-attribute distance as a measure of similarity between classes. We use multiple visual parts at training and either single or multiple visual parts at test time.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the publicly-available MatConvNet library<ref type="bibr" target="#b40">[41]</ref> <ref type="bibr" target="#b1">2</ref> We have empirically found that 50×50 performs well for the task</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that<ref type="bibr" target="#b1">[2]</ref> reports slightly better performance using GoogLeNet features instead of VGG as here.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labelembedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of Output Embeddings for Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Label embedding trees for large multi-class tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Detecting actions, poses, and objects with relational phraselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovering localized attributes for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attribute-centric recognition for cross-category generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Weakly supervised scale-invariant learning of models for visual recognition. IJCV, 71</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<idno>1954. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Z. Harris. Distributional structure. Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online incremental attribute-based zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kankuekul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kawewong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tangruamsub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fisher vectors derived from hybrid gaussian-laplacian mixture models for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONLL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn). In ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person recognition in personal photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition with holistic and pose based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image question answering: A visual semantic embedding model and a new dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluating knowledge transfer and zero-shot learning in a large-scale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning words from sights and sounds: A computational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="146" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4564</idno>
		<title level="m">Matconvnet -convolutional neural networks for matlab</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint learning of visual attributes, object classes and visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A discriminative latent model of object classes and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
	</analytic>
	<monogr>
		<title level="j">Caltech</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Large scale image annotation: Learning to rank with joint word-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3D object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attribute-based transfer learning for object categorization with zero or one training example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Partbased R-CNNs for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno>NIPS. 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
