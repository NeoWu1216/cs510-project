<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BORDER: An Oriented Rectangles Approach to Texture-less Object Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering (SCE)</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Block N4 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Addison</forename><surname>Lee</surname></persName>
							<email>jalee@i2r.a-star.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR) 1 Fusionopolis Way, Connexis (South Tower</orgName>
								<address>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Kemao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering (SCE)</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Block N4 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BORDER: An Oriented Rectangles Approach to Texture-less Object Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This paper presents an algorithm coined BORDER (Bounding Oriented-Rectangle Descriptors for Enclosed</head><p>Regions) for texture-less object recognition. By fusing a regional object encompassment concept with descriptor-based pipelines, we extend local-patches into scalable object-sized oriented rectangles for optimal object information encapsulation with minimal outliers. We correspondingly introduce a modified line-segment detection technique termed Linelets to stabilize keypoint repeatability in homogenous conditions. In addition, a unique sampling technique facilitates the incorporation of robust angle primitives to produce discriminative rotation-invariant descriptors. BORDER's high competence in object recognition particularly excels in homogenous conditions obtaining superior detection rates in the presence of high-clutter, occlusion and scale-rotation changes when compared with modern state-of-the-art texture-less object detectors such as BOLD and LINE2D on public texture-less object databases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognition of texture-less objects has become increasingly significant in modern times as the world diversifies into 2D-3D research areas such as Augmented Reality and 3D reconstruction/printing. Yet, these objects make detection challenging even for state-of-the-art descriptor-based detectors like the popular SIFT (Scale Invariant Feature Transform) <ref type="bibr" target="#b0">[1]</ref>, and SURF (Speeded Up Robust Features) <ref type="bibr" target="#b1">[2]</ref>. The key factor that inhibits decent performance in homogeneity originates from the scarcity of local salient information, which impedes the effectiveness of keypoint registration and ultimately degrading the performance of these local patch-based description-matching paradigms. Consequently, this led to many variants of texture-less detection schemes, therefore potentially excluding many descriptor-based virtues such as scale-rotation invariance, model scalability, and the high distinctiveness in the presence of occlusion and clutter.</p><p>Current contemporary texture-less object detectors mainly fall into two categories to cope with the lackluster information that these objects resonate. The first technique involves edge/gradient-based template matching <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, where objects are trained with various indifferent methods and windowed through the scene to find the best matched location. The popularity of this approach stems from its ability to encompass the object in its entirety, thus granting optimal object description in both textured and homogenous domains at efficient runtimes. However, its robustness quickly diminishes in occluding circumstances, and would need sophisticated amounts of training data to uphold invariances such as rotation, scale and various vantage viewpoints. The second technique assumes an edge-feature aggregation approach <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, often adopting a partial SIFT-like pipeline to describe its grouped edge-features. These algorithms typically form interest-points by engaging a line-based representation of the edges, while exploiting various methods to aggregate and describe their associative properties. Despite incorporating virtues like scale-rotation invariance as well as model scalability, this technique often suffers from stability issues especially during occlusion where edges become altered, affecting its size and potentially causing shifts in the interest-points. Furthermore, its spatial feature-grouping approach often degrades due to nearby clutter, corrupting the aggregations and eventually their descriptors.</p><p>In this paper, we aim to design a detector that combines texture-less based techniques with qualities from the descriptor-based pipeline to robustly recognize homogenous objects in high clutter and occlusion. Our proposed work BORDER, commences its SIFT-like pipeline with a detection scheme to meaningfully divide elongated line-segments into smaller equal-sized fragments (termed Linelets) to stabilize interest-point shifts in occlusion. Next, we capture regional object information using an encapsulation concept by acclimating descriptor-based local patches into larger, object-sized scalable oriented rectangles. These rectangles undergo a unique rotational search technique to find ideal positions to optimally describe the object from a keypoint's "point-of-view". Furthermore, we deploy BORDER in a multi-sized rectangle scheme to incorporate sub-sectional encapsulation for added occlusion resistivity. Subsequently, encompassed regions undertake a linear sampling process to accumulate state-of-the-art rotation-invariant angle primitives <ref type="bibr" target="#b9">[10]</ref> to form its descriptors. Finally, we match BORDER descriptors by exercising the randomized kd-tree forest <ref type="bibr" target="#b14">[15]</ref> after a pre-processing procedure. <ref type="figure" target="#fig_0">Fig. 1</ref> presents an object detection instance of BORDER with its multi-sized oriented rectangle scheme in a cluttered scene.</p><p>The rest of the paper is organized as follows. Section 2 discusses the related work. Section 3 presents the BORDER methodology. Section 4 showcases the experiment results. Lastly, section 5 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Given its impressive detection rates even in the presence of clutter and occlusion, BOLD (Bunch of Line Descriptors) <ref type="bibr" target="#b9">[10]</ref> is arguably the current leading texture-less descriptor-based detector. It uses detected line-segment midpoints as interest-points to aggregate nearby segments via k nearest neighbors (kNN), while describing them using their unique angle primitives. However, as revealed in their paper, nearby distractors tend to cause aggregations to include unwanted segments. Furthermore, line-segment midpoints may be susceptible to shifts due to various factors making the repeatability of the interest-points questionable. Another work that resembles BOLD is <ref type="bibr" target="#b10">[11]</ref>, where lines are similarly employed for detection, but are described by a pairwise structure. Geometrical, color and distance relationships within each line pair then proceed to train its classifier. Other works of similar nature such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> likewise use line aggregations, though with indifferent primitives and training methods. Damen et al. <ref type="bibr" target="#b13">[14]</ref> proposed using edgelets (short straight segments defined by its midpoint and orientation) to accumulate traced angled paths by reflecting off other edgelets to form constellations. The resultant collection of path orientations and distances are subsequently encoded to form its descriptors. Even though this method feels similar to BOLD, it lacks descriptor distinctiveness. Moreover, its path reflection method is very sensitive to occlusion and noise, as constellation paths would differ due to missing or additional edgelets. Other pioneering algorithms of significance in this genre includes a triple-edge feature with a point-based geometric hashing comparison method by Proctor and Illingworth <ref type="bibr" target="#b15">[16]</ref>, and a cubist approach by Nelson and Selinger <ref type="bibr" target="#b16">[17]</ref> using boundary segments called key curves to generate patches for feature extraction.</p><p>Template-based matchers are another widely researched area of texture-less object detection. One of the most significant work termed LINE2D (2D templates) or LINEMOD (2D templates+depth maps) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> uses quantized gradient orientations to form response maps for input windowing comparisons through a similarity measure. LINE2D achieved real-time detection while producing decent results in homogeneity, but quickly degrades under clutter and occlusion. Hsiao and Herbert <ref type="bibr" target="#b17">[18]</ref> followed up by attaching an occlusion reasoning component to LINE2D, enhancing its detection rates under highly distractive scenes. Even so, LINE2D's template-based origin leads to scalability issues, often requiring voluminous number of templates for recognition in variances such as rotation, scale and perspective changes. Other notable template-based works include techniques such as chamfer matching <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref> and Hausdorff distance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, which utilize edge-point templates to find correspondences using a similarity measure. Additionally, 3D template-based algorithms using 3D CAD models <ref type="bibr" target="#b19">[20]</ref> and depth map templates <ref type="bibr" target="#b20">[21]</ref> were respectively introduced in recent years due to the availability of auxiliary depth information.</p><p>Finally, shaped-based learning methods also made substantial impact in texture-less detection. Ferrari et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> introduced a local contour feature called k-Adjacent Segments (kAS) by linking edgel-chains (fairly straight contour segments) and describing them using their orientations and lengths. These kAS features consolidate into class-specific codebooks to learn the object's shape model for detection using a Hough voting scheme. Other contour-based shape detectors comprise of <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>, where regions are similarly detected using contours and learned using shape context descriptors. Although these works displayed decent performance in clutter and scale, rotation and/or occluding results were largely unreported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The BORDER Methodology</head><p>BORDER's recognition scheme follows the 3-step SIFT-like pipeline of detection, description and matching. This is accompanied by a regional object encompassment concept by first detecting interest-points via linelets, followed by a description scheme focused about oriented rectangles. We use linelets as pivot anchor-points for oriented rectangles to discover good locations to describe the object, and subsequently match them based on a unique "point-of-view" scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linelet Detection</head><p>Line-segments have shown in modern times to effectively present a low-level stable edge feature representation for texture-less objects <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. The Line Segment Detector (LSD) developed by Von Gioi et al. <ref type="bibr" target="#b26">[27]</ref>, enables images to be expressed in terms of lines with minimal need for manual parameter tweaks. However, when purely used as a keypoint detector, it encounters a major complication particularly with elongated lines. These lines often materialize in low-curvature areas, thus facilitating region-growth through their related neighboring orientated pixels. Consequently, as line-feature based detectors tend to exploit segment-centers as interest-points <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, any occluding alterations of these lines will render stability issues due to midpoint shifts. Hence, to counteract this shortcoming, we propose an adaptation of LSD to generate equal fragmented versions of extensive line-segments termed Linelets.</p><p>The underlying principle behind linelets is to intuitively fragment stable LSD-produced line-segments based on a model-scene proportion concept. This is done as opposed to sampling edges at regular intervals <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>, to reduce redundancy and meaningfully tune fragment-width according to the severity of clutter and occlusion. We begin by applying an initial LSD detection step for both input images to obtain the line segments of the model L model and the scene L scene respectively. Subsequently, we initiate the modification by defining a width threshold</p><formula xml:id="formula_0">ω = τ R min ,<label>(1)</label></formula><p>where τ ≥ 1 refers to the width-limit factor with τ = 1 denoting that fragment widths are restricted to R min . The variable R min indicates the minimum region size for line-segment validation established through the NFA (Number of False Alarms) computation theorized by Desolneux et al. <ref type="bibr" target="#b27">[28]</ref>, and extensively tested in <ref type="bibr" target="#b28">[29]</ref> for automatic detection without tuning intervention for LSD. This parameter is an ideal base parameter for ω since all lines produced from LSD region-growing must contain at least R min aligned pixels to be deemed as a valid line-segment. Next, to incorporate our proportion-based fragmentation concept, we define</p><formula xml:id="formula_1">τ = L max R min · n(L model ) n(L scene ) · |L scene | |L model | ,<label>(2)</label></formula><p>where L max is the model object's longest line-segment, n(L scene ) and n(L model ) refers to the total model-scene line-segments detected, while |L scene | and |L model | denotes the average lengths of all model-scene line-segments respectively. The first term in Eq. (2), L max /R min indicates the maximum fragments producible from the model object's longest line-segment L max using R min . We chose L max due to its fragmentation priority, as the longest uninterrupted model line-segment would likely need the most partitions to keep it stable against scene occlusions. The subsequent expressions incorporate our proportion estimates where n(L model )/n(L scene ) takes account of the model-scene line-segment density to estimate clutter, and |L scene |/|L model | approximates the line-sizing ratio to verify occlusion breakages using the model-scene average lengths. Lastly, we finalize ω by setting its boundary to</p><formula xml:id="formula_2">ω ℓ = min[max(ω, R min ), L max ],<label>(3)</label></formula><p>where ω ℓ represents the standardized width of each fragment for both input images, which is used to divide the previously materialized line-segments that have lengths ≥ 2ω ℓ into equal ω ℓ widths to produce linelets. Note that if ω ℓ = L max , it implies that linelets fall back to line-segments. This happens when clutter and occlusion are both presumably low, thus requiring no partitions. Finally, each linelet ℓ is attached with properties such as its midpoint m ℓ , its orientation direction θ ℓ , and the aligned pixels a i ∈ A ℓ gathered from LSD region-growing. <ref type="figure" target="#fig_1">Fig. 2</ref> shows a stability comparison between line-segments and linelets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Oriented Rectangle Template</head><p>Before beginning the description process, the initial shape of the oriented rectangle OR must be pre-determined. This preliminary step aims to attain the ideal dimensions of the OR to optimally encapsulate the input model object. We surveyed numerous public databases and branded template images into three main categories. The first type is basically an undistracted template of the object, the second involves a scene with the object prominently presented alongside various insignificant distractors, and the last category suffices a scene with an object mask. All three models can be automatically enclosed by the combination of linelet detection and the minimum enclosing box algorithm <ref type="bibr" target="#b29">[30]</ref>, with the second model requiring an additional salient region detector with automatic threshold <ref type="bibr" target="#b30">[31]</ref> to obtain its object mask. Alternatively, BORDER also permits for manual OR dimensional definition. Note that all models in our experiments were encapsulated by the automatic method, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The OR prototype serves as the basis for several descriptive purposes. First, it enables length/breadth tuning for generating multi-sized ORs for promoting diverse sub-area object encapsulation to facilitate robustness to occlusion. Next, it also caters for OR scale factor σ adjustments for finer scale-space variations (e.g. σ = 0.7) as opposed to image pyramids. Finally, ORs are sub-divided into equal 4×4 blocks for description purposes. Although other dimensions can be set, we have found that this setting offers the best trade-off between descriptor performance and vector size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Linelet-OR Association and Revolution</head><p>Typically, it is customary for a local-patch based detector to center its keypoint on a description region (e.g. SIFT <ref type="bibr" target="#b0">[1]</ref> and SURF <ref type="bibr" target="#b1">[2]</ref>). However, given the object encapsulation concept of BORDER, linelet midpoints m ℓ are instead placed at the corner of the OR. This is done with the assumption that linelets would habitually be detected around object edges. Therefore, by aligning the corner of an OR with a linelet, it could hypothetically promote idealistic encompassment of the object with minimal outliers. For standardization, we align the OR according to the linelet's direction at its longer side. After association, the OR proceeds for a scheduled full revolution about m ℓ . This rotation step plays an important role in BORDER as it allows each m ℓ to search for good locations to describe the object from its own "point-of-view". The rotation takes place n r times, angled equally at θ = 2π/n r per sample r. Moreover, as each placement is essentially a halfspace due to its corner-based association, we correspondingly take account of its flipside to ensure all facets of the revolution equivalently cover the entire spectrum. Consequently, this brings the total rotations to 2n r . <ref type="figure" target="#fig_3">Fig. 4</ref> demonstrates the OR revolution about a linelet midpoint after association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Descriptor Formulation Criterion</head><p>For a rotation sample r to be deemed describable it has to attain an encapsulation standard, otherwise the rotation sample is skipped without having a descriptor being built. The criterion governing descriptor formulation is as follows</p><formula xml:id="formula_3">r = −# r = −$ r = −% r = −&amp; r = −' r = −( r = −) r = −* r = ( r = ' r = * r = &amp; r = % r = $ r = # r = ) + , + ,</formula><formula xml:id="formula_4">desc ℓ (r) = n ℓ (r) σn(ℓ model ) · log n B [n(v B (r))] ≥ γ ,<label>(4)</label></formula><p>where desc ℓ (r) returns a boolean regarding the description validity of the r th rotated OR, n ℓ (r) is the total linelets within the r th rotated OR of the ℓ th linelet, n(ℓ model ) denotes the model's total linelets, σ is the current scale of the OR, n B refers to number of OR blocks (e.g. 4 × 4 = 16), n(v B (r)) represents the number of valid blocks (blocks with at least one linelet), and γ is the description validity factor. Note that the log-function in Eq. (4) balances the weight of the block validity ratio due to the relatively-low counts of texture-less objects. Also, if n(v B (r)) = 0, or n ℓ (r) &gt; σn(ℓ model ) the rotation sample will be omitted, as the former constitutes an empty OR, while the latter signifies corruption because rotation samples should never contain more linelets than the model. Overall, we apply this authoritative condition to each model/scene image pair to enforce the abundance and distribution of linelets within the ORs for strong descriptor significance. Any rotation sample that meets the threshold is labeled as a prospective descriptor, therefore potentially producing multiple keypoints at the same location with each tagged with a different r. <ref type="figure" target="#fig_4">Fig. 5</ref> demonstrates the full descriptor validity process. Before the online application of the condition, the appropriate γ has to be established. For this, all model linelets undergo a pre-revolution step using the prototype (section 3.2) while applying Eq. (4) to obtain the maximum score γ max for each linelet revolution. Subsequently, we gather all linelets' γ max and assign the minimum value of the collection as the description validity factor γ. This asserts that all model linelets would produce at least one descriptor from its maximum score after applying Eq. (4). After establishing γ, we re-iterate the pre-revolution step with different OR sizes by reducing one row/column at a time (e.g. 4×3, 3×4, . . ., 2×2) from the original OR prototype and re-dividing it to 4×4 blocks. Any reduced OR's encapsulation found to surpass γ will also have its length/breadth deployed as part of the multi-sized descriptor scheme within each scale space iteration. This is another key characteristic of BORDER in addition to linelets to greatly enhance detection rates during occlusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">BORDER Description</head><p>For each approved rotational position desc ℓ (r) = 1, empty blocks are assigned with zeros in its particular histogram block vector, whereas valid blocks proceed with the description. For each valid block, we start by consolidating the encapsulated linelets' aligned pixels A ℓ for sampling. This is done as opposed to using every pixel within the block to keep updates to a minimal. Furthermore, as pixels within each A ℓ have closely related orientations from LSD region-growing, we are able to sample A ℓ using a stepsize approach without sacrificing performance. Let B be the histogram block scheduled for update and A B = [A ℓ ∈ B] be the set of aligned pixels within B, then we define the number of update samples n s for B as</p><formula xml:id="formula_5">n s = σγR min n(ℓ B ),<label>(5)</label></formula><p>where σ is the current scale of the OR, 0 ≤ γ ≤ 1 is the description validity factor (section 3.4) used as the object-complexity indicator to normalize the sample size based on the model object's information richness, n(ℓ B ) is the number of linelets in the current B, and R min refers to the minimum region size validation parameter (section 3.1) used as a standardized A ℓ size for all linelets. Subsequently, the block's stepsize can be represented by</p><formula xml:id="formula_6">stepsize = max n(A B )/n s , 1 ,<label>(6)</label></formula><p>where n(A B ) refers to the total aligned pixels within the current B. Finally, each sampling pixel is retrieved using</p><formula xml:id="formula_7">s i = A B (stepsize · i), 0 ≤ i ≤ n s ,<label>(7)</label></formula><p>where s i refers to i th sampled pixel. In general, this stepsize approach normalizes the sampling rate for each linelet's A ℓ according to its size within B to minimize updates. For the descriptors, the key factor besides delivering exclusiveness is rotation invariance. Out of the existing texture-less description methods <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, BOLD's pairwise geometric primitives has proved to be the most robust as presented in their paper <ref type="bibr" target="#b9">[10]</ref>. Therefore, to acclimatize our block samples into similar robust angle primitives, we simulate each sample s i and its origin linelet keypoint ℓ j as point pairs while extending each s i into unit vectors using its aligned direction as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. This creates two angles, but we only use the primitive at s i as the angle at ℓ j has already been encoded by the OR's rotation index and its block sequencing. Contrary to BOLD, information gathered during linelet detection allows us to simplify the derivation of our single-sided angle primitive α i to</p><formula xml:id="formula_8">αi =          arccos ŝi · tij tij ,ĝi · tij &lt; 0 2π − arccos ŝi · tij tij , otherwise ,<label>(8)</label></formula><p>where · is the dot product, t ij refers to the imaginary vector between s i and its origin linelet keypoint ℓ j , while the unit vectorsŝ i andĝ i represents the aligned direction and gradient orientation of s i respectively. The effectiveness of this primitive can be attributed to its contrast polarity property, which is incoporated by verifying the pointing-directions ofĝ i and t ij with respect to itŝ s i axis. For instance, if bothĝ i and t ij point to different halfspaces withŝ i as their separating axis (e.g.ŝ 1 in <ref type="figure" target="#fig_5">Fig. 6</ref>), then we assign the smaller angle betweenŝ i and t ij to α i , otherwise the larger inverse angle is allocated instead (e.g.ŝ 0 andŝ 2 in <ref type="figure" target="#fig_5">Fig. 6</ref>). By doing so, it enables additional robustness by embedding the direction of s i in the descriptors. All α i gathered from each sample-origin pair form votes with weights corresponding to the gradient magnitude of its respective samples. We assign each histogram block with 8 orientation bins (θ = 2π/8), while employing bilateral accumulation to minimize effects of quantization. Finally, all block histograms are concatenated to form a 16 × 8 = 128 dimensional vector with the first block sequence being nearest to the origin as indicated in <ref type="figure" target="#fig_5">Fig. 6</ref>. As for normalization, tests showed that L2 norm applied individually block-wise rather than the entire concatenated descriptor offered the best results. This is primary because ORs span over large regions, therefore becoming susceptible to vastly contrasting magnitudes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Matching BORDER Descriptors</head><p>BORDER asserts that each descriptor can only be matched with another that has been produced at a similar "point-of-view". This means that descriptors are only compared when they are created at the same rotation index r, as illustrated in <ref type="figure" target="#fig_6">Fig. 7</ref>. We introduce this rule due to the tendency of texture-less objects to be symmetrical, hence increasing its matching ambiguity. To accommodate this condition, we distribute the descriptor vectors into 2n r spaces according to its tagged r in the description phase. Therefore, no additional memory or work is sacrificed for the preparation as opposed to accumulating into a single large space. As for the actual matching phase, only vector spaces that correspond to the same r-tag between the train and query descriptors gets matched. BORDER utilizes the randomized kd-tree forest from FLANN <ref type="bibr" target="#b14">[15]</ref> for Euclidean distance matching followed by a geometric verification process for object localization within the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section compares BORDER against other contemporary detectors in both texture-less and textured genres. To ensure comprehensiveness, we employ modern texture-less object detectors such as BOLD <ref type="bibr" target="#b9">[10]</ref> and LINE2D <ref type="bibr" target="#b2">[3]</ref>, as well as popular textured-based keypoint detectors like SIFT <ref type="bibr" target="#b0">[1]</ref>, SURF <ref type="bibr" target="#b1">[2]</ref> and ORB <ref type="bibr" target="#b31">[32]</ref>. A total of three datasets have been chosen for our experiments, the D-Textureless dataset <ref type="bibr" target="#b9">[10]</ref> to challenge BOLD's high detection rates, the CMU Kitchen Occlusion dataset (CMU-KO8) <ref type="bibr" target="#b17">[18]</ref> for its highly cluttered and occlusive scenes, and finally a textured-based assessment using The Stanford Mobile Visual Search (SMVS) Data Set <ref type="bibr" target="#b32">[33]</ref>. A preview of the datasets can be seen from the feature matching results of BORDER in <ref type="figure" target="#fig_0">Fig. 12, 13</ref>, and 14 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">OR Rotations and Scale Space Intervals</head><p>Before the comparisons, internal tests were conducted on BORDER to validate its undetermined parameters.</p><p>OR rotations test This first test involves n r , the number of rotation samples (section 3.3) needed for each linelet-OR association. This experiment iterates all the mentioned datasets while increasing n r in multiples of two to record its detection versus rotation samples trend. From the graph in <ref type="figure">Fig. 8</ref>, it can be observed that greater samples of n r offer better detection rates at the expense of increased complexity. However, as rotation numbers climb, saturation starts to occur, thus rendering high samples impractical. Therefore, we deduce that n r = 8 offers the best compromise between detection and complexity rates.</p><p>Scale space test Mentioned in section 3.2, BORDER exploits the OR's scale factor σ for scale invariance. Therefore, it is paramount to uncover the scale tolerance of the OR in order to minimize iterations. For this experiment, we repeat BORDER using the same object for both inputs with the query image resized at 0.05 intervals, while keeping both ORs' σ = 1. We apply this method to all the dataset models and conclude that keeping σ at intervals of 0.25 (e.g. σ = 1, 0.75, 0.5, 0.25) provides the best tolerable scale space coverage. <ref type="figure">Fig. 9</ref> demonstrates the detection degradation as the object is resized. Note that BORDER only scales σ at the query image during actual execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparative Results</head><p>With BORDER's parameters all tested and automated, we proceed with the experimentations. For the competing algorithms such as LINE2D, SIFT, SURF and ORB, they were all implemented in C++ set with their proposed parameters. BOLD on the other hand was realized by the library from their project site <ref type="bibr" target="#b9">[10]</ref>. Apart from ORB, all interest-point detectors including BORDER uses the Euclidean distance-based FLANN-randomized kd-tree for descriptor matching. ORB instead employs the FLANN-LSH matcher as recommended in their paper <ref type="bibr" target="#b31">[32]</ref>. To inscribe further fairness in the findings, all keypoint-point based detectors follow the Hough-voting scheme <ref type="bibr" target="#b0">[1]</ref> for incremental accumulation of the curves. The processors used for the tests runs at 1.7GHz dual-core from the Intel Core i7 Haswell line, with RAM up to 8GBs.</p><p>D-Textureless dataset experiment The first experiment engages the D-Textureless dataset by the creators of BOLD. It contains 9 templates of commonly used tools, accompanied by 55 scenes. Besides being texture-less, this dataset challenges algorithms on properties such as translation, rotation, and up to about 50% scale and occlusion. We line up all participating algorithms including LINE2D, as D-Textureless provides its training data, to obtain the ROC plot as shown in <ref type="figure" target="#fig_0">Fig. 10a</ref>. From the results, we observe that the texture-less based detectors clearly outperform the others, with BORDER able to slightly edge out BOLD to claim top spot. Although the result between the two may be marginal, it is very significant upon analysis. As prior to the experiment, we found that BOLD had already achieved an impressive 86% true positive rate at its default settings on D-Textureless. Therefore, to be able to surpass BOLD in this dataset, truly exemplifies BORDER's robustness in detecting such texture-less objects. Deeper investigations uncover that BORDER performs better in cases shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, where objects have their extensive lines occluded. This can be accredited to linelets for its line breakage resistance, together with BORDER's isolative description methods. CMU-KO8 dataset experiment The next experiment involves the extremely cluttered and occluded CMU Kitchen Occlusion dataset (CMU-KO8). Assembled by Hsiao and Herbert, this dataset was built to evaluate their occlusion reasoning model, which was used in conjunction with LINE2D for enhancing performance under highly occlusive scenes. It models 8 texture-less kitchenwares with object masks provided, together with 100 scene images for each object in both single (8x100) and multi-view situations (8x100). The key challenge this dataset presents is the various challenging levels of clutter and occlusion, set in a texture-less domain. Hence, very little emphasis was placed in scale, rotation and even translation. We consolidated the results of BORDER, BOLD, LINE2D and LINE2D+Occlusion reasoning (rLINE2D, rLINE2D+OPP and rLINE2D+OCLP) in a recall vs. FPPI (False Positives Per Image) scheme as portrayed in <ref type="bibr" target="#b17">[18]</ref> to compare the average detection results of both the single and multi-view cases. Textured-based detectors are not compared because of the lack of keypoint registration for most of the model objects. <ref type="figure" target="#fig_0">Fig. 10b and 10c</ref> shows the performance of BORDER over the others in both the single and multi-view databases respectively. In this case, there is a clearer distinction between the detection rates of BORDER and BOLD, with the former having about a 7% lead over   the latter in both datasets. LINE2D's performance was quite mediocre, but addition of the occlusion reasoning models propelled it to a much more competitive level. Overall, BORDER achieved the best detection rates among the texture-less detectors, as well as obtaining almost similar results against the LINE2D+occlusion reasoning models without any reasoning intervention. Therefore, this establishes BORDER's recognition robustness in heavy occlusion as well. <ref type="figure" target="#fig_0">Fig. 13</ref> exhibits some remarkable feature matching results of BORDER on CMU-KO8. Additionally, the complete BORDER's feature matching results of D-Textureless and CMU-KO8 can be found in our database 1 . SMVS dataset experiment Thus far, with the preeminence of BORDER in both the D-Textureless and the CMU-KO8 datasets, we can safely acknowledge its robustness in detecting texture-less objects. Consequently, to have a complete evaluation in terms of general object detection, we conduct an experiment on a textured database from the SMVS dataset. This dataset contains 8 categories such as book covers, business cards, cd covers, dvd covers, museum paintings, print, landmarks and video frames. Among the 8 we have left out the last two, as they are not object-based. Each remaining category comprises of 100 models for training, and 400 query images for testing respectively. We choose this dataset primarily due to its diversity as it examines algorithms on different textures such as texts, artworks as well as posterized images. <ref type="figure" target="#fig_0">Fig. 11</ref> reports the average ROC results of the detectors used in this dataset, while <ref type="figure" target="#fig_0">Fig. 14</ref> shows some BORDER feature matching results. Note that LINE2D is left out due to insufficient templates for its training. As anticipated from the results, SIFT/SURF completely dominates, while BORDER on the other hand outperforms BOLD. One aspect that texture-less detectors falls short is in high perspective viewpoints situations. This is due to their large region-based detection schemes, which suffers severe degradation in affine changes as seen in <ref type="figure" target="#fig_0">Fig. 14b</ref>.</p><p>Although achieving state-of-the-art detection rates, BORDER's high rotations/samples per linelet ultimately costs its runtime to be the higher than BOLD as shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. It is however quicker than SIFT, and would be multitudes faster after parallel/GPU intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a detector termed BORDER, which combines a regional object encompassment concept with descriptor-based pipelines to recognize texture-less objects in the presence of high clutter and occlusion. The algorithm stabilizes interest-points in the form of linelets and delivers effective descriptor formation with its oriented-rectangle revolution scheme. BORDER is also invariant to scale and rotation which is vital in today's real-world applications. Results from three datasets revealed BORDER's superior recognition rates among the state-of-the-art texture-less detectors, while displaying high competence in textured instances as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>BORDER's texture-less object detection under clutter and occlusion. (Top) input images, (Bottom-left) feature matching and detection, (Bottom-right) multi-sized oriented rectangle matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Stability comparison between line-segment and linelet detection. (Left) LSD detection where a line has been split into two by an overlaid object. (Right) Linelets on the other hand demonstrates better resistance in the presence of occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Automatic OR prototype detection schemes. (Top-row) Undistracted model template. (Middle-row) Salient model template. (Bottom-row) Scene-mask model template.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Revolution sequence (nr = 8) of an associated OR about a linelet midpoint (left) and its flipside (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Rotation sequence of the oriented rectangle (top) and its flipside (bottom) with nr = 8. The rectangle rotates at an incremental 2π/nr per sample. In this case, samples 2 and -4 (flipside) will be chosen for description (highlighted in green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Three samples and a linelet forming pairs and angle primitives. Each αi updates its respective block histogram vector. Blocks are concatenated according to its assigned block number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>BORDER deployed with multi-sized ORs matched with similar r-tag rule. The first match (purple) is at r = 3, the second match (blue) at r = 1, and the third match (yellow) at r =−6. Each linelet's direction is indicated by the green arrow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>Experiment for optimal rotation samples nr. Example of the scale space experiment. (a), (b) and (c) shows resized versions of the query images with object detection remaining valid. (d) demonstrates that at scale more than 0.25, object detection starts to degrade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Texture-less object database experimental results. (a) D-Textureless Tools dataset, (b) and (c) CMU-KO8 Kitchen Occlusion single-view and multi-view datasets respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Experiment results from the textured SMVS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>Recognition results from D-Textureless where BORDER (a), (c) successfully detects, whereas BOLD (b), (d) falls short.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :</head><label>13</label><figDesc>Some BORDER impressive feature matching results from the CMU-KO8 dataset.(a) BORDER's positive feature matching results for textured objects.(b) Failed to detect under high affine change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 :</head><label>14</label><figDesc>BORDER's SMVS dataset textured sample results. Favorable results from these textured objects can be seen from (a), (b) demonstrates a futile attempt to detect under a high angled viewpoint.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.dropbox.com/sh/87trs7j798ottbq/ AADY4bgAor9G5IOAzHcXCTQ0a?dl=0</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surf: speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3951</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Gradient response maps for real-time detection of textureless objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dominant orientation templates for real-time detection of texture-less objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2257" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Occlusion, clutter, and illumination invariant object recognition. Intl Archives of Photogrammetry and Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical chamfer matching: a parametric edge matching algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borgefors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="849" to="865" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape context and chamfer matching in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thayananthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">127</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficiently locating objects using the hausdorff distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Rucklidge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCV</title>
		<meeting>IJCV</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="251" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic target recognition by matching oriented edge pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bold features to detect texture-less objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Franchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L. Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Preliminary development of a line feature-based object recognition system for textureless indoor objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICAR</title>
		<meeting>ICAR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="255" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object recognition in high clutter images using line features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dementhon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1581" to="1588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature pairs connected by lines for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICAR</title>
		<meeting>ICAR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3093" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time learning and detection of 3d texture-less objects: a scalable approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bunnun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="23" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VISAPP</title>
		<meeting>VISAPP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Foresight: fast object recognition using geometric hashing with edge-triple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Illingworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="889" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A cubist approach to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="614" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Occlusion reasoning for object detection under arbitrary viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast directional chamfer matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1696" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Combining scale-space and similarity-based aspect graphs for fast 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1902" to="1914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d object detection and localization using multimodal point pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DIMPVT</title>
		<meeting>3DIMPVT</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Groups of adjacent contour segments for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fevrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="284" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From images to shape models for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="284" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scale-invariant shape features for recognition of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Shape-based recognition of wiry objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Carmichael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1537" to="1552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="509" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lsd: a fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">From gestalt theory to image analysis: a probabilistic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desolneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page">387726357</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lsd: a line segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPOL</title>
		<meeting>IPOL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="35" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Finding minimal enclosing boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orourke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="199" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Saliency filters: contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krhenbhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Orb: an efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The stanford mobile visual search data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reznik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MMSys</title>
		<meeting>MMSys</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="117" to="122" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
