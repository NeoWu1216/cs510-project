<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Region and Multi-label Learning for Facial Action Unit Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaili</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Comm. and Info. Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecom</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Comm. and Info. Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecom</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Region and Multi-label Learning for Facial Action Unit Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Region learning (RL) and multi-label learning (ML)   have recently attracted increasing attentions in the field of facial Action Unit (AU) detection. Knowing that AUs are active on sparse facial regions, RL aims to identify these regions for a better specificity. On the other hand, a strong statistical evidence of AU correlations suggests that ML is a natural way to model the detection task. In this paper, we propose Deep Region and Multi-label Learning (DRML), a unified deep network that simultaneously addresses these two problems. One crucial aspect in DRML is a novel region layer that uses feed-forward functions to induce important facial regions, forcing the learned weights to capture structural information of the face. Our region layer serves as an alternative design between locally connected layers (i.e., confined kernels to individual pixels) and conventional convolution layers (i.e., shared kernels across an entire image). Unlike previous studies that solve RL and ML alternately, DRML by construction addresses both problems, allowing the two seemingly irrelevant problems to interact more directly. The complete network is end-to-end trainable, and automatically learns representations robust to variations inherent within a local region. Experiments on BP4D and DISFA benchmarks show that DRML performs the highest average F1-score and AUC within and across datasets in comparison with alternative methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The face reveals thoughts and feelings. Facial expressions, in particular, tell a person's internal states, psychopathology, and social behavior. Facial Action Unit (AU) detection plays a fundamental role in describing comprehensive facial expressions, and has become an important problem in computer vision. In automated facial AU detection, two problems have attracted an increasing attention: Region Learning (RL) and Multi-label Learning <ref type="bibr">(ML)</ref>. Given the definition that an AU is active only on sparse facial regions, RL aims at identifying specific regions to improve detection performance. For example, AU 12 is re-  ferred as lip corner puller, and by definition is identified only around the region of lip corners. On the other hand, there has been strong statistics showing evidence of correlations between AUs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>. For instance, AUs 6 and 12 have been observed to frequently co-occur in a Duchenne smile. Building upon these AU correlations, ML attempts to jointly learn multiple AUs as one classification problem. However, it remains unclear how these two problems can interact with each other and jointly be solved.</p><p>Recent work on patch learning is a particular example of RL. Conventional patch-based methods divide face images into uniform patches, as shown in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, and then model the importance for each patch as the magnitude of corresponding model parameters (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref>). In general, higher importance implies higher relevance for such patches to a particular AU. The selected patches, due to their spatial dependencies, are shown more effective and robust to noise than individual feature values. Nevertheless, the patches are manually defined and the majority of existing work ignores the relationship among AUs. More recently, multi-label learning showed possibilities of utilizing such AU correlations, e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38]</ref>. These works derive AU correlations from FACS heuristics <ref type="bibr" target="#b4">[5]</ref> or the statistics from ground truth labels, and then plug the AU correlations into learning, encouraging AUs with high correlation to cooccur more frequently. However, these derived AU correlations can be biased due to coder's subjectiveness or vary from one dataset to another.</p><p>In this paper, we propose Deep Region and Multi-label Learning (DRML), a design of neural network that addresses the above issues by construction. <ref type="figure" target="#fig_1">Fig. 1</ref> illustrates our main idea. Instead of learning importance on uniform facial grids as shown in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, DRML propagates contributing value from higher perceptive fields to lower ones. As a result, the more influential "regions" can be discovered, as shown in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>. Due to the multi-label nature of the network, RL and ML can naturally interact with each other within the network, rather than being solved sequentially <ref type="bibr" target="#b38">[39]</ref> or alternatively <ref type="bibr" target="#b37">[38]</ref>. In addition, we introduce a new region layer that serves as an alternative design between locally connected layers (i.e., confined kernels to individual pixels) and conventional convolution layers (i.e., shared kernels across an entire image). The final network is end-to-end trainable, and converges faster with better learned AU relations than alternative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Automated facial AU detection has been a vital research field for objectively describing facial actions. To tackle AU detection under complex conditions, a majority of studies have been devoted to various features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref> and classifiers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. This study is motivated by convolutional neural networks (CNN), and closely related to region learning (RL) and multi-label learning (ML) for AU detection. Below we review each in turn.</p><p>Region learning (RL): Conventional methods for AU detection utilize geometric features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, appearance features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref> or both <ref type="bibr" target="#b2">[3]</ref>. Such features are typically quantified as histograms, losing the specificity about facial regions that are critical to indicate existence of AUs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Region learning has thus attracted an increasing attention. Zhong et al. <ref type="bibr" target="#b38">[39]</ref> and Liu et al. <ref type="bibr" target="#b17">[18]</ref> divided a face image into uniform patches, which are then categorized into common and specific patches to describe different expressions. However, dividing a face image into uniform patches would easily fail on faces with modest or large pose. Taheri et al. <ref type="bibr" target="#b26">[27]</ref> defined regions for different AUs, and proposed a two-layer group sparsity coding to recover facial expressions using the composition rule of AUs. These regions are pre-defined, and thus can not be learned. Since then, learning the region-AU relation and adaptation to viewpoint changes became a rising demand. Recently, Zhao et al. <ref type="bibr" target="#b37">[38]</ref> exploited patches centered at facial landmarks, and pro-posed a multi-label learning framework to infer discriminative patches.</p><p>Multi-label learning (ML): Conventional AU detection methods, such AdaBoost <ref type="bibr" target="#b14">[15]</ref>, GentleBoost <ref type="bibr" target="#b10">[11]</ref>, or linear SVMs <ref type="bibr" target="#b19">[20]</ref>, perform detection on individual AUs. On the other hand, given the prior that the occurrence of AUs are strongly correlated <ref type="bibr" target="#b4">[5]</ref>, multi-label learning (ML) uses the assumption that the correlation exists between labels, so as to improve the detection performance <ref type="bibr" target="#b7">[8]</ref>. In addition, AUs are generally unbalanced, i.e., positive samples are outnumbered by negative ones. ML shows potential to address imbalance data learning <ref type="bibr" target="#b34">[35]</ref>. For studies considering relationships between AUs, Bayesian Networks (BN) <ref type="bibr" target="#b29">[30]</ref> and dynamic BN <ref type="bibr" target="#b31">[32]</ref> have been used to learn AU correlations. Recently, Stefanos et al. <ref type="bibr" target="#b5">[6]</ref> adopted a latent variable CRF to jointly detect multiple AUs. However, they only focus on AUs that co-occur frequently (positive correlation), regardless of the ones that unlikely co-occur (negative competition). Zhao et al. <ref type="bibr" target="#b37">[38]</ref>, instead, statistically derived positive correlations and negative competitions from annotations, and jointly learned multiple AUs using both correlations. Considering pairs or triplets of co-occurring AUs, Zhang et al. <ref type="bibr" target="#b35">[36]</ref> proposed a multi-task learning approach to learn a common kernel representation that describes the AU correlations.</p><p>The aforementioned studies leveraged AU correlations through either FACS heuristic <ref type="bibr" target="#b4">[5]</ref> or the statistics from annotations. Such derived AU relations can, thus, be biased due to subjectiveness or data imbalance and could be less transferable. Instead, DRML by construction learns the AU relations and active regions in a unified way. In addition, compared to the closest work Joint Patch and Multi-label Learning (JPML) <ref type="bibr" target="#b37">[38]</ref>, DRML is an end-to-end trainable and non-linear model, providing a more powerful model to describe AUs under complex conditions. As DRML is inspired by the huge success in convolutional neural networks, we review them below.</p><p>Convolutional neural networks (CNNs): CNNs have drastically improved the performance of vision systems, including face verification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, object detection <ref type="bibr" target="#b13">[14]</ref>, and video tracking <ref type="bibr" target="#b30">[31]</ref>. A standard convolutional layer applies one filter bank to an entire image. For face verification, this spatial stationarity assumption would not hold because different regions have different local statistics for face images <ref type="bibr" target="#b27">[28]</ref>. Considering the details of local regions and intra-personal differences, Taigman <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> proposed a locally connected layer (LCN), confining different filters to each pixel location. An LCN, thus, results in a burden of a large number of parameters. For facial expression analysis, Liu et al. <ref type="bibr" target="#b16">[17]</ref> and Liu et al. <ref type="bibr" target="#b15">[16]</ref> used convolutional models to learn discriminative local regions for holistic expressions. Liu et al. <ref type="bibr" target="#b15">[16]</ref> greedily selected AU-aware receptive regions by iteratively learning feature maps with the highest Aligned face image <ref type="figure" target="#fig_2">Figure 2</ref>. An outline of the proposed DRML architecture. From left to right, a standard convolution layer filtering on an aligned face image, followed by the region layer, one pooling layer and four convolution layers, three fully connected layers, and one multi-label cross-entropy loss layer at the end. Colors illustrate feature maps produced at each layer.</p><p>relevance to the target label, then used RBM for classification; CNNs are only used to extract feature maps. Instead of greedily learning local regions, Liu et al. <ref type="bibr" target="#b16">[17]</ref> proposed an end-to-end framework, utilizing multiple DBNs to learn features with respect to different face regions and strengthening these weak classifiers to top layer in a boosting way. Compared to these models, DRML concentrates on learning discriminative regions. The structural information in local regions is more prominent for AU detection, because AUs depict the local appearance change of faces <ref type="bibr" target="#b4">[5]</ref>.</p><p>Considering dependencies of both local features and AUs correlations, we propose a region layer embedded in DRML. The region layer confines the same filter for each local region, making the weights in the each region updated individually. Meanwhile, as filters are learned for local regions instead of each pixel <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> or an entire image <ref type="bibr" target="#b13">[14]</ref>, the updated parameters stand as an alternative between a locally connected layer and a standard convolutional layer. On the other hand, DRML takes both domain knowledge and computation efficiency into account, resulting in an efficient model with comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Region and Multi-label Learning (DRML)</head><p>A common assumption for standard convolutional layers is the shared kernels, or filters, for an entire image. However, for structured objects like faces, such assumption would fail to capture local (and could thus be subtle) appearance changes. To remedy this limitation and make use of AU correlations, we construct a DRML network, with a newly proposed region layer, for multi-label AU detection. In this section, we first discuss the DRML architecture. Then we illustrate the effectiveness of the region layer on learning important regions for different AUs. Finally, we compare similarities and differences between DRML and alternative methods.  <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Because facial appearance changes of AUs are regional and could be subtle, a rule of thumb is to ensure each layer preserves sufficient facial information from its previous layer. Unlike most expression analysis studies that use small face images as input (e.g., 48×48 in <ref type="bibr" target="#b21">[22]</ref>), we set the input image to 170×170. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, conv7 still maintain a rough face outline to pass to subsequent fully connected layers. Below we detail each layer throughout this network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DRML architecture</head><p>The input is an aligned RGB face image, which is then passed to a convolutional layer (conv1) with 32 filters of size 11×11×3. In this paper, we use the notion 32×11×11×3@160×160. The conv1 layer generates 32 feature maps, which are fed into a region layer (region2). Sec. 3.2 provides more details of the region layer, which outputs 32 160×160 feature maps. Following up is a maxpooling layer (pool3), which takes a max operator over 2×2 spatial neighborhoods with a stride 2, separately for each channels of feature maps from the region layer. Because the input face image could obtain modest head pose, the pool3 layer makes the network more robust to small translation errors caused by face alignment. In DRML, we use only one max-pooling layer to avoid losing too much spatial information. The pool3 layer is followed by another four convolutional layers (conv4∼conv7), performing local abstraction as regular CNNs. Finally, two fully connected layers (fc8 and fc9) are placed on top of the conv layers to capture the global correlations across the entire face images. Note that the number of output AUs are relatively small compared to the 1,000 categories in ImageNet <ref type="bibr" target="#b13">[14]</ref> or 4,300 identities in DeepFace <ref type="bibr" target="#b27">[28]</ref>, we keep fc9 as 2048-D instead of 4096-D. The fc9 layer will be extracted as a feature vector for each image. Let the number of AUs be C, the number of samples be N , the ground truth Y ∈ {−1, 0, 1} N ×C , @160x160x32 conv1</p><p>(2,2) @20x20x32 (7,3) @20x20x32 @20x20x32 @20x20x32 @20x20x32 @20x20x32 <ref type="figure">Figure 3</ref>. An illustration of the proposed region layer. A feature map is inputed from conv1, and uniformly divided into 8×8 patches. Each 20×20-pixel patch (Pj) is applied with a convolution layer. Re-weight each original patch by adding the convolved one. The output of region layer is a concatenation of all re-weighted patches.</p><p>Y ij indicate the (i, j)-th element of Y, and the predictions Y ∈ R N ×C . The output layer was designed as a multi-label sigmoid cross-entropy loss:</p><formula xml:id="formula_0">L(Y, Y) = − 1 N N n=1 C c=1 {[Y nc &gt; 0] log Y nc + [Y nc &lt; 0] log(1 − Y nc )},</formula><p>where [x] is an indicator function returning 1 if the statement x is true, and 0 otherwise. It is noteworthy that our resulting model has about 56 million parameters, which is 7% less than AlexNet <ref type="bibr" target="#b13">[14]</ref> (60 million) and 53% less than DeepFace <ref type="bibr" target="#b27">[28]</ref> (120 million).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Region layer</head><p>One crucial aspect of DRML is to the usage of a region layer that captures local appearance changes for different facial regions. Such regional information has shown to provide unique cues to recognize AUs and holistic expressions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Inspired by these works, we designed a region layer, whose weights are shared only within a local facial region. Below we interpret its construction and effects on detecting facial AUs.</p><p>Most deep learning literature utilize standard convolutional layers to learn image representations (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>), and assume weights are shared across an entire image. However, for face images, the spatial stationarity assumption does not hold: Face is more structured than natural images, and, thus, different facial regions follow different local statistics. Motivated by this observation, the authors of DeepFace <ref type="bibr" target="#b27">[28]</ref> introduced locally connected layers for face verification. The locally connected layers confine each kernel at each pixel location, resulting in performance that closely approaches humans. However, due to its exhaustive nature, such layers cause a huge number of parameters in the network, i.e., &gt;120 million in DeepFace. For the AU detection task in hand, AU annotations are typically insufficient even for contemporary datasets. For example, there are only ∼140,000 frames in BP4D dataset <ref type="bibr" target="#b36">[37]</ref>. Having such a large network could, thus, easily lead to overfitting. <ref type="figure">Fig. 3</ref> depicts the proposed region layer, which contains three components: patch clipping, local convolution, and identity addition. The patch clipping component uniformly slices a 160×160 response map into a 8×8 grid. We enumerated different clipping parameters starting from 5×5, and found 8×8 performed the best for our datasets. Each mini-batch is normalized using Batch Normalization (BN), and passed through ReLU <ref type="bibr" target="#b22">[23]</ref>. A local convolution component learns to capture local appearance changes, forcing the learned weights in each patch to be updated independently. An identity addition component is introduced along with a "skip connection" from the input patch, which helps avoid vanishing gradient issues during training the network <ref type="bibr" target="#b8">[9]</ref>. Imposing the skip connection also simplifies the learning hypothesis: If an input patch contains no useful information for detecting a particular AU, it would be easier to directly forward the patch than learning a filter bank to reduce the patch's effect. As we will see in our experiments, adding this layer helps preserve sparse facial regions activated by particular AUs <ref type="bibr" target="#b4">[5]</ref>.</p><p>What does region layer capture for AU detection?</p><p>Here we illustrate that region layer can induce important facial regions for identifying different AUs. Specifically, we adopt a "saliency map" <ref type="bibr" target="#b25">[26]</ref> to visualize the regions selected by different models with and without a region layer. The saliency map is image-specific, and computed as the <ref type="bibr">AU</ref>   magnitude of per-pixel gradients with respect to a particular AU. We treat such gradient magnitude as the "active region" of a face image. In this way, we are able to investigate the important and active regions for each AU.</p><p>To show the specificity of facial regions learned using the region layer, we compare DRML with a standard Con-vNet (DRML architecture without the region layer) and the AlexNet <ref type="bibr" target="#b13">[14]</ref>. All networks were trained on the BP4D dataset <ref type="bibr" target="#b36">[37]</ref> and used the multi-label sigmoid cross-entropy loss. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the learned active patches of 10 common AUs. For illustration purpose, the face images were selected manually with apparent AUs from the CK+ dataset <ref type="bibr" target="#b19">[20]</ref>. As can be seen, DRML learns a more specific and concentrated regions for the corresponding AUs. Below we summarize our observations: • AUs 1, 2: For AU1, DRML identifies important regions around inner eyebrow, emphasizing the appearance changes by pulling inner eyebrows. On the contrary, ConvNet and AlexNet emphasize some eye regions, but not as concentrated as DRML. For AU2, DRML has a high saliency on the forehead and strong outer brows.</p><p>The presence with slight saliency on inner brows indicates its likely co-occurrence with AU1. ConvNet marks some lower face and AlexNet fail to concentrate. • AUs 6, 7: For AU6 (check raiser), DRML identifies the center of the mouth due to the strong positive correlation of AU6 and AU12 <ref type="bibr" target="#b37">[38]</ref>. For AU7, DRML gives much more importance on eyelids than the other identified regions. ConvNet also identifies mouth regions for AU6, but check regions for AU7. AlexNet fails to identify saliency for AU6 and AU7. • AU12: AU12 depicts lip corner puller, commonly seen in smiley face. DRML concentrates more on the teeth and slight on the eyes and cheek. ConvNet not only identified the mouth but also some chin regions. AlexNet fails to identify meaningful regions for AU12.</p><p>• AU14: AU14 is dimpler, causing appearance changes of mouth corners. DRML emphasizes the regions around nose; ConvNet regions around mouth. AlexNet fails to identify the subtle appearance changes of AU14. • AUs 15, 17: AU15 and AU17 depict lip corner depressor and chin raiser, which both could cause appearance changes around lower mouth. We observe that DRML is able to concentrate salient regions on lower mouth for AU15 and AU17. ConvNet emphasizes regions around mouth and some regions on the upper face. AlexNet shows saliency over the whole face for AU15 and AU17. • AUs 23, 24: AUs 23 and 24 depict lip tightener and lip pressor. DRML identifies strong saliency around mouth, while ConvNet emphasizes on regions of both mouth and the upper face. AlexNet fails these two AUs. In all, DRML identifies concentrated and sparse regions than alternative methods. These identified regions also coincide with the important patches in JPML <ref type="bibr" target="#b37">[38]</ref>. We found AlexNet consistently fails to identify specific active regions. One reason is the per-pixel contribution of gradient could look like salt-and-pepper noise. Adding further regularization (e.g., <ref type="bibr" target="#b32">[33]</ref>) might help the visualization. Recall that ConvNet is a special case of DRML without the region layer. From this perspective, adding the region layer can be regarded as an regularizer that helps reveal the sparse and discriminative regions. We thus infer that the architecture constructed in this paper is more suitable for AU detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with related work</head><p>DRML shares similarities with patch-based methods for AU detection, i.e., Active Patch Learning (APL) <ref type="bibr" target="#b38">[39]</ref>, JPML <ref type="bibr" target="#b37">[38]</ref>, AUDN <ref type="bibr" target="#b15">[16]</ref>, and Boosted DBN (BDBN) <ref type="bibr" target="#b16">[17]</ref>. All aims to select a discriminative subset of facial regions for better AU detection. However, they differ in several aspects. <ref type="table" target="#tab_3">Table 1</ref> summarizes these differences. APL <ref type="bibr" target="#b38">[39]</ref> selects patches for different expression by inducing sparsity on "groups", which are defined over uniform patches. Different from expression recognition, AU is a multi-label learning problem. In view of dependencies among features and AUs, JPML <ref type="bibr" target="#b37">[38]</ref> jointly learns discriminative patches for multiple AUs. The proposed DRML is inspired by JPML. However, they bear several differences. First, JPML defines AU relations through dataset statistics; DRML by construction learns correlations among AUs. Second, JPML uses manually-crafted feature (i.e., SIFT); DRML learns the features that adapts to multi-label AU detection. Third, JPML learns the PL and ML alternatively; DRML naturally fuses two tasks into one framework, allowing ML and PL to interact more directly. Finally, JPML is linear; DRML stacks non-linear functions that potentially better models the non-linearity of facial AUs.</p><p>Learning representations from raw face images is another crucial property of DRML. AUDN <ref type="bibr" target="#b15">[16]</ref> and BDBN <ref type="bibr" target="#b16">[17]</ref> also have this property for expression recognition. AUDN <ref type="bibr" target="#b15">[16]</ref> sequentially combined three modules that respectively learn expression-specific representation, search subset of the representation that best simulates an AU, and concatenate the subset for recognition. However, these three modules are trained independently; DRML is endto-end trainable. BDBN <ref type="bibr" target="#b16">[17]</ref> integrated feature learning, patch selection and classifier construction into one end-toend trainable framework. Each patch is associated with one DBN. The selection process was done by forming the DBNs as a strong boosted classifier. However, building a network for each patch can be very expensive. Instead, DRML performs the selection through a region layer, containing much smaller units and an identity connection that allows more direct gradient flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Datasets: We evaluated DRML on two spontaneous datasets: BP4D <ref type="bibr" target="#b36">[37]</ref> and DISFA <ref type="bibr" target="#b20">[21]</ref>. For BP4D, we adopted a 3-fold partition to ensure subjects were mutually exclusive in train/val/test sets. For DISFA, we reported results using the best model obtained from BP4D.</p><p>(1) BP4D <ref type="bibr" target="#b36">[37]</ref> contains 2D and 3D videos of 41 young adults during various emotion inductions while interact-ing with an experimenter. We used 328 videos (41 participants×8 videos each) with 10 AUs coded, resulting in ∼140,000 valid face images. For each AU, we sampled 100 positive frames and 200 negative frames for each video.</p><p>(2) DISFA <ref type="bibr" target="#b20">[21]</ref> contains 27 subjects watching video clips, and provides 8 AU annotations with intensities. There were ∼130,000 valid face images. We used the frames with AU intensities with C-level or higher as positive samples, and the rest as negative ones. To be consistent with the 8video setting of BP4D, we sampled 800 positive frames and 1600 negative frames for each video.</p><p>Metrics: The performance was evaluated on two common frame-based metrics: F1-frame and AUC. F1-frame is the harmonic mean of precision and recall, and widely used in AU detection. AUC quantifies the relation between true and false positives. For each method, we computed average metrics over all AUs (denoted as Avg.).</p><p>Implementation: We registered face images to 200×200 using similarity transform <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38]</ref>. Each face was randomly cropped into 170×170, or horizontally mirrored for data augmentation. All models were initialized with learning rate of 0.001, which was further reduced after 8000 iterations. A momentum of 0.9 and weight decay of 0.0005 was used. All implementations were based on the Caffe toolbox <ref type="bibr" target="#b11">[12]</ref> with modifications to support the region layer and multi-label cross-entropy loss. All experiments were performed on one NVIDIA Tesla K40c GPU. Our implementation is available online 1 .</p><p>Comparative methods: We compared DRML to alternative methods, including a baseline linear SVM (LSVM) <ref type="bibr" target="#b6">[7]</ref>, a patch-learning method APL <ref type="bibr" target="#b38">[39]</ref>, and the state-ofthe-art Joint Patch and Multi-label Learning (JPML) <ref type="bibr" target="#b37">[38]</ref>. For baseline networks, we compared to AlexNet <ref type="bibr" target="#b13">[14]</ref>, Con-vNet (DRML excluding the region layer), and LCN (Con-vNet with locally connected layer <ref type="bibr" target="#b27">[28]</ref>). These alternative approaches were picked to answer several questions: (1) whether the learned features are more descriptive than handcrafted ones, (2) whether using AU correlations and/or region layer improves the performance of AU detection, <ref type="bibr" target="#b2">(3)</ref> whether, compared to JPML, the proposed DRML provides a more direct and effective way to jointly learn RL and ML. We excluded JPML in the DISFA experiments due to the lack of reported AU correlations; instead we only reported APL. Below we discuss the results. <ref type="table" target="#tab_4">Tables 2 and 3</ref> show the results of 12 AUs for BP4D and 8 AUs for DISFA, respectively. Below we discuss the results from five perspectives: feature representation, multilabel learning, effects of region layer, joint learning of regions and multi-label, and running time. Feature representation: This paragraph discusses the benefits of the learned features. Comparing the results of AlexNet and LSVM in <ref type="table" target="#tab_4">Table 2</ref>, 9 out of 12 AUs in F1-frame and 12 out of 12 AUs in AUC are higher for AlexNet. The improvement of AlexNet became larger on DISFA, showing its better generalizability in a cross-dataset scenario. Recall that the results on DISFA were reported using a cross-dataset protocol, i.e., we selected the best performing model from BP4D to report the results. As shown in <ref type="table">Table 3</ref>, compared to LSVM, AlexNet achieved about 2% and 13% higher F1-frame and AUC. It is worth noticing that the feature dimensions for LSVM, AlexNet, LCN, DRML are 6272 (128 SIFT features for 49 landmarks), 4096, 2048, and 2048, respectively. In fact, even though the learned features are of lower dimension, more than 40% of learned features for AlexNet, LCN, and DRML, are zeros. We can infer that the learned features, compared to the best performing handcrafted SIFT feature <ref type="bibr" target="#b39">[40]</ref>, capture more discriminative and sparse characteristics for detecting AUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Multi-label learning: Multi-label learning could improve AU detection by taking AU correlations into account. In our experiments, this improvement is more obvious for highly skewed AUs, given the skewness factor defined as the ratio of the number of negative samples to the number of positive samples. Take the BP4D dataset for example. The skewness for frequently occurring AUs <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b11">12)</ref> are (0.7, 0.8) respectively; for infrequently occurring AUs <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24)</ref>, their skewness are (3.8, 4.9, 5.0, 5.5). Compared to the baseline LSVM results, the improvement of methods using multi-label learning (i.e., AlexNet, ConvNet, LCN, and DRML) improves more on AUs with larger skewness. For instance, for both the F1-frame and AUC, the performance of these methods on AUs (1,2,23,24) are 1.5 to 1.8 times higher than the baseline LSVM. That being said, when the training data are relatively rare, multi-label learning helps reduce the effects of the imbalance nature for AU detection. Regardless of the overall improvement across 12 AUs, it is noticeable that, for AUs <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b11">12)</ref>, the LSVM baseline achieved satisfactory performance. One possible explanation is that AUs 10 and 12 have relatively abundant training samples compared to other AUs.</p><p>Region layer: This paragraph discusses the effectiveness of the region layer. Observing the results of ConvNet and LCN in <ref type="table" target="#tab_4">Table 2</ref>, LCN reached higher F1-frame in 11 out of 12 AUs and higher AUC in 7 out of 12 AUs. It validates the observation that LCN learns more discriminative information of face regions than a standard convolutional layer as ConvNet <ref type="bibr" target="#b27">[28]</ref>. In BP4D, DRML outperformed LCN in 6 out 12 AUs for F1-Frame, and 8 out 12 AUs for AUC. In DISFA <ref type="table">(Table 3)</ref>, on average, DRML outperformed LCN with 11.3% higher in F1-frame and 11.7% higher in AUC. This justifies that, compared to LCN applied to individual pixels, the region layer better expresses the structural information in local facial regions. Recall that ConvNet is a special case of DRML without the region layer, we confirmed the effectiveness of DRML. Qualitatively, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the saliency maps of DRML show better specificity than alternative models. All results suggest that the proposed region layer helps AU detection by considering structural information in facial regions.</p><p>Joint learning of regions and multi-label: To better understand the effects of a joint learning framework, we compared the proposed DRML with an AlexNet <ref type="bibr" target="#b13">[14]</ref>. Both networks were trained on the BP4D dataset using 12 AUs. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the convergence curves and learned relation tables of both models. As shown in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>, DRML convergences faster than AlexNet and obtains lower training loss. <ref type="figure" target="#fig_4">Fig. 5(b)-(d)</ref> show the table of correlation coefficients between pairwise AUs for ground truth, DRML and AlexNet, respectively. The element-wise Euclidean distance between DRML and ground truth is 0.0068, while it is 0.0077 for AlexNet. This shows that DRML was able to learn AU relations close to ground truth statistics. In addition, we compared DRML with the state-of-the-art JPML <ref type="bibr" target="#b37">[38]</ref>. Note that one difference is that JPML <ref type="bibr" target="#b37">[38]</ref> reported their results using a 10-split partition, while, for fairness, we implemented JPML using a 3-split partition. Observing the results in <ref type="table" target="#tab_4">Table 2</ref>, on average, DRML achieved 5.0% higher in F1-frame and 11.7% higher in AUC. The results suggest that the direct interaction between RL and ML, along with the non- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>An illustration of (a) a conventional patch-based method, and (b) the proposed DRML. DRML by construction models both important facial regions and relations among multiple AUs, showing a better capability of localization and classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>shows the outline of the proposed DRML architecture. The principle of designing this network is inspired by the networks for face verification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of AU-specific saliency maps for three networks: DRML (second row), ConvNet (third row), and AlexNet<ref type="bibr" target="#b13">[14]</ref> (bottom row). The top row illustrates the appearance of 10 AUs. Colors on each map indicate saliency intensity from low high.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Comparison between DRML and AlexNet<ref type="bibr" target="#b13">[14]</ref>: (a) training loss, (b)-(d) relation tables, where each entry (i, j) is computed as the coefficient correlation between the i-th and the j-th AUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Comparisons between DRML and alternative methods ET: end-to-end trainable, ML: multi-label learning, LR: learning representation, NL: non-linearity, OU: online update.</figDesc><table>Methods 
ET 
ML 
LR 
NL 
OU 

APL [39] 
× 
× 
× 
× 
× 
AUDN [16] 
× 
× 
× 
BDBN [17] 
× 
JPML [38] 
× 
× 
× 
× 
DRML 

*</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Results on the BP4D dataset. Bracketed and bold numbers indicate the best performance; bold numbers indicate the second best.</figDesc><table>F1-frame 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/zkl20061823</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: We thank Prof. Shiguang Shan for initializing this work. Prof. Shan is with VIPL group of Institute of Computing Technology Chinese Academy of Sciences, where most of this work was done. We also thank Jayakorn Vongkulbhisal for helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Avg. <ref type="bibr" target="#b34">35</ref> linearity, bring more advantages to DRML over JPML.</p><p>Running time: We evaluated the running speed of DRML and alternative networks using a NVIDIA Tesla K40c GPU. <ref type="table">Table 4</ref> shows the execution time (ms) for both training and test phases. Specifically, using the same settings as described in Sec. 4.1, we ran each network for 20 trials over 1,000 iterations, evaluated the running time for each iteration, and then computed the mean and standard deviation over the 20 trials. Because DRML serves an alternative architecture between ConvNet and LCN, both training and test time of DRML falls between them. Note that DRML is significantly faster than LCN, which was proposed for face verification <ref type="bibr" target="#b27">[28]</ref>. It is worth noticing that, even ConvNet has slightly smaller number of parameters than AlexNet, the computation complexity could vary, causing the running time of ConvNet slightly larger. In particular, the 11×11 filters in conv1 lead to the major FLOP (multiply-adds) operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents Deep Region and Multi-label Learning (DRML) for facial AU detection. DRML is a unified architecture for AU detection, and allows two seemingly irrelevant tasks, region learning (RL) and multi-label learning (ML), to interact directly. DRML is end-to-end trainable, and able to identify more specific regions for different AUs than conventional patch-based methods. To this end, we introduce a region layer that uses feed-forward functions to capture structural information in different facial regions. Experiments conducted on within-and across-dataset scenarios manifest the effectiveness of DRML. Future work includes imposing group sparsity loss into the objective of DRML to learn sparser facial regions. The proposed region layer introduces potential applications to more structured objects, such as cats, cars, and pedestrians.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic recognition of facial actions in spontaneous expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lainscsek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="22" to="35" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facial action unit event detection by cascade of tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="1454" to="1462" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Facs manual. A Human Face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiconditional latent variable model for joint facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep convolutional ranking for multilabel image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno>abs/1312.4894</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to combine local models for facial AU detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AFGR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action unit detection using sparse appearance descriptors in space-time video volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AFGR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamics of facial expression extracted automatically from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="615" to="625" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AU-aware deep networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AFGR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted DBN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature disentangling machine-a novel approach of feature selection and disentangling in facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Investigating spontaneous facial action recognition through AAM representations of the face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>INTECH Open Access Publisher</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The extended cohn-kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TAFFC</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust representation and recognition of facial emotions using extreme sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shojaeilangari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2140" to="2152" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human facial expression recognition using stepwise linear discriminant analysis and hidden conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1386" to="1398" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structure-preserving sparse decomposition for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3590" to="3603" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning bayesian networks with qualitative constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeung</forename><surname>Dit-Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Capturing global semantic relationships for facial AU recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Confidence preserving machine for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards classimbalance aware multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Task-dependent multi-task multiple kernel learning for facial au detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AFGR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning multiscale active facial patches for expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic cascades with bidirectional bootstrapping for AU detection in spontaneous facial behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TAFFC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="91" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
