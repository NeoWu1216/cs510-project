<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When VLAD met Hilbert</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NICTA * and Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NICTA * and Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When VLAD met Hilbert</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In many challenging visual recognition tasks where training data is limited, Vectors of Locally Aggregated</head> Descriptors (VLAD)  <p>have emerged as powerful image/video representations that compete with or outperform state-ofthe-art approaches. In this paper, we address two fundamental limitations of VLAD: its requirement for the local descriptors to have vector form and its restriction to linear classifiers due to its high-dimensionality. To this end, we introduce a kernelized version of VLAD. This not only lets us inherently exploit more sophisticated classification schemes, but also enables us to efficiently aggregate nonvector descriptors (e.g., manifold-valued data) in the VLAD framework. Furthermore, we propose an approximate formulation that allows us to accelerate the coding process while still benefiting from the properties of kernel VLAD. Our experiments demonstrate the effectiveness of our approach at handling manifold-valued data, such as covariance descriptors, on several classification tasks. Our results also evidence the benefits of our nonlinear VLAD descriptors against the linear ones in Euclidean space using several standard benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper introduces a nonlinear formulation of Vectors of Locally Aggregated Descriptors (VLAD) that generalizes their use to manifold-valued local descriptors, such as symmetric positive definite (SPD) matrices, and allows them to inherently exploit more sophisticated classification algorithms. Modern visual recognition techniques typically represent images by aggregating local descriptors, which, compared to image intensity, provide robustness to varying imaging conditions. From a historical point of view, this trend gained momentum since the introduction of the Bag-of-Words (BoW) model <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>, which had a significant impact on recognition performance. Notable recent developments include dictionary-based solutions <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>, Fisher Vectors (FV) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>, VLAD <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1]</ref> and Convolutional Neural Networks (CNN) <ref type="bibr" target="#b22">[23]</ref>.</p><p>Among the aforementioned techniques, VLAD stands out for the following reasons:</p><p>• VLAD is computed via primitive operations. This makes VLAD extremely attractive when computational complexity is a concern, and requires virtually no parameter-tuning, except the size of the codebook. • In contrast to CNNs, training a VLAD encoder is straightforward and not contingent on having a large training set. • VLAD can be considered as a special case of FVs and hence inherits several of their properties. The most eminent one is its theoretical connection to the Fisher kernel <ref type="bibr" target="#b18">[19]</ref>. • From an empirical point of view, VLAD has been shown to either deliver state-of-the-art accuracy, or compete with the state-of-the-art methods when training data is limited. For instance, for scene classification on the MIT Indoor dataset, multi-scale VLAD, with only 4096 features, comfortably outperforms the mixture of FV and bag-of-parts, which relies on 221550 features <ref type="bibr" target="#b12">[13]</ref>.</p><p>Despite its unique properties, VLAD comes with its own limitations. In particular, VLAD is designed to work with local descriptors in the form of vectors. Yet, several recent studies in computer vision suggest that structural data (e.g., SPD matrices <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>, graphs <ref type="bibr" target="#b45">[46]</ref>, orthogonal matrices <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>) have the potential to provide more robust descriptors. Furthermore, since VLAD typically yields a highdimensional image representation, it is mostly restricted to be employed with linear classifiers. The effectiveness of kernel-based methods, however, has been proven many a time in visual recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>In this paper, we introduce a nonlinear formulation of VLAD that addresses the aforementioned shortcomings. In particular, we first derive a kernelized version of VLAD that relies on mapping each local descriptor to a Reproducing Kernel Hilbert Space (RKHS). We then show that aggregation can be performed in the RKHS, which only involves computing kernel values. Since several valid kernel functions have recently been defined for non-vector data <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>, our formulation ultimately generalizes the use of VLAD to non-vector spaces, such as the SPD manifold and the Grassmannian (the manifold of linear subspaces). Furthermore, the inherent nonlinearity of mappings to RKHS effectively translates to exploiting more advanced classifiers within the VLAD framework.</p><p>In the spirit of computational efficiency, we then introduce a novel nonlinear approximation to our kernel VLAD, which makes use of local subspaces in the Hilbert space. This approximation enjoys properties similar to those of kernel VLAD, yet has the additional benefit of providing us with faster coding schemes. Importantly, both kernel VLAD and its approximation essentially preserve the simplicity of VLAD, in the sense that the extra computations merely consist of kernel evaluations potentially followed by projections (i.e., matrix multiplications). To give a concrete example, for head-pose estimation with manifold-valued data (see Section 4), our nonlinear approximation of kernel VLAD not only outperforms the Riemannian version of VLAD <ref type="bibr" target="#b10">[11]</ref>, but also encodes images 10 times faster.</p><p>Our experimental evaluation demonstrates the effectiveness of our approach at handling manifold-valued data in a VLAD framework. Furthermore, we evidence the benefits of exploiting nonlinear classifiers for visual recognition by comparing the performance of our nonlinear VLAD with the standard one on several benchmark datasets, where the local descriptors have a vector form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Most of the popular image classification methods extract local descriptors at patch level, and aggregate these descriptors into a global image representation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1]</ref>. When large amounts of training data are available, CNNs have now emerged as the method of choice to learn local descriptors. With a limited number of training samples, existing methods typically opt for handcrafted features, such as SIFT.</p><p>To aggregate local features, in addition to operations such as average-pooling and max-pooling, histogram-based solutions (e.g., BoW) have proven successful. Going beyond simple histograms has been an active topic of research in the past decade. For instance, <ref type="bibr" target="#b23">[24]</ref> aggregates histograms computed over different spatial regions. More recent developments, such as FVs <ref type="bibr" target="#b29">[30]</ref> and VLAD <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9]</ref>, suggest that first-and potentially second-order statistics should be encoded in the aggregation process.</p><p>In a separate line of research, structured descriptors (e.g., covariance descriptors, linear subspaces) have been shown to provide robust visual models <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref>. Being of a non-vectorial form, aggregating such descriptors is hard to achieve beyond simple histograms. Nonetheless, one would like to benefit from the best of both worlds, that is, using robust non-vectorial descriptors in conjunction with stateof-the-art aggregation techniques, such as VLAD. This, in essence, is what we propose to achieve in this paper via kernelization. Furthermore, our approach has the additional advantage of allowing us to inherently exploit nonlinear classifiers that have proven powerful for visual recognition.</p><p>The recent work of Faraki et al. <ref type="bibr" target="#b10">[11]</ref> also aims at extending the VLAD framework to non-vector data. Specifically, <ref type="bibr" target="#b10">[11]</ref> makes use of the tangent bundle of a Riemannian manifold to aggregate manifold-valued data in a similar manner as VLAD. By contrast, our work is not limited to Riemannian manifolds. That is, while in <ref type="bibr" target="#b10">[11]</ref> the data must lie on a Riemannian manifold, we only require the existence of a positive definite kernel defined over the data. For instance, our framework therefore also applies to local descriptors represented as graphs, thanks to the available kernel of, e.g., <ref type="bibr" target="#b45">[46]</ref>. Furthermore, several studies suggest that embedding Riemannian manifolds into RKHS boosts recognition performance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. As a matter of fact, this is also demonstrated by our experiments, where our approach outperforms the method of <ref type="bibr" target="#b10">[11]</ref>.</p><p>While a full review of kernel-based methods in computer vision is beyond the scope of this paper, the recent work of <ref type="bibr" target="#b25">[26]</ref> is of particular relevance here. <ref type="bibr" target="#b25">[26]</ref> introduces an approach to employing kernels within a CNN framework. Here, we perform a similar analysis within the VLAD framework, with the additional benefit of obtaining a representation that lets us work with manifold-valued data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Nonlinear VLAD</head><p>In this section, after briefly reviewing the conventional VLAD, we derive our two nonlinear VLAD formulations: kernel VLAD and its local subspace-based approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conventional VLAD</head><formula xml:id="formula_0">Let X = {x i } N i=1 , x i ∈ R d</formula><p>be a set of local descriptors extracted from a query image or a video. In VLAD <ref type="bibr" target="#b20">[21]</ref>, the input space R d is partitioned into m Voronoi cells by means of a codebook C with centers {c j } m j=1 , c j ∈ R d , obtained from training data. Typically, this codebook is computed using the k-means algorithm. Note that supervised algorithms have also recently been employed to build more discriminative codebooks <ref type="bibr" target="#b27">[28]</ref>. In any event, given the codebook, the VLAD code v ∈ R md for the query set X is obtained by concatenating m Local Difference Vectors (LDV) δ j storing, for each center, the sum of the differences between this center and each local descriptor assigned to it. This can be written as</p><formula xml:id="formula_1">v(X ) = δ T 1 (X ), δ T 2 (X ), · · · , δ T m (X ) T ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">δ j (X ) = N i=1 a i j c j − x i ,<label>(2)</label></formula><p>with a i j a binary weight encoding whether the local descriptor x i belongs to the Voronoi cell with center c j or not, i.e., a i j = 1 if and only if the closest codeword to x i is c j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Kernel VLAD (kVLAD)</head><p>As mentioned earlier, the conventional VLAD is designed to work with local descriptors of vector form. As such, it cannot handle structured data representations, such as SPD matrices, or subspaces. While such representations could in principle be vectorized, this would (i) yield impractically high-dimensional VLAD vectors; and (ii) ignore the geometry of these structured representations, which has been demonstrated to result in accuracy loss <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b19">20]</ref>. Here, we address this problem by kernelizing VLAD.</p><p>To this end, let us redefine the query set of local descriptors as</p><formula xml:id="formula_3">X = {x i } N i=1 , x i ∈ X,</formula><p>where each descriptor lies in the space X, which, in contrast to VLAD, is not restricted to be R d . In fact, the only constraint we impose is that X comes with a valid positive definite (pd) kernel k : X×X → R. For example, X could be the space of SPD matrices, with the Gaussian kernel defined in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b19">20]</ref>. According to the Moore-Aronszajn Theorem <ref type="bibr" target="#b1">[2]</ref>, a pd kernel k(·, ·) induces a unique Hilbert space on X, denoted hereafter by H, with the property that there exists a mapping φ :</p><formula xml:id="formula_4">X → H, such that k(x, y) = φ(x), φ(y) H = φ(x) T φ(y).</formula><p>Here, we propose to make use of this property to map the local descriptors to H, which is a vector space, and perform a VLADlike aggregation in Hilbert space. The main difficulty arises from the fact that H may be infinite-dimensional, and, more importantly, that the mapping φ corresponding to a given kernel k is typically unknown.</p><p>Let us suppose that we are given a codebook</p><formula xml:id="formula_5">C = {φ(c i )} m i=1 in H.</formula><p>For instance, this codebook can be computed using kernel k-means. To compute a VLAD code in H, we need means to perform the following operations:</p><formula xml:id="formula_6">1. Determine the assignments {a i j } in H.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Express the LDVs in H.</head><p>To determine the assignments, we note that</p><formula xml:id="formula_7">φ(x) − φ(y) 2 = k(x, x) − 2k(x, y) + k(y, y) . (3)</formula><p>Therefore, for each local descriptor, the nearest codeword can be found using kernel values only, i.e., without having to know the mapping φ, which lets us directly determine the assignments.</p><p>Unfortunately, expressing the LDVs in H is not as straightforward. Clearly, the form of the LDVs, given by</p><formula xml:id="formula_8">δ j (X ) = a i j φ(c j ) − φ(x i ) ,</formula><p>with a i j obtained using Eq. 3, cannot be computed explicitly if the mapping φ is unknown, which is typically the case for popular kernels, such as RBF kernels. However, in most practical applications, the VLAD vector is not important by itself; What really matters for visual recognition is a notion of distance between two VLAD vectors. We therefore turn to the problem of computing the distance of two VLAD vectors in Hilbert space.</p><p>To this end, let</p><formula xml:id="formula_9">X = {x i } N X i=1 , x i ∈ X and Y = {y i } N Y i=1 , y i ∈ X be two sets of local descriptors. The im- plicit VLAD code of X in H can be expressed as v H (X ) = δ T 1 (X ), δ T 2 (X ), · · · , δ T m (X ) T , and similarly for v H (Y). Now, we have vH(X ), vH(Y) H = m s=1 δ T s (X )δs(Y) (4) = m s=1 N X i=1 N Y j=1 a i s a j s φ(cs) − φ(xi) T φ(cs) − φ(y j ) = m s=1 N X i=1 N Y j=1</formula><p>a i s a j s k(xi, y j )+k(cs, cs)−k(xi, cs)−k(y j , cs) , which again only depends on kernel values. With this inner product, a linear SVM, in its dual form, can directly be used for classification <ref type="bibr" target="#b0">1</ref> . In our experiments, we rely on this approach, which we refer to as kernel VLAD or kVLAD for short.</p><p>This inner product, however, also allows us to employ an RBF-based kernel SVM, since</p><formula xml:id="formula_10">v H (X ) − v H (Y) 2 = v H (X ), v H (X ) H − 2 v H (X ), v H (Y) H + v H (Y), v H (Y) H .</formula><p>Note that this essentially yields two layers of kernels, i.e., the RBF kernel of the SVM makes use of the distance, which itself is expressed in terms of kernel values.</p><p>While effective in practice, our kVLAD algorithm, as any kernel method, becomes computationally expensive when dealing with large datasets. In the remainder of this section, we therefore introduce an approximation to kVLAD that addresses this limitation while still benefiting from the nice properties of kVLAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Nonlinear VLAD via Local Subspaces (sVLAD)</head><p>Here, we introduce a nonlinear formulation of VLAD that approximates our kVLAD algorithm. To this end, we propose to make use of local subspaces to derive a novel approximation of a Hilbert space H. This approximation is motivated by the following observation: By looking at Eq. 2, we can see that the contribution of each codeword in the VLAD vector is independent of the other codewords, particularly since each local descriptor is assigned to a single codeword. As such, there is no reason for the approximation of H to be shared across all the codewords and descriptors. We therefore propose to define approximate Hilbert spaces for each codeword individually.</p><p>To this end, let {t s,j } Ns j=1 be the set of training samples that generate the codeword c s . In other words, as in the conventional VLAD where c s = 1</p><formula xml:id="formula_11">Ns j t s,j , we have φ(c s ) = 1</formula><p>Ns i φ(t s,j ). While, due to the unknown nature of φ, such a codeword cannot be explicitly computed, we can still evaluate the kernel function at this codeword, since</p><formula xml:id="formula_12">k(x, c s ) = φ(x) T φ(c s ) = 1 N s j k(x, t s,j ) .</formula><p>Here, we therefore propose to exploit the subspaces spanned by the training samples associated to each individual codeword to obtain an approximate representation of H. More specifically, let S s = span({φ(t s,j )} Ns j=1 ). We then define</p><formula xml:id="formula_13">δ s (X ) = N i=1 a i s π s φ(c s ) − π s φ(x i ) ,<label>(5)</label></formula><p>with π s : H → S s the projection onto S s . This projection can be obtained as follows. Let K s be the kernel matrix estimated from the training samples generating</p><formula xml:id="formula_14">c s , i.e., [K s ] i,j = k(t s,i , t s,j ). By eigendecomposition, we can write K s = U s Λ s U T s . Then, Φ s U s Λ −1/2 s , with Φ s = [φ(t s,1 ), · · · , φ(t s,Ns )]</formula><p>, forms a basis for S s . As such, we can write</p><formula xml:id="formula_15">π s (x) = Λ −1/2 s U s k(x, t s,1 ), · · · , k(x, t s,Ns ) . (6)</formula><p>The LDVs δ s (X ) can then be obtained for all codewords, and concatenated to form the final sVLAD representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Normalization</head><p>Recent studies have shown that the discriminative power of VLAD can be boosted by additional post-processing steps, such as ℓ 2 power normalization and signed square rooting normalization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. The ℓ 2 power normalization, where each block in VLAD is normalized individually, can easily be performed in kVLAD, since</p><formula xml:id="formula_16">δs(X) 2 H = N X i,j=1 a i s a j s k(xi, xj)+k(cs, cs)−k(xi, cs)−k(xj, cs)</formula><p>only depends on kernel values. As a result, the inner product of Eq. 4 after normalizing each VLAD block indepen-</p><formula xml:id="formula_17">dently, i.e., v H (X),v H (Y) H = k s=1 δ s (X), δ s (Y) δ s (X) H δ s (Y) H ,</formula><p>will also only depend on kernel values. By contrast, however, the signed square rooting normalization can only be achieved when explicit forms of the descriptors are available, i.e., in sVLAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Further Discussions</head><p>Our sVLAD formulation makes use of several local approximations of a Hilbert space. Other approaches have been proposed in the past to speed up kernel methods via Hilbert space approximations. Note, however, that these techniques yield one global approximation of the Hilbert space. In particular, the Nyström approximation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b30">31]</ref> makes use of data to approximate the kernel values. Furthermore, other methods approximate the inner product of specific kernel functions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>. For the sake of completeness, below, we derive approximations to kVLAD based on the aforementioned techniques. Note that our experiments demonstrate the benefit of our sVLAD formulation over these other approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Nyström Approximation (nVLAD)</head><p>We start by deriving an approximation to kVLAD via the Nyström method <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b30">31]</ref>. Such an approximation yields an explicit form for the mapping φ to the Hilbert space H, and thus allows us to approximate a given kernel.</p><p>More specifically, let T = {t i } M i=1 , t i ∈ X be a collection of M training examples, and let K be the corresponding kernel matrix, i.e., <ref type="bibr">[K]</ref> i,j = k(t i , t j ). We seek to approximate the elements of K as inner products between r-dimensional vectors. In other words, we aim to find a matrix Z ∈ R r×M , such that K ≃ Z T Z. The best such approximation in the least-squares sense is given by Z = Σ 1/2 V , with Σ and V the top r eigenvalues and corresponding eigenvectors of K. From the Nyström method, for a new sample x ∈ X, the r-dimensional vector representation of the space induced by k(x, ·) can be written as</p><formula xml:id="formula_18">z N (x) = Σ −1/2 V k(x, t 1 ), · · · , k(x, t M ) T . (7)</formula><p>Given a set of local descriptors X = {x i }, our nVLAD algorithm then consists of computing the corresponding {z N (x i )}, and making use of Eq. 1 and Eq. 2 with this new representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fourier Approximation (fVLAD)</head><p>The previous approximation applies to general kernels defined on both Euclidean and non-Euclidean data. In the Euclidean case, however, other approximations have been proposed for specific kernels <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>. Since our experiments on Euclidean data all rely on RBF kernels, here, we discuss an approximation of this type of kernels based on the Bochner Theorem <ref type="bibr" target="#b33">[34]</ref>.</p><p>According to the Bochner Theorem <ref type="bibr" target="#b33">[34]</ref>, a shiftinvariant kernel 2 , such as the Euclidean RBF kernel, can be expressed with the Fourier integral. As shown in <ref type="bibr" target="#b32">[33]</ref>, for real-valued kernels, this can be written as</p><formula xml:id="formula_19">k(x i − x j ) = R d p(ω)z F (x i )z F (x j )dω,<label>(8)</label></formula><p>where z F (x) = √ 2 cos(ω T x+b), with b a random variable drawn from [0, 2π]. In other words, k(</p><formula xml:id="formula_20">x i , x j ) = k(x i −x j ) is the expected value of z F (x i )z F (x j ) under the distribu- tion p(ω). For the RBF kernel k(x i , x j ) = exp(− (x i − x j ) 2 /2σ 2 ), we have p(ω) = N (0, σ −2 I d ). Let {ω i } r i=1</formula><p>, ω i ∈ R d , be i.i.d. samples drawn from the normal distribution N (0, σ −2 I d ), and {b i } r i=1 be samples uniformly drawn from [0, 2π]. Then, the r-dimensional estimate of φ(x) ∈ H is given by</p><formula xml:id="formula_21">z F (x) = 2 r cos(ω T 1 x + b 1 ), · · · , cos(ω T r x + b r ) .<label>(9)</label></formula><p>Similarly to nVLAD, we can then compute z F (x i ) for each local descriptor x i , and use Eq. 1 and Eq. 2 to obtain a code. In our experiments, we refer to this approach, which only applies to Euclidean data, as fVLAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Kernelizing Fisher Vectors</head><p>Due to the connection between VLAD and FVs, it seems natural to rely on the ideas discussed above to kernelize FVs. One difficulty in kernelizing FVs, however, arises from the fact that Gaussian distributions, which are required to model the probability distributions in FVs, are not welldefined in RKHS. More specifically, to fit a Gaussian distribution in a d-dimensional space, at least d independent observations (training samples) are required, to ensure that the covariance matrix of the distribution is not rank deficient. Obviously, for an infinite dimensional RKHS, this requirement cannot be met. While, in principle, it is possible to regularize the distributions, e.g., <ref type="bibr" target="#b51">[52]</ref>, we believe that an in-depth analysis of this approach to kernelize FVs goes beyond the scope of this paper. Note, however, that our approximations of H can be applied verbatim to derive approximate formulations of kernel FVs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We now evaluate our algorithms, i.e., kVLAD and sVLAD, on several recognition tasks. As mentioned before, our main motivation for this work was to be able to exploit the power of the VLAD aggregation scheme to tackle problems where the input data is not in vectorial form. Therefore, we focus on two such types of data, which have become increasingly popular in computer vision, namely Covariance Descriptors (CovDs), which lie on SPD manifolds, and linear subspaces, which form Grassmann manifolds. Nevertheless, in addition to this manifold-valued data, we also evaluate our algorithms in Euclidean space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">SPD Manifold</head><p>In computer vision, SPD matrices have been shown to provide powerful representations for images and videos via region covariances <ref type="bibr" target="#b40">[41]</ref>. Such representations have been successfully employed to categorize, e.g., textures <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b16">17]</ref>, pedestrians <ref type="bibr" target="#b41">[42]</ref> and faces <ref type="bibr" target="#b16">[17]</ref>.</p><p>SPD matrices can be thought of as an extension of positive numbers and form the interior of the positive semidefinite cone. It is possible to directly employ the Frobenius norm as a similarity measure between SPD matrices, hence analyzing problems involving such matrices via Euclidean geometry. However, as several studies have shown, undesirable phenomena may occur when Euclidean geometry is utilized to manipulate SPD matrices <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b19">20]</ref>. Here, instead, we make use of the Stein divergence defined as</p><formula xml:id="formula_22">δ 2 S (A, B) = ln det A + B 2 − 1 2 ln det AB .<label>(10)</label></formula><p>This divergence was shown to yield a positive definite Gaussian kernel <ref type="bibr" target="#b38">[39]</ref>, named the Stein kernel given by k S : S n ++ × S n ++ → R such that k S (A, B) = exp(−σδ 2 S <ref type="figure">(A, B)</ref>). In all our experiments on SPD manifolds, the bandwidth of this kernel was determined by crossvalidation on the training data.</p><p>A standard approach when dealing with an SPD manifold consists of flattening the manifold using the diffeo-morphism log : S n ++ → Sym(n), where log and Sym(n) denote the principal matrix logarithm and the space of symmetric matrices of size n, respectively. Given that Sym(n) is a vector space, one can then directly employ tools from Euclidean geometry, here the VLAD algorithm, to analyze SPD matrices mapped to that space. We refer to this baseline as log-Euclidean VLAD or lE-VLAD following the terminology used in <ref type="bibr" target="#b2">[3]</ref>. Note that this strategy has been successfully employed in several recent studies (e.g., for semantic segmentation <ref type="bibr" target="#b4">[5]</ref>). We also report the results of the Nyström approximation (nVLAD) of Section 3.1. <ref type="bibr" target="#b2">3</ref> Furthermore, we also compare our algorithms against the state-of-the-art Weighted ARray of COvariances (WARCO) <ref type="bibr" target="#b39">[40]</ref>, Riemannian-VLAD (R-VLAD) <ref type="bibr" target="#b10">[11]</ref>, Covariance Discriminative Learning (CDL) <ref type="bibr" target="#b46">[47]</ref>, and Riemannian Sparse Representation using the Stein divergence (RSR-S) <ref type="bibr" target="#b14">[15]</ref>. In WARCO, an image is decomposed into a number of overlapping patches, each of which is represented with a CovD. Classification is then performed by combining the output of a set of kernel classifiers trained on local patches. R-VLAD makes use of the tangent bundle of a Riemannian manifold to aggregate manifold-valued data in a similar manner to VLAD. In essence, WARCO and R-VLAD pursue the same goal as us, i.e., to aggregate local non-vectorial descriptors, which makes them probably the most relevant baselines, here. By contrast, following <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b14">15]</ref>, we have used both CDL and RSR-S holistically, i.e., every image was described by one SPD matrix.</p><p>In the following experiments on the SPD manifold, we used a codebook of size 32 for all variants of the VLAD algorithm. Empirically, we observed that, for any algorithm, larger codebooks did not significantly improve the performance. To provide a fair comparison against WARCO, we used the same set of features as <ref type="bibr" target="#b39">[40]</ref>. Specifically, from a local patch, a 13×13 CovD was extracted using the features</p><formula xml:id="formula_23">f (x, y) = [h1(Y ), · · · , h8(Y ), Y, C b , Cr, g(Y ) , ∠(g(Y ))] T ,</formula><p>where f (x, y) denotes the feature vector at location (x, y) and Y , C b and C r are the three color channels from the CIELab color space at (x, y). h i (·) is the scaled symmetric Difference of Offset Gaussian filter bank, and g(Y ) and ∠(g(Y )) are the gradient magnitude and orientation calculated on the Y channel (see <ref type="bibr" target="#b39">[40]</ref> for details). The same set of features was used for all the algorithms.</p><p>Head Orientation Classification. As a first experiment, we considered the problem of classifying head orientation using the QMUL and HOCoffee datasets <ref type="bibr" target="#b39">[40]</ref>. The QMUL head dataset contains 19292 images of size 50 × 50, captured in an airport terminal. The HOCoffee dataset contains 18117 head images of size 50 × 50. The images typically include a margin of 10 pixels on average, so that the ac- <ref type="bibr" target="#b2">3</ref> The Fourrier approximation fVLAD only applies to Euclidean data. The results of all the algorithms on both datasets are reported in <ref type="table">Table 1</ref>. Note that kVLAD outperforms the state-of-the-art on both datasets, and that sVLAD even outperforms kVLAD on QMUL. This can be attributed to the square root normalization, which is not possible for kVLAD. Without this normalization, the performance of sVLAD drops by roughly 1%, and thus remains close to, but slightly lower than that of kVLAD. Among the approximations, sVLAD is superior to nVLAD. This is not really surprising, since nVLAD uses a single subspace for all its codewords, whereas sVLAD exploits local representations.</p><p>Body Orientation Classification. As a second task on the SPD manifold, we considered the problem of determining body orientation from images using the Human Orientation Classification (HOC) dataset <ref type="bibr" target="#b39">[40]</ref>. The HOC dataset contains 11881 images of size 64 × 32 and comprises 4 orientation classes (Front, Back, Left, and Right). In <ref type="table">Table 1</ref>, we compare the performance of our algorithms with the baselines. First, we note that all VLAD variants, including R-VLAD, lE-VLAD and nVLAD, are superior to WARCO. This demonstrates the effectiveness of the VLAD aggregation scheme. Moreover, we note that our algorithms outperform R-VLAD, lE-VLAD and nVLAD. The highest accuracy is obtained by sVLAD, which, again, in comparison to kVLAD, benefits from the square root normalization.</p><p>Altogether, our experiments on SPD manifolds demonstrate that our approach offers an attractive solution to exploiting the information from local patches. Note that, except for a handful of studies (e.g., WARCO, R-VLAD), CovDs are usually extracted from entire images, hence making them questionable for challenging classification tasks. This is typically due to the fact that aggregating nonvectorial data is an open problem, to which we provide a solution in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Grassmann Manifold</head><p>The space of p dimensional subspaces in R d for 0 &lt; p ≤ d is not a Euclidean space, but a Riemannian manifold known as the Grassmann manifold G(d, p). A point U ∈ G(d, p) is typically represented by a d × p matrix U with orthonormal columns, such that U = Span(U ). The choice of the basis to represent U is arbitrary and metrics on G(d, p) are defined so as to be invariant to this choice. The projection distance is a typical choice of such metric. It was recently shown to induce a valid positive definite kernel on G(d, p) <ref type="bibr" target="#b17">[18]</ref>, i.e., the projection RBF kernel defined as</p><formula xml:id="formula_24">k p (A, B) = exp(σ A T B 2 F ), σ &gt; 0 .<label>(11)</label></formula><p>As for the SPD manifold, the bandwidth of this kernel was obtained by cross-validation on the training data. Several state-of-the-art image-set matching methods model sets of images as subspaces <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. However, to the best of our knowledge, all these methods rely on a holistic subspace representation. The only exception is again R-VLAD <ref type="bibr" target="#b10">[11]</ref>, which, as our approach, can aggregate local subspaces obtained by breaking an image-set into smaller blocks to form a complete image-set descriptor.</p><p>In our experiments, in addition to R-VLAD and nVLAD, we compare the results of our algorithms with four baselines: First, similarly to the log-Euclidean approach on SPD manifolds, we propose to flatten G(d, p) at I d×p 4 and perform conventional VLAD in the resulting Euclidean space. We refer to this method as lE-VLAD. As a second baseline, we make use of the state-of-the-art Grassmannian Sparse Coding (gSC) algorithm of <ref type="bibr" target="#b15">[16]</ref>, which describes each image-set with a single linear subspace. We also employ the kernel version of the Affine Hull Method (kAHM) introduced in <ref type="bibr" target="#b5">[6]</ref> and the CDL algorithm <ref type="bibr" target="#b46">[47]</ref> as other state-ofthe-art baselines for image-set matching. Below, we evaluate the performance of our algorithms and of the baselines on three different classification problems, i.e., action classification, object recognition and pose categorization from image-sets.</p><p>Action Recognition. As a first experiment on the Grassmannian, we made use of the Ballet dataset <ref type="bibr" target="#b47">[48]</ref>. The Ballet dataset consists of 8 complex motion patterns performed by 3 subjects. We extracted 1200 image-sets by grouping 5 frames depicting the same action into one image-set. The local descriptors for each image-set were obtained by splitting the set into small blocks of size 32×32×3 and computing a Histogram of Oriented Gradient (HOG) <ref type="bibr" target="#b7">[8]</ref> for each block. We then created subspaces of size 31 × 3, hence points on G(31, 3). We randomly chose 50% of the imagesets for training and used the remaining sets as test samples. We report the average accuracy over 10 such random splits.</p><p>We report the accuracy of all the algorithms in <ref type="table">Table 2</ref>. Note that all the local approaches outperform the holistic gSC method. The maximum accuracy is obtained by sVLAD, thus showing the power of our approximation.</p><p>Given the simplicity of lE-VLAD, it is interesting to verify if it can measure up to our algorithms by enlarging its <ref type="bibr" target="#b3">4</ref> We use I d×p to denote the truncated identity matrix. dictionary. To this end, we increased the size of the dictionary in lE-VLAD up to the point where the performance started to decrease (256 atoms). While this indeed improved the accuracy of lE-VLAD up to, at best, 91.7%, it remains significantly below the performance of sVLAD.</p><p>Object Recognition. For the task of object recognition from image-sets, we used the CIFAR dataset <ref type="bibr" target="#b21">[22]</ref>. The CI-FAR dataset contains 60000 images (32 × 32 pixels) from 10 different object categories. From this dataset, we generated 6000 image-sets, each one containing 10 random images of the same object. In our experiments, we used 1500 image-sets for training and the remaining 4500 image-sets as test data. We report accuracies averaged over 10 random image-set generation processes. To generate local descriptors, we decomposed each image-set into small blocks of size 8 × 8 × 5. Each block was then represented by a point on G(64, 5) using SVD.</p><p>In <ref type="table">Table 2</ref>, we compare the results of our algorithms with those of the baselines. Here, kVLAD yields the best accuracy, closely followed by sVLAD.</p><p>Pose Classification. As a last experiment on the Grassmannian, we evaluated the performance of our algorithms on the task of pose categorization using the CMU-PIE face dataset <ref type="bibr" target="#b36">[37]</ref>. The CMU-PIE face dataset contains images of 67 subjects under 13 different poses and 21 different illuminations. The images were closely cropped to enclose the face region and resized to 64 × 64. We extracted 1700 image-sets by grouping 6 images with the same pose, but different illuminations, into one image-set. The local descriptors for each image-set were obtained by splitting the set into small blocks of size 32×32×3 from which we computed Histogram of LBP <ref type="bibr" target="#b26">[27]</ref>. We then created subspaces of size 58 × 3, hence points on G(58, 3). <ref type="table">Table 2</ref> compares the results of our algorithms with those of the baselines. The highest accuracy is obtained by kVLAD, this time by a larger margin over sVLAD, which nonetheless remains the second best. Note that, here, flattening the manifold through its tangent space at I 58×3 seems to incur strong distortions, as indicated by the low performance of lE-VLAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Euclidean Space</head><p>Our final experiments are devoted to Euclidean spaces. To this end, we made use of the Pascal VOC 2007 dataset <ref type="bibr" target="#b9">[10]</ref> and of the Flicker Material Database (FMD) <ref type="bibr" target="#b35">[36]</ref>. The Pascal VOC 2007 dataset <ref type="bibr" target="#b9">[10]</ref> contains 9963 images from 20 object categories. The FMD dataset contains 1000 images from 10 different material categories <ref type="bibr" target="#b35">[36]</ref>. Both datasets have been extensively used to benchmark coding techniques. For these datasets, the computational cost of kVLAD becomes overwhelming because of the large amount of local descriptors they involve. Therefore, we only report the results of sVLAD. We compare  these results with those of the two approximations fVLAD and nVLAD, as well as with those of conventional VLAD (implementation provided in <ref type="bibr" target="#b43">[44]</ref>). Note that lE-VLAD and R-VLAD both boil down to conventional VLAD in the Euclidean case.</p><p>For these experiments, we set the size of the codebooks to 256 and used SIFT descriptors as local features, with dimensionality reduced to 80 using PCA. For fVLAD and nVLAD, the size of the RKHS was chosen to be 256 (almost 3 times larger than the original space). While increasing the dimensionality of the RKHS could potentially improve the results, it would come at the expense of increasing the computational burden of coding.</p><p>For Pascal VOC 2007, <ref type="table" target="#tab_3">Table 3</ref> compares the recognition accuracies of the above-mentioned techniques with the following additional baselines: Spatial Pyramid Matching (SPM) <ref type="bibr" target="#b23">[24]</ref>, Object-Centric spatial Pooling (OCP) <ref type="bibr" target="#b34">[35]</ref> and supervised dictionary learning for VLAD (Sup-VLAD) <ref type="bibr" target="#b27">[28]</ref>. Similarly to our experiments on manifolds, sVLAD outperforms the fixed approximation techniques (i.e., fVLAD and nVLAD). Importantly, we observe that the three approximations outperform traditional methods such as SPM and VLAD. Furthermore, sVLAD also outperforms the state-of-the-art pooling method OCP <ref type="bibr" target="#b34">[35]</ref>, and performs roughly on par with the supervised Sup-VLAD. This latter comparison motivates an interesting future research direction to learn a supervised dictionary in RKHS.</p><p>For FMD, <ref type="table">Table 4</ref> compares the recognition accuracies of our algorithms with nVLAD, fVLAD, VLAD and the state-of-the-art methods augmented Latent Dirichlet Allocation (aLDA) <ref type="bibr" target="#b35">[36]</ref>, Multi-Scale Spike-and-Slab Sparse Coding (MS4C) <ref type="bibr" target="#b24">[25]</ref>, and Describable attributes (DTD RBF ) <ref type="bibr" target="#b6">[7]</ref>. In essence, we can see that sVLAD outperforms (i) VLAD and the other approximations; and (ii) the state-of-the-art aLDA, MS4C and DTD methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Coding Times</head><p>Below, we report the coding times of our two algorithms. For sVLAD, this means the time to build one image descriptor, which is virtually the time to classify one sample. For kVLAD, however, since no descriptor is explicitly built, we report the time to compute Eq. 4. Note that this timing can-not truly be compared to that of sVLAD, but still gives an indication of the speed of kVLAD. All these coding times were obtained on a quad-core machine using Matlab.</p><p>When measuring these timings, we used the following parameters (corresponding to our previous experiments). We used a codebook of size 32 for the SPD and Grassmann manifolds, and a codebook of size 256 in Euclidean space. Note that, for the Euclidean case, we assumed that 1000 local descriptors were computed on each image, while, for the SPD and Grassmann manifolds, this number was set to 100. All the coding times are reported in <ref type="table">Table 5</ref>. For kVLAD, the time to classify one image can be roughly obtained by multiplying the values in the table by the number of training samples. As mentioned before, this makes kVLAD illsuited for large datasets. By contrast, these timings show that sVLAD takes roughly one second to classify each image, which is quite competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>SPD Grassmann Euclidean kVLAD 80ms 155ms 45ms sVLAD 750ms 1700ms 950ms <ref type="table">Table 5</ref>. Coding times for kVLAD and sVLAD (see text for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we have introduced a kernel extension of the VLAD encoding scheme. We have also proposed a novel approximation to this kernel formulation in the interest of speeding up the coding process. Not only do the resulting algorithms let us exploit more sophisticated classification schemes in the VLAD framework, but they also allow us to aggregate local descriptors that do not lie in Euclidean space. Our experiments have evidenced that our algorithms outperform state-of-the-art methods, such as WARCO <ref type="bibr" target="#b39">[40]</ref> and R-VLAD <ref type="bibr" target="#b10">[11]</ref>, on several manifold-based recognition tasks. Furthermore, they have also shown that our new coding schemes yield superior results compared to the conventional VLAD algorithm. In the future, we plan to explore possible ways of kernelizing the Fisher vector method <ref type="bibr" target="#b29">[30]</ref>. Since our local approximation of a Hilbert space has empirically proven superior to other approximation, we also intend to study its use in other kernel-based algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Remark 1. Note that one can also use only the top r eigenvectors of K s to construct an r-dimensional local subspace in H. This would not only yield the same dimensionality for all local subspaces, but could also potentially help discarding the noise associated to the {t s,i } Ns i=1 . Remark 2. Recall that in FV the coding scheme takes into account the Gaussian distribution centered at each codeword. In VLAD, coding is simplified by assuming that the Gaussian distributions are isotropic and all have the same variance. Interestingly, our sVLAD formulation relaxes this assumption by explicitly considering the eigenvalues of the kernel (covariance) computed from the data associated to each codeword.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 2. Accuracies for Ballet, CIFAR and CMU-PIE.</figDesc><table>Method 

Ballet 
CIFAR 
CMU-PIE 

R-VLAD [11] 93.9% 
50.5% 
69.5% 
gSC [16] 
79.7% 
59.9% 
75.5% 
kAHM [6] 
85.8% 
36.1% 
55.3% 
CDL [47] 
73.1% 
54.7% 
64.6% 

lE-VLAD 
91.1% 
46.2% 
59.6% 
nVLAD 
88.9% 
62.2% 
79.5% 

kVLAD 
92.2% 
67.9 % 
86.3 % 
sVLAD 
94.4% 
65.2% 
80.1% 

Method 
mAP 

SPM [24] 
54.3% 
OCP [35] 
57.2% 
Sup-VLAD [28] 
60.9% 

VLAD 
54.7% 
nVLAD 
56.2% 
fVLAD 
55.8% 

sVLAD 
60.3% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Mean Average Precision (mAP) for the VOC 2007 dataset.</figDesc><table>Method 
CCR 

aLDA [36] 
44.6% 
MS4C [25] 
50.0% 
DTD RBF [7] 
53.1% 

VLAD 
49.4% 
nVLAD 
52.3% 
fVLAD 
50.3% 

sVLAD 
55.2% 

Table 4. Correct Classification Rate 
(CCR) for the FMD dataset. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that this yields a slightly different optimization problem than the standard kernel SVM formulation, since in our case the inner product itself depends on several kernel values.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A kernel function is shift invariant if k(x i , x j ) = k(x i − x j ).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">All about VLAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American mathematical society</title>
		<imprint>
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A log-Euclidean framework for statistics on diffeomorphisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Commowick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="924" to="931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kernel descriptors for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition based on image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cevikalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2567" to="2573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Revisiting the VLAD image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delhumeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Multimedia</title>
		<meeting>the ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="653" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">More about VLAD: A leap from Euclidean to Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse coding on symmetric positive definite manifolds using Bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extrinsic methods for coding and dictionary learning on Grassmann manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From manifold to manifold: Geometry-aware dimensionality reduction for SPD matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Expanding the family of Grassmannian kernels: An embedding perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<title level="m">Kernel methods on Riemannian manifolds with gaussian rbf kernels. TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning multi-scale representations for material classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<editor>X. Jiang, J. Hornegger, and R. Koch</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8753</biblScope>
			<biblScope unit="page" from="757" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2627" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Boosting VLAD with supervised dictionary learning and high-order statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Riemannian framework for tensor computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale image categorization with explicit data embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fourier analysis on groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Objectcentric spatial pooling for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognizing materials using perceptually inspired features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The CMU pose, illumination, and expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discovering objects and their location in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A new metric on the manifold of kernel matrices with application to matrix geometric means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tosato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<title level="m">Characterizing humans on Riemannian manifolds. TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Region covariance: A fast descriptor for detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pedestrian detection via classification on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
		<title level="m">Visual word ambiguity. TPAMI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Borgwardt. Graph kernels. JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Covariance discriminative learning: A natural and efficient approach to image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2496" to="2503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Human action recognition by semilatent topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Using the Nyström method to speed up kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>T. Leen, T. Dietterich, and V. Tresp</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">From sample similarity to ensemble similarity: Probabilistic distance measures in reproducing kernel Hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="917" to="929" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
