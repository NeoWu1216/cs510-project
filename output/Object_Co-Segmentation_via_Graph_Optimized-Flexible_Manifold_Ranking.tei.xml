<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aiming at automatically discovering the common objects contained in a set of relevant images and segmenting them as foreground simultaneously, object co-segmentation has become an active research topic in recent years. Although a number of approaches have been proposed to address this problem, many of them are designed with the misleading assumption, unscalable prior, or low flexibility and thus still suffer from certain limitations, which reduces their capability in the real-world scenarios. To alleviate these limitations, we propose a novel two-stage co-segmentation framework, which introduces the weak background prior to establish a globally close-loop graph to represent the common object and union background separately. Then a novel graph optimized-flexible manifold ranking algorithm is proposed to flexibly optimize the graph connection and node labels to co-segment the common objects. Experiments on three image datasets demonstrate that our method outperforms other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given a set of images containing the same or similar objects from the same semantic class, the goal of object co-segmentation is to discover and segment out such common objects from all images, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. The problem of object co-segmentation is first proposed by Rother et al. <ref type="bibr" target="#b2">[3]</ref>, which demonstrates that simultaneously segmenting out the common objects in an image pair can achieve higher accuracy than segmenting in either single image alone. Following this work, a number of researchers make their efforts to develop more effective computational models <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> for co-segmenting objects in such image pairs. However, these methods only seek to co-segment objects in two images at a time, which results in direct limitations when extending beyond pairwise relations. By realizing this problem, more recent object co-segmentation approaches <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> propose to discover the common patterns of the co-occurring objects in group-level and thus can segment out them in more than two images. With such important progress, object co-segmentation has become to be more practical for the real-world problems because there are rich collections of multiple related pictures sharing the common objects or events in reality <ref type="bibr" target="#b16">[17]</ref>, such as the photo-sharing websites like Flickr and Facebook. However, when performing object co-segmentation in such real-world scenarios, the existing methods still suffer from certain limitations, which are mainly lied in the following aspects:</p><p>Misleading assumption: Some existing methods are based on a misleading assumption that the common regions contained by the given image group should be the objects of interest. However, lots of real-world image groups containing similar objects are collected in similar scenes and thus they also contain similar and co-occurring image background which may confuse these methods seriously. In this case, these methods always wrongly segment out the similar co-occurring image backgrounds, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. For example, Faktor et al. <ref type="bibr" target="#b1">[2]</ref> proposed to discover the co-occurring regions firstly, and then perform co-segmentation by mapping between the co-occurring</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Co-segmentation via Graph Optimized-Flexible Manifold Ranking</head><p>Rong Quan 1 , Junwei Han 1 , Dingwen Zhang 1 , Feiping Nie <ref type="bibr" target="#b1">2</ref> 1 School of Automation, 2 School of Computer Science and Center for OPTIMAL, Northwestern Polytechnical University, Xi'an, 710072, P. R. China rongquan0806, junweihan2010, zhangdingwen2006yyy, feipingnie@gmail.com (b) The co-segmentation results of <ref type="bibr" target="#b0">[1]</ref>. (c) The co-segmentation results of <ref type="bibr" target="#b1">[2]</ref>. (d) Our results. regions across the different images, whereas those extracted co-occurring regions also contain the common backgrounds. The models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> did not explicitly consider the common foreground and background in separate formulations, which leads to the failure results where arbitrarily shaped background regions (with similar appearance in both images) are segmented out as the common objects.</p><p>To solve this problem, we introduce a novel concept in this paper, i.e., the union background, which indicates the collections of image background regions in each image group, and then proposes to formulate the common objects and union background separately. Specifically, we represent the image group as a globally close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries via a novel GO-FMR (graph optimized-flexible manifold ranking) algorithm which can infer both the optimal labels and connections of the nodes to precisely distinguish the common objects from the union background even when some background regions are similar and co-occurring in the image group (see <ref type="figure" target="#fig_0">Figure 2</ref>(d)).</p><p>Unscalable prior: For alleviating the confusion of common foreground and background, some approaches adopt certain prior knowledge, e.g. saliency and objectness in co-segmentation. For example, Rubinstein et al. <ref type="bibr" target="#b0">[1]</ref> proposed to establish reliable correspondences between pixels in different images based on the extracted saliency regions. Vicente et al. <ref type="bibr" target="#b13">[14]</ref> showed that requiring the foreground segment to be an object can significantly improve the co-segmentation performance and thus they introduced the objectness in their model via generating a pool of object-like proposal segmentations. However, such prior knowledge may not always guarantee to provide adequate and precise information when scaling up for the real-world scenarios due to the inestimable complexity and diversity in real world.</p><p>To solve this problem, we propose a novel two-stage framework in this paper, which can weaken the strong prior knowledge used in the previous work to a much more scalable prior, i.e., the background prior. As mentioned in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, the background prior comes from the basic rule of photographic composition, that is, most photographers do not crop objects of interest along the view frame. In other words, the image boundary is mostly background. Based on this prior, we initialize the graph in the first stage by connecting the superpixel nodes located in the image boundaries of the entire image group, i.e., the union background. By inferring via the proposed GO-FMR algorithm, we can obtain the image regions which are more different from the union background. It further provides informative knowledge for co-segmenting the common objects in the second stage.</p><p>Low flexibility: Some previous methods rely heavily on certain model configurations which are manually designed and kept fixed during the exploration of the common objects. However, such strategies are typically subjective and cannot generalize well to flexibly adapt to various real-world scenarios encountered in practice. Take the graph-based object co-segmentation approaches as the example. Joulin et al. <ref type="bibr" target="#b8">[9]</ref> proposed a discriminative clustering algorithm, which combined the fixed Laplacian matrix and kernel matrix to formulate the spatial consistency and discriminative clustering, respectively. For better taking advantage of the information available from other images in <ref type="figure">Figure 3</ref>: Overview of our two-stage object co-segmentation framework. the group, Kim et al. <ref type="bibr" target="#b19">[20]</ref> carefully modelled the inter-image relationships via connecting the image regions of one image to all other images in an image cluster. As can be seen, these methods build the graph connections based on the certain aspects of human knowledge and keep them fixed during the optimization processes. However, the limited human knowledge we have may not always guarantee an optimal graph connection in various scenarios. Thus, such strategies keeping the graph connections unchanged during the optimization process may suffer from the low flexibility when dealing with the real-world scenarios.</p><p>To solve this problem, we propose a novel GO-FMR algorithm, which can alternatively optimize the superpixel labels as well as the connections in the image group. Specifically, given an initialized graph with the nodes connected based on certain human knowledge, the proposed GO-FMR algorithm can be guided by the human knowledge and further flexibly infer the optimal graph connection for the specific scenario rather than blindly trusting the human knowledge and keeping the manually designed graph connection fixed in all cases. Moreover, the prior information can also be easily incorporated into the proposed GO-FMR algorithm by initializing the seed nodes correspondingly. Thus, it can be adapted to the various real-world scenarios flexibly.</p><p>The concrete framework proposed in this paper to reduce above mentioned problems is shown in <ref type="figure">Figure 3</ref>. Given images within an image group, we first decompose each image into superpixels. Then, for each superpixel, we extract the low-level appearance features and high-level semantic features, respectively. Afterwards, the properties of the common objects are inferred via the GO-FMR based two-stage scheme to generate the probability maps for the different types of features separately. Finally, the obtained probability maps are integrated to generate the final object co-segmentation results. In summary, the contributions of this paper are three-folds:</p><p> We make one of the earliest efforts to formulate the common object and union background separately, which can effectively suppress the co-segmentation of the common background in the real-world scenarios.  We proposed a novel two-stage object co-segmentation scheme which relies on a much weaker background prior and thus can better scale up for more complex scenarios.  We propose a novel GO-FMR algorithm to optimize the established globally close-loop graph, which can simultaneously infer both the labels of all the superpixel nodes and the optimal graph connection to best explore the relationships among all image regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Approach</head><p>For a set of images   <ref type="bibr">1 2</ref> , ,..., m I I I  that contain a common object, our goal is to segment the common object instance in each image. As a pre-processing step, each image i I in  is first over-segmented into i n superpixels by the SLIC algorithm <ref type="bibr" target="#b20">[21]</ref>. Then the whole image set </p><formula xml:id="formula_0">contains 1 m i i nn    superpixels.</formula><p>Our object co-segmentation process is performed on superpixel level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The image features</head><p>In this paper, we adopt two kinds of image features, i.e., the low-level appearance features and the high-level semantic features, to capture different characteristics of each superpixel.</p><p>The low-level appearance features are sensitive to the appearance variations of images. The common objects detected based on this kind of features should have consistent appearance and well-defined object boundaries within each image, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>(b). Three kinds of low-level appearance features are used in this work, including color, texture, and dense SIFT descriptors <ref type="bibr" target="#b21">[22]</ref>. They are denoted by 1 c , 2 c and 3 c , respectively.</p><p>As mentioned by Zhang et al. <ref type="bibr" target="#b16">[17]</ref>, the common objects in different images may share strong homogeneity in semantic level. Thus, we also apply the deep semantic feature in this paper. Specifically, we employ the 'CNN-S' <ref type="bibr" target="#b22">[23]</ref> model which is pre-trained on the ImageNet <ref type="bibr" target="#b23">[24]</ref> dataset to extract the high-level semantic representations. Firstly, we feed each image into the pre-trained CNN and extract the responses from the last convolutional layer as the higher-level image representations, which consists of 512 feature maps with size of 17×17. Next, we resize the feature maps to the size of the original image, and then use a max pooling operation on each superpixel to generate a 512-D CNN feature vector. Subsequently, an auto-encoder is further used to reduce the feature dimension to 24. The c . With such semantic feature, we can precisely locate the common objects in each image, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>(c). However, as the original size of the feature maps is very small, even after the resizing operation, the feature maps are still very coarse. Thus the obtained co-segmentation results based on such CNN tend to be somewhat blurry.</p><p>As the low-level appearance features and the high-level semantic features describe different attributes of the images and play different roles in the co-segmentation process, we first perform our co-segmentation framework based on each of them separately. Then those two preliminary co-segmentation results are integrated into the final one, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The graph construction</head><p>We construct a globally close-loop graph</p><formula xml:id="formula_1">  ,, G V E  A on  ,</formula><p>where each node in V corresponds to a superpixel in  , the edges in E connect all the related superpixels, and the affinity matrix A measures the similarities among all superpixels. As spatially neighboring nodes with similar features tend to belong to the same class, we connect each node with not only its spatial neighbors, but also the neighbors of its neighbors to model the intra-image constraints in each image. In addition, as we formulate the common object and union background separately, we connect all the potential common object nodes together and union background nodes together to model their consistency relationships, respectively. The initial node connections in our graph can be illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. </p><formula xml:id="formula_2">3 2 1 exp ij ij t t t t a             cc (1) or   2 44 exp ij ij a    cc (2)</formula><p>where i c is the feature vector of node i . Eq. (1) is used for the low-level appearance features and Eq. (2) is used for the high-level semantic feature. Empirically, setting <ref type="bibr" target="#b0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The GO-FMR algorithm</head><p>After modeling the similarity relationships among all superpixels in  through the graph</p><formula xml:id="formula_3">  ,, G V E </formula><p>A , we further partition all superpixels into the common object and the union background based on the graph. Here we formulate the graph labeling problem as a graph-based manifold ranking problem <ref type="bibr" target="#b24">[25]</ref> and propose a novel semi-supervised learning technique called the graph optimized-flexible manifold ranking algorithm (GO-FMR), to infer the class labels of all superpixels. The graph-based manifold ranking problem <ref type="bibr" target="#b24">[25]</ref> refers to the problem that given a node as query, all other nodes in the graph are ranked based on their correlations to the given query. Suppose that some superpixels in  have already been labelled as 1 (or used as queries). We use the GO-FMR algorithm to rank all superpixels based on their relevance to the labeled superpixels, where the ranking scores can be treated as the probabilities of these superpixels being labeled as 1, i.e. their prediction labels corresponding to the query superpixels.</p><p>Traditional graph labeling algorithms directly predict the class labels of all the superpixels based on the manually established affinity matrix A . However, just depending on the manually established affinity matrix may not represent the real similarity relationships among all the superpixels. Even if it can, the process of computing the affinity matrix from the original image features itself will lose some information. Instead of just depending on the affinity matrix A , the proposed GO-FMR algorithm additionally uses a projection to directly infer the predict labels from the original image features. It also automatically learns the optimal graph connection for specific scenario during the label prediction process to infer the final predict labels more accurately.  </p><formula xml:id="formula_4">          * * * * , , ,<label>, , , 2 2 2</label></formula><p>, , , arg min , , , is a vector with all elements being 1, and T b X w +1 is a linear projection that directly maps X to the prediction labels f . The first two terms in Eq. (4) constrain the label fitness (i.e., f should be close to the given labels of the labeled nodes) and manifold smoothness (i.e., f should be smooth on the entire graph of both the unlabeled and labeled nodes), respectively, which are normally used in traditional manifold ranking algorithms. In addition, the linear projection function   T h b  X X w +1 is used to directly map X to the prediction labels f . We use both the manifold ranking and the linear classification projection to predict the labels of all superpixels. The residue between the prediction results of these two methods is constrained to be as small as possible. 0  SA measures the difference between the learned optimal affinity matrix S and the human established affinity matrix A . As we infer the optimal affinity matrix S under the guidance of the human established affinity matrix A , this term ensures that S will not change too much from A .</p><formula xml:id="formula_5">arg min tr tr b T T T b T F b b b             f w S S f w S f w S f w S f y U f y f L f X w 1 f S A<label>(4)</label></formula><formula xml:id="formula_6">T b    f X w 1 f is</formula><p>As those four variables in Eq. (4) cannot be solved simultaneously, we use an iterative optimization process to alternate between optimizing S and f , w , b . We first initialize the optimal affinity matrix S as A . The detailed iterative process is as follows:</p><p>(1). Fix S , and compute f , w , b . When S is fixed, this label prediction problem can be solved through the FME algorithm in <ref type="bibr" target="#b25">[26]</ref>. As the objective function is proved to be jointly convex with respect to f , w , and b , there exists the optimal solutions for them, which are denoted as:</p><formula xml:id="formula_7">      1 1 1 c T cc TT b n          S f U L H N Uy w X X X f f 1 w X1 (5) where     1 1 2 T T T T c c c c c           N X X X X X X X X I , c c  X XH ,   1/ T c n  H I 11 , and nn   I</formula><p>is an identity matrix. For more details about the computation process, please refer to <ref type="bibr" target="#b25">[26]</ref>.</p><p>(2). Fix f , w , and b , and compute S . When f , w , b are fixed, with the prediction label vector f computed in the last step, S can be computed by solving the following optimization problem:</p><formula xml:id="formula_8">  2 * arg min tr T F     S S S f L f S A<label>(6)</label></formula><p>which can also be written as: Initialize the optimal similarity matrix S as A . Afterwards, we can obtain the optimal affinity matrix S that corresponds to the current prediction label vector f . Next, we update f based on S . These two processes alternate until convergence. Algorithm 1 summarizes the proposed GO-FMR algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Our two-stage co-segmentation framework</head><p>In our work, we define the common objects as the regions in different images that are similar to each other and different from the union backgrounds. We develop a two-stage co-segmentation framework to first detect some potential common objects by comparing with the union backgrounds, and then refine the obtained potential common objects by further considering the similarity between them, as shown in the middle part of <ref type="figure">Figure 3</ref>.</p><p>The first stage: The union backgrounds are first initialized as the superpixels on the image boundaries of all images. Then we construct a graph   1 G with all the initialized union background nodes connected together and all the spatially neighboring nodes connected. Next, by treating all the initialized union background nodes as the labeled nodes with label 1, we use the GO-FMR algorithm to compute all superpixel nodes' prediction labels</p><formula xml:id="formula_9">  1 f .</formula><p>As the potential common objects are defined as the superpixels that are different from the union backgrounds, we extract the potential common object superpixels from the prediction labels   1 f as:</p><formula xml:id="formula_10">          1 1 1 *mean    l f f (9) where     1 1</formula><p> f 1 f , and  controls the extraction of the common objects. A smaller  means that more superpixels will be extracted as potential common objects. The second stage. During this stage, we further compute the more accurate prediction labels for all superpixels by additionally considering the similarity among the common objects.</p><p>Specifically, we build a more comprehensive graph   2 G , where all potential common object nodes are further connected except for the initialized union background nodes and the spatially neighboring nodes. Then, we treat all potential common object nodes as the labeled nodes with label 1, and use the GO-FMR algorithm to infer the final label predictions   2 f of all superpixels. Finally, we generate a probability map for each image from   2 f , where each pixel value represents the likelihood of this pixel being the common object.</p><p>As shown in <ref type="figure">Figure 3</ref>, we perform our two-stage co-segmentation framework based on both the low-level appearance features and the high-level semantic features, respectively. Consequently, for each image, two probability maps are obtained based on these two kinds of image features. We multiply these two probability maps to obtain a more accurate one, where only the pixels detected as common objects by both the low-level appearance features and the high-level semantic features can have high probability of being labeled as common object. Finally, we apply a Grab-cut <ref type="bibr" target="#b26">[27]</ref> algorithm to the final probability maps to obtain the final binary object co-segmentation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate our method on a widely used benchmark dataset, the iCoseg <ref type="bibr" target="#b27">[28]</ref> dataset, and two more challenging datasets, the Internet <ref type="bibr" target="#b0">[1]</ref> dataset and the PASCAL-VOC dataset <ref type="bibr" target="#b1">[2]</ref>. Two widely used evaluation metrics are utilized: Precision, P (the percentage of correctly labeled pixels of both common object and union background), and Jaccard index, J (the intersection over union of the resulted co-segmentation map and the ground truth segmentation).</p><p>In our experiments, for each object class in the dataset, we first divide it into several smaller groups, where each group contains images with similar scenes, and then perform our co-segmentation framework on each group. Specifically, we use a k -means clustering algorithm to cluster the images based on their GIST <ref type="bibr" target="#b28">[29]</ref> descriptors. Here, k is set to make sure that each group has about ten images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> </head><p>for the high-level semantic features. The parameter  in Eq. (9) controls the extraction of the possible common object superpixels from   1 f . Empirically, we set  to 2 for the low-level appearance features and 1.5 for the high-level semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experiments on the iCoseg dataset</head><p>The iCoseg <ref type="bibr" target="#b27">[28]</ref> dataset is a widely used benchmark dataset for evaluating co-segmentation approaches. It contains 38 object classes of totally 643 images with human-given pixel-level segmentation ground-truth. The common objects of each class belong to the same object instance under different viewpoints and illumination. The images in each object class have the same theme and similar backgrounds. Some example images are shown in <ref type="figure">Figure 6</ref>. We first evaluate our method on all these 38 object classes and then conduct experiments on a subset of the iCoseg dataset (16 classes of 122 images). This subset (sub-iCoseg) is also often used in previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14]</ref> to evaluate their co-segmentation approaches.</p><p>In <ref type="table">Table 1</ref>, we show the comparison results of our method and two state-of-the-art co-segmentation algorithms of <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b29">[30]</ref> on the whole iCoseg dataset. <ref type="bibr" target="#b29">[30]</ref> is a supervised co-segmentation approach. Additionally, we also compare our results with <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14]</ref> on the sub-iCoseg dataset, where <ref type="bibr" target="#b13">[14]</ref> is a supervised co-segmentation method. The comparison results are shown in <ref type="table">Table 2</ref>. <ref type="table">Table 1</ref>. Comparison results of the proposed method and two state-of-the-art co-segmentation methods on the iCoseg dataset in terms of average Precision (denoted by P) and Jaccard index (denoted by J). Because <ref type="bibr" target="#b29">[30]</ref> does not provide their Jaccard index results, we thus do not present them here. As shown in <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref>, our method outperform all other co-segmentation methods. Compared with the second best approach <ref type="bibr" target="#b1">[2]</ref>, our method has slightly higher Precision and much higher Jaccard index. Note that the results of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14]</ref> are taken from <ref type="table">Table 1</ref> in <ref type="bibr" target="#b1">[2]</ref>. Some results of our co-segmentation method are visualized in <ref type="figure">Figure 6</ref>. We can see that our co-segmentation method can accurately detect the common object instances in different images. Besides, almost no distracting background regions are detected as common objects any more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experiments on the Internet dataset</head><p>The Internet dataset <ref type="bibr" target="#b0">[1]</ref> consists of thousands of images from the Internet through three query expansions: car, horse, and airplane. The common objects of each object class in this dataset are similar objects from the same semantic class. This dataset is a challenging dataset. Some example images are shown in <ref type="figure">Figure 6</ref>. We can see that the common objects in different images have quite different colors, scales, poses, and viewing-angles, and the backgrounds in each object class are also different from each other. In addition, each object class in this dataset contains some noisy images that do not contain the common objects. In our experiment, we follow <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b30">[31]</ref> to utilize a subset of 100 images per class for evaluation. All those images in this subset are with human-given segmentation ground-truth.</p><p>We compare our method with four state-of-the-art approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> and <ref type="table" target="#tab_8">Table 3</ref> shows the comparison results of each object class. The comparison results show that our method outperforms all other methods on all three object classes. Note that the results of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> are taken from <ref type="table">Table 2</ref> in <ref type="bibr" target="#b30">[31]</ref>.  <ref type="figure">Figure 6</ref> shows some sample results. As can be seen, our co-segmentation method can accurately detect the common objects of each class. However, for the noisy images of each object class, we cannot always successfully recognize them. For example, the last image of each object class shown in the third row of <ref type="figure">Figure 6</ref> is a noisy image, which does not contain the common object. We successfully recognize the noisy image in the plane class, but fail in other two object classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiments on the PASCAL-VOC dataset</head><p>The PASCAL-VOC dataset formed in <ref type="bibr" target="#b1">[2]</ref> is also a benchmark for evaluating co-segmentation approaches. It consists of 1037 images of 20 object classes from the well-known PASCAL-VOC 2010 dataset. This dataset is more challenging due to extremely large intra-class variability and distracting background clutter. <ref type="figure">Figure 6</ref> shows some example images from this dataset. In this database, many common objects have similar colors with the backgrounds. For example, in the bird class (the last example in the fifth row of <ref type="figure">Figure 6</ref>), the branches have very similar colors with the birds. <ref type="table" target="#tab_9">Table 4</ref> presents the comparison results of our method and <ref type="bibr" target="#b1">[2]</ref> on the PASCAL-VOC dataset. As can be seen from the results, our method outperforms <ref type="bibr" target="#b1">[2]</ref> and both our average Precision and Jaccard index are much higher than <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model component analysis</head><p>In this section, we implement experiments on the PASCAL-VOC dataset to further analyze some components in the proposed framework. Firstly, we demonstrate the effectiveness of integrating both the low-level appearance features and high-level semantic features. Specifically, we report the performance based on each individual feature in <ref type="table" target="#tab_10">Table 5</ref>, respectively. As can be seen, the performance of integrating both features is significantly better than that of using either individual feature, which demonstrates the importance of leveraging the complementary information existed in different kinds of features as we pointed in Section 2.1.</p><p>Secondly, we demonstrate the effectiveness of the graph optimization step. Specifically, we report the performance of our method without graph optimization in <ref type="table" target="#tab_10">Table 5</ref>. As can be seen, by executing graph optimization, the co-segmentation performance of our method improves a lot. This demonstrates that flexibly inferring the optimal graph connections to fit different scenarios can obtain much encouraging co-segmentation performance.</p><p>Lastly, we compare the performance of our method with a baseline method which adopts the conventional manifold ranking strategy as used in <ref type="bibr" target="#b24">[25]</ref>. As shown in <ref type="table" target="#tab_10">Table 5</ref>, the baseline method achieves performance much worse than the proposed method, which demonstrates the effectiveness of the proposed GO-FMR algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we have proposed a novel computational framework for object co-segmentation. By introducing a new concept of weak background prior, we constructed globally close-loop graphs to formulate the common object and union background separately. Afterwards, we designed a graph optimized-flexible manifold ranking algorithm to flexibly optimize the graph connection and node labels, which finally yielded the co-segmentation results. The comprehensive evaluations on three publically available benchmarks and comparisons with a number of state-of-the-art approaches have demonstrated the superiority of the proposed work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) Several original images from the same object class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The first row: example images containing similar objects from the same semantic class. The second row: the co-segmentation results of our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the combination of the two kinds of image features. (a) The input images from the same object class. (b) The probability maps obtained from the low-level appearance features. (c) The probability maps obtained from the high-level semantic feature. (d) The probability maps obtained from the combination of low-level appearance features and high-level semantic feature. (e) The final binary masks of the input images.24-D CNN feature vector is denoted by<ref type="bibr" target="#b3">4</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of the node connections in our graph. The green lines show that the potential common object nodes of all the images are connected together. The blue lines show that the potential union background nodes of all the images are connected together. The yellow lines within each image show that the neighbor nodes within each image are connected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>to infer the prediction labels f , where each 0 ij s  . In addition, to simplify the computation, from S . The two parameters  and  are used to balance different terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Compute f , w , b by Eq. (5) 4: Compute S by Eq. (8) 5: until the prediction label vector f stops changing Output: The prediction label vector f and the i th superpixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>means that the i th superpixel belongs to the potential common object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>The affinity matrix</figDesc><table>ij nn 

a  
 
  
A 

measures the weights of 

× 
ij nn 

Ee  
  . For each edge ij 
e that connects two nodes, we 

compute the similarity weight ij 
a as follows: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>can produce good results in our experiments. After obtaining A , we further define the diagonal matrix D as the row sums of A , and the graph Laplacian matrix</figDesc><table>1 
  and 

23 

0.7 
  

L as： 
 
L D A 
(3) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>corresponds to a superpixel in  and t is the feature dimension. Suppose that some superpixels in  have already been labeled. We define a binary indicator vector</figDesc><table>Let 

us 
denote 
 
as 
a 
sample 
set 

 
 

12 , ,..., 

tn 
n 

 

 
X = x x 
x 
, where each sample 

1 
t 
i 

 

 
x 

 
 

1 

12 , ,..., 

T 
n 
n 

y y 
y 

 

 
y= 
, where 
1 

i 

y  means that i 

x is 

labeled as 1, and 
0 

i 

y  means that i 

x is unlabeled. Then, 

the prediction labels (or ranking scores) of all superpixels 

 
 

1 

12 , ,..., 

T 
n 
n 

f f 
f 

 

 
f= 

can be computed by solving the 

following optimization problem: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>the residue between f and  </figDesc><table>h X , and 

2 

0 2 

f 
is a penalty term for the 

mismatch between them. In addition, the last term 

2 

F 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>The two parameters  and  in Eq. (4) are set</figDesc><table>empirically: 
0.01 

  

and 
5 

  

for the low-level 
appearance features, and 
0.05 

  

and 
10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>. Comparison results of the proposed method and three state-of-the-art co-segmentation methods on the iCoseg dataset in terms of average Precision and Jaccard index.</figDesc><table>iCoseg 
Ours 
[2] 
[30] 

P 
93.3 
% 

92.8 
% 

91.4 
% 
J 
0.76 
0.73 
-
Table 2sub-iCoseg 
Ours 
[2] 
[1] 
[14] 

P 
94.8 
% 

94.4 
% 

89.6 
% 

85.4 
% 
J 
0.82 
0.79 
0.68 
0.62 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Comparison results of the proposed method and four state-of-the-art co-segmentation methods on the subset of Internet dataset in terms of average Precision and Jaccard index.</figDesc><table>Car 
Horse 
Airplane 

P 
J 
P 
J 
P 
J 
[9] 
58.7 
0 

37.1 
5 

63.8 
4 

30.1 
6 

49.2 
5 

15.3 
6 
[32] 
68.8 
5 

0.04 
75.1 
2 

6.43 
80.2 
0 

7.90 
[1] 
85.3 
8 

64.4 
2 

82.8 
1 

51.6 
5 

88.0 
4 

55.8 
1 
[31] 
87.6 
5 

64.8 
6 

86.1 
6 

33.3 
9 

90.2 
5 

40.3 
3 
Ours 
88.5 
7 

66.8 
3 

89.3 
9 

58.1 
3 

91.0 
3 

56.3 
1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Comparison results of the proposed method and [2] on the PASCAL-VOC dataset in terms of the average Precision and Jaccard index.</figDesc><table>PASCAL-VOC 
Ours 
[2] 
P 
89 
84 
J 
0.52 
0.46 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 .</head><label>5</label><figDesc>The results for model component analysis. Note that 'Low', 'High', 'w/o OP', 'MR' and 'Ours' denote the image co-segmentation results that only based on the low-level appearance features, only based on the high-level semantic features, without graph optimization, the conventional manifold ranking strategy and our complete framework.PASCAL-VOC 'Low' 'High'</figDesc><table>'w/o 
OP' 
'MR' 'Ours 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Co-segmentation by composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An efficient algorithm for co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Half-integrality based algorithms for cosegmentation of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Co-segmentation of image pairs with quadratic global constraint in MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cosegmentation revisited: Models and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From co-saliency to co-segmentation: An efficient and fully unsupervised energy minimization model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-class cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scale invariant cosegmentation for image groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyzing the subspace structure of related images: Concurrent segmentation of image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised co-segmentation through region matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object-based RGBD image co-segmentation with mutex constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object co-segmentation based on shortest path algorithm and saliency model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1429" to="1441" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Co-saliency detection via looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Background prior-based salient object detection via deep reconstruction residual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A hierarchical image clustering cosegmentation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flexible manifold embedding: A framework for semi-supervised and unsupervised dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1921" to="1932" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">icoseg: Interactive co-segmentation with intelligent scribble guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building the gist of a scene: The role of global image features in recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog Brain Res</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="36" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Segmentation propagation in imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuettel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enriching visual knowledge bases via object discovery and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed cosegmentation via submodular optimization on anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
