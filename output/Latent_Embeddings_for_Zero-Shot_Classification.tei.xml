<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Embeddings for Zero-shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MPI for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MPI for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MPI for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IIT</orgName>
								<address>
									<settlement>Kanpur</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quynh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MPI for Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Embeddings for Zero-shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Zero-shot classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref> is a challenging problem. The task is generally set as follows: training images are provided for certain visual classes and the classifier is expected to predict the presence or absence of novel classes at test time. The training and test classes are connected via some auxiliary, non visual source of information e.g. attributes.</p><p>Combining visual information with attributes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref> has also supported fine grained classification. In fine grained image collections, images that belong to different classes are visually similar to each other, e.g. different bird species. Image labeling for such collections is a costly process, as it requires either expert opinion or a large number of attributes. To overcome this limitation, recent works have explored distributed text representations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24]</ref> which are learned from general (or domain specific) text corpora. <ref type="bibr">*</ref>   Substantial progress has been made for image classification problem in the zero-shot setting on fine-grained image collections <ref type="bibr" target="#b1">[2]</ref>. This progress can be attributed to (i) strong deep learning based image features <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref> and (ii) learning a discriminative compatibility function between the structured image and class embeddings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref>. The focus of this work is on the latter, i.e. on improving the compatibility learning framework, in particular via unsupervised auxiliary information.</p><p>The main idea of structured embedding frameworks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref> is to first represent both the images and the classes in some multi-dimensional vector spaces. Image embeddings are obtained from state-of-the-art image representations e.g. those from convolutional neural networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref>. Class embeddings can either (i) be obtained using manually specified side information e.g. attributes <ref type="bibr" target="#b19">[20]</ref>, or (ii) extracted automatically <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> from an unlabeled large text corpora. A discriminative bilinear compatibility function is then learned that pulls images from the same class close to each other and pushes images from different classes away from each other. Once learned, such a compatibility function can be used to predict the class (more precisely, the embedding) of any given image (embedding). In particular, this prediction can be done for images from both seen and unseen classes, hence enabling zero-shot classification.</p><p>We address the fine-grained zero-shot classification problem while being particularly interested in more flexible unsupervised text embeddings. The state-of-the-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref> use a unique, globally linear compatibility function for all types of images. However, learning a linear compatibility function is not particularly suitable for the challenging fine-grained classification problem. For fine-grained classification, a model that can automatically group objects with similar properties together and then learn for each group a separate compatibility model is required. For instance, two different linear functions that separate blue birds with brown wings and from other blue birds with blue wings can be learned separately. To that end, we propose a novel model for zero-shot setting which incorporates latent variables to learn a piecewise linear compatibility function between image and class embeddings. The approach is inspired by many recent advances in visual recognition that utilize latent variable models, e.g. in object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>, human pose estimation <ref type="bibr" target="#b38">[39]</ref> and face detection <ref type="bibr" target="#b42">[43]</ref> etc.</p><p>Our contributions are as follows. <ref type="formula" target="#formula_0">(1)</ref> We propose a novel method for zero-shot learning. By incorporating latent variables in the compatibility function our method achieves factorization over such (possibly complex combinations of) variations in pose, appearance and other factors. Instead of learning a single linear function, we propose to learn a collection of linear models while allowing each image-class pair to choose from them. This effectively makes our model non-linear, as in different local regions of the space the decision boundary, while being linear, is different. We use an efficient stochastic gradient descent (SGD) based learning method. (2) We propose a fast and effective method for model selection, i.e. through model pruning. (3) We evaluate our novel piecewise linear model for zero-shot classification on three challenging datasets. We show that incorporating latent variables in the compatibility learning framework consistently improves the state-of-the-art.</p><p>The rest of the paper is structured as follows. In Sec. 3 we detail the bilinear compatibility learning framework that we base our method on. In Sec. 4 we present our novel Latent Embedding (LatEm) method. In Sec. 5 we present our experimental evaluation and in Sec. 6 we conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We are interested in the problem of zero-shot learning where the test classes are disjoint from the training classes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. As visual information from such test classes is not available during training, zero-shot learning requires secondary information sources to make up for the missing visual information. While secondary information can come from different sources, usually they are derived from either large and unrestricted, but freely available, text corpora, e.g. word2vec <ref type="bibr" target="#b22">[23]</ref>, glove <ref type="bibr" target="#b28">[29]</ref>, or structured textual sources e.g. wordnet hierarchies <ref type="bibr" target="#b23">[24]</ref>, or costly human annotations e.g. manually specified attributes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref>. Attributes, such as 'furry', 'has four legs' etc. for animals, capture several characteristics of objects (visual classes) that help associate some and differentiate others. They are typically collected through costly human annotation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref> and have shown promising results <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> in various computer vision problems.</p><p>The image classification problem, with a secondary stream of information, could be either solved by solving related sub-problems, e.g. attribute prediction <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, or by a direct approach, e.g. compatibility learning between embeddings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref>. One such factorization could be by building intermediate attribute classifiers and then making a class prediction using a probabilistic weight of each attribute for each sample <ref type="bibr" target="#b19">[20]</ref>. However, these methods, based on attribute classifiers, have been shown to be suboptimal <ref type="bibr" target="#b0">[1]</ref>. This is due to their reliance on binary mappings (by thresholding attribute scores) between attributes and images which causes loss in information. On the other hand, solving the problem directly, by learning a direct mapping between images and their classes (represented as numerical vectors) has been shown to be better suited. Such label embedding methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref> aim to find a mapping between two embedding spaces, one each for the two streams of information e.g. visual and textual. Among these methods, CCA <ref type="bibr" target="#b12">[13]</ref> maximizes the correlation between these two embedding spaces, <ref type="bibr" target="#b25">[26]</ref> learns a linear compatibility between an fMRI-based image space and the semantic space, <ref type="bibr" target="#b34">[35]</ref> learns a deep non-linear mapping between images and tags, ConSe <ref type="bibr" target="#b24">[25]</ref> uses the probabilities of a softmax-output layer to weight the vectors of all the classes, SJE <ref type="bibr" target="#b1">[2]</ref> and ALE <ref type="bibr" target="#b0">[1]</ref> learn a bilinear compatibility function using a multiclass <ref type="bibr" target="#b3">[4]</ref> and a weighted approximate ranking loss <ref type="bibr" target="#b15">[16]</ref> respectively. DeViSE <ref type="bibr" target="#b11">[12]</ref> does the same, however, with an efficient ranking formulation. Most recently, <ref type="bibr" target="#b31">[32]</ref> proposes to learn this mapping by optimizing a simple objective function which has closed form solution.</p><p>We build our work on multimodal embedding methods. However, instead of learning a linear compatibility function, we propose a nonlinear compatibility framework that learns a collection of such linear models making the overall function piecewise linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background: Bilinear Joint Embeddings</head><p>In this section, we describe the bilinear joint embedding framework <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>, on which we build our Latent Embedding Model (LatEm) (Sec. 4).</p><p>We work in a supervised setting where we are given an annotated training set T = {(x, y)|x ∈ X ⊂ R dx , y ∈ Y ⊂ R dy } where x is the image embedding defined in an image feature space X , e.g. CNN features <ref type="bibr" target="#b18">[19]</ref>, and y is the class embedding defined in a label space Y that models the conceptual relationships between classes, e.g. attributes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. The goal is to learn a function f : X → Y to predict the correct class for the query images. In previous work <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>, this is done via learning a function F : X × Y → R that measures the compatibility between a given input embedding (x ∈ X ) and an output embedding (y ∈ Y). The prediction function then chooses the class with the maximum compatibility, i.e.</p><formula xml:id="formula_0">f (x) = arg max y∈Y F (x, y).<label>(1)</label></formula><p>In general, the class embeddings reflect the common and distinguishing properties of different classes using sideinformation that is extracted independently of images. Using these embeddings, the compatibility can be computed even with those unknown classes which have no corresponding images in the training set. Therefore, this framework can be applied to zero-shot learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>In previous work, the compatibility function takes a simple form,</p><formula xml:id="formula_1">F (x, y) = x ⊤ W y<label>(2)</label></formula><p>with the matrix W ∈ R dx×dy being the parameter to be learnt from training data. Due to the bilinearity of F in x and y, previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b37">38]</ref> refer to this model as a bilinear model, however one can also view it as a linear one since F is linear in the parameter W. In the following, these two terminologies will be used interchangeably depending on the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Latent Embeddings Model (LatEm)</head><p>In general, the linearity of the compatibility function (Eq. <ref type="formula" target="#formula_1">(2)</ref>) is a limitation as the problem of image classification is usually a complex nonlinear decision problem. A very successful extension of linear decision functions to nonlinear ones, has been through the use of piecewise linear decision functions. This idea has been applied successfully to various computer vision tasks e.g. mixture of templates <ref type="bibr" target="#b14">[15]</ref> and deformable parts-based model <ref type="bibr" target="#b9">[10]</ref> for object detection, mixture of parts for pose estimation <ref type="bibr" target="#b38">[39]</ref> and face detection <ref type="bibr" target="#b42">[43]</ref>. The main idea in most of such models, along with modeling parts, is that of incorporating latent variables, thus making the decision function piecewise linear, e.g. the different templates in the mixture of templates <ref type="bibr" target="#b14">[15]</ref> and the different 'components' in the deformable parts model <ref type="bibr" target="#b9">[10]</ref>. The model then becomes a collection of linear models and the test images pick one from these linear models, with the selection being latent. Intuitively, this factorizes the decision function into components which focus on distinctive 'clusters' in the data e.g. one component may focus on the profile view while another on the frontal view of the object.</p><p>Objective. We propose to construct a nonlinear, albeit piecewise linear, compatibility function. Parallel to the latent SVM formulation, we propose a non-linear compatibility function as follows,</p><formula xml:id="formula_2">F (x, y) = max 1≤i≤Kw ⊤ i (x ⊗ y),<label>(3)</label></formula><p>where i = 1, . . . , K, with K ≥ 2, indexes over the latent choices andw i ∈ R dxdy are the parameters of the individual linear components of the model. This can be rewritten as a mixture of bilinear compatibility functions from Eq. (2) as</p><formula xml:id="formula_3">F (x, y) = max 1≤i≤K x ⊤ W i y.<label>(4)</label></formula><p>Our main goal is to learn a set of compatibility spaces that minimizes the following empirical risk,</p><formula xml:id="formula_4">1 N |T | n=1 L(x n , y n ),<label>(5)</label></formula><p>where L : X × Y → R is the loss function defined for a particular example (x n , y n ) as L(x n , y n ) = y∈Y max{0, ∆(y n , y)+F (x n , y)−F (x n , y n )} (6) where ∆(y, y n ) = 1 if y = y n and 0 otherwise. This ranking-based loss function has been previously used in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref> such that the model is trained to produce a higher compatibility between the image embedding and the class embedding of the correct label than between the image embedding and class embedding of other labels.</p><p>Optimization. To minimize the empirical risk in Eq. <ref type="formula" target="#formula_4">(5)</ref>, one first observes that the ranking loss function L from Eq. <ref type="formula">(6)</ref> is not jointly convex in all the W i 's even though F is convex. Thus, finding a globally optimal solution as in the previous linear models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> is out of reach. To solve this problem, we propose a simple SGD-based method that works in the same fashion as in the convex setting. It turns out that our algorithm works well in practice and achieves state-of-the-art results as we empirically show in Sec. 5.</p><p>We explain the details of our Algorithm 1 as follows. We loop through all our samples for a certain number of epochs T . For each sample (x n , y n ) in the training set, we randomly select a y that is different from y n (step 3 of Algorithm 1). If the randomly selected y violates the margin (step 4 in Algorithm 1), then we update the W i matrices (steps 5 − 13 in Algorithm 1). In particular, we find the W i Algorithm 1 SGD optimization for LatEm</p><formula xml:id="formula_5">T = {(x, y)|x ∈ R dx , y ∈ R dy } 1: for all t = 1 to T do 2:</formula><p>for all n = 1 to |T | do <ref type="bibr">3:</ref> Draw (x n ,y n ) ∈ T and y ∈ Y \ {y n } <ref type="bibr">4:</ref> if F (x n , y) + 1 &gt; F (x n , y n ) then 5: end for 16: end for that leads to the maximum score for y and the W j that gives the maximum score for y. If the same matrix gives the maximum score (step 7 in Algorithm 1), we update that matrix. If two different matrices lead to the maximum score (step 9 in Algorithm 1), we update them both using SGD.</p><formula xml:id="formula_6">i * ← arg max 1≤k≤K x ⊤ n W k y 6: j * ← arg max 1≤k≤K x ⊤ n W k y n 7: if i * = j * then 8: W t+1 i * ← W t i * − η t x n (y − y n ) ⊤ 9: end if 10: if i * = j * then 11: W t+1 i * ← W t i * − η t x n y ⊤ 12: W t+1 j * ← W t j * + η t x n y ⊤</formula><p>Model selection. The number of matrices K in the model is a free parameter. We use two strategies to select the number of matrices. As the first method, we use a standard cross-validation strategy -we split the dataset randomly into disjoint parts (in a zero-shot setup) and choose the K with the best cross-validation performance. While this is a well established strategy which we find to work well experimentally, we also propose a pruning based strategy which is competitive while being faster to train. As the second method, we start with a large number of matrices and prune them as follows. As the training proceeds, each sampled training examples chooses one of the matrices for scoringwe keep track of this information and build a histogram over the number of matrices counting how many times each matrix was chosen by any training example. In particular, this is done by increasing the counter for W j * by 1 after step 6 of Algorithm 1. With this information, after five passes over the training data, we prune out the matrices which were chosen by less than 5% of the training examples, so far. This is based on the intuition that if a matrix is being chosen only by a very small number of examples, it is probably not critical for performance. With this approach we have to train only one model which adapts itself, instead of training multiple models for cross-validating K and then training a final model with the chosen K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion. LatEm builds on the idea of Structured Joint</head><p>Embeddings (SJE) <ref type="bibr" target="#b1">[2]</ref>. We discuss below the differences between LatEm and SJE and emphasize our technical contributions.</p><p>LatEm learns a piecewise linear compatibility function through multiple W i matrices whereas SJE <ref type="bibr" target="#b1">[2]</ref> is linear. With multiple W i 's the compatibility function has the freedom to treat different types of images differently. Let us consider a fixed classŷ and two substantially visually different types of images x 1 , x 2 , e.g. the same bird flying and swimming. In SJE <ref type="bibr" target="#b1">[2]</ref> these images will be mapped to the class embedding space with a single mapping W ⊤ x 1 , W ⊤ x 2 . On the other hand, LatEm will have learned two different matrices for the mapping i.e. W ⊤ 1 x 1 , W ⊤ 2 x 2 . While in the former case a single W has to map two visually, and hence numerically, very different vectors (close) to the same point, in the latent case such two different mappings are factorized separately and hence are arguably easier to perform. Such factorization is also expected to be advantageous when two classes sharing partial visual similarity are to be discriminated e.g. while blue birds could be relative easily distinguished from red birds, to do so for different types of blue birds is harder. In such cases, one of the W i 's could focus on color while another one could focus on the beak shape (in Sec. 5.2 we show that this effect is visible). The task of discrimination against different bird species would then be handled only by the second one, which would also arguably be easier.</p><p>LatEm uses the ranking based loss <ref type="bibr" target="#b37">[38]</ref> in Eq. (6) whereas SJE <ref type="bibr" target="#b1">[2]</ref> uses the multiclass loss of Crammer and Singer <ref type="bibr" target="#b3">[4]</ref> which replaces the in Eq. (6) with max. The SGD algorithm for multiclass loss of Crammer and Singer <ref type="bibr" target="#b3">[4]</ref> requires at each iteration a full pass over all the classes to search for the maximum violating class. Therefore it can happen that some matrices will not be updated frequently. On the other hand, the ranking based loss in Eq. (6) used by our LatEm model ensures that different latent matrices are updated frequently. Thus, the ranking based loss in Eq. (6) is better suited for our piecewise linear model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the proposed model on three challenging publicly available datasets of Birds, Dogs and Animals. First, we describe the datasets, then give the implementation details and finally report the experimental results.</p><p>Datasets. Caltech-UCSD Birds (CUB), Stanford Dogs (Dogs) are standard benchmarks of fine-grained recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b17">18]</ref> and Animals With Attributes (AWA) is another popular and challenging benchmark dataset <ref type="bibr" target="#b19">[20]</ref>. All these three datasets have been used for zero-shot learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41]</ref>. Tab. 1 gives the statistics for them.</p><p>In zero-shot setting, the dataset is divided into three dis-   <ref type="bibr" target="#b1">[2]</ref>. In zero-shot learning, where training and test classes are disjoint sets, to get a more stable estimate in our own results, we make four more splits by random sampling, while keeping the number of classes the same as before. We average results over the total of five splits. The average performance over the five splits is the default setting reported in all experiments, except where mentioned otherwise, e.g. in comparison with previous methods.</p><p>Image and class embeddings. In our latent embedding (LatEm) model, the image embeddings (image features) and class embeddings (side information) are two essential components. To facilitate direct comparison with the state-of-the-art, we use the embeddings provided by <ref type="bibr" target="#b1">[2]</ref>. Briefly, as image embeddings we use the 1, 024 dimensional outputs of the top-layer pooling units of the pre-trained GoogleNet <ref type="bibr" target="#b35">[36]</ref> extracted from the whole image. We do not do any task specific pre-processing on images such as cropping foreground objects. As class embeddings we evaluate four different alternatives, i.e. attributes (att), word2vec (w2v), glove (glo) and hierarchies (hie). Attributes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9]</ref> are distinguishing properties of objects that are obtained through human annotation. For fine-grained datasets such as CUB and Dogs, as objects are visually very similar to each other, a large number of attributes are needed. Among the three datasets used, CUB contains 312 attributes, AWA contains 85 attributes while Dogs does not contain annotations for attributes. Our attribute class embedding is a vector per-class measuring the strength of each attribute based on human judgment.</p><p>In addition to human annotation the class embeddings can be constructed automatically from either a large unlabeled text corpora or through hierarchical relationship between classes. This has certain advantages such as we do not need any costly human annotation, however as a drawback, they tend not to perform as well as supervised attributes. One of our motivations for this work is that the class embeddings captured from a large text corpora contains latent relationships between classes and we would like to automatically learn these. Therefore, we evalu-   <ref type="bibr" target="#b1">[2]</ref> method. We report average per-class Top-1 accuracy on unseen classes. We use the same data partitioning, same image features and same class embeddings as SJE <ref type="bibr" target="#b1">[2]</ref>. We cross-validate the K for LatEm.</p><p>ate three common methods for building unsupervised text embeddings. Word2Vec <ref type="bibr" target="#b22">[23]</ref> is a two-layer neural network which predicts words given the context within a skip window slided through a text document. It builds a vector for each word in a learned vocabulary. Glove <ref type="bibr" target="#b28">[29]</ref> is another distributed text representation method which uses co-occurrence statistics of words within a document. We use the pre-extracted word2vec and glove vectors from wikipedia provided by <ref type="bibr" target="#b1">[2]</ref>. Finally, another way of building a vectorial structure for our classes is to use a hierarchy such as WordNet <ref type="bibr" target="#b23">[24]</ref>. Our hierarchy vectors are based on the hierarchical distance between child and ancestor nodes, in WordNet, corresponding to our class names. For a direct comparison, we again use the hierarchy vectors provided by <ref type="bibr" target="#b1">[2]</ref>. In terms of size, w2v and glo are 400 dimensional whereas hie is ≈ 200 dimensional.</p><p>Implementation details. Our image features are z-score normalized such that each dimension has zero mean and unit variance. All the class embeddings are ℓ 2 normalized. The matrices W i are initialized at random with zero mean and standard deviation 1 √ dx <ref type="bibr" target="#b0">[1]</ref>. The number of epochs is fixed to be 150. The learning rates for the CUB, AWA and Dog datasets are chosen as η t = 0.1, 0.001, 0.01, respectively, and kept constant over iterations. For each dataset, these parameters are tuned on the validation set of the default dataset split and kept constant for all other dataset folds and for all class embeddings. As discussed in Sec 4, we perform two strategies for selecting the number of latent matrices K: cross-validation and pruning. When using crossvalidation, K is varied in {2, 4, 6, 8, 10} and the optimal K is chosen based the accuracy on a validation set. For pruning, K is initially set to be 16, and then at every fifth epoch during training, we prune all those matrices that support less than 5% of the data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with State-of-the-Art</head><p>We now provide a direct comparison between our LatEm and the state-of-the-art SJE <ref type="bibr" target="#b1">[2]</ref>    <ref type="bibr" target="#b24">25</ref>.2% from 24.3%, the improvement is more significant on CUB (24.2% from 20.6%) and on AWA (57.5% from 51.2%). These results establish our novel Latent Embeddings (LatEm) as the new state-ofthe-art method for zero-shot learning on three datasets in ten out of eleven test settings. They are encouraging, as they quantitatively show that learning piecewise linear latent embeddings indeed capture latent semantics on the class embedding space.</p><p>Following <ref type="bibr" target="#b1">[2]</ref> we also include a comparison when combining supervised and unsupervised embeddings. The results are given in Tab 3. First, we combine all the embeddings, i.e. att,w2v,glo,hie for AWA and CUB. LatEm improves the results over SJE significantly on AWA (76.1% vs 73.9%). Second, we combine the unsupervised class embeddings, i.e. w2v,glo,hie, for all datasets. LatEm consistently improves over the combined embeddings obtained with SJE in this setting. On CUB combining w2v,glo,hie achieves 34.9% (vs 29.9%), on AWA, it achieves 66.2% (vs 60.1%) and on Dogs, it obtains 36.3% (vs 35.1%). These experiments show that the embeddings contain non-redundant information, therefore the results tend to improve by combining them.  <ref type="table">Table 4</ref>: Average per-class top-1 accuracy on unseen classes (the results are averaged on five folds). SJE: <ref type="bibr" target="#b1">[2]</ref>, LatEm: Latent embedding model (K is cross-validated).</p><p>Stability evaluation of zero-shot learning. Zero-shot learning is a challenging problem due to the lack of labeled training data. In other words, during training time, neither images nor class relationships of test classes are seen. As a consequence, zero-shot learning suffers from the difficulty in parameter selection on a zero-shot set-up, i.e. train, val and test classes belong to disjoint sets. In order to get stable estimates of our predictions, we experimented on additional (in our case four) independently and randomly chosen data splits in addition to the standard one. Both with our LatEm and the publicly available implementation of SJE <ref type="bibr" target="#b1">[2]</ref> we repeated the experiments five times.</p><p>The results are presented in Tab 4. For all datasets, all the result comparisons between SJE and LatEm hold and therefore the conclusions are the same. Although the SJE outperforms LatEm with supervised attributes on CUB, LatEm outperforms the SJE results with supervised attributes on AWA and consistently outperforms all the SJE results obtained with unsupervised class embeddings. The details of our results are as follows. Using supervised class embeddings, i.e. attributes, on AWA, LatEm obtains an impressive 72.5% (vs 70.5%) and using unsupervised embeddings the highest accuracy is observed with w2v with 52.3% (vs 49.3%). On CUB, LatEm with w2v obtains the highest accuracy among the unsupervised class embeddings with 33.1% (vs 27.7%) On Dogs, LatEm with hie obtains the highest accuracy among all the class embeddings, i.e. 25.6% (vs 24.6%). These results insure that our accuracy improvements reported in Tab 2 were not due to a dataset bias. By augmenting the datasets with four more splits, our LatEm obtains a consistent improvement on all the class embeddings on all datasets over the state-of-the-art.</p><p>Note that, for completion, in this section we provided a full comparison with the state-of-the-art on all class embeddings, including supervised attributes. However, there are two disadvantages of using attributes. First, since finegrained object classes share many common properties, we need a large number of attributes which is costly to obtain. Second, attribute annotations need to be done on a dataset basis, i.e. the attributes collected for birds do not work with dogs. Consequently, attribute based methods are not gen-  eralizable across datasets. Therefore, we are interested in the unsupervised text embeddings settings, i.e. w2v, glo, hie. Moreover, with these unsupervised embeddings, our LatEm outperforms the SJE on nine out of nine cases in all our datasets. For the following sections, we will present results only with w2v, glo and hie.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Interpretability of latent embeddings</head><p>In Sec. 5.1, we have demonstrated that our novel latent embedding method improves the state-of-the-art of zeroshot classification on two fine-grained datasets of birds and dogs, i.e. CUB and Dogs, and one dataset of Animals, i.e. AWA. In this section, we zoom into the challenging CUB dataset and aim to investigate if individual W i 's learn visually consistent and interpretable latent relationships between images and classes. We use word2vec and glove as text embeddings. <ref type="figure" target="#fig_3">Fig 2 shows</ref> the top scoring images retrieved by three different W i for the two embeddings i.e. w2v and glo.</p><p>For w2v, we observe that the images scored highly by the same W i (each row) share some visual aspect. The images in the first row are consistently of birds which have long and pointy beaks. Note that they belong to different classes; having a long and pointy beak is one of the shared aspect of birds of these classes. Similarly, for the second row, the retrieved images are of small birds with brown heads and light colored breasts and the last row contains large birds with completely black plumage. These results are interesting because although w2v is trained on wikipedia in an unsupervised manner with no notion of attributes, our LatEm is able to (1) infer hidden common properties of classes and (2) support them with visual evidence, leading to a data clustering which is optimized for classification, however also performs well in retrieval.</p><p>For glo, similar to the results with w2v, the top-scoring images using the same W i consistently show distinguish-ing visual properties of classes. The first row shows blue birds although belonging to different species, are clustered together which indicates that this matrix captures the "blue"ness of the objects. The second row has exclusively aquatic birds, surrounded by water. Finally, the third row has yellow birds only. Similar to w2v, although glo is trained in an unsupervised manner, our LatEm is able to bring out the latent information that reflect object attributes and support this with its visual counterpart.</p><p>These results clearly demonstrate that our model factorizes the information with visually interpretable relations between classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Pruning vs. cross-validation for model selection</head><p>In this section we evaluate the performances obtained with the number of matrices in the model is fixed with pruning vs. cross-validation.</p><p>Tab. 5 presents the number of matrices selected by two methods along with their performances on three datasets. In terms of performance, both methods are competitive. Pruning outperforms cross validation on five cases and is outperformed on the remaining six cases. The performance gaps are usually within 1-2% absolute, with the exception of AWA dataset with att and w2v with 72.5% vs. 70.7% and 52.3% vs. 49.3%, respectively for cross validation and pruning. Hence neither of the methods has a clear advantage in terms of performance, however cross validation is slightly better.</p><p>In terms of the model size, cross validation seems to have a slight advantage. It selects a smaller model, hence more space and time efficient one, seven cases out of eleven. The trend is consistent for all class embeddings for the AwA dataset but is mixed for CUB and Dogs. The advantage of pruning over cross-validation is that it is much faster to train -while cross validation requires training and testing with multiple models (once each per every possible choice   of K), pruning just requires training once. There is however another free parameter in pruning i.e. choice of the amount of training data supporting a matrix for it to survive pruning. Arguably, it is more intuitive than setting directly the number of matrices to use instead of cross validating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluating the number of latent embeddings</head><p>In Sec. 5.1, when we use multiple splits of the data, although the relative performance difference between the state of the art and our method has not changed, for some cases we observe a certain increase or decrease in accuracy. In this section, we investigate the experiments performed with five-folds on the CUB dataset and provide further analysis for a varying number of K. For completeness of the analysis, we also evaluate the single matrix case, namely K ∈ {1, 2, 4, 6, 8, 10} using unsupervised embeddings, i.e. w2v, glo, hie. <ref type="figure" target="#fig_4">Fig. 3</ref> shows the performance of the model with a different number of matrices. We observe that the performance generally increases with increasing K, initially, and then the patterns differ with different embeddings. With w2v the performance keeps increasing until K = 6 and then starts decreasing, probably due to model overfitting. With glo the performance increases until K = 10 where the final accuracy is ≈ 5% higher than with K = 1. With the hie embedding the standard errors do not increase significantly in any of the cases, are similar for all values of K and there is no clear trend in the performance. In conclusion, the variation in performance with K seems to depend of the embeddings used, however, in the zero-shot setting, depending on the data distribution the results may vary up to 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented a novel latent variable based model, Latent Embeddings (LatEm), for learning a nonlinear (piecewise linear) compatibility function for the task of zero-shot classification. LatEm is a multi-modal method: it uses images and class-level side-information either collected through human annotation or in an unsupervised way from a large text corpus. LatEm incorporates multiple linear compatibility units and allows each image to choose one of themsuch choices being the latent variables. We proposed a ranking based objective to learn the model using an efficient and scalable SGD based solver.</p><p>We empirically validated our model on three challenging benchmark datasets for zero-shot classification of Birds, Dogs and Animals. We improved the state-of-the-art for zero-shot learning using unsupervised class embeddings on AWA up to 66.2% (vs 60.1% )and on two fine-grained datasets, achieving 34.9% accuracy (vs 29.9%) on CUB as well as achieving 36.3% accuracy (vs 35.1%) on Dogs with word2vec. On AWA, we also improve the accuracy obtained with supervised class embeddings, obtaining 76.1% (vs 73.9%). This demonstrates quantitatively that our method learns a latent structure in the embedding space through multiple matrices. Moreover, we made a qualitative analysis on our results and showed that the latent embeddings learned with our method leads to visual consistencies. Our stability analysis on five dataset folds for all three benchmark datasets showed that our method can generalize well and does not overfit to the current dataset splits. We proposed a new method for selecting the number of latent variables automatically from the data. Such pruning based method speeds the training up and leads to models with competitive space-time complexities cf. the cross-validation based method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>LatEm learns multiple W i 's that maximize the compatibility between the input embedding (image, text space) and the output embedding (label space) of all training examples. The different W i 's may capture different visual characteristics of objects, i.e. color, beak shape etc. and allow distribution of the complexity among them, enabling the model to do better classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Top images ranked by the matrices using word2vec and glove on CUB dataset, each row corresponds to different matrix in the model. Qualitative examples support our intuition -each latent variable captures certain visual aspects of the bird. Note that, while the images may not belong to the same fine-grained class, they share common visual properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Effect of latent variable K, with unsupervised class embeddings (on CUB dataset with five splits).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Currently with CSE, Indian Institute of Technology Kanpur. Majority of this work was done at Max Planck Institute for Informatics.</figDesc><table>cerulean 
warbler 

W 3 

W 2 

Label space 

pine 
grosbeak 
cardinal 

W 1 

Image space 

Text space 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the three datasets used. CUB and Dog are fine-grained datasets whereas AWA is a more general concept dataset.</figDesc><table>joint sets of train, val and test. For comparing with 
previous works, we follow the same train/val/test set 
split used by </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Latent Embeddings (LatEm) method with the state-of-the-art SJE</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc>LatEm improves over SJE on AWA (71.9% vs. 66.7%) significantly. However, as our aim is to reduce the accuracy gap between supervised and unsupervised class embeddings, we focus on unsupervised embeddings, i.e. w2v, glo and hie. On all datasets, LatEm with w2v, glo and hie improves the state-of-the-art SJE [2] significantly. With w2v, LatEm achieves 31.8% accuracy (vs 28.4%) on CUB, 61.1% accuracy (vs 51.2%) on AWA and finally 22.6% (vs 19.6%) on Dogs. Similarly, using glo, LatEm achieves 32.5% accuracy (vs 24.2%) on CUB, 62.9% accuracy (vs. 58.8%) on AWA and 20.9% accuracy (vs. 17.8%) on Dogs. Finally, while LatEm with hie on Dogs improves the result to</figDesc><table>Combining embeddings either including or not in-
cluding supervision in the combination. w: the combination 
includes attributes, w/o: the combination does not include 
attributes. 

a bilinear function that maximizes the compatibility be-
tween image and class embeddings. Our LatEm on the 
other hand learns a nonlinear, i.e. piece-wise linear func-
tion, through multiple compatibility functions defined be-
tween image and class embeddings. 
The results are presented in Tab. 2. Using the text 
embeddings obtained through human annotation, i.e. at-
tributes (att), </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 5 :</head><label>5</label><figDesc>(Left) Number of matrices selected (on the original split) and (right) average per-class top-1 accuracy on unseen classes (averaged over five splits). PR: proposed model learnt with pruning, CV: with cross validation.</figDesc><table>K 

1 
2 
4 
6 
8 
10 
Top-1 Acc. (in %) 

20 

25 

30 

35 

w2v 
glo 
hie 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labelembedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of Output Embeddings for Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What&apos;s in a name? first names as facial attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the learnability and design of output codes for multiclass problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fine-grained crowdsourcing for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining attributes and Fisher vectors for efficient image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discovering localized attributes for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attribute-centric recognition for cross-category generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Springer Series in Statistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning hypergraph-regularized attribute predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature sets and dimensionality reduction for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training linear svms in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online incremental attribute-based zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kankuekul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kawewong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tangruamsub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://vision.stanford.edu/aditya86/ImageNetDogs/.4" />
		<title level="m">Stanford dogs dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training object class detectors from eye tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluating knowledge transfer and zero-shot learning in a large-scale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What helps here -and why? Semantic relatedness for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-attribute spaces: Calibration for attribute fusion and similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image ranking and retrieval based on multi-attribute queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
	</analytic>
	<monogr>
		<title level="j">Caltech</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attribute-based transfer learning for object categorization with zero or one training example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
