<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Benefits of Selection Diversity via Bilevel Exclusive Sparsity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Goergen Institute for Data Science</orgName>
								<orgName type="department" key="dep3">Department of Industrial and Systems Engineering</orgName>
								<orgName type="institution" key="instit1">University of Rochester</orgName>
								<orgName type="institution" key="instit2">University of Rochester</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Huang</surname></persName>
							<email>huangyj0@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Goergen Institute for Data Science</orgName>
								<orgName type="department" key="dep3">Department of Industrial and Systems Engineering</orgName>
								<orgName type="institution" key="instit1">University of Rochester</orgName>
								<orgName type="institution" key="instit2">University of Rochester</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Tran</surname></persName>
							<email>lam.c.tran@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Goergen Institute for Data Science</orgName>
								<orgName type="department" key="dep3">Department of Industrial and Systems Engineering</orgName>
								<orgName type="institution" key="instit1">University of Rochester</orgName>
								<orgName type="institution" key="instit2">University of Rochester</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Goergen Institute for Data Science</orgName>
								<orgName type="department" key="dep3">Department of Industrial and Systems Engineering</orgName>
								<orgName type="institution" key="instit1">University of Rochester</orgName>
								<orgName type="institution" key="instit2">University of Rochester</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Huang</surname></persName>
							<email>shuai.huang.ie@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Goergen Institute for Data Science</orgName>
								<orgName type="department" key="dep3">Department of Industrial and Systems Engineering</orgName>
								<orgName type="institution" key="instit1">University of Rochester</orgName>
								<orgName type="institution" key="instit2">University of Rochester</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On Benefits of Selection Diversity via Bilevel Exclusive Sparsity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparse feature (dictionary) selection is critical for various tasks in computer vision, machine learning, and pattern recognition to avoid overfitting. While extensive research efforts have been conducted on feature selection using sparsity and group sparsity, we note that there has been a lack of development on applications where there is a particular preference on diversity. That is, the selected features are expected to come from different groups or categories. This diversity preference is motivated from many real-world applications such as advertisement recommendation, privacy image classification, and design of survey.</p><p>In this paper, we proposed a general bilevel exclusive sparsity formulation to pursue the diversity by restricting the overall sparsity and the sparsity in each group. To solve the proposed formulation that is NP hard in general, a heuristic procedure is proposed. The main contributions in this paper include: 1) A linear convergence rate is established for the proposed algorithm; 2) The provided theoretical error bound improves the approaches such as L 1 norm and L 0 types methods which only use the overall sparsity and the quantitative benefits of using the diversity sparsity is provided. To the best of our knowledge, this is the first work to show the theoretical benefits of using the diversity sparsity; 3) Extensive empirical studies are provided to validate the proposed formulation, algorithm, and theory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays people can easily extract tons of features in computer vision applications, for example, the wellknown open source library VLFeat <ref type="bibr" target="#b33">[34]</ref> provides a large collection of feature extraction algorithms, while the number of labeled samples are usually limit. Without restricting the number of active features, the learned model  <ref type="figure" target="#fig_1">Figure 1</ref>. Illustration of different sparsity preferences. The orange color indicates the active (or selected) feature. "No sparsity" potentially selects all features; "overall sparsity" (commonly used sparsity such as L1 norm and L0 norm regularization) only restricts the number of selected features without any restriction on the structure; "group sparsity" prefers selects features staying in the same group; and "diversity sparsity" restricts selected features in different groups.</p><p>will overfit the training data and dramatically degrades the performance on testing data. Sparse feature (dictionary) selection is one of key steps in various tasks in computer vision, machine learning, and pattern recognition to overcome the overfitting issue (e.g., visualtracking <ref type="bibr" target="#b46">[47]</ref>, face recognition <ref type="bibr" target="#b37">[38]</ref>, sparse coding for image classification <ref type="bibr" target="#b36">[37]</ref>, joint restoration and recognition <ref type="bibr" target="#b44">[45]</ref>). Extensive research efforts have been conducted on sparse feature selection by explicitly or implicitly restricting the number of active features (e.g., LASSO <ref type="bibr" target="#b30">[31]</ref>) or the number of active feature group (e.g., group LASSO <ref type="bibr" target="#b41">[42]</ref>), where the assumption is that active features (or active groups of features) should be sparse.</p><p>In the paper, we are interested in the sparse feature selection with a particular preference on diversity, that is, the selected features are expected from different groups or categories. <ref type="figure" target="#fig_1">Figure 1</ref> provides an illustration for the different sparsity preference that are commonly used and the diversity preference considered in this paper. This diversity preference is motivated from many real-world applications such as privacy image classification, design of survey, and advertisement recommendation:</p><p>• (Private image detection) Private image detection is a new emerging task with the thriving activity of posting photos on social media. Sometimes the user uploads photos with the intent to limit the viewership to only friends and family but not to the general public due to personal information in the photos. The reason why an image is considered as private may vary, as shown in <ref type="figure" target="#fig_6">Figure 4</ref>. Different privacy type is usually better to indicate with different type of features. To better identify the general private image, diversity in feature selection is highly preferred. • (Design of survey) Survey is a very useful data collection technique that has been widely used in many fields such as HCI, education, healthcare, marketing, and etc. On one hand, the total number of questions should be restricted in a certain range. On the other hand, the principle for the design of the survey is to be thorough and comprehensive to cover all aspects. • (Advertisement recommendation) Many recommendation systems are designed to help users to find interesting and relevant items from a large information space. From a user perspective, users will feel frustrated when they are facing a monotonous recommendation list. On one hand, the total number of recommended advertisements is given. On the other hand, users expect the recommended advertisements from diverse categories.</p><p>To emphasize the diversity in feature selection, a popular approach is to use L 1,2 norm regularization <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref> such as exclusive LASSO. It is a soft diversity regularization and the selection is seriously affected by the magnitiute of weights of true features. Thus, it does not fit well for applications with strict requirement on diversity such as design of survey and advertisement recommendation. More importantly, so far there is no any theoretical result showing the benefit of emphasizing the selection diversity. This paper proposes a novel general bilevel exclusive sparsity formulation to pursue a diverse feature selection, which directly restricts the numbers of the total selected features and the features selected from each group:</p><formula xml:id="formula_0">min w2R p f (w) (1a) s.t. kwk 0  s (1b) kw g k 0  t g g 2 G (1c)</formula><p>w is the model we are pursuing. Its nonzero elements indicate the selected features (or items). f (w) is a convex smooth function characterizing the empirical loss on the training samples. For example, f (w) can take the form of the least square loss 1 2 P n i=1 (X i − y i ) 2 or logistic regression loss P n i=1 log(1 + exp(−y i X &gt; i w)). The first constraint (1b) controls the overall sparsity, that is, the total number of selected features, where the L 0 norm kwk 0 is defined as the number of nonzeros in w. The second constraint (1c) maintains the selection diversity by restricting the maximal number of selected features from each group, where w g is a sub-vector of w indexed by g ⇢ {1, 2, ···,p} and G is a super set containing all g's. (Note that t g 's are positive integers and their sum is greater than s; otherwise the constraint (1b) can be removed.) The group information usually comes from the prior information which is usually decided by the specific applications and domain knowledge. For example, in many computer vision applications, the group is automatically decided by the kind of features such as SIFT features, HOG features, GIST features, and etc. When group information is unavailable, a typical way is to cluster the features using algorithms like K-means <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this paper, we derive an efficient computational algorithm to solve (1) which is NP-hard in general, and further provide theoretical results that show convergence of the algorithm and consistency of the learning formulation. More importantly, our analysis shows the clear advantages of using bilevel exclusive sparsity than using the single overall sparsity in two senses: 1) for the noiseless case, our approach needs fewer samples to find the true solution than the approach only using a single level of sparsity; 2) our approach improves the estimation error bound from O(n −1 s log p) to O(n −1 s log(p/|G|)) under certain assumptions where n is the number of training samples, p is the total number of features, and |G| is number of groups. To the best of our knowledge, this is the first work to show the benefits of using the diversity sparsity structure, which provides fundamental understanding on diversity preference. Extensive numerical studies have been conducted on both synthetic data and real-world data which show that the proposed method is superior over existing methods such as LASSO <ref type="bibr" target="#b45">[46]</ref>, L 0 norm approach <ref type="bibr" target="#b42">[43]</ref>, and exclusive LASSO <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation and definition</head><p>• Ω(s, t) is used to denote the set {w 2 R p | kwk 0  s, kw g k 0  t g 8g 2 G}; • t 2 R |G| a vector consisting of all t g in G in a certain order; •w 2 Ω(s, t) denotes the target solution which takes an arbitrary point in this region Ω(s, t);</p><p>• P Ω(s,t) (u) = arg min w2Ω(s,t) kw − uk 2 . P Ω(∞,t) (w) removes the overall sparsity restriction and can be simply calculated by keeping the top t g largest elements in group g. Similarly, P Ω(s,∞) (w) removes the sparsity restriction for all groups and can be calculated by keeping the top s largest elements from w; • supp(w) denotes the support index set, that is, the index set of nonzero elements in w; • [w] S denotes the subvector indexed by the set S; • |S| returns the cardinality of S if S is a set or the absolute value of S if it is a real number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Extensive research efforts have been conducted on the sparse learning and sparsity regularizer. In this section, we provide a brief overview of the methods that are mostly related to our work, which include algorithms and models restricting the overall sparsity and the structured sparsity.</p><p>Overall sparsity. The most popular approaches to enforce the overall sparsity is to use the L 1 norm regularization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7]</ref>, which leads to a convex problem. L 0 norm approaches directly restrict overall sparsity, but it turns out to be NP hard problem in general. Iteratively hard threshold (IHT) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b42">43]</ref> is a commonly used approach to solve it, which is nothing but the projected gradient descent. The model L 0 norm with least square loss is considered in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref>, while <ref type="bibr" target="#b42">[43]</ref> considers the general convex smooth loss and provides theoretical error bounds and convergence rate. Our work is also based on IHT, but we consider bilevel exclusive sparsity structure simultaneously. In <ref type="bibr" target="#b25">[26]</ref>, IHT is categorized as greedy pursuits algorithm, together with Compressive Sampling Matching Pursuit (CoSaMP) <ref type="bibr" target="#b23">[24]</ref>, Orthogonal Matching Pursuit (OMP) <ref type="bibr" target="#b32">[33]</ref>, Regularized OMP (ROMP) <ref type="bibr" target="#b24">[25]</ref>, and forward-backward greedy algorithms <ref type="bibr" target="#b21">[22]</ref>. All of these approaches essentially admit the same theoretical error bound. One interesting work <ref type="bibr" target="#b17">[18]</ref> shows that by considering projection with a relaxed L 0 constraint, some assumptions which are needed for most methods can be released. Although some method (e.g. CoSaMP) can achieve better accuracy by designing more crafted procedures, there are not much theoretical improvements, and it usually brings considerable computational cost. The reason we choose IHT is that it is a straightforward approach to directly control the sparsity at all levels.</p><p>Structured sparsity. Besides of the overall sparsity, there is another type of structured sparsity. Group LASSO <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref> was proposed to extend LASSO for selecting groups of features. Fused LASSO <ref type="bibr" target="#b31">[32]</ref> penalizes the sparsity of features and their difference. Exclusive LASSO <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref> also seeks the diversity in selection and uses the convex regularizer L 1,2 norm to characterize the exclusive sparsity structure. The key difference from this paper lies on two folds: 1) L 1,2 norm is a soft constraint for diversity preference, which is not suitable for applications with strict requirement on diversity and is not robust when weights of true features are in very different scales; and 2) They do not have any theoretical guarantee or error bound to show the benefit by using the exclusive sparsity. To our best knowledge, this paper is the first work to show the theoretical guarantee and benefits of using the diversity preference. Bach et.al. show the extension of the usage of L 1 norm to the modeling of a series of structured sparsity constraint composing with groups of features <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr" target="#b40">[41]</ref> presented a bilevel sparsity model. In addition, there are some work about pursuing sparsity of multiple levels <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36]</ref>. The difference with our "bilevel" concept is their models restrict the overall sparsity and the group sparsity while our model restricts the overall sparsity and exclusive group sparsity. More importantly, previous works do not show any theoretical benefit by using the bilevel sparsity structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithm</head><p>The problem in (1) is NP hard in general. In this section, we derive the IHT algorithm for solving <ref type="bibr" target="#b0">(1)</ref>. The IHT algorithm is an iterative algorithm which is able to directly control the sparsity of the solution unlike the convex relaxation based approaches. It consists of two steps per iteration: gradient descent and projection. The gradient descent step aims to decrease the objective and the projection uses to satisfy all constraints:</p><formula xml:id="formula_1">w k+1 = P Ω(s,t) (w k − ηrf (w k ) | {z } gradient descent ) | {z } projection (2)</formula><p>where w k are the weights in the k-th iteration. The combination of these two steps per iteration will monotonically decreases the objective function value as long as the step length η is small enough. We will show it converges in a linear rate to a ball of the target solution. It is also worth to mention that the IHT algorithm is essentially the projected gradient descent algorithm from the perspective of optimization. It is simple and efficient as long as the projection step is easy to calculate. In <ref type="bibr" target="#b1">(2)</ref>, the projection step is to project a point to the set Ω(s, t) which is the intersection between the sets Ω(s, 1) and Ω(1, t). In general, the projection to the intersection between two sets would be difficult to cal-culate efficiently, but in our case due to the special structure, this projection can be decomposed into two simple projects. Lemma 1. The projection on P Ω(s,t) defined in (2) can be calculated from two simple sequential projections as following</p><formula xml:id="formula_2">P Ω(s,t) (w)=P Ω(s,∞) P Ω(∞,t) (w) .</formula><p>The projection operation P Ω(s,∞) sets all the elements to zero except the s elements which have the largest magnitude, and P Ω(∞,t) makes the same operation to each group.</p><p>Note that the lemma above greatly simplifies the computational procedure of (1) by decomposing the procedure into two simpler operations P Ω(∞,t) (w) and P Ω(s,∞) (w). Specifically, P Ω(∞,t) (w) basically removes the total sparsity restriction and it is simply to calculate by keeping the top t g largest elements in group g. Similarly, P Ω(s,∞) (w) removes the sparsity restriction for all groups which is also simply to calculate by keeping the top s largest elements of w.</p><p>The η is the step length in <ref type="bibr" target="#b1">(2)</ref>. To guarantee the convergence, η can be dynamically decided by linear search <ref type="bibr" target="#b5">[6]</ref>. The idea behinds linear search is to decrease the step length η if w k+1 does not reduce the objective function value from w k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Main Results</head><p>This section discusses the convergence rate of (2) and the advantages of the proposed algorithm and the formation by using two levels of sparsity constraints. All proofs are provided in the supplemental materials.</p><p>The main theoretical results and significance can be summarized as follows:</p><p>• (Convergence) The general analysis <ref type="bibr" target="#b42">[43]</ref> for the IHT update guarantees its convergence, but it is unclear whether it converges to the true solution. We show that the IHT update in (2) converges to a ball around the true solution with a linear rate and the radius of this ball converges to zero when the number of samples goes to infinity. This means that the solution provided by (2) will be driven to the true solution when more and more samples are received. • (Benefits of using bilevel exclusive sparsity) It is a core problem in machine learning and sparse learning (or compressed sensing) to understand the dependence of the error bound and the sample complexity. Under some commonly used assumptions in sparse learning for the least squares objective function, we prove the error bound of the proposed algorithm is</p><formula xml:id="formula_3">kŵ − w * k O ⇣ p s log(p/|G|)/n ⌘ ,</formula><p>whereŵ is the output of running the IHT updates for a certain number of iterations. To the best of our knowledge, this work is the first to provide an error bound for the exclusive sparsity motivated approaches. More importantly, our analysis shows the advantages over the approaches (such as L 0 norm approaches and L 1 norm approaches) only using the overall sparsity with the well known error bound</p><formula xml:id="formula_4">O ⇣ p s log(p)/n ⌘ .</formula><p>We believe that this analysis provides some fundamental understanding on the exclusive sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Convergence Rate</head><p>For the convergence property, we need to make some assumptions similar to most papers in compressed sensing and sparse learning. The following two assumptions essentially assume the lower bound ρ − (s, t) and the upper bound ρ + (s, t) for the curvature of f (·) in a low dimensional space Ω(s, t). However, the key difference from existing assumption lies on that our assumptions are less restrictive. While in many existing works such as <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b21">22]</ref>, where ρ + and ρ − need to hold for any pairs (w, u) satisfying w − u 2 Ω(s, t). Our assumptions only needs to hold for a subset (w,w) with w −w 2 Ω(s, t), wherew is a fixed target model. There exists ρ + (s, t) &lt; +1 which satisfies:</p><formula xml:id="formula_5">hrf (w) −rf (w), w −wi≥ ρ + (s, t) −1 k[rf (w) −rf (w)] S k 2 , 8supp(w −w) ✓ S, S 2 Ω(s, t).</formula><p>For readers who are familiar with RIP constant δ defined in Eq. (1.7) <ref type="bibr" target="#b7">[8]</ref> for the least square loss function, ρ − (s, t) and ρ + (s, t) are actually nothing but 1 − δ and 1+δ respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2. Letw be an arbitrary target model and</head><formula xml:id="formula_6">α = 2(1 − 2ηρ − (3s, 3t)+η 2 ρ + (3s, 3t)ρ − (3s, 3t)) 1/2 .<label>(3)</label></formula><p>If the step length η in <ref type="bibr" target="#b1">(2)</ref> can appropriately set to a value such that α is less than 1 and w 0 is initialized as a feasible point in Ω(s, t), we have the following results</p><p>• The k-th iteration satisfies</p><formula xml:id="formula_7">kw k −wk ↵ k kw 0 −wk+ 2 (1 − ↵)⇢ + (3s, 3t) ∆; • If k ≥ l log 2∆</formula><p>(1−α)ρ+(3s,3t)kw 0 −wk /log ↵ m , then the target features inw can be identified by w k is at least</p><formula xml:id="formula_8">|supp(w k ) \ supp(w)|≥ ⇢ j ||w j |&gt; 4∆ (1 − ↵)⇢ + (3s, 3t) where ∆ = P Ω(2s,2t) (rf (w)) .</formula><p>This theorem suggests a few important observations:</p><p>1) w k in <ref type="formula">(2)</ref> converges to the ball Bw Acute readers may notice that the implicit assumption in our theory is ↵ &lt; 1, which essentially requires the ratio ⇢ − /⇢ + &gt; 3/4 by choosing the optimal steplength. This assumption is consistent with former studies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8]</ref>. There is a recent analysis for L 0 minimization <ref type="bibr" target="#b17">[18]</ref> which can relax this requirement (⇢ − /⇢ + &gt; 0) by enlarging the scope (i.e. larger Ω) of the Assumptions 1 and 2. By applying their strategy, our result can be potentially relaxed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Theoretical Benefits of Using Bilevel Exclusive Sparsity</head><p>We explain the benefits of using the bilevel sparsity structures in this subsection. In particular, we show the advantage over the model only using the overall sparsity such as Dantzig Selector <ref type="bibr" target="#b6">[7]</ref>, LASSO <ref type="bibr" target="#b45">[46]</ref> and L 0 minimization <ref type="bibr" target="#b42">[43]</ref>.</p><p>To simplify the following analysis, we consider the least square loss f (w)= 1 2n kXw − yk 2 . Assume that w ⇤ 2 Ω(s, t) ⇢ R p is a sparse vector, and y is generated from y = Xw ⇤ + ✏ ✏ ✏ 2 R n where ✏ ✏ ✏ i are i.i.d Guassian noise N (0, σ 2 ) 1 . The data matrix X has i.i.d samples (or rows) and each sample follows the sub-Guassian distribution.</p><p>We will consider two cases to show the advantage of the proposed model: noiseless case ✏ ✏ ✏ =0and noisy case ✏ ✏ ✏ 6 =0. First for the noiseless case, we have the following result:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3. (Noiseless linear regression) For the least square loss without observation noise (that is, ✏ ✏ ✏ =0 ), assume that matrix X is sub-Guassian and has independent rows or columns. If the number of samples n is more than</head><formula xml:id="formula_9">O 0 @ min 8 &lt; : s log p, log(max g2G |g|) X g2G t g 9 = ; 1 A (4)</formula><p>then by appropriately choosing ⌘ (for example, ⌘ = 1/⇢ + (3s, 3t)) such that ↵ defined in <ref type="formula" target="#formula_6">(3)</ref> is less than 1, we have with high probability 2 that the sequence {w k } generated from <ref type="bibr" target="#b1">(2)</ref> converges to identify all features in the true solution w ⇤ .</p><p>This theorem basically suggests that the true solution w ⇤ can be exactly recovered when the number of samples is more than the quantity in (4). For the models which only consider the overall sparsity, for example, Dantzig selector <ref type="bibr" target="#b6">[7]</ref>, LASSO <ref type="bibr" target="#b45">[46]</ref>, and L 0 minimization <ref type="bibr" target="#b42">[43]</ref>, they have a similar exact recovery condition when the number of samples is more than O(s log p). Apparently, the required number of samples for our model is fewer in the order sense. Particularly, when P g∈G t g = O(s) and log(max g2G |g|) ⌧ log p, the required number of samples in (4) is much less than s log p required by Dantzig selector, LASSO, and L 0 minimization. For example, taking s = p/log p and |g|= log p, we have s log p = p and (4) = p log log p log p which suggest that the proposed model constantly improved the sample complexity. In other words, it implies the quantitative benefits of using the additional exclusive sparsity. Next, we will observe similar benefits in the noisy case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4. (Noisy linear regression) Under the same setting as in Theorem 3 except that the noise allows to be nonzero, we have with high probability</head><p>• There exists a number k 0 , such that To see the benefits, recall the error bound for Dantzig selector <ref type="bibr" target="#b6">[7]</ref>, LASSO <ref type="bibr" target="#b45">[46]</ref>, and L 0 minimization <ref type="bibr" target="#b42">[43]</ref> is O( p n −1 s log p). Our result actually improves it to Eq. (5). To see a clear quantitative improvement, we can assume that all groups have comparable sizes and t g is chosen appropriately ( P g2G t g = O(s)), then the error bound in (5) becomes O( p n −1 log(p/|G|)). Considering the same scenario as in the noiseless case, we can obtain the constant improvement as well.</p><formula xml:id="formula_10">kw k − w ⇤ k<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Although this paper mainly focuses on the theoretical side, we provide an empirical study to validate the proposed model and algorithm with L 0 norm based approach, L 1 norm based approach, and L 1,2 norm based approach. Experiments are conducted on synthetic data, real data, and a real application problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic Data</head><p>In this section, we evaluate our method with synthetic data. Our sparsity method can be used in many algorithms which pursue a sparse solution. In the experiments, we conduct two classical models in machine learning and artificial intelligence, i.e., the least square and logistic regression models. These two algorithms are both linear models which allow us to use a n ⇥ p measurement matrix X to measure the data for n dimensional observations. For both algorithms, the measurement matrix X is generated with i.i.d. standard Gaussian distribution. For generating the true sparse vector w ⇤ , we first generate a dense p dimensional vector with i.i.d. standard Gaussian distribution, then we randomly set its elements to zeros until it satisfies our requirement (i.e., the bilevel sparse constraint). For the sparsity parameters correspond to s and t, we use random integers from 1 to bp/|G|c to set elements of t, where G is the set of all groups. Elements are uniformly assigned to |G| groups. The overall sparsity s is set as 0.75 ⇥ sum(t), where the function sum(·) means the sum all of elements in the vector. The settings of group structure and sparsity s are the same for all the methods. All the synthetic experiments are repeated 30 times and we report the averaged performance.</p><p>Linear regression Firstly, we demonstrate the experiment using linear regression to recover the sparse vector w ⇤ . The dimension of the vector w ⇤ is set as p = 1000 and the number of measurements is set as n = 600. After generating the true sparse vector w ⇤ , we can get the observations y 2 R n by y = Xw ⇤ + ✏, where ✏ is a n dimensional noise vector generated from i.i.d. Gaussian distribution with mean 0 and variance 0.01. The cost function of least square is f (w)= 1 2n kXw − yk 2 . We compare our methods with three methods, i.e., L 0 norm minimization <ref type="bibr" target="#b42">[43]</ref>, L 1 norm regularization (LASSO <ref type="bibr" target="#b45">[46]</ref>), and L 1,2 norm regularization (Exclusive LASSO <ref type="bibr" target="#b18">[19]</ref>). While LASSO and L 0 minimization only considers the overall sparsity, exclusive LASSO only restrict the exclusive sparsity. For fair comparison, methods with regularization will be truncated and we only keep the largest s elements. <ref type="figure" target="#fig_4">Figure 2</ref> shows the accuracy of feature selection (i.e. |supp(w ⇤ ) \ supp(w)|/|supp(w ⇤ )|) and the relative error rate which is kw − w ⇤ k/kw ⇤ k. The step size ⌘ of projected gradient descent is chosen by line search method. For all methods, the relative error rate decreases just with the improvement of feature selection accuracy. Within these four methods, the proposed bilevel method is superior when compared to other methods.  Logistic Regression Next we move to the logistic regression model which is a classical classification model. The model learns a linear classifier by maximizing the likelihood of sample-label pairs in the training set. The ith sample X i is generated from Gaussian distribution and its label y i is decided by sign(X i w ⇤ ). We set the feature dimension p to 1000 and generate the true sparse model w ⇤ in a similar fashion to the linear regression experiment. The number of training samples is n = 2000. Besides the comparison with true sparse vector w ⇤ , we need to estimate the classification error of our learned classifier. In addition, we generate a matrix X test 2 R n⇥p by using the mean of the generated matrix X, then its label vector is sign(X test w ⇤ ). <ref type="figure" target="#fig_5">Figure 3</ref> shows the feature selection accuracy and classification error rate of our method, and its counterparts, L 0 logistic regression and L 1 logistic regression. The result of L 1 norm regularization method only keep the s largest <ref type="table">Table 1</ref>. Testing accuracy comparison among L1 logistic regression, L0 logistic regression, and the proposed Bilevel exclusive sparsity model on real datasets: Computer (ISOLET <ref type="bibr" target="#b2">[3]</ref>), Handwritten Digits (GISETTE and MNIST <ref type="bibr" target="#b20">[21]</ref>), Cancer (LEU <ref type="bibr" target="#b14">[15]</ref>, ALLAML <ref type="bibr" target="#b12">[13]</ref>, Colorectal <ref type="bibr" target="#b0">[1]</ref> and Prostate-GE <ref type="bibr" target="#b29">[30]</ref>) and Social Media (TwitterHealthData <ref type="bibr" target="#b28">[29]</ref>  <ref type="bibr" target="#b19">(20)</ref> elements, and all of the other methods are conducted in a standard L 2 norm regularized logistic regression with the selected features. The L 1 norm and L 2 norm regularized logistic regression models are implemented with LIBLINEAR <ref type="bibr" target="#b11">[12]</ref>. From <ref type="figure" target="#fig_5">Figure 3</ref>, we can see that our method consistently outperforms other methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Real Data Sets</head><p>We compare the proposed method to exclusive LASSO, L 1 norm and L 0 norm logistic regression on real datasets. For each dataset, we randomly sampled 50% of the data for training and the remaining data for testing. We repeat all experiments 10 times and reported the averaged performance for three algorithms in <ref type="table">Table 1</ref>. For our approach, s is set to sum(t) roughly for all experiments. We observed that the proposed bilevel sparsity algorithm is the best performer compared to L 1 norm and L 0 norm in logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Application on Privacy Image Classification</head><p>Photo privacy is an important problem in the digital age. Many photos are shared on social media websites such as Facebook, Twitter, Flickr, and etc. Many time, the user uploaded photos without noticing privacy sensitive information is leaked in their photos. This can lead on identify theft, stolen private photos, or cause embarrassment. Solving the photo privacy problem helps to alleviate the aforementioned problems and alerts the user before they post their photos online.  This experiment use two datasets for photo privacy detection. The first data set is a public dataset from <ref type="bibr" target="#b43">[44]</ref> which consists of with roughly 4K private photos and 33.5K public photos from Flickr. Due to the imbalance number of photos between the two classes, we randomly sampled 4K photos from the public photos set. For the second dataset, we collected photos <ref type="bibr" target="#b19">[20]</ref> that are considered as private risk in general which come from 6 classes, i.e., nude photos, identification card, wedding photos, documents, people smoking, and family (group) photos. A sample of photos in our data set is shown in <ref type="figure" target="#fig_6">Figure 4</ref>. This data set consists of roughly 3.4K private photos. We randomly sampled 3.4K public photos from the public photos of dataset from <ref type="bibr" target="#b43">[44]</ref> and used them as public photos.</p><p>We extracted 5 types of features: color histogram, histogram of oriented gradients (HOG) <ref type="bibr" target="#b10">[11]</ref>, linear binary pattern (LBP) <ref type="bibr" target="#b26">[27]</ref>, GIST <ref type="bibr" target="#b27">[28]</ref>, and bag of visual words (BoVW) with SIFT points <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10]</ref>  Different types (groups) of features depict different aspects of the visual information and emphasize different attributes of the images. As shown in <ref type="figure" target="#fig_6">Figure 4</ref>, the six private images classified into the "private" image due to different reasons look very different visually. For example, nude photos and documents photos have very different visual information. Therefore, we need to use different kinds of features to classify all of them into the general private image framework. Although traditional overall sparsity can avoid this, but the selected features may concentrate in just a few groups due to the biased dataset. For example, if the training set is dominanted by the number of nude images as private images, the selected features are also dominanted by nude images as well while important features from other types of private images could be absent in the selection procedure. This motivates us to emphasize the diversity in feature selection.</p><p>We demonstrate the performance of learning models with a relatively small training data (i.e., number of data is much smaller than the number of features). In our experiment, we randomly chose 1% ∼ 5% samples from all the samples as the training set and leave the rest as the testing set. We construct the features into 5 groups based on the 5 different feature types mentioned above. By tuning the parameters on training set, we set the sparsity t g as half of the number of total features in the group g and the overall sparsity s =0.75×sum(t). The classification error rate is shown in <ref type="figure" target="#fig_7">Figure 5</ref>. Exclusive Lasso is also compared here by setting the observations y as 1 and −1 for positive and negative samples respectively. We can see that our bilevel sparse model consistently outperforms the other sparse learning models. We also notice that all the error rates decrease significantly as the size of training set grows from 1% to 2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Application on a Complex Survey Dataset</head><p>We implemented our method on a survey dataset that was used for understanding which business practices drive firm's performance. This is a complex problem since many factors and practices could affect a firm performance. Thus, the original survey was quite comprehensive that consists of more than 20 categories (each category can be a group in the context of this paper) such <ref type="bibr" target="#b7">8</ref>  <ref type="formula" target="#formula_6">9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29</ref>   as human resources, supply chain, information technology, to name a few. To measure each category, several related items (each item is a variable) were used to measure the same latent construct. The survey was assigned to 197 companies while these companies were classified as two classes, the high-performing company and low-performing company. The dataset comprised 1701 variables and 197 observations. The primary objective of the experiment is show how the proposed method can help to improve the design of complex survey. For our approach, s is set to 0.5 × sum(t). Thus, as shown in <ref type="figure" target="#fig_8">Figure 6</ref>, the proposed method is more powerful to capture the inherent structure in the survey data. For a given number of feature that will be selected, the proposed method provides better approximation of the original dataset. Therefore, it implies that our method could help optimize the design of the survey, minimize the redundancy, and maximize the information collection power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel bilevel exclusive sparsity formulation to emphasize the diversity in feature or item in general selection, motivated by diversity preference in many real problems such as private image classification, advertisement recommendation, and design of survey. The proposed formulation can be specified by many common tasks such as regression and classification. We propose to use IHT to solve this formulation which is NP-hard in general, and prove a linear convergence rate of IHT and the error bound between the output of IHT and the true solution. Our theoretical analysis shows the improvement of using diversity sparsity over the commonly used sparse learning approaches such as LASSO and L 0 norm approaches. To the best of our knowledge, this is the first work to show such error bound and theoretical benefit of using the diversity sparsity. Experiments on synthetic data and real-world data demonstrate the effectiveness of our method and validate and the correctness of our theory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Assumption 1 .</head><label>1</label><figDesc>(Target Restricted Strong Convexity.) There exists ρ − (s, t) &gt; 0 which satisfies: hrf (w) −rf (w), w −wi≥ρ − (s, t)kw −wk 2 , 8 w −w 2 Ω(s, t) Assumption 2. (Restricted Lipschitz Gradient.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>selected features by w k (i.e., |supp(w k ) \ supp(w)|), should be more than |{j || w j |&gt;4∆(1−α)ρ+(3s,3t) }|; 3) If all channels (that is, all nonzeros) of the target solutionw are strong enough, all true features can be identified correctly after a few iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Feature selection accuracy and relative error rate for linear regression loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Feature selection accuracy and classification error rate for logistic regression loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Samples of private images data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Classification error rate on privacy image datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Empirical accuracy vs. # of selected features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>for all k&gt;k 0 , that is, all true features are identified after a certain number of iterations.</figDesc><table>O 

0 

@ min 

8 
&lt; 

: 

r 
s log p 
n 
, 

s 
log(max g2G |g|) 
P 
g2G t g 
n 

9 
= 

; 

1 

A , 

• If |w ⇤ 
j |&gt;O 
⇣ 
min 
n 
s log p, log(max g2G |g|) 
P 
g2G t g 
o⌘ 
, 

there exists a number k 00 , such that supp(w k )= 
supp(w ⇤ ) for all k&gt;k 00 , that is, all true features are 
identified after a certain number of iterations. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>).</figDesc><table>Data 
#Samples 
#Features 
#Selected features 
L1 logistic regression 
L0 logistic regression 
Exclusive LASSO 
Bilevel sparsity(#Groups) 

ISOLET 
1560 
617 
18 
74.74%±0.1127% 
80.63%±0.1013% 
77.96%±0.3559% 
81.14%±0.0729 % (10) 
MNIST 
3119 
784 
8 
96.40%±0.0670% 
97.13%±0.1011% 
95.03%±0.0957% 
97.29%±0.1138 % (10) 
ALLAML 
72 
7129 
4 
83.33%±0.0728% 
85.28%±0.1785% 
79.44%±0.1642% 
85.28%±0.0561 % (4) 
Colorectal 
112 
16331 
6 
92.14%±0.0275% 
92.14%±0.2203% 
80.71%±0.2168% 
93.75%±0.0060 % (8) 
GISETTE 
6000 
5000 
20 
90.82%±0.0109% 
93.78%±0.1528% 
91.92%±0.0743% 
94.17%±0.1360 % (20) 
Prostate-GE 
102 
5966 
7 
84.12%±0.0814% 
88.24%±0.1720% 
86.67%±0.0792% 
88.63%±0.0118 % (8) 
LEU 
72 
571 
16 
93.06%±0.0384% 
93.61%±0.3812% 
83.13%±0.0114% 
95.56%±0.0673 % (8) 
TwitterHealthData 
6873 
3846 
36 
81.96%±0.2071% 
83.39%±0.0601% 
83.33%±0.1957% 
83.50%±0.0241% </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>. For each image, we generate 5 sub feature vectors which are 768-dimensional color histogram, 1984-dimensional HOG feature vector, 3712-dimensional LBP feature vector, 512-dimensional GIST feature vector, and 512dimensional BoVW vector. These features capture the different type of image statistics such as colors, textures, and other patterns. (a) Zerr et.al.'s [44] privacy image dataset.</figDesc><table>Training percentage 

0.01 
0.02 
0.03 
0.04 
0.05 

Error rate 

0.005 

0.01 

0.015 

0.02 

L0 logistic regression 
L1 logistic regression 
Exclusive LASSO 
Bilevel sparsity 

Training percentage 

0.01 
0.02 
0.03 
0.04 
0.05 

Error rate 

0.01 

0.02 

0.03 

0.04 

0.05 

L0 logistic regression 
L1 logistic regression 
Exclusive LASSO 
Bilevel sparsity 

(b) Our privacy image dataset. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The Gaussian noise can be relaxed to sub-Gaussian noise; for example, any bounded random variable is sub-Gaussian</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">"With high probability" is a typical statement to simply the complicated probability definition and explanation. It basically says that the probability will converge to 1 when the problem dimension goes to infinity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors from University of Rochester are supported by the NSF grant CNS-1548078 and the NEC fellowship. Shuai Huang is supported by NSF grant CMMI-1505260.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biomarker discovery in maldi-tof serum protein profiles using discrete wavelet transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Deelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A T P</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="643" to="649" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured sparsity through convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="450" to="468" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adaptive regression and model selection in data mining problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative thresholding for sparse approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="629" to="654" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2313" to="2351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4203" to="4215" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilabel visual classification with label exclusive context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="834" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Learning in Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sequencing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Massively Parallel Genomics. Science</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<biblScope unit="page" from="393" to="395" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hard thresholding pursuit: an algorithm for compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Foucart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2543" to="2563" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gaasenbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Mesirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Coller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Caligiuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A tail inequality for quadratic forms of subgaussian random vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Commun. Probab</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">52</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exclusive sparsity norm minimization with random groups via cone projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07925</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On iterative hard thresholding methods for high-dimensional m-estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="685" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exclusive feature learning on arbitrary structures via`1 ,2 -norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fujimaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1655" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Privacy-cnh: A framework to detect photo privacy with convolutional neural network using hierarchical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Forward-backward greedy algorithms for general convex smooth functions over a cardinality constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fujimaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cosamp: Iterative signal recovery from incomplete and inaccurate samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Needell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="301" to="321" />
		</imprint>
	</monogr>
	<note>Applied and Computational Harmonic Analysis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Signal recovery from incomplete and inaccurate measurements via regularized orthogonal matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Needell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vershynin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="310" to="316" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Linear convergence of stochastic iterative greedy algorithms with sparse constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Needell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Woolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.0088</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadilek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Silenzio</surname></persName>
		</author>
		<title level="m">Which restaurants should you avoid today? First AAAI Conference on Human Computation and Crowdsourcing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gene expression correlates of clinical prostate cancer behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Febbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ladd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Amico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Richie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Lander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Kantoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Sellers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="209" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparsity and smoothness via the fused lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="108" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Greed is good: Algorithmic results for sparse approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2231" to="2242" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to the non-asymptotic analysis of random matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vershynin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.3027</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online feature selection with group structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient sparse group feature selection via nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="284" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simultaneous feature and feature group selection through hard thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="532" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bi-level multi-source learning for heterogeneous block-wise missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="192" to="206" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Gradient hard thresholding pursuit for sparsity-constrained optimization. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Privacy-aware image classification and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Demidova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;12</title>
		<meeting>the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Close the loop: Joint blind image restoration and recognition with sparse representation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="770" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Some sharp performance bounds for least squares regression with l1 regularization. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2109" to="2144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Robust visual tracking via multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2042" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exclusive lasso for multi-task feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAIS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="988" to="995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
