<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Scale Hard Sample Mining with Monte Carlo Tree Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Canévet</surname></persName>
							<email>olivier.canevet@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">2É cole Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<orgName type="institution">Idiap Research Institut</orgName>
								<address>
									<country>Switzerland, Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Fleuret</surname></persName>
							<email>francois.fleuret@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">2É cole Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<orgName type="institution">Idiap Research Institut</orgName>
								<address>
									<country>Switzerland, Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large Scale Hard Sample Mining with Monte Carlo Tree Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate an efficient strategy to collect false positives from very large training sets in the context of object detection. Our approach scales up the standard bootstrapping procedure by using a hierarchical decomposition of an image collection which reflects the statistical regularity of the detector's responses.</p><p>Based on that decomposition, our procedure uses a Monte Carlo Tree Search to prioritize the sampling toward sub-families of images which have been observed to be rich in false positives, while maintaining a fraction of the sampling toward unexplored sub-families of images. The resulting procedure increases substantially the proportion of false positive samples among the visited ones compared to a naive uniform sampling.</p><p>We apply experimentally this new procedure to face detection with a collection of ∼100,000 background images and to pedestrian detection with ∼32,000 images. We show that for two standard detectors, the proposed strategy cuts the number of images to visit by half to obtain the same amount of false positives and the same final performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning techniques for object detection require very large sets of negative examples, which are usually used through a bootstrapping procedure. The training process constructs a sequence of predictors of increasing performance, each trained from a fixed set of positive samples and a collection of so called "hard" negative samples that fool the previous predictor.</p><p>Such an approach enriches the training set with negative samples that get closer and closer to the boundary between the positive and the negative populations, which are the ones that matter for a discriminative criterion. From a computational perspective, bootstrapping decouples the selection of the interesting (negative) samples from their use for training the model. The latter usually has a cost linear with the number of selected samples, which is far less than the number  of samples in the full set. However, the selection process from the full set requires one evaluation of the predictor per candidate sample, and remains linear with the total number of samples.</p><p>In practice one observes that the frequency of falsepositives in images is highly structured: certain types of images exhibit statistical regularities that generate more or less frequent false-positives. Similar structures can be observed in the images themselves: Large uniform patches (sky, empty walls) can be ignored, while high-frequency or highly-structured parts (trees, buildings, bookshelves) should be examined in detail. If one has to collect images from the web to create a "good" set of background images, she/he would quickly get a good intuition about which images to select and which to ignore. Indeed, the quality of images as sources of hard samples is strongly related to the geographical environment or type of events they depict, or indirectly to the time period, photographer, or even the web site they originate from. In a video for instance, timeconsistency induces a strong regularity of the proportion of hard samples in contiguous frames.</p><p>The existence of such structures motivates the use of a hierarchical process able to concentrate computation recursively, figuring out automatically at what scale (image sets, image sub-sets, image) it should make a decision about investing or not more computation in the corresponding samples.</p><p>We propose to formalize the problem by first defining a tree-structure whose leaves are individual images, and whose nodes correspond to small groups of content/temporal related images in the bottom level (street, flowers, indoor, etc.), and larger groups of dataset related images in the top level (dataset, origin, etc., see <ref type="figure" target="#fig_2">figure 2</ref> for an example). If the structure is given (temporal structure, keywords, etc.) then no pre-processing is needed. Given such a tree and an existing predictor, each leaf is labeled with a score that reflects how many false positives it contains.</p><p>Our objective is to use that tree-structure to efficiently sample among false-positives, that is to maximize the fraction of false positives we find among the samples we actually look at.</p><p>Without an additional structure, this problem amounts to an exploration-exploitation dilemma: We want at the same time to "exploit" the groups of images we have already identified as promising, that is are rich in false-positives, but we also want to invest a fraction of our computational effort to "explore" new groups of images.</p><p>Framed in such a way, a natural response to the problem is the use of the Monte Carlo Tree Search (MCTS). This technique associates a multi-arm bandit to each node of the tree, and uses them to sample paths down the tree based on the current estimates of rewards, or in our case, of proportion of hard samples in the sub-trees. While MCTS is traditionally used to characterize the good choice to make at the top node, the by-product we use here is the list of leaves it has visited during sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Object detection and bootstrapping</head><p>Object detection aims at predicting the position and the scale of all the instances of an object class in an input image. Most detectors use a binary classifier which discriminates the object from the background, and evaluates it at all positions and scales in the image. Multiple detections are removed with a non-maxima suppression post-processing.</p><p>The binary classifier is trained with a population of positive samples corresponding to location and scales in images where the object is visible, and a population of negative samples uniformly taken in (parts of) images where the object of interest is not visible. Then a bootstrapping approach <ref type="bibr" target="#b31">[30]</ref> is used to improve the classifier by assembling better negative sample sets: The training set is augmented by a collection of misclassified samples and used to train a new classifier. This procedure emphasizes difficult samples that lie at the boundary between the two classes and can be repeated multiple times. A hard sample can be defined as being on the wrong side of the boundary <ref type="bibr" target="#b33">[32]</ref> or in the margin of the classifier <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>The number of bootstrapping steps varies depending on the complexity or the nature of the classifier and on the number of hard samples that are added: Dalal and Triggs <ref type="bibr" target="#b9">[10]</ref> perform only one step of bootstrapping but add all the false positives that are found until it no longer fits in memory. The pedestrian detector of Dollar et al. <ref type="bibr" target="#b12">[13]</ref> is trained in a soft cascade fashion with three rounds of bootstrapping, each time adding 5,000 new samples. The deformable part based model (DPM) <ref type="bibr" target="#b15">[16]</ref> is trained with a maximum of ten rounds for each re-labeling of the positive samples, with a stopping criterion based on the variation of the objective function. Finally when a detector is trained in a cascade fashion, the bootstrapping procedure is applied at each level. Henriques et al. <ref type="bibr" target="#b17">[18]</ref> showed that in the particular case of linear classifiers, the Gram matrix of translated samples can efficiently be computed with non overlapping windows which allows to train with the fully translated set of samples without any bootstrapping step.</p><p>Most of the works in object detection have focused on proposing new features to improve the performances of the final detector such as Haar-wavelets <ref type="bibr" target="#b33">[32]</ref>, histograms of oriented gradients (HOG) <ref type="bibr" target="#b9">[10]</ref>, optical-flow based features and self-similarity <ref type="bibr" target="#b34">[33]</ref> or aggregated channel features (ACF) <ref type="bibr" target="#b12">[13]</ref>. Besides, other works have concentrated on speeding up the detection at test time. Cascades <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b2">3]</ref> or coarse-to-fine approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">28]</ref> allow to reject many windows in the early stages and concentrate most of the computation on promising parts of the image. In the case of the DPM, the convolutions can be efficiently computed in the Fourier domain <ref type="bibr" target="#b14">[15]</ref> or hashed into tables for fast access <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">29]</ref>. Nevertheless, little work has been done to efficiently build the set of hard samples during the bootstrapping step. For instance, the works by Canévet et al. <ref type="bibr" target="#b4">[5]</ref> and Kalal et al. <ref type="bibr" target="#b18">[19]</ref> sample respectively in the image plan, and in sample sets, without prior structure over the databases nor explicit concern for the exploration/exploitation trade-off. The structure of the data-set is never exploited in order to find the hard samples faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Monte Carlo Tree Search (MCTS)</head><p>Monte Carlo Tree Search is a method to find the optimal solution in a given and potentially huge search space <ref type="bibr" target="#b3">[4]</ref>. MCTS balances between analyzing promising moves in the space (exploitation) and expanding the tree randomly (exploration). It has gained a lot of interest in the Artificial Intelligence community in the last decade because of the huge improvement it brought in the game of Computer Go.</p><p>MCTS has successfully been applied to games to make a computer play against a human. Previous strategies such as αβ <ref type="bibr" target="#b20">[21]</ref> or A * <ref type="bibr" target="#b19">[20]</ref> have shown to be efficient against humans for the game of chess or checker because it is quite easy to evaluate the outcome of the game given the current state. But for games such as Go or Backgammon, computers have long been unable to defeat non-professional players until MCTS appeared. By doing randomized simulations of the game and biasing simulations towards a successful end for the computer, MCTS is able to find the next best move to be made. Many works have been done to formulate MCTS for games <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7]</ref> and computers are now able to defeat human on small boards for the game of Go.</p><p>Another example of successful use of MCTS is the optimization of a "black-box" function <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b7">8]</ref> where the goal is to get a good estimate of the maximum of the function (deterministic or stochastic) by evaluating it only a limited number of times. The idea is to design a sequence of input samples on which the function should be evaluated given the previously observed values. The space is split in a hierarchical manner, and MCTS determines in which subspace the function should be evaluated next. As the process goes on, the procedure converges to the subspace where the function is maximal.</p><p>In all these applications, the input space can be represented with a tree that serves as the support for MCTS. We now describe more precisely the basic run of MCTS in the case of a two-player game, in which the machine tries to determine the most promising move to be made against the human player.</p><p>For every move, one considers a tree whose root node corresponds to the current configuration of the game, whose internal nodes correspond to possible future configurations, and whose leaves are winning configurations. Using this tree, the sampling procedure of MCTS (see <ref type="figure" target="#fig_3">Figure 3</ref>) recursively goes down the tree as follows: In every node, if some of the children have never been visited, one is selected at random uniformly. If all children have been visited at least once, the selection is framed as a multi-armed bandit problem <ref type="bibr" target="#b1">[2]</ref> to optimally tackle the exploitation/exploration dilemma. When a leaf is reached, that is a wining configuration for one of the two players, the reward is backpropagated up to the root, and the statistics at each node regarding the number of times it was visited and the fraction of winning outcomes are updated.</p><p>This sampling is repeated until a computational/time budget is exhausted, at which point the next move is made by selecting the best child of the root which is the one with</p><formula xml:id="formula_0">Algorithm 1 UCB1 [2] ∀ k,X k ← 0, n k ← 0 for t = 1 to T do Select arm k t = argmax kXk + 2 ln t n k</formula><p>Observe reward X t from arm k t UpdateX kt and n kt end for the maximum proportion of wins.</p><p>MCTS and bandit algorithms have nice theoretical properties. In particular, one such property is the guarantee that they only expand the optimal part of the tree. Moreover the tree structure of MCTS allows to deal with very large spaces leaving unexpanded unpromising parts of the domain.</p><p>As explained in detail in § 3.2, we propose in this paper to formulate the problem of mining hard samples in a MCTS way. We associate to each leaf an image, and a positive reward if it contains false-positives. However, instead of using the MCTS to eventually select a good child at the root node, we keep track of the "good" samples to retrain the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>For clarity, we first recall below the basics of multi-arm bandits and Monte Carlo Tree Search. Then, in § 3.3, we present how we adapt such strategies to bootstrapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-armed bandit (MAB)</head><p>As explained in section 2.2, the selection of the next child to visit is formulated as a MAB problem, that we now describe using the analogy of a gambler in a casino. Given a slot machine with K arms, at each iteration t ∈ 1, T , the player selects one arm and plays it. This generates a reward following an unknown distribution described by θ k with support in [0, 1], and of unknown expectation µ k . The arm with the largest expectation is called optimal and is the one that the player would play all the time, had he knew it was optimal. The goal of the player is to maximize his cumulative payoff, or equivalently, to minimize his cumulative regret, that is the loss due to not playing the optimal machine all the time. Lay and Robbins <ref type="bibr" target="#b24">[24]</ref> proved that the regret grows at least logarithmically with the number of plays. Solving the MAB problem consists in finding a policy to select the next arm to pull given past observations and to achieve a logarithmic regret.</p><p>Many algorithms have been proposed to select the best arm at a given iteration <ref type="bibr" target="#b1">[2]</ref>. Although the usual policy used for the MAB in MCTS is UCB1, we will also present Thompson sampling because the reward we aim at modeling -namely the proportion of false positives -is strongly biased toward very small values which is inconsistent with the standard assumptions justifying the use of UCB1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Thompson Sampling [6]</head><p>1: D ← ∅ 2: for t = 1 to T do <ref type="bibr">3:</ref> For each arm, draw θ k ∼ P (θ|D) ∝ P (D|θ)P (θ) <ref type="bibr" target="#b3">4</ref>:</p><formula xml:id="formula_1">Select arm k t = argmax k E[X k | θ k ] 5:</formula><p>Observe reward X t from arm k t 6:</p><formula xml:id="formula_2">D ← D ∪ (k t , X t ) 7: end for 3.1.1 Upper Confident Bound (UCB1)</formula><p>The UCB1 <ref type="bibr" target="#b1">[2]</ref> selects the arm maximizinḡ</p><formula xml:id="formula_3">X k + 2 ln t n k ,</formula><p>whereX k is the average reward of arm k (estimated from the previous plays), n k the number of times arm k has been played and t the total number of plays done so far. The first term is the exploitation term and is larger for arms with more rewards. The second term is the exploration term and tends to be larger for less frequently pulled arms. As the exploration term is a decreasing function of the time, the beginning of the process is dominated by the exploration term while the end of the process is driven by the exploitation one. Asymptotically, only the best arms are pulled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Thompson sampling for MAB (TS)</head><p>Thompson Sampling <ref type="bibr" target="#b32">[31]</ref> was introduced to address the exploration/exploitation trade-off in a purely Bayesian manner. It was applied recently for the MAB problem in <ref type="bibr" target="#b5">[6]</ref>, and then proved to achieve a logarithmic regret <ref type="bibr" target="#b0">[1]</ref>. The idea of TS is to assume a prior distribution on the parameter θ k of the distribution of each arm, and at each iteration t, to play the arm according to its posterior probability of being optimal, that is choosing the arm maximizing E[X k |θ k ], where θ k is drawn from the posterior at each iteration. The use of TS is motivated by the fact that it can explicitly embeds a model of the rewards with a long tail distribution as opposed to UCB1 which are constrained to be in [0, 1].</p><p>In this setup, the exploration phase occurs in the beginning of the sequence when the posterior distributions are not estimated with many observations, and as the number of observations increases, the estimation of the posterior is better, and as for UCB1, only the best arm is pulled asymptotically. Algorithms 1 and 2 summarize the selection of the next arm for both presented strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Monte Carlo Tree Search (MCTS)</head><p>In MCTS, the traversal of the tree is performed from top to bottom. Starting from the root node, one iteration of MCTS consists in associating a MAB on the children of the current node, selecting the best child (that is the one maximizing the score of MAB), and going further down with the new selected node. When the policy reaches a leaf, a reward is drawn from it.</p><p>If the MAB policy is UCB1 ( § 3.1.1), the child selected is the one maximizingX</p><formula xml:id="formula_4">i + 2 ln p n i ,</formula><p>where p (resp. n i ) is the number of times the current node (resp. child i) has been visited.</p><p>If the MAB policy is based on TS ( § 3.1.2), the next child is chosen by sampling according to its posterior probability, based on the passed traversals of the current node.</p><p>The reward is then back-propagated and the statistics of the nodes between the leaf and the root are updated (see <ref type="figure" target="#fig_3">figure 3</ref>). The number of visits are incremented; for UCB1, the mean of the nodes are recomputed given the outcome of the simulation (win or loss) and for TS, the posterior is updated for future draws.</p><p>The MCTS policy will asymptotically visit the best branches that lead to more wins leaving unexpanded non promising parts of the search space. The MCTS framework seems therefore well suited for the task of mining hard samples in a large structured collection of images.</p><p>We next present how MCTS is ported to bootstrapping. We define the rewards obtained at the leaves (i.e. in the images) after detecting false positives and how the tree is updated to avoid going back to the same images twice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MCTS Bootstrapping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Image dataset structure</head><p>As said in the introduction, image datasets inherently have a hierarchical structure by the way images were collected, and our procedure builds upon this structure (see <ref type="figure" target="#fig_2">figure 2)</ref>.</p><p>In the top level, the children of the root corresponds to a specific image dataset, such as Pascal, INRIA, or any image directory available on one's hard disk.</p><p>Further down the tree, nodes would correspond to subparts of each datasets, such as the year for Pascal <ref type="bibr">(2007, 2008, etc.)</ref> or the name of the semantic object contained in the sub-directory for SUN (desert, abbey, etc.). Finally, at the bottom of the tree, leaves are individual images.</p><p>When an image dataset comes with no explicit structure (such as Microsoft Coco <ref type="bibr" target="#b25">[25]</ref> or INRIA Person <ref type="bibr" target="#b9">[10]</ref>), a preclustering can be applied build sub-groups of visually similar images, which correspond in practice to coarse semantic categories of similar structural complexity (see § 4.2.2). This is what is depicted by figure 2 below the "INRIA" node, where images taken from the same place (city, forest, etc.) have a common ancestor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Procedure</head><p>We now explain in detail how MCTS Bootstrapping works to train an object detector. We assume we have this very large structured database.</p><p>After training the initial detector with the collection of positive samples and a collection of negative samples uniformly taken in the dataset, the detector is bootstrapped several times by adding false detections.</p><p>We recall that in the traditional setting, the detector is applied on random images from the dataset until finding enough hard samples.</p><p>In MCTS Bootstrapping, the next image on which to apply the detector is chosen by traversing the tree from the root in an MCTS fashion. A first MAB selects the dataset from which the image will be chosen. Then a second MAB selects from which sub-part of this dataset, etc., until eventually reaching an image. The detector is applied on it, the hard samples (if any) are kept (that is receiving a reward) and the outcome of the play is eventually back-propagated up to the root by updating the various statistics of all the nodes that were traversed. The image is marked as "exhausted" not to be selected anymore in the future steps.</p><p>This process is then repeated to select another image, this time based on the new updated statistics, until enough hard samples are found. As hard samples are found, the MCTS policy progressively concentrates its sampling on more promising parts of the tree, hence dataset, that is on sub-groups of images which are rich in hard samples.</p><p>Statistics are reset to 0 before beginning a new bootstrapping phase because images that produced false positives in previous rounds may no longer be informative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Scores</head><p>As described in § 3.2, each node contains the number of wins that were obtained after traversing it, and the number of times it was traversed. We propose to adapt these scores to our scheme of MCTS Bootstrapping. We also recall that the rewards should be in the full range [0, 1] <ref type="bibr" target="#b1">[2]</ref>.</p><p>As the goal is to find false detections, the reward obtained after applying the detector should be a function of this amount h of hard samples in the image which was eventually selected. The first score that we can defined is</p><formula xml:id="formula_5">win = 1 if h &gt; 0 0 otherwise.</formula><p>This "win" score corresponds to the vanilla setup of the MAB where the reward is either 0 or 1 ("did the player win or not?"). The back-propagation rule to update the score of node p with the scores of its children C(p) is win p = c∈C(p) win c .</p><p>However, the score should reflect the size of an image, to leverage the fact that finding the same amount of false detections in two images of different size does not have the same (computational) cost. A natural score would be d = h/S, where S is the number of times the detector is evaluated in the image, which is proportional to its size in pixels. d is thus the true density of false positives in the image.</p><p>Preliminary experiments show that the true density does not suit UCB1 policy because the rewards are small. A detector can be evaluated ∼100, 000 times on a 640 × 480 image so finding 10 hard samples leads to a reward of 0.0001. The exploration term dominates the small exploitation score and there is no exploitation.</p><p>We thus normalize this score so that it ranges in [0, 1] and finally, we definẽ</p><formula xml:id="formula_6">d = min 1, 1 2Z h S ,</formula><p>where Z is the mean of the density of hard samples that is estimated as the images are visited. The min ensures that the score lies in [0, 1]. Basically, when the density of hard samples is around the mean, the reward will be 0.5, and when an image is rich in false positives (i.e. h/S much larger than Z), the score will be close to 1. The score is back-propagated withd p = c∈C(p)d c . So each node i contains the number of times n i it was visited (0 or 1 for an image), the number of wins win i (0 or 1 for an image), the number of hard samples h i found below that node, and the number of times S i the detector was evaluated. In addition to that, a Boolean flag indicates if there are still non-visited images below a node, thus avoiding going to "exhausted" branches.</p><p>Given the various scores, we define 3 different policies for the MAB: two of them for UCB1 and one for TS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">MCTS-strategies</head><p>MCTS-UCB1-win (win) The first strategy is based on UCB1 ( § 3.1.1) with the "win" score. At a given node p, the next node to select among its children C(p) is the one maximizing win c n c + 2 ln p n c .</p><p>win c /n c is the average number of images in which at least one hard sample was found. This score reflects the probability of finding at least one hard sample in an image and corresponds to the simplest bandit scenario in which rewards are either 0 or 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCTS-UCB1-dense (dense)</head><p>The second one is based on the normalized density of hard samples and the next child In both UCB1 policies, the exploration score 2 ln p/n c simply reflects time, just as in vanilla MCTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCTS-Thompson-sampling (ts)</head><p>For Thompson sampling ( § 3.1.2) as described in <ref type="bibr" target="#b5">[6]</ref>, a model of the distribution of the observations is required as well as a prior on its parameter. As previously stated, TS does not require the rewards to lie in [0, 1] so we here directly use the true density.</p><p>Some simple experiments (see <ref type="figure" target="#fig_4">figure 4)</ref> show that the distribution of the density of false positives in images has an exponential shape. We thus model the true density d i of hard samples with an exponential distribution p(D|λ) = λe −λx (with notations of algorithm 2, θ = λ). If we use the conjugate prior of the exponential distribution, that is the Gamma prior Γ(α, β), then the posterior is also a Gamma distribution with parameter α ′ = α + n i and β ′ = β + d i , with n i being the number of times node i was visited and d i = h i /S i the true density of hard samples.</p><p>So at a given node p with children C(p), the MCTS Bootstrapping based on TS selects the next child by:</p><p>1. Drawing λ c from Γ(α + n c , β + d c ), 2. Selecting argmin c λ c (the expectation of an exponential distribution is</p><formula xml:id="formula_7">1/λ, so argmax c E[X c |λ c ] = argmin c λ c ).</formula><p>We now compare these three strategies to the traditional uniform bootstrapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present the results of our experiments to train a face detector and a pedestrian detector with a large dataset of images. We show that our MCTS-based bootstrapping approach is able to leverage the tree structure of the dataset to efficiently find hard samples. We will make our code publicly available at the time of publication to allow the reproduction of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Detectors</head><p>The ACF detector <ref type="bibr" target="#b12">[13]</ref> belongs to the state-of-the-art detectors for pedestrian detection <ref type="bibr" target="#b35">[34]</ref> and face detection <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b37">36]</ref>. It consists of a series of channels which are combined with boosted small depth trees. We used in part the implementation provided by the original author <ref type="bibr" target="#b11">[12]</ref>.</p><p>The DPM <ref type="bibr" target="#b15">[16]</ref> also reaches state-of-the-art performance for face detection <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b36">35]</ref> and we used the Fourier-based implementation of <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image datasets with a tree structure 4.2.1 CaltechPedestrians</head><p>The pedestrian detector is trained on the CaltechPedestrians dataset <ref type="bibr" target="#b13">[14]</ref> as described in <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b11">12]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Face-free images</head><p>The face detector ACF (resp. DPM) is trained with 15, 000 (resp. 7, 000) images of faces from AFLW <ref type="bibr" target="#b23">[23]</ref> and we use a collection of 102, 230 background (face-free) images. We have used 7, 537 images from Pascal and 24, 685 from Microsoft Coco <ref type="bibr" target="#b25">[25]</ref>. In addition to that, we have downloaded images from Flickr using keywords which a priori are useful to train a face detector (animal, trees, etc.) or useless (sky, desert, landscape, etc.). On average, we collected 3, 000 images of these categories.</p><p>The structure of the tree is obvious for "keyword" images: images of desert, trees or animals will each make a node (see <ref type="figure" target="#fig_2">figure 2</ref>). As for Coco and Pascal, there is no inherited structure. To make one, we perform a pre-clustering of these datasets: We make a 48 × 64 thumbnail of each image, compute its features (gradient or GIST) and recursively perform a k-mean clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>MCTS Bootstrapping is faster than the traditional uniform approach because it is able to concentrate its search for hard samples on promising parts of the dataset.</p><p>We present our results by looking at the number of images required to find the targeted number of hard samples. For the ACF detector, averaged computing times estimated on 40, 000 images give 0.047s for computing the features and 0.011s for evaluating the detector per image. The Fourier-based DPM on 640 × 480 images requires 0.041s to compute the HOG features and 0.1s to do the convolutions. This justifies the use of the number of images instead  of the number of evaluation of the detector as a reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Reduction of the number of visited images</head><p>The main performance measure for the proposed methods, that is the number of images to visit to collect a required number of hard samples, is presented in tables 1 and 2. Our MCTS-based methods need roughly half as many images than the traditional uniform approach, without hurting the performance of the resulting trained detector (last columns).   <ref type="figure">Figure 5</ref>: These plots show how often the datasets are visited by the procedure during the second bootstrapping step to collect 5, 000 hard samples with the ACF detector. The uniform method selects a dataset proportionally to the number of images in the set (to be uniform over the entire dataset) and requires more than 2, 500 images to find enough false positives (x axis cut). Our MCTS bootstrapping approaches identify the most promising datasets (Coco, Pascal, animals) while leaving the less interesting (sky, fingerprints, etc.). Note that "coco-plane" contains many images of plane/bird on blue sky (i.e. less dense in hard samples), so within a (structured) dataset, MCTS is also able to discard sub-branches and visit it less often. <ref type="figure">Figure 5</ref> shows how often the datasets are visited over time when training an ACF-face detector. The uniform approach selects images uniformly in the whole dataset, that is proportionally to the number of images in each sub-sets. The Pascal dataset is more visited because it contains 7, 537 images while the other sets have around 3, 000. Each dataset is visited linearly with time. Unsurprisingly the MCTS approaches identify the datasets Pascal, Coco, animals or stairs are being rich in hard samples and visit them more often. Datasets of sky, desert of fingerprints are quickly identified as being useless. The speed-up is therefore due to the identification of good branches of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Behavior of the sampling</head><p>When MCTS methods are applied on a pre-clustered dataset (suffix "clust" and "gist" in table 1) the speed-up is even more than on non-clustered dataset. This is due to the fact that in Coco and Pascal sets, the pre-clustering put uninformative images in the same clusters such as planes over a blue sky or landscapes. MCTS thus identifies uninformative sub-branches. We did not use this pre-clustering step on CaltechPedestrians because the temporal structure is consistent in itself. MCTS on a shuffled data-set (suffix "shuf") performs as poorly as the uniform approach. <ref type="figure">Figure 6</ref> shows how many times a sequence of Caltech-Pedestrians was selected as a function of its average number of hard samples. One point is a sequence. Sequences with few hard samples (left part of x-axis) are less visited, while sequences with more hard samples are visited more often (right part). MCTS methods have concentrated their sampling on rich sequences hence the speed-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel approach to collect hard negatives from large databases of images. Instead of visiting images in a uniform and unstructured manner, we use a hierarchical  <ref type="figure">Figure 6</ref>: All sequences are equally visited by the uniform approach whereas richer sequences in hard samples are much more selected by the bandit based strategies. structure that goes from the level of collections of databases down to the individual images, and to leverage that architecture with a bandit-base exploration strategy. This formulation ensures a proper scalability by relying on sound procedures for balancing exploration and exploitation. Experiments on a collection of more than 100, 000 images show that this approach properly concentrates computation on "good images" and reduces the number of samples to visit to find the same amount of false positives by a factor of two for 2 types of detectors, ACF and DPM.</p><p>MCTS is well suited to concentrate properly computational resources on large databases for machine learning at large. It could be combined with active learning, or extended to other tasks where the availability of handprovided labels is not critical, for instance large-scale unsupervised feature learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Number of images visited vs. bootstrapping iterations for a the ACF-face detector (top) and a DPM-face detector (bottom). Our methods using a Monte-Carlo tree search (blue and green) focus on difficult images and visit roughly half the number of images the traditional approach needs (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Example of a structured image database. The structure can be explicit such as in the SUN dataset (facade, sky, etc.) or implicit like in the INRIA Person dataset, where images can be grouped either based on their names or after a pre-clustering of the images. Videos have a temporal structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>One run of the traditional MCTS<ref type="bibr" target="#b3">[4]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Distribution c is the normalized density of false positives as described in 3.3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>by using 24, 498 positive examples and 3 rounds of bootstrapping each time adding 25, 000 hard samples. The structure of the tree is pretty straight forward because of the temporal structure of the dataset. The sequences (set00-V000, set00-V001, etc.) are at the top of the tree while the images are arranged chronologically at the bottom of the tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The top plot of figure 1 summarizes table 1 by showing the cumulated number of images required to train an ACFface detector. Regarding DPM, we only present the cumulated number of images on the bottom plot of figure 1 for space consideration. For all re-labeling steps of the positive examples, MCTS strategies require half as many images as the uniform approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Number of images visited to train an ACF pedestrian detector (averaged over 10 runs). The temporal structure of the video was kept to build the tree. The performance on the CaltechPedestrians test set is the miss rate at a given number of false positive per image (fppi).</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Analysis of thompson sampling for the multi-armed bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1111.1797</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of monte carlo tree search methods. Computational Intelligence and AI in Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tavener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient sample mining for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Canévet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Machine Learning (ACML)</title>
		<meeting>the Asian Conference on Machine Learning (ACML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical evaluation of thompson sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2249" to="2257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Van Den Herik. Monte-carlo strategies for computer go</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaslot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bouzy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uiterwijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th BeNeLux Conference on Artificial Intelligence</title>
		<meeting>the 18th BeNeLux Conference on Artificial Intelligence<address><addrLine>Namur, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bandit algorithms for tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Coquelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient selectivity and backup operators in monte-carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers and games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast, accurate detection of 100,000 object classes on a single machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1814" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Piotr&apos;s Image and Video Matlab Toolbox (PMT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<ptr target="http://vision.ucsd.edu/˜pdollar/toolbox/doc/index.html.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exact acceleration of linear object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dubout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coarse-to-fine face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="85" to="107" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond hard negative mining: Efficient detector learning via blockcirculant decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weighted sampling for large-scale boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A parsing: fast exact viterbi parse selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An analysis of alpha-beta pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="293" to="326" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bandit based monte-carlo planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: ECML 2006</title>
		<imprint>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Asymptotically efficient adaptive allocation rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A coarse-to-fine approach for fast deformable object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1353" to="1360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">30hz object detection with dpm v5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<biblScope unit="page" from="65" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Example-based learning for viewbased human face detection. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="34" to="47" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">New features and insights for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR), 2010 IEEE conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1030" to="1037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H H</forename><surname>Woonhyun Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The fastest deformable part model for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2497" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Biometrics (IJCB)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
