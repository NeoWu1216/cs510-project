<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Less is more: zero-shot learning from online textual documents with noise suppression *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Qiao</surname></persName>
							<email>ruizhi.qiao@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
							<email>lingqiao.liu@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<email>chunhua.shen@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<email>anton.vandenhengel@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Less is more: zero-shot learning from online textual documents with noise suppression *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classifying a visual concept merely from its associated online textual source, such as a Wikipedia article, is an attractive research topic in zero-shot learning because it alleviates the burden of manually collecting semantic attributes. Recent work has pursued this approach by exploring various ways of connecting the visual and text domains. In this paper, we revisit this idea by going further to consider one important factor: the textual representation is usually too noisy for the zero-shot learning application. This observation motivates us to design a simple yet effective zero-shot learning method that is capable of suppressing noise in the text.</p><p>Specifically, we propose an l 2,1 -norm based objective function which can simultaneously suppress the noisy signal in the text and learn a function to match the text document and visual features. We also develop an optimization algorithm to efficiently solve the resulting problem. By conducting experiments on two large datasets, we demonstrate that the proposed method significantly outperforms those competing methods which rely on online information sources but with no explicit noise suppression. Furthermore, we make an in-depth analysis of the proposed method and provide insight as to what kind of information in documents is useful for zero-shot learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unlike traditional object classification tasks in which the training and test categories are identical, zero-shot learning aims to recognize objects from classes not seen at the training stage. It is recognized as an effective way for large scale visual classification since it alleviates the burden of collecting sufficient training data for every possible class. The key component ensuring the success of zero-shot learning is to find an intermediate semantic representation to bridge the * The first two authors contributed to this work equally. Correspondence should be addressed to C. Shen. gap between seen and unseen classes. In a nutshell, with this semantic representation we can first learn its connection with image features and then transfer this connection to unseen classes. So once the semantic representation of an unseen class is given, one can easily classify the image through the learned connection.</p><p>Attributes, which essentially represent the discriminative properties shared among both seen and unseen categories, have become the most popular semantic representation in zero-shot learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26]</ref>. Although the recent use of attributes has led to exciting advances in zero-shot learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27]</ref>, the creation of attributes still relies on much human labour. This is inevitably discouraging since the motivation for zero-shot learning is to free large-scale recognition tasks from cumbersome annotation requirements.</p><p>To remedy this drawback and move towards the goal of fully automatic zero-shot learning, several recent works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref> have explored the possibility of using the easily accessed online information sources to create the intermediate semantic representation. One possible choice is to directly use online textual documents, e.g., those found in Wikipedia, to build such a representation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref>. This is promising because online text documents can be easily obtained and contain rich information about the object. To conduct zero-shot learning with textual documents, existing works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> develop various ways to measure the similarity between text and visual features. Our work is also based on this idea. We take a step further, however, to consider one additional important factor: the document representation is much more noisy than the human specified semantic representation and negligence of this fact would inevitably lead to inferior performance. For example, when the bagof-words model is adopted as the document representation, the occurrence of every word in a document will trigger a signal in one dimension of the document representation. However, it is clear that most words in a document are not directly relevant for identifying the object category. Thus it is necessary to design a noise suppression mechanism to down weight the importance of those less relevant words for zero-shot learning. This mechanism is closely related to feature selection. However, it is not exactly the same. As will be discussed in the following sections, the solution of our method does not discard the less relevant dimensions of the document representation but only suppress their impact for zero-shot learning.</p><p>To this end, we propose a zero-shot learning method which particularly caters for the need for noise suppression. More specifically, we proposed a simple yet effective l 2,1norm based objective function which simultaneously suppresses the noisy signal within text descriptions and learns a function to match the visual and text domains. Furthermore, we develop an efficient optimization algorithm to solve this problem. By conducting experiments on two large scale zero-shot learning evaluation benchmarks, we demonstrate the benefit of the proposed noise suppression mechanism as well as its superior performance over other zero-shot learning methods which also rely on online information sources. In addition, we also conduct an in-depth analysis of the proposed method which provides an insight as to what kinds of information within a document are useful for zero-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Most zero-shot learning approaches rely on human specified attributes. As one of the earliest attempts in zero-shot learning, Lampert et al. <ref type="bibr" target="#b11">[12]</ref> adopted a set of attributes obtained from a psychology study. By learning probabilistic predictors of those attributes, they developed a framework to estimate the posterior of the test class. Later, a number of works has been proposed to improve the way of learning the connection between attributes and object categories. For example, the work in <ref type="bibr" target="#b10">[11]</ref> addresses unreliability of attributes by exploring the idea of random forest. The work in <ref type="bibr" target="#b0">[1]</ref> converted the zero-shot learning into a cross-domain matching problem and they proposed to learn a matching function to compare the attribute and the image feature. Built upon this idea, Romera-Paredes and Torr <ref type="bibr" target="#b19">[20]</ref> proposed a simpler but more effective objective function to learn the matching function. Zhang and Saligrama <ref type="bibr" target="#b26">[27]</ref> advocated the benefits of using attribute-attribute relationships, termed semantic similarity, as the intermediate semantic representation and they learn a function to match the image features with the semantic similarity.</p><p>To go beyond the human specified attributes, recent works also explore the use of other forms of semantic representations which can be easily obtained <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. For example, the co-occurrence statistics of words has been explored in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref> to capture the semantic relevance of two concepts. The distributed word representation, e.g., word2vec, has been utilized as a substitution of attributes <ref type="bibr" target="#b8">[9]</ref> and more recently the word2vec representation has been shown to be complementary to the human specified attributes <ref type="bibr" target="#b9">[10]</ref>.</p><p>The other information source for creating the semantic representation is the online textual document, such as Wikipedia articles. In an earlier work, Berg et al. <ref type="bibr" target="#b3">[4]</ref> attempt to discover attribute representation from a noisy web source by ranking the visual-ness scores of attribute candidates. Rohrbach et al. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> mine semantic relatedness for attribute-class association from different internet sources. More recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref> directly learn a function to measure the compatibility between documents and visual features. However, compared with the state-of-the-art zeroshot learning methods, their performance seems to be disappointing even though some advanced technologies, such as deep learning, has been applied <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The overview of our method is depicted in <ref type="figure">Figure 1</ref>. It starts with a raw document representation which is simply a binarized histogram of words. This representation is fed into our zero-shot learning algorithm to generate a classifier to detect relevant images. In the process of generating this classifier, the noise suppression regularizer in our method will automatically suppress the impact of less relevant words (illustrated as the red words in <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Text representation</head><p>We extract our text representation based on a simple bagof-words model. We start by a preprocessing step of tokenizing the words and removing stop words and punctuations. Then a histogram of the remaining word occurrences is calculated and is subsequently binarized as the text representation. In other words, once a word appears in a document, its corresponding dimension within the text representation is set to "1". One more commonly used choice for the text representation is based on TF-IDF as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref>. However, we find that it produces worse performance 1 than directly using the binarized representation. This is probably because the weighting calculated of TF-IDF is not suitable for our zero-shot learning although it is considered to be less noisy for applications like document classification. In the binarized histogram we essentially treat each word in a document equally and this inevitably introduces a lot of noisy signals. However, thanks to our noise suppressing zeroshot learning algorithm, we can substantially down-weight the less relevant words and achieve good performance even with a noisy document representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning to match text and visual features</head><p>We first formally define our problem and introduce the notation used in the following sections. At the training An antelope is a member of a number of even-toed ungulate species indigenous to various regions in Africa and Eurasia. A group of antelope is called a herd …</p><p>The beaver is a primarily nocturnal, large, semiaqua&gt;c rodent. Beavers are known for building dams. They are the second-largest rodent in the world … CaCles are the most common type of large domes&gt;cated ungulates. They are a prominent modern member of the subfamily Bovinae … … W z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise Suppression</head><p>An antelope is a member of a number of even-toed ungulate species indigenous to various regions in Africa and Eurasia. A group of antelope is called a herd …</p><p>The beaver is a primarily nocturnal, large, semiaqua&gt;c rodent. Beavers are known for building dams. They are the second-largest rodent in the world … CaCles are the most common type of large domes&gt;cated ungulates. They are a prominent modern member of the subfamily Bovinae … … Classifier Genera&gt;ng W x <ref type="figure">Figure 1</ref>. Overview of our zero-shot learning approach. The text representations are processed by the noise suppression mechanism to generate a classifier to detect relevant images and the noisy components of text representations are suppressed to gain better performance. stage, both image features and document descriptions for C seen categories are available. Let X ∈ R d×N denote the image features of N training examples and Z ∈ {0, 1}d ×C the aforementioned document representations for C seen classes, whered and d are the dimensionality of the document representation and the image features respectively. We also define Y ∈ {0, 1} N ×C as the indicator matrix for the C seen classes. Each row of Y has a unique "1" indicating its corresponding class label. At the test stage, the document representations of theĈ unseen classes are given and our task is to assignĈ unseen class labels to the test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Formulation</head><p>Our method is inspired by a recently proposed zero-shot learning approach <ref type="bibr" target="#b19">[20]</ref> which has demonstrated impressive performance despite a very simple learning process. More specifically, it learns a matrix V which optimizes the following objective function.</p><formula xml:id="formula_0">min V X ⊤ VS − Y 2 F + λ VS 2 F + γ X ⊤ V 2 F + λγ V 2 F<label>(1)</label></formula><p>where S denotes the semantic attribute matrix and it can be either a binary matrix or a real value matrix. The scalars γ and λ are weights controlling the prominence of the various terms. The underlying idea of this algorithm can be understood as follows. If the task is to classify X into C categories, we can simply learn a linear classifier by fitting to Y, that is, min W X ⊤ W − Y 2 F . However, in this case W cannot be transferred to unseen classes. Thus we fur-ther impose that W = VS. In other words, the classifier of a class is generated from its attributes. With this requirement, the classifier of an unseen class can be easily obtained and utilized to predict the category of a test image. Similarly, we can also treat X ⊤ V as the classifier operated on the attributes S. The above understanding naturally gives rise to the regularization terms λ VS 2 F and γ X ⊤ V 2 F which play the same role of the Frobenius norm regularizer as commonly introduced in multi-class classification or regression. Since our document representation can also be seen as an attribute vector, the method in <ref type="bibr" target="#b19">[20]</ref> can be readily applied to our problem by simply setting S = Z. However, this naive solution ignores an important fact that the document representation is much more noisy than the human specified attribute vectors. To handle this issue, we introduce a noise suppression mechanism into Eq. (1). More specifically, we first decompose V into two terms:</p><formula xml:id="formula_1">V = W x ⊤ W z ,<label>(2)</label></formula><p>where W x ∈ R m×d and W z ∈ R m×d . These two matrices will play different roles in our method. W z is used to suppress the noisy components of Z and transform Z into a m × C intermediate representation. W x is used to generate the image classifier from the noise-suppressed intermediate representation. Thus, two different regularization terms are imposed to suit these two different roles. The first term is the l 2,1 -norm of W z ⊤ which achieves the noise suppression effect. The second term is the Frobenius norm of</p><formula xml:id="formula_2">W x ⊤ W z Z which is similar to the λ VS 2 F term in Eq. (1).</formula><p>The formulation of our method is expressed as follows:</p><formula xml:id="formula_3">min Wx,Wz L(W x , W z ) + λ 1 W ⊤ x W z Z 2 F + λ 2 W ⊤ z 2,1 ,<label>(3)</label></formula><formula xml:id="formula_4">L(W x , W z ) = X ⊤ W ⊤ x W z Z − Y 2 F . The l 2,1 -norm is defined as W T z 2,1 = d i=1 w i z 2 , where w i z denotes the i-th column of W z .</formula><p>It is known that the l 2,1 -norm will encourage the column vectors of W z to have few large values, which means that the impact of noisy dimensions of Z will be substantially suppressed or even completely eliminated. In fact, if λ 2 becomes sufficient large, it achieves the effect of feature selection on the document representation. However, by cross-validating λ 1 and λ 2 , our method does not lead to an exactly sparse solution as it seems that the algorithm prefers to keep the majority of the dimensions in Z for zero-shot learning. This is probably due to the joint regularization effect of W ⊤</p><p>x W z Z 2 F or the fact that dimensions corresponding to lower values of w i z 2 are still useful for zero-shot learning. Therefore we consider the use of the l 2,1 -norm here as a noise suppression mechanism rather than a feature selection mechanism. We drop out the other regularization terms in Eq. (1) since we find them have little impact on performance.</p><p>Similar to <ref type="bibr" target="#b19">[20]</ref>, once V, in our case V = W x ⊤ W z , is learned, we can infer the class label of a test image x using the following rule:</p><formula xml:id="formula_5">c * = max c x ⊤ W ⊤ x W z z c ,<label>(4)</label></formula><p>where z c is the document representation for the c-th candidate test class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Optimization</head><p>Eq. <ref type="formula" target="#formula_3">(3)</ref> is convex for W x and W z individually but not convex for both of them. Therefore we can solve it using an alternating method, that is, we first fix W x and solve for W z ; then fix W z and solve for W x .</p><p>(1) Fix W x and solve for W z :</p><p>This sub-problem is a regression problem with l 2,1 -norm regularization. Nie et al. <ref type="bibr" target="#b14">[15]</ref> proposes an iterative framework to efficiently solve it. It has been shown that the original problem is equivalent to sequentially solving the following problem until convergence</p><formula xml:id="formula_6">min Wz,D L(W x , W z ) + λ 1 W ⊤ x W z Z 2 F + (5) λ 2 T r(W z D t W ⊤ z ),</formula><p>where D t is a diagonal matrix whose i-th diagonal element is 1/(2 (w i z ) (t−1) 2 ) 2 at the t-th iteration, where <ref type="bibr" target="#b1">2</ref> In practice, we relax 1/(2||w i z || 2 ) to 1/(2 w i z ⊤ w i z + σ), σ → 0, Algorithm 1 Fix W x and solve W z Input: W x ; X of seen classes; Z of seen classes; λ 1 and λ 2 ; maximum number of iterations τ . Initialize D 0 as identity matrix I ∈ Rd ×d . for t = 1 · · · τ do · Solve Sylvester equation <ref type="formula" target="#formula_8">(6)</ref> for</p><formula xml:id="formula_7">W t z with D t−1 . · Update the diagonal matrix D t with its i-diagonal element as 1/(2||(w i z ) (t) || 2 ), where (w i z ) (t) is the i-th column of W t z . if Converge then · Break. end end Output: W z . (w i z ) (t−1)</formula><p>is the i-th column of the optimal W z solved at the (t − 1)-th iteration. The problem in Eq. (5) further reduces to a Sylvester equation of W z</p><formula xml:id="formula_8">AW z + W z B = C,<label>(6)</label></formula><formula xml:id="formula_9">A = λ 2 (W x XX ⊤ W ⊤ x + λ 1 W x W ⊤ x ) −1 , B = ZZ ⊤ (D) −1 , C = 1 λ 2 AW x XYZ ⊤ (D) −1 .</formula><p>The Sylvester equation has a unique solution if and only if A and −B do not share any eigenvalues. Many stateof-the-art toolboxes are able to solve it efficiently. In our setting, since both A and B are positive definite, A has only positive eigenvalues and −B has only negative eigenvalues. Therefore Eq. (6) has a unique solution. In summary, the sub-problem of fixing W x to solve W z can be solved via the algorithm listed in Algorithm 1.</p><p>(2) Fix W z and solve for W x : This sub-problem is a conventional least squares minimization problem which has the following closed-form solution</p><formula xml:id="formula_10">W ⊤ x = (XX ⊤ + λ 1 I) −1 XYZ ⊤ W ⊤ z (W z ZZ ⊤ W ⊤ z ) −1 .<label>(7)</label></formula><p>By alternating between the above two matrices, the overall alternating optimization algorithm for Eq. (3) is listed in Algorithm 2.</p><p>as the i-th diagonal element to avoid the case of zero columns, and the l 2,1 norm is therefore approximated by</p><formula xml:id="formula_11">d i=1 w i z ⊤ w i z + σ.</formula><p>It has been proved in <ref type="bibr" target="#b14">[15]</ref> that this approximation guarantees the convergence and the result approaches to that of l 2,1 -norm as σ → 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Alternating algorithm for solving Eq. (3)</head><p>Input: X of seen classes; Z of seen classes; λ 1 and λ 2 ; maximum number of iterations τ . Initialize W 0</p><p>x with Gaussian distribution. for t = 1 · · · τ do · Solve (5) iteratively for W t z with W t−1</p><p>x according to Algorithm 1.</p><formula xml:id="formula_12">· Solve (7) for W t x with W t z . if Converge then · Break. end end Output: W x , W z .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We divide our experiments into two parts. In the first part we evaluate the proposed method and compare it against both of the methods utilizing online textual sources and human-specified semantic attributes. In the second part we analyse in-depth the noise suppression effect of the proposed method and provide insight into what kind of information in a document is useful for zero-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setting</head><p>Datasets: We test our approach on two widely used benchmarks for attribute learning and zero-shot learning: Animals with Attributes <ref type="bibr" target="#b11">[12]</ref> (AwA) and Caltech-UCSD birds-200-2011 <ref type="bibr" target="#b24">[25]</ref> (CUB-200-2011). AwA consists of 30,475 images of 50 mammals classes with 85 attributes including color, skin texture, body size, body part, affordance, food source, habitat, and behaviour. CUB-200-2011 contains 11,788 images of 200 categories of bird subspecies with 312 fine-grained attributes such as color/shape/texture of body parts. We follow the train/test split according to <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b24">[25]</ref>, where 10 and 50 testing classes are treated as unseen for AwA and CUB-200-2011, respectively.</p><p>Textual document sources: We extract the text representation according to scheme introduced in Section 3.2. The raw textual sources are collected from Wikipedia articles describing each of the categories. When constructing the vocabulary, we use the articles of seen classes only. The dimensionality of the text representation is 3506 for AwA and 6815 for CUB-200-2011, respectively.</p><p>Image features: To make fair a comparison, two types of image features, the low-level features in <ref type="bibr" target="#b18">[19]</ref> and the fully connected layer activations from the "imagenet-vggverydeep-19" <ref type="bibr" target="#b21">[22]</ref> CNN are used in our experiments.</p><p>Implementation details: The Sylvester equation in Eq. (6) is solved by a MATLAB built-in function, which takes only around 5 seconds on an Intel Core i7 CPU at 3.40GHz. The number of rows of matrices W x and W z is equal to the number of seen classes. We choose the 29.12 ± 0.07 <ref type="table">Table 2</ref>. Zero-shot learning classification results of AwA, measured by mean accuracy. In <ref type="bibr" target="#b18">[19]</ref>, the approach mines attributes names from WordNet and additionally mines class-attribute from online sources of Wikipedia, WordNet, Yahoo, and Flickr. All methods in this table use the same low-level features in <ref type="bibr" target="#b18">[19]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance evaluation</head><p>We first compare our method against <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b18">[19]</ref>. The former is most relevant to our work in the sense that it learns a mapping to match images and textual documents. The work in <ref type="bibr" target="#b18">[19]</ref> is a comprehensive comparison study of various information sources for zero-shot learning. Besides these two method, we also treat S = Z in Eq. (1), and apply the ESZSL method in <ref type="bibr" target="#b19">[20]</ref> to our zero-shot learning problem. To make a fair comparison, we use the same lowlevel features in <ref type="bibr" target="#b18">[19]</ref> when comparing with it and then use the "imagenet-vgg-verydeep-19" to compare with <ref type="bibr" target="#b2">[3]</ref>. The comparison results are given in <ref type="table">Table 1 and Table 2</ref>. As can be seen in <ref type="table">Table 1</ref>, the proposed method significantly outperforms the methods in <ref type="bibr" target="#b2">[3]</ref>, although they have used a more complicated deep learning framework. Also, we find that our baseline ESZSL achieves good performance. However, it is still 5% inferior to our approach, which clearly demonstrates the advantage of the noise suppression mechanism introduced in this paper. The results in <ref type="table">Table 2</ref> further show that our method is superior over other approaches which rely on automatically mined information from the web. Again, our method achieves a significant improvement (more than 4%) over ESZSL.</p><p>We now compare our work with a few other state-of-theart approaches on zero-shot learning, even though some of them are not based on online information sources. The results are summarized in <ref type="table">Table 3</ref>. Results <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1]</ref> listed in the upper part of the table utilize hand-crafted features and not surprisingly their performance is much inferior to that of the proposed method. The lower part of Table 3 are methods with visual features extracted from a pretrained CNN and thus are more comparable to our method. In this setting, we find that our method is comparable to most of the state-of-the-art results on AwA and results better than ours are all obtained from the methods using cleaner human defined attributes. The work in <ref type="bibr" target="#b1">[2]</ref> evaluates various semantic representations such as Word2Vec embedding, GloVe word co-occurrence from Wikipedia sources, taxonomy embedding inferred from WordNet Hierarchy, and pre-defined binary and real-valued attributes. Our approach outperforms all methods that use online text sources. This shows that although online text sources provide transferable semantic representations, their discriminative ability is affected by the inherent noise and our method is better at handling the noisy information source for zero-shot learning.</p><p>Similar results are observed on the CUB-200-2011 dataset. Our approach again outperforms the methods using online sources and those methods that beat ours are all based on human specified fine-grained attributes. Note that many of the bird categories in CUB-200-2011 have very subtle differences which may not be well captured in Wikipedia articles. However, better performance may be expected by using a higher quality text corpus, such as bird watching articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">In-depth analysis of the proposed method</head><p>In this section we provide an in-depth analysis of the proposed method by examining its noise suppression mechanism and the words that are most discriminative in the view of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Effectiveness of the noise suppression method</head><p>In our method, the l 2,1 -norm is expected to allow only a few dimensions of the document representation to have large values. The importance of each individual dimension of the document representation can therefore be measured by the l 2 -norm of each column of learned W Z (we call it the im-portance weight in the following). We visualize this measurement for each dimension of the document representation in the top two subfigures in <ref type="figure" target="#fig_1">Figure 2</ref>. As can be seen, most of the importance weights are not exactly zero as one might expect given that the l 2,1 -norm is applied. In fact, there are only 702 zero columns (out of 3506) for AwA and 949 (out of 6815) for CUB-200-2011. As also mentioned in Section 3, this is probably because of the joint regularization effect of ||W ⊤ x W z Z|| 2 F in Eq. (3) and/or because by cross-validation most dimensions are still identified as being useful although their weighting should be very low. The second postulate might be supported by the observation that poorer performance will be obtained if we manually remove the dimensions which have low importance weights.</p><p>Although our formulation does not achieve the feature selection effect, it does only assign large importance weights to a small number of dimensions. To visually compare its effect, we replace the l 2,1 -norm and with the Frobenius norm and carry out our learning algorithm again. The resulting importance weights are shown in the two subfigures at the bottom of <ref type="figure" target="#fig_1">Figure 2</ref>. As can be seen, large importance weights appear in more dimensions in this case. This observation verifies the noise suppression effect of the regularizer introduced in Eq. (3) and explains the superior performance of our method over other text-based zero-shot learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Understanding the important dimensions of the document representation</head><p>Since each individual dimension of the textual document representation corresponds to an unique word, we can visualize the dimensions/words with large importance weights for better understanding our zero-shot learning algorithm. <ref type="table">Table 4</ref> lists at most 15 top scored words for 15 out of 40 seen classes in AwA and we could make several observations from it: (1) even though the document representations are extremely noisy, most of the top-ranked words are semantically meaningful to describe discriminative properties of a category (an animal in this case), such as body parts, habitat, behaviour, affordance, taxonomy, and environment. In fact, we find many top weighted words are consistent with some of the human specified attributes in AwA. <ref type="bibr" target="#b1">(2)</ref> Many top-ranked words are not explicitly "visualizable" but they imply visual information of a category. For example, the abstract concept "ruminant" implicitly tells that the creature with this property is "deer-like" or "cattle-like" and builds a visual connection between antelope and deer in <ref type="table">Table 4</ref>. This observation has also been made in the literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref>. <ref type="formula" target="#formula_3">(3)</ref> Interestingly, we also notice that although some concepts are not commonly considered as attributes, they exhibit large importance weight as inferred by our algorithm. By taking a close examina-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method/Dataset</head><p>AwA CUB Rohrbach et al. <ref type="bibr" target="#b17">[18]</ref> 42.7 Jayaraman &amp; Grauman <ref type="bibr" target="#b10">[11]</ref> 43.01 Mensink et al. <ref type="bibr" target="#b13">[14]</ref> 14.4 Akata et al. <ref type="bibr" target="#b0">[1]</ref> 43.5 18.0 Lampert et al. <ref type="bibr" target="#b12">[13]</ref> (attr. real) 57.5 Deng et al. <ref type="bibr" target="#b4">[5]</ref>   tion, we categorize these words into two types. The first (labelled green in <ref type="table">Table 4</ref>) are some concepts that are more likely to co-occur with meaningful attributes. For example, the word "stomach" is only shared by antelope and deer in <ref type="table">Table 4</ref>, despite its existence in all mammals. This is probably because "stomach" is more likely to be co-occurred with  <ref type="table">Table 4</ref>. Category-wisely top ranked words, sorted by average importance weights within each class. The blue words are generally considered as meaningful attributes of this class. The green words are concepts somewhat related to this class, but are less informative to define it. The red words are concepts that are not semantically related to the corresponding class.</p><p>"ruminant", a discriminative property of ruminant animals. Another type of words (labelled red in <ref type="table">Table 4</ref>) are not sufficiently meaningful for human interpreter. For example, "belong" and "general" are assigned with high importance weight for all cetaceans (blue whale, dolphin, killer whale etc.) and rodents (mouse, rabbit, hamster etc.), respectively. We suspect the reason is due to the dataset bias of documents. For example, documents of similar categories may be edited by authors from the same background who prefer a certain word choice. In sum, we find most of the top ranked words carry weak information by their own, but it seems that using them collaboratively produces impressive discriminative power for zero-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have introduced a noise suppression mechanism to text-based zero-shot learning. The proposed l 2,1 -norm based objective function generates classifiers that are robust against textual noise and achieve state-of-theart zero-shot learning performance. We have made several findings in the experiments. (1) The inherent noise within text sources has a significant impact on zero-shot learning performance. As all the text-methods without noise suppression are inferior to our approach, we speculate that noise in a component of the mid-level representation decreases its discriminative power. (2) Most noisy components are suppressed rather than completely eliminated by our mechanism. Some words, although unimportant individually, can produce meaningful discriminative power when put together. (3) We find three kinds of words in the de-noised representation that can provide useful information for zero-shot learning. The first kind are the attributelike words that explicitly describe the category. The second are words that are weakly related to the category. They usually occur with definitive words. The last kind of words is non-informative to humans, but shows certain distribution patterns among related categories.</p><p>Overall, this paper points out an important factor in textbased zero-shot learning that has been previously ignored. By dealing directly with the inevitable variations in human expression, and suppressing words that contain little or no value, the performance of text-based automatic zero-shot learning can be significantly improved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>hyper-parameters with a five-fold cross-validation on the seen classes, where 20% (5 for AwA and 30 for CUB-200-2011) of the seen classes are held out for validation and the remaining seen classes are used for training. The hyperparameters are tuned within the range of all cases of 10 b , where b = {−2, −1, · · · , 5, 6}. Once the hyper-parameters are selected, we use all seen classes to train the final model. All of our reported results are averaged over 10 trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The two subfigures at the top show column-wise l2-norms of Wz learned with l2,1-norm regularization. The two subfigures at the bottom show column-wise l2-norms of Wz learned with Frobenius-norm regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Index of columns of Wz regularized by l2,1-norm for AwA Index of columns of Wz regularized by l2,1-norm for CUBIndex of columns of Wz regularized by Frobenius-norm for AwAIndex of columns of Wz regularized by Frobenius-norm for CUB</figDesc><table>(hierarchy) 
44.2 
ESZSL [20] (attr. bin) 
62.85 
Akata et al. [2] (Word2Vec) 
51.2 
28.4 
Akata et al. [2] (GloVe) 
58.8 
24.2 
Akata et al. [2] (WordNet) 
51.2 
20.6 
Akata et al. [2] (attr. bin) 
52.0 
37.8 
Akata et al. [2] (attr. real) 
66.7 
50.1 
Fu et al. [10] (attr. &amp; words) 
66.0 
Zhang &amp; Saligrama [27] (attr. real) 
76.33 
30.41 
ESZSL [20] (Wikipedia) 
58.53 
23.80 
Ours (Wikipedia) 
66.46 ± 0.42 29.00 ± 0.28 

Table 3. Zero-shot learning classification results on AwA and CUB-200-2011. Blank spaces indicate these methods are not tested on the 
corresponding datasets. Contents in braces indicate the semantic sources which these methods use for zero-shot learning. Methods in the 
upper part of the table use low-level features and the remaining methods in the lower part use deep CNN features. 

500 
1000 
1500 
2000 
2500 
3000 
3500 

Normalized l2-norm 

0 

0.5 

1 

1000 
2000 
3000 
4000 
5000 
6000 

Normalized l2-norm 

0 

0.5 

1 

500 
1000 
1500 
2000 
2500 
3000 
3500 

Normalized l2-norm 

0 

0.5 

1 

1000 
2000 
3000 
4000 
5000 
6000 

Normalized l2-norm 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Class Top Ranked Words/Dimensions Antelope antler, woodland, fight, stomach, spike, antelope, escape, mate, night, variety, ruminant, ridge, broad, scent, herd Beaver river, protect, semiaquatic, web, branch, eurasian, american, land, insular, hunt, fur, extant, adult, stream, pond Blue Whale ton, whale, flipper, kilometre, marine, ocean, belong, mph, shape, dive, earth, worldwide, indian, travel, pacific Buffalo climate, extant, herd, indian, cattle, dairy, animate, bc, trade, behaviour, human, milk, northern, southeast, field Cow draft, milk, cattle, widespread, product, meat, domestic, strong, cart, plow, oxen, bullock, cow, animate, india Deer antler, fight, mate, elk, palmate, moose, wolf, season, bear, woodland, herd, ruminant, deer, stomach, spike Moose herd, elk, palmate, moose, wolf, fight, deer, compete, alces, temperate, climate, aggressive, sedentary, season Mouse rodent, house, eat, avoid, burrow, general, genetic, popular, breed, wild, small, tail, vermin, nocturnal, prey Dolphin flipper, whale, ton, kilometre, indian, dive, mph, earth, shape, blubber, belong, marine, ocean, capture, prevent Horse draft, strong, milk, meat, ungulate, equip, widespread, loose, past, history, compete, endure, technique, style, flee Hamster mix, underground, fragile, house, bear, seed, worn, silky, rapid, classify, general, tail, flexible, dwarf, pouch Killer Whale ton, whale, dolphin, click, dive, killer, pollution, belong, capture, vocal, calf, tail, threat, fish, fin Otter semiaquatic, branch, eurasian, lake, engage, bed, play, trap, river, deplete, giant, cetacean, mink, weasel, web Rabbit fragile, house, classify, general, introduce, underground, pad, vegetarian, companionship, defensive, shelf, detect S. Monkey agile, arm, walk, tropic, rainforest, primate, source, primary, bark, passage, balance, thumb, moist, threaten</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Using TF-IDF is about 7% and 5% inferior to binarized representations on AwA and CUB, respectively.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labelembedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn. IEEE, IEEE</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn. IEEE, IEEE</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis. IEEE</title>
		<meeting>IEEE Int. Conf. Comp. Vis. IEEE</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="663" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Write a classifier: Zero-shot learning using purely textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2584" to="2591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zero-shot object recognition by semantic manifold distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger</editor>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3464" to="3472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Costa: Cooccurrence statistics for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient and robust feature selection via joint l2,1-norms minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H Q</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1813" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Default probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Osherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wilkie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="269" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What helps where -and why? semantic relatedness for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="910" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zeroshot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<editor>C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger</editor>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient object category recognition using classemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="page" from="776" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis. IEEE</title>
		<meeting>IEEE Int. Conf. Comp. Vis. IEEE</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
