<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
							<email>lcchen@cs.ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
							<email>barron@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
							<email>kpmurphy@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<email>yuille@stat.ucla.edu</email>
						</author>
						<title level="a" type="main">Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep convolutional neural networks (CNNs) are the backbone of state-of-art semantic image segmentation systems.</head><p>Recent work has shown that complementing CNNs with fully-connected conditional random fields (CRFs) can significantly enhance their object localization accuracy, yet dense CRF inference is computationally expensive. We propose replacing the fully-connected CRF with domain transform (DT), a modern edge-preserving filtering method in which the amount of smoothing is controlled by a reference edge map. Domain transform filtering is several times faster than dense CRF inference and we show that it yields comparable semantic segmentation results, accurately capturing object boundaries. Importantly, our formulation allows learning the reference edge map from intermediate CNN features instead of using the image gradient magnitude as in standard DT filtering. This produces task-specific edges in an end-to-end trainable system optimizing the target semantic segmentation quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (CNNs) are very effective in semantic image segmentation, the task of assigning a semantic label to every pixel in an image. Recently, it has been demonstrated that post-processing the output of a CNN with a fully-connected CRF can significantly increase segmentation accuracy near object boundaries <ref type="bibr" target="#b4">[5]</ref>.</p><p>As explained in <ref type="bibr" target="#b25">[26]</ref>, mean-field inference in the fullyconnected CRF model amounts to iterated application of the bilateral filter, a popular technique for edge-aware filtering. This encourages pixels which are nearby in position and in color to be assigned the same semantic label. In practice, this produces semantic segmentation results which are well aligned with object boundaries in the image.</p><p>One key impediment in adopting the fully-connected CRF is the rather high computational cost of the underlying bilateral filtering step. Bilateral filtering amounts to high- * Work done in part during an internship at Google Inc.  <ref type="figure">Figure 1</ref>. A single unified CNN produces both coarse semantic segmentation scores and an edge map, which respectively serve as input multi-channel image and reference edge to a domain transform edge-preserving filter. The resulting filtered semantic segmentation scores are well-aligned with the object boundaries. The full architecture is discriminatively trained by backpropagation (red dashed arrows) to optimize the target semantic segmentation.</p><p>dimensional Gaussian filtering in the 5-D bilateral (2-D position, 3-D color) space and is expensive in terms of both memory and CPU time, even when advanced algorithmic techniques are used.</p><p>In this paper, we propose replacing the fully-connected CRF and its associated bilateral filtering with the domain transform (DT) <ref type="bibr" target="#b15">[16]</ref>, an alternative edge-aware filter. The recursive formulation of the domain transform amounts to adaptive recursive filtering of a signal, where information is not allowed to propagate across edges in some reference signal. This results in an extremely efficient scheme which is an order of magnitude faster than the fastest algorithms for a bilateral filter of equivalent quality.</p><p>The domain transform can equivalently be seen as a recurrent neural network (RNN). In particular, we show that the domain transform is a special case of the recently proposed RNN with gated recurrent units. This connection allows us to share insights, better understanding two seemingly different methods, as we explain in Section 3.4.</p><p>The amount of smoothing in a DT is spatially modulated by a reference edge map, which in the standard DT corresponds to image gradient magnitude. Instead, we will learn the reference edge map from intermediate layer features of the same CNN that produces the semantic segmentation scores, as illustrated in <ref type="figure">Fig. 1</ref>. Crucially, this allows us to learn a task-specific edge detector tuned for semantic image segmentation in an end-to-end trainable system.</p><p>We evaluate the performance of the proposed method on the challenging PASCAL VOC 2012 semantic segmentation task. In this task, domain transform filtering is several times faster than dense CRF inference, while performing almost as well in terms of the mean intersection-over-union (mIOU) metric. In addition, although we only trained for semantic segmentation, the learned edge map performs competitively on the BSDS500 edge detection benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic image segmentation Deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b26">[27]</ref> have demonstrated excellent performance on the task of semantic image segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. However, due to the employment of maxpooling layers and downsampling, the output of these networks tend to have poorly localized object boundaries. Several approaches have been adopted to handle this problem. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5]</ref> proposed to extract features from the intermediate layers of a deep network to better estimate the object boundaries. Networks employing deconvolutional layers and unpooling layers to recover the "spatial invariance" effect of max-pooling layers have been proposed by <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33]</ref>. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> used super-pixel representation, which essentially appeals to low-level segmentation methods for the task of localization. The fully connected Conditional Random Field (CRF) <ref type="bibr" target="#b25">[26]</ref> has been applied to capture long range dependencies between pixels in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref>. Further improvements have been shown in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b37">38]</ref> when backpropagating through the CRF to refine the segmentation CNN. In contrary, we adopt another approach based on the domain transform <ref type="bibr" target="#b15">[16]</ref> and show that beyond refining the segmentation CNN, we can also jointly learn to detect object boundaries, embedding task-specific edge detection into the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge detection</head><p>The edge/contour detection task has a long history <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11]</ref>, which we will only briefly review. Recently, several works have achieved outstanding performance on the edge detection task by employing CNNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>. Our work is most related to the ones by <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref>. While Xie and Tu <ref type="bibr" target="#b43">[44]</ref> also exploited features from the intermediate layers of a deep network <ref type="bibr" target="#b39">[40]</ref> for edge detection, they did not apply the learned edges for high-level tasks, such as semantic image segmentation. On the other hand, Bertasius et al. <ref type="bibr" target="#b2">[3]</ref> and Kokkinos <ref type="bibr" target="#b23">[24]</ref> made use of their learned boundaries to improve the performance of semantic image segmentation. However, the boundary detection and semantic image segmentation are considered as two separate tasks. They optimized the performance of boundary detection instead of the performance of high level tasks. On the contrary, we learn object boundaries in order to directly optimize the performance of semantic image segmentation.</p><p>Long range dependency Recurrent neural networks (RNNs) <ref type="bibr" target="#b11">[12]</ref> with long short-term memory (LSTM) units <ref type="bibr" target="#b19">[20]</ref> or gated recurrent units (GRUs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> have proven successful to model the long term dependencies in sequential data (e.g., text and speech). Sainath et al. <ref type="bibr" target="#b36">[37]</ref> have combined CNNs and RNNs into one unified architecture for speech recognition. Some recent work has attempted to model spatial long range dependency with recurrent networks for computer vision tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43]</ref>. Our work, integrating CNNs and Domain Transform (DT) with recursive filtering <ref type="bibr" target="#b15">[16]</ref>, bears a similarity to ReNet <ref type="bibr" target="#b42">[43]</ref>, which also performs recursive operations both horizontally and vertically to capture long range dependency within whole image. In this work, we show the relationship between DT and GRU, and we also demonstrate the effectiveness of exploiting long range dependency by DT for semantic image segmentation. While <ref type="bibr" target="#b41">[42]</ref> has previously employed the DT (for joint object-stereo labeling), we propose to backpropagate through both of the DT inputs to jointly learn segmentation scores and edge maps in an end-to-end trainable system. We show that these learned edge maps bring significant improvements compared to standard image gradient magnitude used by <ref type="bibr" target="#b41">[42]</ref> or earlier DT literature <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model overview</head><p>Our proposed model consists of three components, illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. They are jointly trained end-to-end to optimize the output semantic segmentation quality.</p><p>The first component that produces coarse semantic segmentation score predictions is based on the publicly available DeepLab model, <ref type="bibr" target="#b4">[5]</ref>, which modifies VGG-16 net <ref type="bibr" target="#b39">[40]</ref> to be FCN <ref type="bibr" target="#b30">[31]</ref>. The model is initialized from the VGG-16 ImageNet <ref type="bibr" target="#b35">[36]</ref> pretrained model. We employ the DeepLab-LargeFOV variant of <ref type="bibr" target="#b4">[5]</ref>, which introduces zeros into the filters to enlarge its Field-Of-View, which we will simply denote by DeepLab in the sequel.</p><p>We add a second component, which we refer to as Ed-geNet. The EdgeNet predicts edges by exploiting features from intermediate layers of DeepLab. The features are resized to have the same spatial resolution by bilinear interpolation before concatenation. A convolutional layer with kernel size 1×1 and one output channel is applied to yield edge prediction. ReLU is used so that the edge prediction is in the range of zero to infinity.</p><p>The third component in our system is the domain transform (DT), which is is an edge-preserving filter that lends itself to very efficient implementation by separable 1-D recursive filtering across rows and columns. Though DT is traditionally used for graphics applications <ref type="bibr" target="#b15">[16]</ref>, we use it to filter the raw CNN semantic segmentation scores to be better aligned with object boundaries, guided by the EdgeNet produced edge map.  We review the standard DT in Sec. 3.2, we extend it to a fully trainable system with learned edge detection in Sec. 3.3, and we discuss connections with the recently proposed gated recurrent unit networks in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Domain transform with recursive filtering</head><p>The domain transform takes two inputs: (1) The raw input signal x to be filtered, which in our case corresponds to the coarse DCNN semantic segmentation scores, and (2) a positive "domain transform density" signal d, whose choice we discuss in detail in the following section. The output of the DT is a filtered signal y. We will use the recursive formulation of the DT due to its speed and efficiency, though the filter can be applied via other techniques <ref type="bibr" target="#b15">[16]</ref>.</p><p>For 1-D signals of length N , the output is computed by setting y 1 = x 1 and then recursively for i = 2, . . . , N</p><formula xml:id="formula_0">y i = (1 − w i )x i + w i y i−1 .<label>(1)</label></formula><p>The weight w i depends on the domain transform density d i</p><formula xml:id="formula_1">w i = exp − √ 2d i /σ s ,<label>(2)</label></formula><p>where σ s is the standard deviation of the filter kernel over the input's spatial domain. Intuitively, the strength of the domain transform density d i ≥ 0 determines the amount of diffusion/smoothing by controlling the relative contribution of the raw input signal x i to the filtered signal value at the previous position y i−1 when computing the filtered signal at the current position y i . The value of w i ∈ (0, 1) acts like a gate, which controls how much information is propagated from pixel i − 1 to i. We have full diffusion when d i is very small, resulting into w i = 1 and y i = y i−1 . On the other extreme, if d i is very large, then w i = 0 and diffusion stops, resulting in y i = x i . Filtering by Eq. (1) is asymmetric, since the current output only depends on previous outputs. To overcome this asymmetry, we filter 1-D signals twice, first left-to-right, then right-to-left on the output of the left-to-right pass.</p><p>Domain transform filtering for 2-D signals works in a separable fashion, employing 1-D filtering sequentially along each signal dimension. That is, a horizontal pass (left-toright and right-to-left) is performed along each row, followed by a vertical pass (top-to-bottom and bottom-to-top) along each column. In practice, K &gt; 1 iterations of the two-pass 1-D filtering process can suppress "striping" artifacts resulting from 1-D filtering on 2-D signals <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Fig. 4</ref>]. We reduce the standard deviation of the DT filtering kernel at each iteration, requiring that the sum of total variances equals the desired variance σ 2 s , following [16, Eq. 14]</p><formula xml:id="formula_2">σ k = σ s √ 3 2 K−k √ 4 K − 1 , k = 1, . . . , K ,<label>(3)</label></formula><p>plugging σ k in place of σ s to compute the weights w i by Eq.</p><p>(2) at the k-th iteration. The domain transform density values d i are defined as</p><formula xml:id="formula_3">d i = 1 + g i σ s σ r ,<label>(4)</label></formula><p>where g i ≥ 0 is the "reference edge", and σ r is the standard deviation of the filter kernel over the reference edge map's range. Note that the larger the value of g i is, the more confident the model thinks there is a strong edge at pixel i, thus inhibiting diffusion (i.e., d i → ∞ and w i = 0). The standard DT <ref type="bibr" target="#b15">[16]</ref> usually employs the color image gradient but we show next that better results can be obtained by computing the reference edge map by a learned DCNN.</p><formula xml:id="formula_4">g i = 3 c=1 ∇I (c) i (5) (a) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Trainable domain transform filtering</head><p>One novel aspect of our proposed approach is to backpropagate the segmentation errors at the DT output y through the DT onto its two inputs. This allows us to use the DT as a layer in a CNN, thereby allowing us to jointly learn DCNNs that compute the coarse segmentation score maps in x and the reference edge map in g.</p><p>We demonstrate how DT backpropagation works for the 1-D filtering process of Eq. (1), whose forward pass is illustrated as computation tree in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>. We assume that each node y i not only influences the following node y i+1 but also feeds a subsequent layer, thus also receiving gradient contributions ∂L ∂yi from that layer during back-propagation. Similar to standard back-propagation in time, we unroll the recursion of Eq. (1) in reverse for i = N, . . . , 2 as illustrated in <ref type="figure" target="#fig_3">Fig. 3(b)</ref> to update the derivatives with respect to y, and to also compute derivatives with respect to x and w,</p><formula xml:id="formula_5">∂L ∂x i ← (1 − w i ) ∂L ∂y i (6) ∂L ∂w i ← ∂L ∂w i + (y i−1 − x i ) ∂L ∂y i (7) ∂L ∂y i−1 ← ∂L ∂y i−1 + w i ∂L ∂y i ,<label>(8)</label></formula><p>where ∂L ∂xi and ∂L ∂wi are initialized to 0 and ∂L ∂yi is initially set to the value sent by the subsequent layer. Note that the weight w i is shared across all filtering stages (i.e., left-to-right/right-to-left within horizontal pass and top-tobottom/bottom-to-top within vertical pass) and K iterations, with each pass contributing to the partial derivative.</p><p>With these partial derivatives we can produce derivatives with respect to the reference edge g i . Plugging Eq. (4) into Eq. (2) yields</p><formula xml:id="formula_6">w i = exp − √ 2 σ k 1 + g i σ s σ r .<label>(9)</label></formula><p>Then, by the chain rule, the derivative with respect to g i is</p><formula xml:id="formula_7">∂L ∂g i = − √ 2 σ k σ s σ r w i ∂L ∂w i .<label>(10)</label></formula><p>This gradient is then further propagated onto the deep convolutional neural network that generated the edge predictions that were used as input to the DT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relation to gated recurrent unit networks</head><p>Equation 1 defines DT filtering as a recursive operation. It is interesting to draw connections with other recent RNN formulations. Here we establish a precise connection with the gated recurrent unit (GRU) RNN architecture <ref type="bibr" target="#b7">[8]</ref> recently proposed for modeling sequential text data. The GRU employs the update rule</p><formula xml:id="formula_8">y i = z iỹi + (1 − z i )y i−1 .<label>(11)</label></formula><p>Comparing with Eq. (1), we can relate the GRU's "update gate" z i and "candidate activation"ỹ i with DT's weight and raw input signal as follows:</p><formula xml:id="formula_9">z i = 1 − w i andỹ i = x i . The GRU update gate z i is defined as z i = σ(f i ),</formula><p>where f i is an activation signal and σ(t) = 1/(1+e −t ). Comparing with Eq. (9) yields a direct correspondence between the DT reference edge map g i and the GRU activation f i :</p><formula xml:id="formula_10">g i = σ r σ s σ k √ 2 log(1 + e fi ) − 1 .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Protocol</head><p>Dataset We evaluate the proposed method on the PASCAL VOC 2012 segmentation benchmark <ref type="bibr" target="#b12">[13]</ref>, consisting of 20 foreground object classes and one background class. We augment the training set from the annotations by <ref type="bibr" target="#b17">[18]</ref>. The performance is measured in terms of pixel intersection-overunion (IOU) averaged across the 21 classes. Training A two-step training process is employed. We first train the DeepLab component and then we jointly finetune the whole model. Specifically, we employ exactly the same setting as <ref type="bibr" target="#b4">[5]</ref> to train DeepLab in the first stage. In the second stage, we employ a small learning rate of 10 −8 for fine-tuning. The added convolutional layer of EdgeNet is initialized with Gaussian variables with zero mean and standard deviation of 10 −5 so that in the beginning the Ed-geNet predicts no edges and it starts to gradually learn edges for semantic segmentation. Total training time is 11.5 hours (10.5 and 1 hours for each stage). Reproducibility The proposed methods are implemented by extending the Caffe framework <ref type="bibr" target="#b21">[22]</ref>. The code and models are available at http://liangchiehchen.com/ projects/DeepLab.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>We first explore on the validation set the hyper-parameters in the proposed model, including (1) features for EdgeNet, (2) hyper-parameters for domain transform (i.e., number of iterations, σ s , and σ r ). We also experiment with different methods to generate edge prediction. After that, we analyze our models and evaluate on the official test set. Features for EdgeNet The EdgeNet we employ exploits intermediate features from DeepLab. We first investigate which VGG-16 <ref type="bibr" target="#b39">[40]</ref> layers give better performance with the DT hyper-parameters fixed. As shown in Tab. 1, baseline DeepLab attains 62.25% mIOU on PASCAL VOC 2012 validation set. We start to exploit the features from conv3 3, which has receptive field size 40. The size is similar to the patch size typically used for edge detection <ref type="bibr" target="#b10">[11]</ref>. The resulting model achieves performance of 65.64%, 3.4% better than the baseline. When using features from conv2 2, conv3 3, and conv4 3, the performance can be further improved to 66.03%. However, we do not observe any significant improvement if we also exploit the features from conv1 2 or conv5 3. We use features from conv2 2, conv3 3, and conv4 3 in remaining experiments involving EdgeNet. Number of domain transform iterations Domain transform requires multiple iterations of the two-pass 1-D filtering process to avoid the "striping" effect [16, <ref type="figure" target="#fig_5">Fig. 4</ref>]. We train the proposed model with K iterations for the domain transform, and perform the same K iterations during test. Since there are two more hyper-parameters σ s and σ r (see Eq. (9)), we also vary their values to investigate the effect of varying the K iterations for domain transform. As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, employing K = 3 iterations for domain transform in our proposed model is sufficient to reap most of the gains for several different values of σ s and σ r . Varying domain transform σ s , σ r and comparison with other edge detectors We investigate the effect of varying σ s and σ r for domain transform. We also compare alternative methods to generate edge prediction for domain trans-   <ref type="formula">5)</ref> is used as in standard domain transform <ref type="bibr" target="#b15">[16]</ref>. We search for optimal σ s and σ r for those methods. First, we fix σ s = 100 and vary σ r in <ref type="figure">Fig. 5(a)</ref>. We found that the performance of DT-Oracle, DT-SE, and DT-Gradient are affected a lot by different values of σ r , since they are generated by other "plugged-in" modules (i.e., not jointly fine-tuned). We also show the performance of baseline DeepLab and DeepLab-CRF which employs dense CRF. We then fix the found optimal value of σ r and vary σ s in <ref type="figure">Fig. 5 (b)</ref>. We found that as long as σ s ≥ 90, the performance of DT-EdgeNet, DT-SE, and DT-Gradient do not vary significantly. After finding optimal values of σ r and σ s for each setting, we use them for remaining experiments. We further visualize the edges learned by our DT-EdgeNet in <ref type="figure">Fig. 6</ref>. As shown in the first row, when σ r increases, the learned edges start to include not only object boundaries but also background textures, which degrades the performance for semantic segmentation in our method (i.e., noisy edges make it hard to propagate information between neighboring pixels). As shown in the second row, varying σ s does not change the learned edges a lot, as long as its value is large enough (i.e., ≥ 90).</p><formula xml:id="formula_11">σ s =170, σ r =1 σ s =130, σ r =1 σ s =90, σ r =1 σ s =50, σ r =1 (a) (b)</formula><p>We show val set performance (with the best values of σ s and σ r ) for each method in Tab. 2. The method DT-Gradient improves over the baseline DeepLab by 1.7%. While DT-SE is 0.9% better than DT-Gradient, DT-EdgeNet further enhances performance (4.1% over baseline). Even though DT-EdgeNet is 1.2% lower than DeepLab-CRF, it is several times faster, as we discuss later. Moreover, we have found that combining DT-EdgeNet and dense CRF yields the best performance (0.8% better than DeepLab-CRF). In this hybrid DT-EdgeNet+DenseCRF scheme we post-process the DT filtered score maps in an extra fully-connected CRF step. Trimap Similar to <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5]</ref>, we quantify the accuracy of the proposed model near object boundaries. We use the "void" label annotated on PASCAL VOC 2012 validation  set. The annotations usually correspond to object boundaries. We compute the mean IOU for the pixels that lie within a narrow band (called trimap) of "void" labels, and vary the width of the band, as shown in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p><p>Qualitative results We show some semantic segmentation results on PASCAL VOC 2012 val set in <ref type="figure" target="#fig_10">Fig. 9</ref>. DT-EdgeNet visually improves over the baseline DeepLab and DT-SE. Besides, when comparing the edges learned by Structured Edges and our EdgeNet, we found that EdgeNet better captures the object exterior boundaries and responds less than SE to interior edges. We also show failure cases in the bottom two rows of <ref type="figure" target="#fig_10">Fig. 9</ref>. The first is due to the wrong predictions from DeepLab, and the second due to the difficulty in localizing object boundaries with cluttered background.</p><p>Test set results After finding the best hyper-parameters, we evaluate our models on the test set. As shown in the top of Tab. 4, DT-SE improves 2.7% over the baseline DeepLab, and DT-EdgeNet can further enhance the performance to 69.0% (3.9% better than baseline), which is 1.3% behind employing a fully-connected CRF as post-processing (i.e., DeepLab-CRF) to smooth the results. However, if we also incorporate a fully-connected CRF as post-processing to our model, we can further increase performance to 71.2%. Models pretrained with MS-COCO We perform another experiment with the stronger baseline of <ref type="bibr" target="#b33">[34]</ref>, where DeepLab is pretrained with the MS-COCO 2014 dataset <ref type="bibr" target="#b28">[29]</ref>. Our goal is to test if we can still obtain improvements with the proposed methods over that stronger baseline. We use the same optimal values of hyper-parameters as before, and report the results on validation set in <ref type="table">Tab</ref> EdgeNet on BSDS500 We further evaluate the edge detection performance of our learned EdgeNet on the test set of BSDS500 <ref type="bibr" target="#b0">[1]</ref>. We employ the standard metrics to evaluate edge detection accuracy: fixed contour threshold (ODS Fscore), per-image best threshold (OIS F-score), and average precision (AP). We also apply a standard non-maximal suppression technique to the edge maps produced by EdgeNet for evaluation. Our method attains ODS=0.718, OIS=0.731, and AP=0.685. As shown in <ref type="figure" target="#fig_9">Fig. 8</ref>, interestingly, our Ed-geNet yields a reasonably good performance (only 3% worse than Structured Edges <ref type="bibr" target="#b10">[11]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented an approach to learn edge maps useful for semantic image segmentation in a unified system that is trained discriminatively in an end-to-end fashion. The proposed method builds on the domain transform, an edge-Method ImageNet COCO DeepLab <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref> 65.1 68.9 DeepLab-CRF <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref> 70. -76.0 DPN <ref type="bibr" target="#b29">[30]</ref> 74.1 77.5 Adelaide Context <ref type="bibr" target="#b27">[28]</ref> 75.3 77.8 <ref type="table">Table 4</ref>. mIOU (%) on PASCAL VOC 2012 test set. We evaluate our models with two settings: the models are (1) pretrained with ImageNet, and (2) further pretrained with MS-COCO.  CRF-RNN (CRF part) <ref type="bibr" target="#b45">[46]</ref> 1482 - <ref type="table">Table 5</ref>. Average inference time (ms/image). Number in parentheses is the percentage w.r.t. the DeepLab computation. Note that EdgeNet computation time is improved by performing convolution first and then upsampling.</p><p>preserving filter traditionally used for graphics applications. We show that backpropagating through the domain transform allows us to learn an task-specific edge map optimized for semantic segmentation. Filtering the raw semantic segmentation maps produced by deep fully convolutional networks with our learned domain transform leads to improved localization accuracy near object boundaries. The resulting scheme is several times faster than fully-connected CRFs that have been previously used for this purpose.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Our proposed model has three components: (1) DeepLab for semantic segmentation prediction, (2) EdgeNet for edge prediction, and (3) Domain Transform to accurately align segmentation scores with object boundaries. EdgeNet reuses features from intermediate DeepLab layers, resized and concatenated before edge prediction. Domain transform takes as input the raw segmentation scores and edge map, and recursively filters across rows and columns to produce the final filtered segmentation scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Computation tree for domain transform recursive filtering: (a) Forward pass. Upward arrows from yi nodes denote feeds to subsequent layers. (b) Backward pass, including contributions ∂L ∂y i from subsequent layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>VOC 2012 val set. Effect of varying number of iterations for domain transform: (a) Fix σs and vary both σr and K iterations. (b) Fix σr and vary both σs and K iterations. form: (1) DT-Oracle, where groundtruth object boundaries are used, which serves as an upper bound on our method. (2) The proposed DT-EdgeNet, where the edges are produced by EdgeNet. (3) DT-SE, where the edges are found by Structured Edges (SE) [11]. (4) DT-Gradient, where the image (color) gradient magnitude of Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 Figure 6 .Figure 5 .</head><label>165</label><figDesc>) σ s = 50, σ r = 0.1 (h) σ s = 90, σ r = 0.1 (i) σ s = 130, σ r = 0.1 (j) σ s = 170, σ r = 0.Effect of varying domain transform's σs and σr. First row: when σs is fixed and σr increases, the EdgeNet starts to include more background edges. Second row: when σr is fixed, varying σs has little effect on learned edges. VOC 2012 val set. Effect of varying σs and σr. (a) Fix σs = 100 and vary σr. (b) Use the best σr from (a) and vary σs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>(a) Some trimap examples (top-left: image. top-right: ground-truth. bottom-left: trimap of 2 pixels. bottom-right: trimap of 10 pixels). (b) Segmentation result within a band around the object boundaries for the proposed methods (mean IOU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Evaluation of our learned EdgeNet on the test set of BSDS500. Note that our EdgeNet is only trained on PASCAL VOC 2012 semantic segmentation task without edge supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Visualizing results on VOC 2012 val set. For each row, we show (a) Image, (b) Baseline DeepLab segmentation result, (c) edges produced by Structured Edges, (d) segmentation result with Structured Edges, (e) edges generated by EdgeNet, and (f) segmentation result with EdgeNet. Note that our EdgeNet better captures the object boundaries and responds less to the background or object interior edges. For example, see the legs of left second person in the first image or the dog shapes in the second image. Two failure examples in the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>either multi-scale inputs<ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7]</ref> or features from intermediate layers of DCNN<ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5]</ref>). Motivated by this, we further combine our proposed discriminatively trained domain transform and the model of<ref type="bibr" target="#b6">[7]</ref>, yielding 76.3% performance on test set, 1.5% behind current best models<ref type="bibr" target="#b27">[28]</ref> which jointly train CRF and DCNN<ref type="bibr" target="#b5">[6]</ref> Table 3. Performance on PASCAL VOC 2012 val set. The models have been pretrained on MS-COCO 2014 dataset.</figDesc><table>. 3. We still 
observe 1.6% and 2.7% improvement over the baseline by 
DT-SE and DT-EdgeNet, respectively. Besides, adding a 
fully-connected CRF to DT-EdgeNet can bring another 1.8% 
improvement. We then evaluate the models on test set in the 
bottom of Tab. 4. Our best model, DT-EdgeNet, improves 
the baseline DeepLab by 2.8%, while it is 1.0% lower than 
DeepLab-CRF. When combining DT-EdgeNet and a fully-
connected CRF, we achieve 73.6% on the test set. Note 
the gap between DT-EdgeNet and DeepLab-CRF becomes 
smaller when stronger baseline is used. 
Incorporating multi-scale inputs State-of-art models on 
the PASCAL VOC 2012 leaderboard usually employ multi-
scale features (Method 

mIOU (%) 

DeepLab 
67.31 
DeepLab-CRF 
71.01 

DT-SE 
68.94 
DT-EdgeNet 
69.96 
DT-EdgeNet + DenseCRF 
71.77 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>in terms of ODS F-score), while our EdgeNet is not trained on BSDS500 and there is no edge supervision during training on PASCAL VOC 2012. Comparison with dense CRF Employing a fullyconnected CRF is an effective method to improve the segmentation performance. Our best model (DT-EdgeNet) is 1.3% and 1.0% lower than DeepLab-CRF on PASCAL VOC 2012 test set when the models are pretrained with Ima-geNet or MS-COCO, respectively. However, our method is many times faster in terms of computation time. To quantify this, we time the inference computation on 50 PAS-CAL VOC 2012 validation images. As shown in Tab. 5, for CPU timing, on a machine with Intel i7-4790K CPU, the well-optimized dense CRF implementation<ref type="bibr" target="#b25">[26]</ref> with 10 mean-field iterations takes 830 ms/image, while our implementation of domain transform with K = 3 iterations (each iteration consists of separable two-pass filterings across rows and columns) takes 180 ms/image (4.6 times faster). On a NVIDIA Tesla K40 GPU, our GPU implementation of domain transform further reduces the average computation time to 25 ms/image. In our GPU implementation, the total computational cost of the proposed method (EdgeNet+DT) is 26.2 ms/image, which amounts to a modest overhead (about 18%) compared to the 145 ms/image required by DeepLab. Note there is no publicly available GPU implementation of dense CRF inference yet.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work wast partly supported by ARO 62250-CS and NIH Grant 5R01EY022247-03.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-05" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-for-low and low-for-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<title level="m">Scene labeling with lstm recurrent neural networks. In CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03339</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge a retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nˆ4-fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain transform for edge-aware image and video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S L</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pixel-wise deep learning for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Statistical edge detection: Learning and evaluating edge cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="57" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional-recursive deep learning for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Filter-based meanfield inference for random fields with higher-order terms and product label-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="290" to="307" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00393</idno>
		<title level="m">Renet: A recurrent neural network based alternative to convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
