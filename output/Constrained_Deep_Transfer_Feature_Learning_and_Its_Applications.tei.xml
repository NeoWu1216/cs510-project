<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Constrained Deep Transfer Feature Learning and its Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECSE Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<addrLine>110 8th street</addrLine>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECSE Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<addrLine>110 8th street</addrLine>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Constrained Deep Transfer Feature Learning and its Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature learning with deep models has achieved impressive results for both data representation and classification for various vision tasks. Deep feature learning, however, typically requires a large amount of training data, which may not be feasible for some application domains. Transfer learning can be one of the approaches to alleviate this problem by transferring data from data-rich source domain to data-scarce target domain. Existing transfer learning methods typically perform one-shot transfer learning and often ignore the specific properties that the transferred data must satisfy. To address these issues, we introduce a constrained deep transfer feature learning method to perform simultaneous transfer learning and feature learning by performing transfer learning in a progressively improving feature space iteratively in order to better narrow the gap between the target domain and the source domain for effective transfer of the data from source domain to target domain. Furthermore, we propose to exploit the target domain knowledge and incorporate such prior knowledge as constraint during transfer learning to ensure that the transferred data satisfies certain properties of the target domain.</p><p>To demonstrate the effectiveness of the proposed constrained deep transfer feature learning method, we apply it to thermal feature learning for eye detection by transferring from the visible domain. We also applied the proposed method for cross-view facial expression recognition as a second application. The experimental results demonstrate the effectiveness of the proposed method for both applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Feature learning with deep models is an active research area. Recent research has demonstrated that with feature learning methods, effective features can be learnt for both representation and classification of the input data for many computer vision tasks including face recognition <ref type="bibr" target="#b19">[20]</ref>, object detection <ref type="bibr" target="#b4">[5]</ref>, and scene classification <ref type="bibr" target="#b23">[24]</ref>. Feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target domain Source domain</head><p>Step 1: Constrained transfer learning Raw data space  learning with deep model, however, typically requires a large amount of training data. Hence, feature learning for domains with scarce data is not feasible.</p><p>Transfer learning can be one of the approaches to address this problem and help feature learning in the data-scarce target domain by transferring data or knowledge from datarich source domain. Transfer learning not only can compensate for the lack of data in the target domain but can also benefit the tasks in the target domain from the experience gained from the source domain. The transfer learning techniques usually involve instance transfer, feature transfer, model parameter transfer, or relational knowledge transfer <ref type="bibr" target="#b11">[12]</ref>. Those transfer learning techniques have been used in natural language processing <ref type="bibr" target="#b0">[1]</ref>, document classification <ref type="bibr" target="#b22">[23]</ref>, etc. However, typical transfer learning techniques usually perform one-shot transfer in a fixed or shallow feature space, while a fixed feature space may not effectively fill the semantic gap between the target and source domains. Another issue with transfer learning is that it is purely data driven, without adequately considering certain inherent properties of the target data.</p><p>To tackle these issues, we propose a constrained deep transfer feature learning method to perform simultaneous transfer learning and feature learning by exploiting the knowledge in both target and source domains. The general framework is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, we propose to iteratively perform constrained transfer learning and feature learning in several increasingly deeper feature spaces to gradually transfer the knowledge from source domain to target domain and learn the features in the target domain. Furthermore, we propose to exploit the target domain knowledge and incorporate such prior knowledge as constraint during transfer learning to ensure that the transferred data satisfies certain properties of the target domain principles.</p><p>The proposed framework has the following merits. First, the progressive transfer allows the creation of several intermediate pseudo domains to bridge the gap between the source and target domains. As the knowledge transfer continues, the intermediate domains gradually approach the target domain. Second, feature learning is performed at each level as knowledge transfer happens, and it is also performed progressively at a higher level as knowledge transfer continues. Hence, knowledge transfer and feature learning intertwine at each step, improving both feature learning and knowledge transfer. Finally, by imposing constraints on the transferred data, we can ensure the transferred data not only possess certain desired characteristics but also to be semantically meaningful.</p><p>The remaining part of this paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the Restricted Boltzmann Machine and Deep Boltzmann Machine models. Section 4 introduces the proposed method. Section 5 shows the experimental results. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, an increasing number of works concentrate on learning good representations for data with deep models. In <ref type="bibr" target="#b17">[18]</ref>, Salakhutdinov et al. build a hierarchical deep model to learn features for object recognition and handwritten character recognition. Similar to the proposed method, some works perform feature learning based on multimodal data in different domains. For example, in <ref type="bibr" target="#b10">[11]</ref>, Ngiam et al. use deep autoencoder to learn common features from audio and video data. In <ref type="bibr" target="#b18">[19]</ref>, a deep multimodal DBM is constructed to learn shared features for images and texts. However, in contrast to the existing works that learn shared representations across domains <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b18">[19]</ref>, our work focuses on transferring knowledge from one domain to help feature learning for another domain. Multimodal feature learning methods usually require a lot of paired training data, while we specifically handle the case where there is limited data in the target domain and rich data in the source domain.</p><p>Transfer learning refers to the learning methods that leverage the knowledge from the source domain to help learning in the target domain. The transferred knowledge includes training instances, features, model parameters and relational knowledge <ref type="bibr" target="#b11">[12]</ref>. For example, in <ref type="bibr" target="#b2">[3]</ref>, Triadaboost is proposed to transfer the instances within the framework of adaboost. In <ref type="bibr" target="#b14">[15]</ref>, feature spaces are found that minimize the inter-domain differences.</p><p>There is also work that combines transfer learning with feature learning. In <ref type="bibr" target="#b22">[23]</ref>, deep features learned with the DBM model using data in the source domain are selected for document classification in the target domain. Different from the work <ref type="bibr" target="#b22">[23]</ref> that first learns deep features and then transfers the learned features among multimodal data, our work preforms transfer feature learning at multiple levels of the deep architecture. More importantly, we add target domain knowledge to constrain the transfer learning and feature learning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Before we introduce the proposed method, we first review the Restricted Boltzmann Machine (RBM) and the Deep Boltzmann Machine (DBM) models. RBM is a undirected probabilistic graphical model <ref type="figure" target="#fig_1">(Figure 2</ref> (a)) that captures the joint probability of the binary input data t (e.g. data in the target domain) with multiple binary hidden nodes h.</p><formula xml:id="formula_0">p(t; ξ) = h exp(−E(t, h; ξ)) Z(ξ) ,<label>(1)</label></formula><formula xml:id="formula_1">− E(t, h; ξ) = t T W h + c T t + b T h,<label>(2)</label></formula><p>where E(.) is the energy function, Z(ξ) = t,h exp(−E(t, h; ξ)) is the partition function, and ξ = {W, c, b} are the parameters. The conditional probabilities are as follows:</p><formula xml:id="formula_2">p(ti = 1|h; ξ) = σ( j Wi,jhj + ci),<label>(3)</label></formula><formula xml:id="formula_3">p(hj = 1|t; ξ) = σ( i tiWi,j + bj),<label>(4)</label></formula><p>where σ(.) denotes the sigmoid function. Given the training data, the parameters are learned by maximizing the log likelihood with stochastic gradient ascend algorithm approximated by the Contrastive Divergence (CD) algorithm <ref type="bibr" target="#b6">[7]</ref>. DBM model <ref type="bibr" target="#b15">[16]</ref> shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c) (assume two layer model thereafter) consists of one layer of visible nodes and multiple layers of binary hidden nodes. It represents the probability of visible nodes (similar as Equation 1) with the new energy function (ignoring the bias terms): <ref type="table">Table 1</ref>. Notations. t, s and t represent target data, source data, and the pseudo target data transferred from the source domain. If necessary, we use the superscript to indicate the pairwise (P)/unpaired (U) data (corresponding data in both domains, and data in one domain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Notations</head><p>Pairwise target and source data</p><formula xml:id="formula_4">Data P = {t k , s k } N P k=1</formula><p>Unpaired target data</p><formula xml:id="formula_5">Data U t = {ti} N U t i=1</formula><p>Unpaired source data</p><formula xml:id="formula_6">Data U s = {sj} N U v j=1</formula><p>Unpaired pseudo target data</p><formula xml:id="formula_7">Data U t = { tn} N U t n=1</formula><p>where ξ = {ξ 1 , ξ 2 } are the parameters for different layers. DBM is learned in a layer-wise manner and then jointly fine tuned <ref type="bibr" target="#b15">[16]</ref>. For feature learning using RBM or DBM, the hidden nodes in the top layer are inferred for each input data using <ref type="bibr">Equation 4</ref> or the mean-field techniques <ref type="bibr" target="#b15">[16]</ref>, and they are considered as the new feature representations. For transfer learning, the input data is the concatenated data from the source s and target domains t, and the RBM <ref type="figure" target="#fig_1">(Figure 2</ref> (b)) will learn their joint probability distribution. Transfer learning can then be performed using the joint probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Constrained deep transfer feature learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The general framework</head><p>The proposed constrained deep transfer feature learning method is motivated by the following intuitions. First, it's straightforward to think of directly applying the DBM model illustrated in section 3 to learn the deep features for the target domain. However, DBM learning usually requires a lot of training data with variations, while there is limited data in the target domain. But, if we can capture the joint distribution of target and source data as a bridge between the source and target domains, we could transfer additional large amount of source data to the target domain as pseudo data for further target domain feature learning. Second, because of the significant domain differences, one time transfer may not be effective as stated above. Therefore, we propose to perform the transfer feature learning progressively at different levels of feature spaces to generate intermediate pseudo target spaces to gradually approach the final target space. Third, because of the underlying mechanism that produces the data, target data must follow certain properties that we want to preserve in the transferred data. We hence constrain the transfer learning to ensure the satisfaction of these properties. Therefore, we propose the constrained deep transfer feature learning method. <ref type="table">Table 1</ref> shows the notations.</p><p>The general framework is shown in <ref type="figure" target="#fig_0">Figure 1</ref> and Algorithm 1. First, in the pre-training stage, we perform the transfer learning and feature learning iteratively. In the transfer learning step, we capture the joint distribution of Algorithm 1: Constrained deep transfer feature learning Data: Pairwise source and target data (Data P ); Unpaired target data (Data U t ); Unpaired source data (Data U s ). Result: Learned features: ξ 1 , ξ 2 ,... /* Pre-training stage */ for Layer l=1 to L do /* Constrained transfer learning */ • Learn the joint probability of target and source data p(t l , s l ; θ l ) with constraint C(.; θ l ). • Transfer the unpaired source data s l i by sampling the pseudo target data t l through p(t l |s l i ; θ l ). /* Feature learning */ • Learn the features ξ l with target and pseudo target data.</p><p>• Project the data to the learned feature space for further constrained transfer feature learning.</p><p>end /* Joint fine-tuning stage */ • Jointly fine-tune the features ξ 1 , ξ 2 , ... based on the deeply transferred pseudo target data and the real target data.</p><p>target and source data p(t, s; θ) using RBM <ref type="figure" target="#fig_1">(Figure 2</ref> (b)) as a bridge and transfer additional large amount of source data as pseudo target data t by sampling through the conditional distribution p(t|s; θ). In addition, we impose constraint C(.; θ) in the learning. Third, in the feature learning step, the transferred pseudo target data are combined with the original target data (shaded area in <ref type="figure" target="#fig_0">Figure 1</ref>) for feature learning using RBM <ref type="figure" target="#fig_1">(Figure 2</ref> (a)). Then, the pseudo target data and real target data are projected to the learned feature space. In the new feature space of the next level, the transferred pseudo target data are considered as "source data" for further constrained transfer feature learning in the next iteration. Finally, similar as the DBM model ( <ref type="figure" target="#fig_1">Figure 2</ref> (c)), the layer-wisely learned features with parameters ξ 1 , ξ 2 ,... are fine-tuned jointly based on the deeply transferred pseudo target data and the real target data. The iterative transfer learning and feature learning should converge, because deep feature learning has been shown converging <ref type="bibr" target="#b7">[8]</ref> and the transfer learning also converges due to the gradually reduced gaps between target domain and source domain through the iterations. In the following subsections, we discuss each step in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Constrained semi-supervised transfer learning in one layer</head><p>In this section, we first discuss how to learn the joint probability of target and source data p(t, s; θ) in a semisupervised manner with constraints. Then, we discuss how to generate the pseudo target data by sampling through p(t|s; θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Semi-supervised learning of the joint probability</head><p>In this work, we use RBM model illustrated in section 3 to learn the joint probability of target and source data p(t, s; θ). As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b), the model defines the joint probability:</p><formula xml:id="formula_8">p(t, s; θ) = h exp(−E(t, s, h; θ)) Z(θ) ,<label>(6)</label></formula><p>where the energy function E(.) has similar format as that in <ref type="bibr">Equation 2</ref>.</p><p>Recall that RBM is usually learned with the Contrastive Divergence (CD) algorithm <ref type="bibr" target="#b6">[7]</ref>. However, standard CD learning for the RBM model requires a large amount of pairwise source and target data, which are limited in our application. To tackle this problem, we propose the semisupervised learning method. Specifically, we learn the model parameters by maximizing the log likelihood w.r.t the pairwise source and target data Data P , the unpaired target data Data U t , and the unpaired source data Data U s .</p><formula xml:id="formula_9">θ * = arg max θ L(θ; Data) (7) L(θ; Data) = L(θ; Data P ) + αL(θ; Data U t ) + βL(θ; Data U s ) (8) L(θ; Data P ) = 1 N P N P k=1 log(p(t k , s k ; θ)) (9) L(θ; Data U t ) = 1 N U t N U t i=1 log(p(ti; θ)) (10) L(θ; Data U s ) = 1 N U s N U s j=1 log(p(sj; θ))<label>(11)</label></formula><p>Here, α and β are two parameters that balance different terms. p(t; θ) and p(s; θ) in Equation <ref type="bibr" target="#b9">10</ref> and Equation <ref type="bibr" target="#b10">11</ref> are the marginal distributions of data in one domain by summing out the missing data in the other domain. Following the CD algorithm, we solve the optimization problem in Equation 7 using gradient ascent algorithm. The gradient of model parameters for each log likelihood term is calculated as:</p><formula xml:id="formula_10">∂L(θ; Data) ∂θ = − ∂E ∂θ P data + ∂E ∂θ P model ,<label>(12)</label></formula><p>where . P data and . P model represent the data dependent and model dependent expectations.</p><p>In the CD algorithm <ref type="bibr" target="#b6">[7]</ref>, to approximately calculate the data dependent expectation . P data , we need to sample the unknown variables from the data dependent probabilities P data . Note that, in the semi-supervised learning setting, the data dependent probabilities P data differ for each data type. Specifically, P data P = p(h|t k , s k ; θ), P data U t = p(h, s|t i ; θ) and P data U s = p(h, t|s j ; θ). For the pairwise data, we could directly sample h from p(h|t k , s k ; θ) using Equation <ref type="bibr" target="#b3">4</ref>. However, for the unpaired data, p(h, s|t i ; θ) and p(h, t|s j ; θ) are intractable. Thus, we use Gibbs sampling to generate h, s from p(h, s|t i ; θ). Similarly, we can  generate samples h, t from p(h, t|s j ; θ). Following the CD algorithm, we estimate the model dependent expectations with k-step (k=5) Gibbs update through the model, starting from the samples calculated using the data dependent probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Transfer learning with constraints</head><p>In every domain, its data only represent the observations of the underlying latent objects. It is often through either a physical or biological process that the latent objects produce the observed data in the target domain. For example, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>, in the case of eye detection on thermal images by transferring the knowledge from visible domain, the intensities of the eyes measure the temperature near the eye skin surface and the eye temperature is determined by the the blood flow to the eye skin surface. Hence, the intensity distribution near the eye on an thermal image is mainly determined by the underlying latent vascular structure distribution. For example, the tear dust area usually has the high temperature due to the blood flow in the artery beneath ( <ref type="figure" target="#fig_3">Figure 3 (c)</ref>). On the other hand, in the pupil area and the hair-insulated eye lash region, the temperature is expected to be low because of lack of blood flow. These unique facial anatomy structures in the eye regions lead to the unique and distinct target data pattern that is universal across subjects. As a result, to ensure a physically and semantically meaningful transfer, we propose to impose certain constraints during the transfer learning in order to preserve such target data properties, such as the certain unique shape and appearance characteristics. Based on the intuitions illustrated above, we modify the parameter learning problem in Equation 7 by adding the constraint C(.; θ):</p><formula xml:id="formula_11">θ * = arg max θ L(θ; Data) − λC(.; θ)<label>(13)</label></formula><p>Here, the first term is the log likelihood function defined in Equation <ref type="bibr" target="#b7">8</ref>. The second term C(.; θ) is a cost function to ensure that the transferred data satisfies certain properties.</p><p>To solve this optimization problem in <ref type="bibr">Equation 13</ref>, depending on the type of C(.; θ), we could still use the gradient ascent algorithm and we have derived the gradient of the first term w.r.t the parameters in section 4.2.1. Then, we only need to calculate the gradient of the second term C(.; θ) w.r.t the parameters, i.e., dC(.;θ) dθ . It's detailed calculated will be discussed in section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Pseudo target data generation by sampling</head><p>Given the learned joint probability p(t, s; θ), we could transfer the large amount of additional source data s j ∈ Data U s to the target domain to generate the pseudo target data t. This can be done by sampling through p(t|s j ; θ), s j ∈ Data U s using Gibbs Sampling method that iteratively calls Equation 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature learning in one layer</head><p>In each iteration, combining the transferred pseudo target data and the real target data, the goal of feature learning is to learn the RBM model <ref type="figure" target="#fig_1">(Figure 2 (a)</ref>) that captures the variations of target and pseudo target data and uses the hidden nodes as new features. More formally, the RBM is defined in <ref type="figure" target="#fig_0">Equation 1</ref> and parameter learning is formulated as:</p><formula xml:id="formula_12">ξ * = arg max ξ L(ξ; Datat) + γL(ξ; Data U t )<label>(14)</label></formula><p>The first term and second term represent the log likelihood w.r.t the real target data Data t = Data P t Data U t , and the pseudo target data Data U t . γ is the parameter that balances the two terms. To learn the features, we apply the standard Contrastive Divergence (CD) algorithm <ref type="bibr" target="#b6">[7]</ref>. The only difference is that the gradient is calculated based on both terms in Equation <ref type="bibr" target="#b13">14</ref>. Given the learned model, for each target and pseudo target data, its new representation can be calculated using Equation <ref type="bibr" target="#b3">4</ref>. Then, for the next iteration in the learned feature space, constrained transfer learning and feature learning continue and interact until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Joint feature fine-tuning</head><p>After multi-layer constrained transfer feature learning in the pre-training stage, we have the initially learned features ξ 1 , ξ 2 , .... In addition, we have the deeply transferred pseudo target data in the top feature layer denoted as t L .</p><p>Then, for joint feature fine-tuning, we need to project the pseudo data back to the original space by repeatedly calling Equation 3 with parameters ξ L ,..., ξ 1 and get the deeply transferred pseudo data in the original space t 1 . Then, based on the pseudo target data t 1 and the real target data t 1 , we apply standard fine-tuning algorithm <ref type="bibr" target="#b15">[16]</ref> for the Deep Boltzmann Machine model, where the mean-field fix point equation is used to estimate the data-dependent expectation and the Persistent Markov Chain is used to estimate the model dependent expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Eye detection on thermal images by transferring from visible domain</head><p>To demonstrate the proposed framework for constrained deep transfer feature learning, we applied it to eye detection on thermal images. Comparing to facial analysis in the visible domain, thermal facial analysis is more sensitive to eye localizations <ref type="bibr" target="#b1">[2]</ref>. However, there are limited works about eye detection on thermal images. Furthermore, the existing thermal eye detection techniques typically use the visible image features, which are suboptimal, since thermal and visible images are formed based on different principles <ref type="bibr" target="#b3">[4]</ref>. For example, thermal images contain limited texture and gradient information due to the heat diffusion phenomenon, while the visible image features usually focus on encoding these detailed information.</p><p>While feature learning on thermal images can alleviate this problem, limited thermal training images make it very difficult to leverage on the existing deep learning models. To address this challenge, we propose to apply the proposed constrained transfer feature learning method to perform thermal feature learning by transferring the eye data in the visible domain to the thermal domain to compensate for the lack of thermal data. The target domain refers to the thermal patches, including the eye patches and the background patches. The source domain refers to the visible eye patches. Note that, we only need to transfer the eye patches as positive data, since we can generate many negative data by sampling from the background. For thermal eye detection, with the learned thermal features using the proposed method, we train SVM classifier to search the eye with a scanning window manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>Databases: We use four databases including the visible and thermal facial behavior database(VTFB), the MAH-NOB laughter database <ref type="bibr" target="#b12">[13]</ref>, the Natural visible and thermal facial expression database (NVIE) <ref type="bibr" target="#b20">[21]</ref>, and the Facial Recognition Technology (FERET) database <ref type="bibr" target="#b13">[14]</ref>. VTFB has synchronized visible and thermal videos (FLIR SC6800 thermal camera) for 7 subjects and additional thermal videos for 13 subjects with spontaneous facial expressions and arbitrary head poses. The MAHNOB Laughter database provides thermal facial sequences of 22 subjects with moderate head poses, neutral and happy facial expressions. The NVIE database contains thermal facial sequences of 215 subjects with spontaneous and posed expressions. FERET database provides visible facial images with different head poses and moderate expressions.</p><p>During training, we use the synchronized thermal and visible images of the first 7 subjects from VTFB (1295 images) as pairwise data. We use the thermal images of ad-ditional 3 subjects from the VTFB (573 images), and visible images from FERET (3830 images) as unpaired data. We test the method on the thermal images of remaining 10 subjects from VTFB (542 images), the MAHNOB database (114 images) and the NVIE database (35550 images).</p><p>The method: We use two layer DBM with 800 and 600 hidden nodes to learn features. The RBM models used to learn the joint distributions p(t, s; θ l ) have 400 hidden nodes. The hyper-parameters α, β, λ and γ are 0.4, 0.3, 0.002, and 0.3, respectively. In our experiments, the height and width of the image patches are about 1/2 the inter-ocular distance and the patches are normalized to 40×40. The background patches are at least 1/4 inter-ocular distance away from the eyes. We augment the training data by rotating and resizing the images. Following <ref type="bibr" target="#b15">[16]</ref>, we learn the Gaussian-binary RBMs <ref type="bibr" target="#b8">[9]</ref> with 1000 hidden nodes for the raw visible and thermal image patches, respectively, and then treat the values of the hidden layers as the preprocessed data to speed up the learning procedure. With Matlab implementation, it takes about 30 hours to train the full model with two layers and the constraint on a single core machine.</p><p>To impose the target domain constraint as discussed in section 4.2.2, we propose to impose the constraint that the transferred thermal eyes must satisfy the general appearance pattern as shown in <ref type="figure" target="#fig_3">Fig. 3(c)</ref> for the thermal eye. To achieve this goal, we propose to capture such a target data pattern by using a thermal mean eye, which can be obtained by averaging the existing target data. The basic idea is that while individual thermal eye may vary because of each person's unique eye structure, through averaging, the thermal mean eye can capture their commonality while canceling out their differences. The commonality is resulted from the shared underlying eye structure. <ref type="figure" target="#fig_3">Figure 3</ref>(c) shows the mean eye as an example. It apparently can capture the unique pattern of a thermal eye, i.e., brighter in the inner eye corner and darker in the eye center that are true across subjects at different conditions.</p><p>For transferring the visible eyes to thermal domain to help the thermal feature learning, the transfer learning step in Equation 13 becomes:</p><formula xml:id="formula_13">θ * = arg max θ L(θ; Data + ) − λ 1 N U + s j t + Q j − m + 2 2 .<label>(15)</label></formula><p>Here, "+" denotes the positive eye data. The second term enforces the mean of the transferred pseudo thermal eye</p><formula xml:id="formula_14">1 N U + s j t +</formula><p>Qj is close to the given mean eye m + . Specifically, for each source data s + j ∈ Data U + s , t Qj represents the expected transferred pseudo thermal eye and Q j = p(t + |s + j ; θ). Then, 1 N U + s j t + Qj will be the mean of the transferred pseudo thermal eyes and it should be close to the given mean eye m + . Assume the gradient of the second constraint term of the objective function in <ref type="bibr">Equation 15</ref> w.r.t θ is denoted as δ. Then, it is calculated as follows:</p><formula xml:id="formula_15">δ =2 * [ 1 N U + s j t + Q j − m + ] T * 1 N U + s j [− t + ∂E ∂θ R j + ∂E ∂θ R j t + Q j ],<label>(16)</label></formula><p>where R j = p(t + , h|s + j ; θ), s + j ∈ Data U + s . Evaluation criterion: The eye detection error is defined as error = max(||D l −G l ||2,||Dr−Gr||2)</p><formula xml:id="formula_16">||G l −Gr||2</formula><p>, where D and G represent the detected and ground truth eye locations, and the subscript denotes left and right eyes. We regard error &lt; 0.15 as the successful detection. The evaluation criteria follows the methods [10] <ref type="bibr" target="#b21">[22]</ref> for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation of the proposed method</head><p>In this section, we evaluate the proposed constrained deep transfer feature learning method on the VTFB database. Note that, in the constrained transfer learning step (discussed in section 4.2), the joint probability of thermal and visible eyes p(t + , s + ; θ) can be learned with different approaches. They are (M1) learning with pairwise data using standard CD algorithm <ref type="bibr" target="#b6">[7]</ref>, (M2) proposed semisupervised learning with paired and unpaired data as discussed in section 4.2.1, (M3) learning with pairwise data and constraints (second term in Equation <ref type="bibr" target="#b12">13</ref>), and (M4) learning with semi-supervised data and constraints (the full model). In the experiments, we compare the four variations and study the other properties of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Evaluation of the constrained transfer learning method in layer one</head><p>First, we evaluate the constrained transfer learning method in layer one. We evaluate the learned joint probabilities p(t + , s + ; θ) with different learning strategies (M1 to M4) based on two criteria, including the reconstruction error and the log likelihood on the hold-out pairwise data. The reconstruction error refers to the average pixel difference between the transferred pseudo thermal eyes sampled from p(t + |s + j ; θ) and the ground truth thermal eyes. We calculate the log-likelihood on the hold-out testing set using the method in <ref type="bibr" target="#b16">[17]</ref>.</p><p>As can be seen in <ref type="table" target="#tab_1">Table 2</ref>, while both the semisupervised learning and the constraint (M2&amp;M3) improve the performances over standard learning method M1, combining them together (M4, the full model) achieves the best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Evaluation of the thermal eye detector with the constrained deep transfer features</head><p>In this subsection, we further evaluate the eye detection performance based on the features learned using the proposed method with two-layer DBM (two iterations in Algorithm 1). For the constrained transfer learning step in each layer of the deep model, we use four different learning strategies (M1-M4).</p><p>(1) Different methods: We compare the proposed method with different strategies (M1-M4) to the baseline method (M0). The baseline method learns features using the standard Deep Boltzmann Machine model based on the thermal data without any knowledge transfer. As shown in <ref type="table" target="#tab_2">Table 3</ref>, even with only the pairwise data, the proposed transfer feature learning method (M1) improves over the baseline (M0). The semi-supervised learning (M2) and constraint (M3) further boost the performances. By combining them together (M4), the constrained deep transferred features learned from multi-modality data increase the detection rate by 6.09%, comparing to the baseline (M0).</p><p>We also implement two other baselines using DBM and visible training data. (V0) refers to DBM feature learning based on the visible data. (V1) fine-tune the visible features learned with visible data on thermal data, so that the model parameters for thermal feature learning are initialized as the parameters of visible features. As can be seen in <ref type="table" target="#tab_2">Table 3</ref>, they are all inferior to the proposed method (M4). (2) Transfer in different layers: <ref type="figure" target="#fig_4">Figure 4</ref> shows that it's important to perform the constrained transfer learning in multiple layers, since it further boosts the performance comparing to one layer transfer. In addition, transferring in the deeper feature space (layer 2) is slightly better than transferring in the shallow feature space (layer 1).  (3) Learning with different amount of training data We vary the training data to train the proposed method (features and classifier) and the baseline method, and the eye detection rates are shown in <ref type="figure" target="#fig_6">Figure 5</ref>. Specifically, we keep reducing the number of training subjects from "P7+UP3" (7 subjects with pairwise data + 3 subjects with unpaired data) to "P2+UP2". Comparing M4 to M0, the proposed method always learns better features than the baseline. Comparing M4 to M2, the constraints are important and they improve the performances with different sets of data.  (4) Comparison with other features: We compare the learned features with the proposed method (M4) to standard image features, such as the Scale Invariant Feature Transform (SIFT) feature, and the Histogram of Oriented Gradients (HOG) feature. For fair comparison, we train the linear SVM with same training data (P7+UP3), and the experimental results only differ due to the features. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>, the learned features with the proposed method significantly outperforms the other designed features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with existing thermal eye detectors on other databases</head><p>To the best of our knowledge, there are only two works <ref type="bibr" target="#b9">[10]</ref>[22] that report eye detection results on publicly available thermal face databases. In <ref type="bibr" target="#b9">[10]</ref>, image patches are represented by Haar wavelet coefficients. Two successive classifiers (coarse and fine) are constructed based on features in different levels of details in a cascade manner.</p><p>In <ref type="bibr" target="#b21">[22]</ref>, information from the whole face region is used to support eye detection on thermal facial images. It identifies 15 sub-regions on the face. Haar-like features and SVM are used to learn the eye detector. <ref type="table" target="#tab_4">Table 4</ref> illustrates that our detector outperforms the method <ref type="bibr" target="#b9">[10]</ref> on the MAHNOB Laughter database <ref type="bibr" target="#b12">[13]</ref>. On the NVIE database <ref type="bibr" target="#b20">[21]</ref>, our detector is significantly better than the method in <ref type="bibr" target="#b21">[22]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Second Application</head><p>The proposed constrained deep transfer feature learning method is not limited to visible and thermal domains. It can be extended to other domains for simultaneous knowledge transfer and deep feature learning. In this section, we apply the proposed framework to cross-view facial expression recognition as the second application. The goal of this application is to learn the effective feature representation to classify neutral and non-neutral face in non-frontal view by transferring the data in frontal view (source) to non-frontal view (target). The unique facial structure in non-frontal view is used to constrain the transferring learning. In the experiments, we used the Multi-Pie database <ref type="bibr" target="#b5">[6]</ref>  <ref type="figure">(Figure 8</ref>  different features, including the learned features using the proposed constrained deep transfer feature learning method, the learned features using the conventional DBM, the handcrafted HOG and LBP features. In addition, we vary the number of training subjects in the target domain (100, 50, 20). There are a few observations. First, the learned features are significantly better than the hand-crafted features. Second, for all settings, by transferring the knowledge in the source domain, the learned features using the proposed method outperform the features learned with DBM.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a constrained deep transfer feature learning framework in order to perform feature learning in a data-scarce target domain. Instead of performing transfer learning in a fixed feature space, we propose to simultaneously perform transfer and feature learning iteratively in increasingly higher level of feature spaces in order to minimize the semantic gap between the source and target domains. Furthermore, to ensure the transferred data to be semantically meaningful and to be consistent with the underlying properties of the target domain, we incorporate target domain knowledge as constraints into the transfer learning.</p><p>We applied the proposed framework for thermal feature learning for thermal eye detection by transferring the knowledge from visible domain. We also applied the it for cross-view facial expression recognition. The experiments demonstrate the effectiveness of the proposed framework for both applications. In the future, we would apply the framework to other vision applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The framework of the proposed constrained deep transfer feature learning method. The algorithm iteratively performs transfer learning and feature learning in more and more deeper feature spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) RBM for data in one domain. (b) RBM for transfer learning. (c) DBM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Constrained deep transfer feature learning for thermal eye detection. (a)(b) Pairwise visible and thermal facial images<ref type="bibr" target="#b12">[13]</ref>. (c) Thermal mean eye.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Transfer in different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Learning with different amounts of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Cumulative error distribution curves using eye detectors with different features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 5 )</head><label>5</label><figDesc>Visualization: Figure 7 (a)(b) show the learned filters (parameters ξ 1 and ξ 2 ) with the proposed method (M4). Filters in the lower level capture local dependencies. They represent small dots, and some of them are similar to the Gabor Filters. Filters in the higher level capture more global variations. (a) learned filters in layer 1 (b) learned filters in layer 2 Figure 7. (a)(b): Learned filters in different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>), which contains facial images with varying head poses, facial expressions, and illumination conditions (training: id 1-200, testing: id 201-337).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 Figure 8 .</head><label>98</label><figDesc>below shows the results. In the experiments, we compared the classification accuracies (linear SVM) with (a) Frontal image (source) (b) Non-frontal image (target) Cross-view neutral (left) and non-neutral (right) facial expression recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Facial expression classification accuracies with different features and numbers of training subjects in the target domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of different constrained transfer learning methods in layer one.</figDesc><table>Methods 
Reconstruction error Log likelihood 
M1: Pairwise data 
0.0627 
-514.1 
M2: Semi-supervised 
0.0601 
-484.8 
M3: Pairwise + constraint 
0.0588 
-492.0 
M4: Semi + constraint 
0.0580 
-476.9 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Eye detection rates using different methods</figDesc><table>Methods 
Eye detection rate 
M0: DBM (baseline) 
87.45% 
M1: Pairwise data 
89.48% 
M2: Semi-supervised 
90.04% 
M3: Pairwise + constraint 
91.88% 
M4: Semi + constraint 
93.54% 

V0: DBM visible (baseline) 
73.25% 
V1: DBM visible finetune (baseline) 
86.72% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Comparison with existing thermal eye detectors on the MAHNOB laughter database and the NVIE database.</figDesc><table>MAHNOB Laughter Database: 
Methods 
reported in [10] Proposed method (M4) 
Detection rate 
83.3% 
87.72% 
NVIE Database: 
Methods 
reported in [22] Proposed method (M4) 
Detection rate 
68% 
81.60% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">− E(t, h 1 , h 2 ; ξ 1 , ξ 2 ) = t T W 1 h 1 + (h 1 ) T W 2 h 2 ,(5)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ir and visible light face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="332" to="358" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boosting for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning, ICML &apos;07</title>
		<meeting>the 24th international conference on Machine learning, ICML &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond cognitive signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Espinosa-Duro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faundez-Zanuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mekyska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="374" to="381" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial component detection in thermal imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Binefa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internation conference on Machine Learing (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The mahnoblaughter database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing Journal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="202" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European conference on Computer vision: Part IV</title>
		<meeting>the 11th European conference on Computer vision: Part IV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning with hierarchical-deep models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1958" to="1971" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A natural visible and infrared facial expression database for expression recognition and emition inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="682" to="691" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eye localization from thermal infrared images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2613" to="2621" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep transfer learning via restricted boltzmann machine for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Applications and Workshops (ICMLA), 10th International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="323" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
