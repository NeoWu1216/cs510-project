<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trace Quotient Meets Sparsity: A Method for Learning Low Dimensional Image Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Wei</surname></persName>
							<email>xian.wei@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<addrLine>Arcisstr. 21</addrLine>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
							<email>hao.shen@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<addrLine>Arcisstr. 21</addrLine>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kleinsteuber</surname></persName>
							<email>kleinsteuber@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<addrLine>Arcisstr. 21</addrLine>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trace Quotient Meets Sparsity: A Method for Learning Low Dimensional Image Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an algorithm that allows to learn low dimensional representations of images in an unsupervised manner. The core idea is to combine two criteria that play important roles in unsupervised representation learning, namely sparsity and trace quotient. The former is known to be a convenient tool to identify underlying factors, and the latter is known as a disentanglement of underlying discriminative factors. In this work, we develop a generic cost function for learning jointly a sparsifying dictionary and a dimensionality reduction transformation. It leads to several counterparts of classic low dimensional representation methods, such as Principal Component Analysis, Local Linear Embedding, and Laplacian Eigenmap. Our proposed optimisation algorithm leverages the efficiency of geometric optimisation on Riemannian manifolds and a closed form solution to the elastic net problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Finding appropriate low dimensional representations of data is a long-standing challenging problem in data processing and machine learning. Recent development in representation learning shows that appropriate data representations are the key to the success of machine learning algorithms, as different representations can entangle different explanatory information of the data, cf. <ref type="bibr" target="#b3">[4]</ref>. The aim of this paper is to construct effective low dimensional representation learning approaches to disentangle various explanatory or discriminative information in the data for solving unsupervised learning problems.</p><p>Sparse representation (SR) was developed as an instrument to leverage the underlying sparse structure of data. It has led to a great success in signal reconstruction, denoising and image super-resolution, cf. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref>. These methods can be considered as data-driven sparse representation approaches. On the other hand, sparse coefficients can also be interpreted as features that are suitable for the tasks of learning, such as face recognition <ref type="bibr" target="#b28">[29]</ref>, subspace clustering <ref type="bibr" target="#b10">[11]</ref>, and image classification <ref type="bibr" target="#b25">[26]</ref>.</p><p>Furthermore, it is also evidential that sparse representation of the data can be further processed by applying other disentangling instruments, in order to reveal task-related information. For example, the work in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> incorporates linear classifiers with sparse representation to jointly learn a sparsifying dictionary and a classifier. Similarly, adopting sparse representations in a classical expected risk minimisation formulation leads to the so-called task-driven dictionary learning approaches, specifically for supervised learning tasks, cf. <ref type="bibr" target="#b23">[24]</ref>. In the unsupervised learning setting, directly applying a Principal Component Analysis (PCA) on sparse representations also results in promising performance in 3D visualisation and clustering, cf. <ref type="bibr" target="#b11">[12]</ref>. From a perspective of representation learning, it is a logical conclusion that sparse representations contain rich distributed information of the data with respect to certain learning tasks, and it also necessitates application of further learning mechanisms to disentangle the underlying explanatory information.</p><p>In this work, we are interested in the problem of unsupervised learning. Among various unsupervised learning techniques, the trace quotient criterion is a simple but powerful instrument for data discrimination, in particular for Dimensionality Reduction (DR). This generic criterion is shared by various classic DR methods, such as PCA, Linear Local Embedding (LLE) <ref type="bibr" target="#b24">[25]</ref>, Orthogonal Neighbourhood Preserving Projection (ONPP) <ref type="bibr" target="#b21">[22]</ref>, Locality Preserving Projections (LPP) <ref type="bibr" target="#b16">[17]</ref>, and Orthogonal LPP (OLPP) <ref type="bibr" target="#b6">[7]</ref>. Our main construction in this work is to apply the trace quotient criterion to further disentangle sparse representations for unsupervised learning tasks.</p><p>The paper is organised as follows. Section 2 provides a brief review on both sparse representations and the trace quotient criterion. In Section 3, we construct a generic cost function for learning both the sparsifying dictionary and the orthogonal DR transformation, and develop a geomet-ric conjugate gradient algorithm on the underlying smooth manifold. Three applications of the proposed generic model are discussed in Section 4, together with their experimental evaluations presented in Section 5. Finally, conclusions and outlooks are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Elastic Net Solution and the Trace Quotient Maximisation</head><p>In this section, we recall some facts about both sparse representations and trace quotient optimisation based dimensionality reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sparse Coding</head><p>Given a set of data samples x i ∈ R m , the aim of sparse coding is to find a collection of atoms d j ∈ R m such that each data sample can be approximated by a linear combination of only a few of the atoms {d j }. According to <ref type="bibr" target="#b3">[4]</ref>, the atoms can be interpreted as underlying factors that are responsible for explaining the discrepancy in the data set. In other words, sparse coding generates sparsely distributed representations of data with respect to the specific atoms.</p><p>The collection of atoms (often as columns in a matrix) is called a dictionary D ∈ R m×r , leading to the model</p><formula xml:id="formula_0">x i = Dφ i + ǫ i ,<label>(1)</label></formula><p>where φ i ∈ R r is the corresponding sparse representation, and ǫ i ∈ R m is additive noise. In this work, we restrict each column d i ∈ R m of D to have unit norm, i.e.</p><formula xml:id="formula_1">S(m, r) := D ∈ R m×r rk(D) = m, d i 2 = 1 ,<label>(2)</label></formula><p>which is a product manifold of (m − 1)-dimensional unit spheres. In the literature, there are several well-established methods for sparse coding, which all depend on a certain sparsity measure, cf. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. Once a dictionary is given, there are several ways of finding the sparse representation. If sparsity is measured by employing a combination of the ℓ 1 -and the ℓ 2 -norm, a solution to the elastic net problem <ref type="bibr" target="#b33">[34]</ref>, i.e.</p><formula xml:id="formula_2">φ * := argmin φ∈R r 1 2 x − Dφ 2 2 + λ 1 φ 1 + λ2 2 φ 2 2<label>(3)</label></formula><p>yields a convenient way to obtain the sparse representation. The two regularisation parameters λ 1 ∈ R + and λ 2 ∈ R + are chosen to ensure stability and uniqueness of the solution. Solutions to the elastic net problem (3) share a convenient fact that, under certain assumptions, there exists a closed from expression. Let us define the set of indices of the non-zero entries of the solution φ * = [ϕ * 1 , . . . , ϕ * r ] ⊤ ∈ R r by Λ := {i ∈ {1, . . . , r}|ϕ * i = 0} and k := |Λ|. Then the solution of the elastic net (3) has a closed-form expression as</p><formula xml:id="formula_3">φ * D (x) := D ⊤ Λ D Λ + λ 2 I k −1 D ⊤ Λ x − λ 1 s Λ ,<label>(4)</label></formula><p>where I k is the k × k identity matrix, s Λ ∈ {±1} k carries the signs of φ * Λ , and D Λ ∈ R m×k is the subset of D in which the index of atoms (columns) fall into the support Λ. With a reasonable assumption that the dictionary D is suitably incoherent, the solution φ * D (x) shares an algorithmically convenient property of being locally twice differentiable with respect to both D and x, cf. <ref type="bibr" target="#b23">[24]</ref>. Such a prominent property leads to the framework of task-driven dictionary learning, which is specifically dedicated to supervised learning problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Low dimensional representations based on the trace quotient criterion</head><p>The goal of low dimensional representation learning is to find representations y i ∈ R l of given data samples x i ∈ R m with l &lt; m, via a mapping</p><formula xml:id="formula_4">µ : R m → R l , x → µ(x),<label>(5)</label></formula><p>which captures certain desired properties of the data to facilitate the specific applications. Many classic DR methods restrict the mapping µ to be an orthogonal transformation, i.e. µ(x) := U ⊤ x with U ∈ St(l, m). Here St(l, m) denotes the Stiefel manifold</p><formula xml:id="formula_5">St(l, m) := U ∈ R m×l |U ⊤ U = I l .<label>(6)</label></formula><p>In the category of unsupervised learning, this model includes various classic DR methods, such as PCA, Orthogonal Locality Preserving Projection (OLPP) <ref type="bibr" target="#b6">[7]</ref>, Orthogonal Linear Graph Embedding (OLGE) <ref type="bibr" target="#b29">[30]</ref>, and Orthogonal Neighbourhood Preserving Projections (ONPP) <ref type="bibr" target="#b21">[22]</ref>. Often, the aforementioned DR methods find the orthogonal transformation U ∈ St(l, m) via maximising the socalled trace quotient or trace ratio, i.e.</p><formula xml:id="formula_6">g : St(l, m) → R, g(U ) := tr(U ⊤ AU ) tr(U ⊤ BU ) ,<label>(7)</label></formula><p>where A ∈ R m×m is assumed to be symmetric positive semi-definite and B ∈ R m×m is often assumed to be symmetric positive definite. Both matrices are constructed according to the specific problems, cf. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, and examples will be given and discussed in Section 4. Due to the rotation invariance of the function g, i.e. g(U Θ) = g(U ) for Θ ∈ R l×l being orthogonal, we can redefine the trace quotient function as</p><formula xml:id="formula_7">f : Gr(l, m) → R, f (P ) := tr(AP ) tr(BP ) ,<label>(8)</label></formula><p>where Gr(l, m) denotes the Graßmann manifold as the set of all m-dimensional rank-l orthogonal projectors, i.e.</p><p>Gr(l, m) := U U ⊤ |U ∈ St(l, m) .</p><p>Various efficient optimisation algorithms over Riemannian manifolds have been developed to maximise the function f , cf. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Joint Learning Framework</head><p>In this section, we firstly present a generic cost function, which adopts the sparsifying dictionary learning in the framework of trace quotient maximisation in Section 3.1. Then a geometric conjugate gradient algorithm is presented in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A generic cost function</head><p>As suggested by the work of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>, further processing on the sparse representation is capable of unveiling taskrelated underlying factors, potentially for both supervised and unsupervised learning tasks. In what follows, we construct a cost function, which allows to jointly learn both the sparsifying dictionary and the orthogonal transformation in the framework of trace quotient maximisation.</p><p>Let us denote by Φ(D, X) := [φ x1 (D), . . . , φ xn (D)] ∈ R r×n the sparse representation of the data X = [x 1 , . . . , x n ] for a given dictionary D. We confine ourselves to the solutions of the elastic net minimisation. Let A : R r×n → R r×r and B : R r×n → R r×r be two smooth functions that serve as generating functions for the matrices A and B in the trace quotient <ref type="bibr" target="#b6">(7)</ref>. The specific constructions of the mappings A and B are given in Section 4. Then we define a generic trace quotient function in sparse representations as</p><formula xml:id="formula_9">f : S(m, r) × Gr(l, r) → R, f (D, P ) := tr(A(Φ(D, X))P ) tr(B(Φ(D, X))P ) + σ ,<label>(10)</label></formula><p>with σ ∈ R + being chosen to guarantee the denominator of f to be positive. manifold S(m, r) × Gr(l, r). In the rest of the paper, we refer to our proposed model as SPARse LOW dimensional representation learning (SparLow). In order to prevent solution dictionaries from being highly coherent, which is critical for guaranteeing the local smoothness of the elastic net solutions, we employ a log-barrier function on the scalar product of all dictionary columns to control the mutual coherence of the learned dictionary D, cf. <ref type="bibr" target="#b15">[16]</ref>, i.e.</p><formula xml:id="formula_10">r(D) := − 1≤i&lt;j≤k log(1 − (d ⊤ i d j ) 2 ).<label>(11)</label></formula><p>Furthermore, the authors in <ref type="bibr" target="#b27">[28]</ref> argue that an appropriate dictionary of choice in sparse representation can reveal the semantics of the data. We propose the following regulariser on the dictionary to be learned</p><formula xml:id="formula_11">h(D) = D − D * 2 F ,<label>(12)</label></formula><p>where D * is the optimal data-driven dictionary learned from sparse coding of the data X, and · F denotes the Frobenius norm. Practically, we use a dictionary D produced by state</p><formula xml:id="formula_12">Algorithm 1: A CG-SparLow Algorithm.</formula><p>Input : X ∈ R m×n and functions A : R r×n → R r×r and B : R r×n → R r×r as specified in Section 4 ; Output: Accumulation point D * ∈ S(m, r) and P * ∈ Gr(l, r) ;</p><p>Step 1: Generate initialization D (0) ∈ S(m, r) and P (0) ∈ Gr(l, r), and set j = 1 ;</p><p>Step 2: Compute</p><formula xml:id="formula_13">G (1) = H (1) ← (∇ J (D (0) ), ∇ J (P (0) )) ;</formula><p>Step 3: Set j = j + 1 ;</p><p>Step 4: Update</p><formula xml:id="formula_14">(D (j+1) , P (j+1) ) ← (D (j+1) , P (j+1) )+λH (j) ,</formula><p>where λ is computed by employing a backtracking line search along geodesics;</p><p>Step</p><formula xml:id="formula_15">5: Update H (j+1) ← G (j+1) + γH (j) , where G (j+1) = (∇ J (D (j+1) ), ∇ J (P (j+1) ))</formula><p>, and γ is chosen according to Eq. (14) ;</p><formula xml:id="formula_16">Step 6: If j mod (r(m − 1) + l(r − l) − 1) = 0, set H (j+1) ← G (j+1) ;</formula><p>Step 7: If G (j+1) is small enough, stop. Otherwise, go to Step 3;</p><p>of the art methods, such as K-SVD. Our experiments have verified that the heuristic regulariser h ensures solutions of the generic cost function J defined in Eq. (13) to be self explanatory to the data, and guarantees stable performance towards the task of learning. By combining the two regularisers discussed above, we construct the following cost function to jointly learn both the sparsifying dictionary and the orthogonal transformation, i.e.</p><p>J : S(m, r)×Gr(l, r) → R,</p><formula xml:id="formula_17">J(D, P ) := f (D, P ) + µ 1 r(D) + µ 2 h(D),<label>(13)</label></formula><p>where the two weighting factors µ 1 , µ 2 ∈ R + control the influence of the two constraints on the final solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A geometric conjugate gradient algorithm</head><p>In this subsection, we present a geometric CG algorithm on the product manifold S(m, r) × Gr(l, r) to maximise the generic cost function J, defined in <ref type="bibr" target="#b12">(13)</ref>. It is well known that CG algorithms offer prominent properties, such as a superlinear rate of convergence and the applicability to large scale optimisation problems with low computational complexity, e.g. in sparse recovery <ref type="bibr" target="#b14">[15]</ref>. We refer to <ref type="bibr" target="#b0">[1]</ref> for further technical details for these computations.</p><p>We denote by T (D,P ) S(m, r) × Gr(l, r) the tangent space of S(m, r) × Gr(l, r) at (D, P ), the Riemannian gradient of J at (D, P ) by G := (∇ J (D), ∇ J (P )) ∈ T (D,P ) S(m, r) × Gr(l, r), and by H ∈ T (D,P ) S(m, r) × Gr(l, r) the conjugate gradient direction.</p><p>Given dim S(m, r) = r(m − 1) and dim Gr(l, r) = l(r − l), we summarise a conjugate gradient algorithm for maximising the function J as defined in <ref type="formula" target="#formula_0">(13)</ref>, cf. Algorithm 1.</p><p>For updating the direction parameter γ in Step 5, we confine ourselves to a formula, which is proposed in <ref type="bibr" target="#b22">[23]</ref>, as</p><formula xml:id="formula_18">γ = G (j+1) , G (j+1) −G (j) H (j) , G (j) ,<label>(14)</label></formula><p>with the inner product ·, · being defined as</p><formula xml:id="formula_19">(P 1 , Q 1 ), (P 2 , Q 2 ) = tr(P ⊤ 1 P 2 ) + tr(Q ⊤ 1 Q 2 ).<label>(15)</label></formula><p>Finally in Step 6, the search direction is periodically reset to the gradient, in order to achieve fast convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications of the SparLow Model</head><p>In the previous section, we propose a generic regularised cost function, and develop a geometric conjugate gradient algorithm to maximise the generic cost function J. In what follows, we present counterparts of three classic unsupervised learning methods, namely, PCA, LLE, and Laplacian Eigenmap. Experimental evaluations are conducted in Section 5, to illustrate the performance of our proposed framework, in comparison to several direct competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">PCA-like SparLow</head><p>The standard PCA method computes an orthogonal transformation U ∈ St(l, m) such that the variance of the low dimensional representations is maximised, i.e. U is the maximiser of the problem</p><formula xml:id="formula_20">max U ∈St(l,m) tr U ⊤ XH n X ⊤ U ,<label>(16)</label></formula><p>where H n = I n − 1 n 1 n 1 ⊤ n is the centring matrix with 1 n = [1, · · · , 1] ⊤ ∈ R n . In the framework of trace quotient, the denominator can be trivially considered to be tr(U ⊤ B pca U ) with B pca = tr(XH n X ⊤ )I n , which is a constant. By adopting the sparse representations Φ(D, X), we construct straightforwardly </p><formula xml:id="formula_21">A pca (Φ(D, X)) := Φ(D, X)H n (Φ(D, X)) ⊤ ,<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">LLE-like SparLow</head><p>The original LLE method aims to find low dimensional representations of the data via fitting directly the barycentric coordinates of a point based on its neighbours constructed in the original data space, cf. <ref type="bibr" target="#b24">[25]</ref>. It is well known that the low dimensional representations in the LLE method can only be computed implicitly. In order to overcome this drawback, the so-called Orthogonal Neighbourhood Preserving Projections (ONPP) is developed in <ref type="bibr" target="#b21">[22]</ref>, by introducing an explicit orthogonal transformation between the original data and its low dimensional representation.</p><p>Specifically, the ONPP method solves the problem</p><formula xml:id="formula_22">max U ∈St(l,m) tr U ⊤ XM X ⊤ U ,<label>(19)</label></formula><p>where M = (I n −W ) ⊤ (I n −W ) with W ∈ R n×n being the matrix of barycentric coordinates of the data. Similar to the previous subsection, we construct the following functions for an LLE-like SparLow approach, i.e. </p><formula xml:id="formula_23">A lle (Φ(D, X)) := Φ(D, X)M (Φ(D, X)) ⊤ ,<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Laplacian SparLow</head><p>Another category of DR methods are the ones involving a Laplacian matrix of the data. It includes, for example, Locality Preserving Projection (LPP) <ref type="bibr" target="#b16">[17]</ref>, Orthogonal LPP (OLPP) <ref type="bibr" target="#b6">[7]</ref>, Linear Graph Embedding <ref type="bibr" target="#b29">[30]</ref>. Similar to the approaches applied in the previous two subsections, we adapt a simple formulation by setting </p><p>with M := {m ij } ∈ R n×n being a real symmetric matrix measuring the similarity between data pairs (x i , x j ), and</p><formula xml:id="formula_25">B lap (Φ(D, X)) := Φ(D, X)W (Φ(D, X)) ⊤ ,<label>(23)</label></formula><p>with W = {w ij } ∈ R n×n being a diagonal matrix having w ii := j =i m ij for all i = 1, . . . , n. Specifically, the similarity matrix M can be computed by applying a Gaussian kernel function on the distance between two data samples, i.e. m ij = exp(− x i − x j 2 2 /t) or constant weights (m ij = 1 if φ i and φ j are adjacent, m ij = 0 otherwise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluations</head><p>In this section, we investigate the performance of our proposed SparLow methods on real image data. We apply the SparLow methods to firstly learn low dimensional representations of real images, and then evaluate their performance in two unsupervised learning scenarios, namely, (1) the 1NN classification using known class labels, cf. <ref type="bibr" target="#b12">[13]</ref>; and (2) 3D visualisation of disentangling factors learned by applying the SparLow. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>In the following of the paper, we refer to the three Spar-Low methods proposed in Section 4.1, 4.2, and 4.3 as PCA-SparLow, LLE-SparLow and Lap-SparLow, respectively. Similarly, we refer to direct applications of the classic DR methods on sparse representations that are generated with respect to a fixed dictionary as SparLDR. Three members of the SparLDR family are investigated in our experiments, namely, SparPCA, SparOLPP, and SparONPP, as the three corresponding counterparts of the SparLow.</p><p>In our experiments, dictionaries are initialised as a column-wise normalised Gaussian matrix and then improved by employing the K-SVD algorithm <ref type="bibr" target="#b1">[2]</ref>. The learned dictionaries D's are used in the regulariser h, as defined in <ref type="bibr" target="#b11">(12)</ref>. Once an initial dictionary D is given, the orthogonal projection P ∈ Gr(l, r) can be obtained by applying classical DR methods on the sparse representations. However, when the size of the training dataset is huge, di-rectly performing classical DR methods is often prohibitive. In order to overcome this difficulty, we propose to randomly select a relatively small number of samples, and then to employ the classical DR methods on their sparse representations to obtain an estimation of the initial orthogonal projection P 0 ∈ Gr(l, r).</p><p>Throughout all experiments, we consistently set σ = 10 −3 in Eq. <ref type="bibr" target="#b9">(10)</ref>. We treat each image as an m-dimensional vector, and normalise it to one. Let n be the number of all signals which contain c classes, we use n train , n test to denote the number of total training samples and the number of total testing samples, respectively. Usually, we set n = n train + n test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Handwritten digit images</head><p>Our first experiment is performed on the handwritten digits from the MNIST database 1 and the USPS <ref type="bibr" target="#b19">[20]</ref>. The MNIST database consists of 60, 000 handwritten digits images for training and 10, 000 digits images for testing. All images are grayscale between 0 and 1 and have a uniform size of 28 × 28 pixels. The USPS database has 7, 291 training images and 2, 007 testing images of size <ref type="bibr">(16 × 16)</ref>. By vectorising the pixel intensity values of the images, each image is represented as a vector of dimension m = 784 or m = 256 for the MNIST database and the USPS database, respectively.</p><p>In this experiment, the parameters for elastic net are set to be λ 1 = 0.2, λ 2 = 2 × 10 −5 , and µ 1 = 5 × 10 −3 , µ 2 = 2.5 × 10 −4 , for both experiments on the MNIST and USPS datasets. The size of dictionary is chosen to be r = 1000. For CS-PCA <ref type="bibr" target="#b11">[12]</ref>, we employ one common strategy of randomly choosing a certain number of data points as dictionary in a given set of training data, cf. <ref type="bibr" target="#b28">[29]</ref>.</p><p>To demonstrate the effectiveness of the proposed algorithms, experiments of 3D visualisation were conducted on the USPS dataset, compared to the classic DR methods and the SparLDR methods, see <ref type="figure" target="#fig_3">Fig. 1</ref>. It is easily seen that the low dimensional representations captured in the original data space, shown in the first row in <ref type="figure" target="#fig_3">Fig. 1</ref>, are very hard to cluster or group. In particular, the boundary between each pair of digits are completely entangled. Direct applications on the sparse representations for a given dictionary, i.e. the second row in <ref type="figure" target="#fig_3">Fig. 1</ref>, show a significant improvement in disentangling the class information. Furthermore, it is evidentially clear that visualisation powered by the SparLow, i.e. the third row in <ref type="figure" target="#fig_3">Fig. 1</ref>, leads to direct clustering of the handwritten digits. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy: USPS Accuracy: MNIST PCA <ref type="bibr" target="#b11">[12]</ref> 86.36%, l = 50 83.43%, l = 50 OLPP <ref type="bibr" target="#b6">[7]</ref> 84.10% 82.50% ONPP <ref type="bibr" target="#b21">[22]</ref> 87.38% 83.08% KPCA <ref type="bibr" target="#b20">[21]</ref> 89.15%, l = 50 − LLE <ref type="bibr" target="#b24">[25]</ref> 68.80% 66.09% LE <ref type="bibr" target="#b20">[21]</ref> 71.88% 68.16% t−SNE <ref type="bibr" target="#b26">[27]</ref> 77.12% 76.59% ISOMAP <ref type="bibr" target="#b20">[21]</ref> 64.80% 60.51% GTM <ref type="bibr" target="#b4">[5]</ref> 56.92% 60.63% PCA-SparLow 92.18%, l = 50 89.42%, l = 50 Lap-SparLow 91.80% 87.19% LLE-SparLow 90.12% 86.55%</p><p>Let us denote by δ i the i th largest eigenvalue of Φ(D, X)H n (Φ(D, X)) ⊤ , and further define "Ratio of eigenvalues" in <ref type="figure" target="#fig_5">Fig. 4</ref> as t l = l i=1 δ i / r j=1 δ j . <ref type="figure" target="#fig_5">Fig. (4)</ref> shows that our proposed PCA-SparLow significantly increase the ratio t l . It also can be seen, our t l are consistently larger than those of CS-PCA and KPCA, which indicates that the PCA-SparLow method captures more structure information, which preserves power in the l dimensional subspace, cf. <ref type="bibr" target="#b6">[7]</ref>. One obvious benefit of the proposed SparLow model is that the learned low dimensional representations share both reconstructive and discriminative capacities. In this experiment, after applying the SparLow methods on the images from the USPS database, we employ the 1NN method to classify the reduced features. Reconstruction errors in terms of Peak Signal-to-Noise Ratio (PSNR) are presented in <ref type="figure">Fig. 2</ref>(a). <ref type="figure">Fig. 2(b)</ref> shows the box plot of results of applying the 1NN classification ten times on the USPS database with random initialisations. It is clear that the regulariser h, defined in Eq. <ref type="bibr" target="#b11">(12)</ref> has the capability of ensuring good reconstruction, and achieving stable discriminations. Finally, we compare the SparLow methods to several state of the art methods, on the task of 1NN classification. For PCA, KPCA and PCA-SparLow, we set l = 50, for other methods, we set l = 20. For USPS, we use the full training and testing database. For MNIST, we randomly choose 20, 000 images for training, and use standard 10, 000 testing database. According to <ref type="figure">Fig. 3</ref> and <ref type="table" target="#tab_0">Table 1</ref>, it is obvious that the SparLow methods consistently outperform the state of the arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">CMU PIE faces analysis</head><p>In this subsection, we test the SparLow methods on the CMU PIE face database <ref type="bibr" target="#b5">[6]</ref>. The CMU PIE face database contains 68 human subjects with 41, 368 face images. As suggested in <ref type="bibr" target="#b5">[6]</ref>, a subset containing 11, 554 PIE faces are chosen, all of which are manually aligned and cropped, thus we nearly get 170 images for each individual, with the scale 32 × 32 and 256 grey levels per pixel. All experiments are repeated ten times with different randomly selected training and test images, and the average of per-class recognition rates is recorded for each run. In our experiments, we set λ 1 = 10 −2 , λ 2 = 10 −5 , µ 1 = 2.5 × 10 −4 , µ 2 = 5 × 10 −3 .</p><p>First of all, similar to the experiments conducted on the handwritten digits, <ref type="figure">Fig. 5</ref> gives the 3D visualisation of low dimensional representations learned by the SparLow methods and their classical counterparts. It unveils a same message that the SparLow methods can disentangle the class information very clearly. <ref type="figure" target="#fig_6">Fig. 6</ref> illustrates the performance of LDR, SparLDR and SparLow in terms of recognition ac-curacy. It is easily seen that the SparLow methods outperform the state of the art algorithms, such as PCA, OLPP and ONPP. Moreover, visualising the facial features is a common approach to assess the performance of DR methods. In order to facilitate this task, we define the j th disentangling factor υ j as</p><formula xml:id="formula_26">υ j = Du j ∈ R m ,<label>(24)</label></formula><p>with u j being the j th column vector of projection matrix U . This construction is similar to the concept of eigenfaces in <ref type="bibr" target="#b2">[3]</ref>, laplacianfaces in <ref type="bibr" target="#b16">[17]</ref>, orthogonal laplacianfaces in <ref type="bibr" target="#b6">[7]</ref>, and orthogonal LLEfaces in <ref type="bibr" target="#b24">[25]</ref>. <ref type="figure">Fig. 7(b)</ref> gives the first 10 basis vectors of learned disentangling factors for PCA-SparLow, Lap-SparLow and LLE-SparLow. As for comparison, <ref type="figure">Fig. 7</ref>(a) shows the first 10 eigenfaces, laplacianfaces, and LLEfaces. It shows that (i) our learned facial features are more prominent, especially for laplacianfaces and LLE faces, (ii) our learned facial features captures richer information, such as varying pose and expression (e.g. smile).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Outlooks</head><p>In this work, we present an unsupervised low dimensional representation learning approach, coined here as SparLow, which leverages both the sparse representation and the trace quotient criterion. It can be considered as a two-step disentangling mechanism, which applies the trace quotient criterion on the sparse representations. Our proposed generic cost function is defined on a sparsifying dictionary and an orthogonal transformation, which form a product Riemannian manifold. A geometric CG algorithm is developed for optimizing the cost function. Our experimental results depict that in comparison with the state of the art unsupervised representation learnings methods, our proposed SparLow method possesses promising performance in data visualisation and 1NN classification. The proposed SparLow is flexible and can be extended to more general cases of low dimensional representation learning models with orthogonal constraints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and B pca (Φ(D, X)) := tr(Φ(D, X)H n (Φ(D, X)) ⊤ )I r .<ref type="bibr" target="#b17">(18)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and B lle (Φ(D, X)) := tr(Φ(D, X)M (Φ(D, X)) ⊤ )I r . (21)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>A</head><label></label><figDesc>lap (Φ(D, X)) := Φ(D, X)M (Φ(D, X)) ⊤ ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 .</head><label>1</label><figDesc>3D visualization using OLPP, PCA and ONPP on USPS handwritten digits. From top to bottom: Applying OLPP/PCA/ONPP in original space, in sparse space with respect to initial dictionary D, and in sparse space with respect to learned dictionary via SparLow, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Box plot of 1NN classification with or without Regularizations Performing SparLow with or without developed regularizations on USPS database. PCA-SparLow/R denotes PCA-SparLow without regularizations, and in same way to Lap-SparLow/R and LLE-SparLow/R. Comparison of 1NN classification using PCA-SparLow, PCA, KPCA, CS-PCA on MNIST &amp; USPS database. Dictionary size is 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Ratio of top l largest eigenvalues against all eigenvalues in learning process of PCA-SparLow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Face recognition on 68 class PIE faces. The classifier is 1NN. Randomly choose ntrain = 8160 and ntest = 3394.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .Figure 7 .</head><label>57</label><figDesc>3D visualization using OLPP, PCA and ONPP on PIE faces. From top to bottom: Applying OLPP/PCA/ONPP in original space, in sparse space with respect to initial dictionary, and in sparse space with respect to learned dictionary, respectively. Visualisation of facial features. The presented features are generated via Eq.<ref type="bibr" target="#b23">(24)</ref>. From top to bottom: (1) PCA eigenfaces; (2) Laplacianfaces; (3) LLEfaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Classification Performance for the MNIST &amp; USPS datasets of the Proposed SparLow methods, with comparisons to some classical unsupervised DR approaches.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://yann.lecun.com/exdb/mnist/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the German Research Foundation (DFG) through grant number KL 2189/9-1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Optimization Algorithms on Matrix Manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K-Svd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: Recognition using class specific linear projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gtm: The generative topographic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Svensén</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="234" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spectral Regression: A Regression Framework for Efficient Regularized Subspace Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Orthogonal laplacianfaces for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3608" to="3614" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linear dimensionality reduction: Survey, insights, and generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2765" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dimensionality reduction via compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Caetano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1163" to="1170" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-pie. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From manifold to manifold: geometry-aware dimensionality reduction for spd matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cartoon-like image reconstruction via constrained ℓp-minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleinsteuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37 th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>the 37 th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="717" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Separable dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleinsteuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face recognition using laplacianfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Projection metric learning on grassmann manifold with application to video based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Logeuclidean metric learning on symmetric positive definite manifold with application to image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="720" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Trace optimization and eigenproblems in dimension reduction methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kokiopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical Linear Algebra with Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="565" to="602" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Orthogonal neighborhood preserving projections: A projection-based dimensionality reduction technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kokiopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2143" to="2156" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient generalized conjugate gradient algorithms, part 1: Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Storey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Task-driven dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="791" to="804" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structured sparse priors for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1763" to="1776" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1031" to="1044" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: a general framework for dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised translationinvariant sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3517" to="3524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse representation based fisher discrimination dictionary learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
