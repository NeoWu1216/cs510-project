<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tensor Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Tensors via Convex Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canyi</forename><surname>Lu</surname></persName>
							<email>canyilu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Chen</surname></persName>
							<email>yudong.chen@cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Operations Research and Information Engineering</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<email>wliu@ee.columbia.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Didi Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<email>zlin@pku.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">360 AI Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tensor Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Tensors via Convex Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies the Tensor Robust Principal Component (TRPCA) problem which extends the known Robust P-CA [4] to the tensor case. Our model is based on a new tensor Singular Value Decomposition (t-SVD) [14] and its induced tensor tubal rank and tensor nuclear norm. Consider that we have a 3-way tensor X ∈ R n1×n2×n3 such that X = L 0 + S 0 , where L 0 has low tubal rank and S 0 is sparse. Is that possible to recover both components? In this work, we prove that under certain suitable assumptions, we can recover both the low-rank and the sparse components exactly by simply solving a convex program whose objective is a weighted combination of the tensor nuclear norm and the ℓ 1 -norm, i.e.,</p><p>where λ =1 / max(n 1 ,n 2 )n 3 . Interestingly, TRPCA involves RPCA as a special case when n 3 =1and thus it is a simple and elegant tensor extension of RPCA. Also numerical experiments verify our theory and the application for the image denoising demonstrates the effectiveness of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of exploiting low-dimensional structure in high-dimensional data is taking on increasing importance in image, text and video processing, and web search, where the observed data lie in very high dimensional spaces. The classical Principal Component Analysis (PCA) <ref type="bibr" target="#b11">[12]</ref> is the most widely used statistical tool for data analysis and dimensionality reduction. It is computationally efficient and * Corresponding author. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tensor of corrupted observations</head><p>Underlying low-rank tensor Sparse error tensor <ref type="figure">Figure 1</ref>: Illustration of the low-rank and sparse tensor decomposition from noisy observations. powerful for the data which are mildly corrupted by small noises. However, a major issue of PCA is that it is brittle to grossly corrupted or outlying observations, which are ubiquitous in real world data. To date, a number of robust versions of PCA were proposed. But many of them suffer from the high computational cost. The recent proposed Robust PCA <ref type="bibr" target="#b3">[4]</ref> is the first polynomial-time algorithm with strong performance guarantees. Suppose we are given a data matrix X ∈ R n1×n2 , which can be decomposed as X = L 0 + E 0 , where L 0 is low-rank and E 0 is sparse. It is shown in <ref type="bibr" target="#b3">[4]</ref> that if the singular vectors of L 0 satisfy some incoherent conditions, L 0 is low-rank and S 0 is sufficiently sparse, then L 0 and S 0 can be recovered with high probability by solving the following convex problem</p><formula xml:id="formula_0">min L,E L * + λ E 1 , s.t. X = L + E,<label>(1)</label></formula><p>where L * denotes the nuclear norm (sum of the singular values of L), E 1 denotes the ℓ 1 -norm (sum of the absolute values of all the entries in E) and λ = 1/ max(n 1 ,n 2 ). RPCA and its extensions have been successfully applied for background modeling <ref type="bibr" target="#b3">[4]</ref>, video restoration <ref type="bibr" target="#b10">[11]</ref>, image alignment <ref type="bibr" target="#b21">[22]</ref>, et al.</p><p>One major shortcoming of RPCA is that it can only handle 2-way (matrix) data. However, the real world data are ubiquitously in multi-dimensional way, also referred to as tensor. For example, a color image is a 3-way object with column, row and color modes; a greyscale video is indexed by two spatial variables and one temporal variable. To use RPCA, one has to first restructure/transform the multi-way data into a matrix. Such a preprocessing usually leads to the information loss and would cause performance degradation. To alleviate this issue, a common approach is to manipulate the tensor data by taking the advantage of its multi-dimensional structure.</p><p>In this work, we study the Tensor Robust Principal Component (TRPCA) which aims to exactly recover a low-rank tensor corrupted by sparse errors, see <ref type="figure">Figure 1</ref> for an illustration. More specifically, suppose we are given a data tensor X , and know that it can be decomposed as</p><formula xml:id="formula_1">X = L 0 + E 0 ,<label>(2)</label></formula><p>where L 0 is low-rank and E 0 is sparse; here, both components are of arbitrary magnitude. Note that we do not know the locations of the nonzero elements of E 0 , not even how many there are. Now we consider a similar problem as R-PCA. Can we recover the low-rank and sparse components exactly and efficiently from X ? It is expected to extend the tools and analysis from the low-rank matrix recovery to the tensor case. However, this is not easy since the numerical algebra of tensors is fraught with hardness results <ref type="bibr" target="#b8">[9]</ref>. The main issue for low-rank tensor estimation is the definition of tensor rank. Different from the matrix rank which enjoys several "good" properties, the tensor rank is not very well defined. Several different definitions of tensor rank have been proposed but each has its limitation. For example, the CP rank <ref type="bibr" target="#b14">[15]</ref>, defined as the smallest number of rank one tensor decomposition, is generally NP-hard to compute. Also its convex relaxation is intractable. Thus, the low CP rank tensor recovery is challenging. Another direction, which is more popular, is to use the tractable Tucker rank <ref type="bibr" target="#b14">[15]</ref> and its convex relaxation. For a k-way tensor X , the Tucker rank is a vector defined as rank tc (X ): = rank X <ref type="bibr" target="#b0">(1)</ref> , rank X <ref type="bibr" target="#b1">(2)</ref> , ··· , rank X (k) , where X (i) is the mode-i matricization of X . The Tucker rank is based on the matrix rank and thus computable. Motivated from the fact that the nuclear norm is the convex envelop of the matrix rank within the unit ball of the spectral norm, the Sum of Nuclear Norms (SNN) <ref type="bibr" target="#b15">[16]</ref>, defined as i X (i) * , is used as a convex surrogate of the Tucker rank. The effectiveness of this approach has been well studied in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref>. However, SNN is not a tight convex relaxation of the Tucker rank <ref type="bibr" target="#b22">[23]</ref>. The work <ref type="bibr" target="#b20">[21]</ref> considers the low-rank tensor completion problem based on SNN,</p><formula xml:id="formula_2">min X k i=1 X (i) * , s.t. P Ω (X )=P Ω (X 0 ),<label>(3)</label></formula><p>where P Ω (X 0 ) is an incomplete tensor with observed entries on the support Ω. It is shown in <ref type="bibr" target="#b20">[21]</ref> that the above model can be substantially suboptimal: reliably recovering a k-way tensor of length n and Tucker rank (r, r, ··· ,r) from Gaussian measurements requires O(rn k−1 ) observations. In contrast, a certain (intractable) nonconvex formulation needs only O(rK + nrK) observations. The work <ref type="bibr" target="#b20">[21]</ref> further proposes a better convexification based on a more balanced matricization of X and improves the bound to O(r ⌊ k 2 ⌋ n ⌊ k 2 ⌋ ). It may be better than (3) for small r and k ≥ 4. But it is still far from optimal compared with the nonconvex model. Another work <ref type="bibr" target="#b9">[10]</ref> proposes an SNN based tensor RPCA model</p><formula xml:id="formula_3">min L,E k i=1 λ i L (i) * + E 1 + τ 2 L 2 F + τ 2 E 2 F s.t. X = L + E, X ∈ R n1×n2×···×n k ,<label>(4)</label></formula><p>where E 1 is the sum of the absolute values of all entries in E, and gives the first exact recovery guarantee under certain tensor incoherence conditions. More recently, the work <ref type="bibr" target="#b27">[28]</ref> proposes the tensor tubal rank based on a new tensor decomposition scheme in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>, which is referred as tensor SVD (t-SVD). The t-SVD is based on a new definition of tensor-tensor product which enjoys many similar properties as the matrix case. Based on the computable t-SVD, the tensor nuclear norm <ref type="bibr" target="#b23">[24]</ref> is used to replace the tubal rank for low-rank tensor recovery (from incomplete/corrupted tensors) by solving the following convex program,</p><formula xml:id="formula_4">min X X * , s.t. P Ω (X )=P Ω (X 0 ),<label>(5)</label></formula><p>where X * denotes the tensor nuclear norm (see Section 2 for the definition).</p><p>In this work, we study the TRPCA problem which aims to recover the low tubal rank component L 0 and sparse component E 0 from X = L 0 + E 0 ∈ R n1×n2×n3 (this work focuses on the 3-way tensor) by convex optimization</p><formula xml:id="formula_5">min L,E L * + λ E 1 , s.t. X = L + E.<label>(6)</label></formula><p>We prove that under certain incoherence conditions, the solution to the above problem perfectly recovers the low-rank and the sparse components, provided, of course that the tubal rank of L 0 is not too large, and that E 0 is reasonably sparse. A remarkable fact as that in TRPCA is that <ref type="bibr" target="#b5">(6)</ref> has no tunning parameter either. Our analysis shows that λ =1 / max(n 1 ,n 2 )n 3 guarantees the exact recovery no matter what L 0 and E 0 are. Actually, as will be seen later, if n 3 =1(X is a matrix in this case), our TRPCA in <ref type="bibr" target="#b5">(6)</ref> reduces to RPCA in <ref type="bibr" target="#b0">(1)</ref>, and also our recovery guarantee in Theorem 3.1 reduces to Theorem 1.1 in <ref type="bibr" target="#b3">[4]</ref>. Another advantage of (6) is that it can be solved by polynomial-time algorithms, e.g., the standard Alternating Direction Method of Multipliers (ADMM) <ref type="bibr" target="#b0">[1]</ref>. The rest of this paper is organized as follows. Section 2 introduces some notations and preliminaries of tensors, where we define several algebraic structures of 3-way tensors. In Section 3, we will describe the main results of TRP-CA and highlight some key differences from previous work. In Section 4, we conduct some experiments to verify our results in theory and apply TRPCA for image denoising. The last section gives concluding remarks and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Notations and Preliminaries</head><p>In this section, we introduce some notations and give the basic definitions used in the rest of the paper.</p><p>Throughout this paper, we denote tensors by boldface Euler script letters, e.g., A. Matrices are denoted by boldface capital letters, e.g., A; vectors are denoted by boldface lowercase letters, e.g., a, and scalars are denoted by lowercase letters, e.g., a. We denote I n as the n × n sized identity matrix. The filed of real number and complex number are denoted as R and C, respectively. For a 3-way tensor A ∈ C n1×n2×n3 , we denote its (i, j, k)-th entry as A ijk or a ijk and use the Matlab notation A(i, :, :), A(:,i,:) and A(:, :,i) to denote respectively the i-th horizontal, lateral and frontal slice. More often, the frontal slice A(:, :,i) is denoted compactly as A (i) . The tube is denoted as A(i, j, :). The inner product of A and B in C n1×n2 is defined as A, B = Tr(A * B), where A * denotes the conjugate transpose of A and Tr(·) denotes the matrix trace. The inner product of A and B in C n1×n2×n3 is defined as</p><formula xml:id="formula_6">A, B = n3 i=1 A (i) , B (i) .</formula><p>Some norms of vector, matrix and tensor are used. We denote the ℓ 1 -norm as A 1 = ijk |a ijk |, the infinity norm as A ∞ =m a x ijk |a ijk | and the Frobenius nor-</p><formula xml:id="formula_7">ma s A F = ijk |a ijk | 2 .</formula><p>The above norms reduce to the vector or matrix norms if A is a vector or a matrix.</p><p>For</p><formula xml:id="formula_8">v ∈ C n , the ℓ 2 -norm is v 2 = i |v i | 2 . The spectral norm of a matrix A ∈ C n1×n2 is denoted as A =max i σ i (A), where σ i (A)'s are the singular values of A. The matrix nuclear norm is A * = i σ i (A).</formula><p>For A ∈ R n1×n2×n3 , by using the Matlab command fft, we denoteĀ as the result of discrete Fourier transformation of A along the 3-rd dimension, i.e.,Ā = fft(A, [], 3). In the same fashion, one can also compute A fromĀ via ifft(Ā, [], 3) using the inverse FFT. In particular, we denoteĀ as a block diagonal matrix with each block on diagonal as the frontal sliceĀ (i) ofĀ, i.e.,</p><formula xml:id="formula_9">A = bdiag(Ā)= ⎡ ⎢ ⎢ ⎢ ⎣Ā (1)Ā (2) . . .Ā (n3) ⎤ ⎥ ⎥ ⎥ ⎦ .</formula><p>The new tensor-tensor product <ref type="bibr" target="#b13">[14]</ref> is defined based on an important concept, block circulant matrix, which can be regarded as a new matricization of a tensor. For A ∈ R n1×n2×n3 , its block circulant matrix has size n 1 n 3 ×n 2 n 3 , i.e.,</p><formula xml:id="formula_10">bcirc(A)= ⎡ ⎢ ⎢ ⎢ ⎣ A (1) A (n3) ··· A (2) A (2) A (1) ··· A (3) . . . . . . . . . . . . A (n3) A (n3−1) ··· A (1) ⎤ ⎥ ⎥ ⎥ ⎦ .</formula><p>We also define the following operator</p><formula xml:id="formula_11">unfold(A)= ⎡ ⎢ ⎢ ⎢ ⎣ A (1) A (2) . . . A (n3) ⎤ ⎥ ⎥ ⎥ ⎦ , fold(unfold(A)) = A.</formula><p>Then t-product between two 3-way tensors can be defined as follows.</p><formula xml:id="formula_12">Definition 2.1 (t-product) [14] Let A ∈ R n1×n2×n3 and B ∈ R n2×l×n3 . Then the t-product A * B is defined to be a tensor of size n 1 × l × n 3 , A * B = fold(bcirc(A) · unfold(B)).<label>(7)</label></formula><p>Note that a 3-way tensor of size n 1 ×n 2 ×n 3 can be regarded as an n 1 ×n 2 matrix with each entry as a tube lies in the third dimension. Thus, the t-product is analogous to the matrixmatrix product except that the circular convolution replaces the product operation between the elements. Note that the tproduct reduces to the standard matrix-matrix product when n 3 =1. This is a key observation which makes our TRPCA involve RPCA as a special case.</p><p>Definition 2.2 (Conjugate transpose) <ref type="bibr" target="#b13">[14]</ref> The conjugate transpose of a tensor A of size n 1 ×n 2 ×n 3 is the n 2 ×n 1 × n 3 tensor A * obtained by conjugate transposing each of the frontal slice and then reversing the order of transposed frontal slices 2 through n 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.3 (Identity tensor) [14]</head><p>The identity tensor I ∈ R n×n×n3 is the tensor whose first frontal slice is the n × n identity matrix, and whose other frontal slices are all zeros.</p><formula xml:id="formula_13">Definition 2.4 (Orthogonal tensor) [14] A tensor Q ∈ R n×n×n3 is orthogonal if it satisfies Q * * Q = Q * Q * = I.<label>(8)</label></formula><p>Definition 2.5 (F-diagonal Tensor) <ref type="bibr" target="#b13">[14]</ref> A tensor is called f-diagonal if each of its frontal slices is a diagonal matrix. .</p><formula xml:id="formula_14">Theorem 2.1 (T-SVD) [14] Let A ∈ R n1×n2×n3 .</formula><p>Then it can be factored as</p><formula xml:id="formula_15">A = U * S * V * ,<label>(9)</label></formula><p>where U ∈ R n1×n1×n3 , V ∈ R n2×n2×n3 are orthogonal, and S ∈ R n1×n2×n3 is a f-diagonal tensor. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the t-SVD factorization. Note that t-SVD can be efficiently computed based on the matrix SVD in the Fourier domain. This is based on a key property that the block circulant matrix can be mapped to a block diagonal matrix in the Fourier domain, i.e.,</p><formula xml:id="formula_16">(F n3 ⊗ I n1 ) · bcirc(A) · (F −1 n3 ⊗ I n2 )=Ā,<label>(10)</label></formula><p>where Definition 2.6 (Tensor multi rank and tubal rank) <ref type="bibr" target="#b27">[28]</ref> The tensor multi rank of A ∈ R n1×n2×n3 is a vector r ∈ R n3 with its i-th entry as the rank of the i-th frontal slice of A, i.e., r i = rank Ā (i) . The tensor tubal rank, denoted as rank t (A), is defined as the number of nonzero singular tubes of S, where S is from the t-SVD of A = U * S * V * . That is</p><formula xml:id="formula_17">rank t (A)=#{i : S(i, i, :) = 0} =max i r i .<label>(11)</label></formula><p>The tensor tubal rank has some interesting properties as the matrix rank, e.g., for A ∈ R n1×n2×n3 , rank t (A) ≤ min (n 1 ,n 2 ), and rank t (A * B) ≤ min(rank t (A), rank t (B)).</p><p>Definition 2.7 (Tensor nuclear norm) The tensor nuclear norm of a tensor A ∈ R n1×n2×n3 , denoted as A * ,i s defined as the average of the nuclear norm of all the frontal slices ofĀ, i.e., A * :</p><formula xml:id="formula_18">= 1 n3 n3 i=1 Ā (i) * .</formula><p>With the factor 1/n 3 , our tensor nuclear norm definition is different from previous work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. Note that this is important for our model and analysis in theory. The above tensor nuclear norm is defined in the Fourier domain. It is closely related to the nuclear norm of the block circulant matrix in the original domain. Indeed,</p><formula xml:id="formula_19">A * = 1 n 3 n3 i=1 Ā (i) * = 1 n 3 Ā * = 1 n 3 (F n3 ⊗ I n1 ) · bcirc(A) · (F −1 n3 ⊗ I n2 ) * (12) = 1 n 3 bcirc(A) * .</formula><p>The above relationship gives an equivalent definition of the tensor nuclear norm in the original domain. So the tensor nuclear norm is the nuclear norm (with a factor 1/n 3 ) of a new matricization (block circulant matrix) of a tensor. Compared with previous matricizations along certain dimension, the block circulant matricization may preserve more spacial relationship among entries.</p><p>Definition 2.8 (Tensor spectral norm) The tensor spectral norm of A ∈ R n1×n2×n3 , denoted as A , is defined as A := Ā .</p><p>If we further define the tensor average rank as rank a (A)= 1 n3 n3 i=1 rank(Ā), then it can be proved that the tensor nuclear norm is the convex envelop of the tensor average rank within the unit ball of the tensor spectral norm. Definition 2.9 (Standard tensor basis) <ref type="bibr" target="#b26">[27]</ref> The column basis, denoted ase i , is a tensor of size n × 1 × n 3 with its (i, 1, 1)-th entry equaling to 1 and the rest equaling to 0. Naturally its transposee * i is called row basis. The tube basis, denoted asė k , is a tensor of size 1 × 1 × n 3 with its (1, 1,k)-entry equaling to 1 and the rest equaling to 0.</p><p>For simplicity, denote e ijk =e i * ė k * e * j . Then for any A ∈ R n1×n2×n3 ,w eh a v eA = ijk e ijk , A e ijk = ijk a ijk e ijk .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tensor RPCA and Our Results</head><p>As in low-rank matrix recovery problems, some incoherence conditions need to be met if recovery is to be possible for tensor-based problems. Hence, in this section, we first introduce some incoherence conditions of the tensor L 0 extended from <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref>. Then we present the recovery guarantee of TRPCA (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tensor Incoherence Conditions</head><p>As observed in RPCA <ref type="bibr" target="#b3">[4]</ref>, the exact recovery is impossible in some cases. TRPCA suffers from a similar issue.</p><p>For example, suppose X =e 1 * ė 1 * e * 1 (x ijk =1when i = j = k =1and zeros everywhere else). Then X is both low-rank and sparse. We are not able to identify the lowrank component and the sparse component in this case. To avoid such pathological situations, we need to assume that the low-rank component L 0 is not sparse. To this end, we assume L 0 to satisfy some incoherence conditions. Definition 3.1 (Tensor Incoherence Conditions) Fo r L 0 ∈ R n1×n2×n3 , assume that rank t (L 0 )=r and it has the skinny t-SVD L 0 = U * S * V * , where U ∈ R n1×r×n3 and V ∈ R n2×r×n3 satisfy U * * U = I and V * * V = I, and S ∈ R r×r×n3 is a f-diagonal tensor. Then L 0 is said to satisfy the tensor incoherence conditions with parameter μ if</p><formula xml:id="formula_20">max i=1,··· ,n1 U * * e i F ≤ μr n 1 n 3 ,<label>(13)</label></formula><formula xml:id="formula_21">max j=1,··· ,n2 V * * e j F ≤ μr n 2 n 3 ,<label>(14)</label></formula><p>and U * V * ∞ ≤ μr n 1 n 2 n 2 3 .</p><p>(</p><p>As discussed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, the incoherence condition guarantees that for small values of μ, the singular vectors are reasonably spread out, or not sparse. As observed in <ref type="bibr" target="#b4">[5]</ref>, the joint incoherence condition is not necessary for low-rank matrix completion. However, for RPCA, it is unavoidable for polynomial-time algorithms. In our proofs, the joint incoherence (15) condition is necessary. Another identifiability issue arises if the sparse tensor has low tubal rank. This can be avoided by assuming that the support of S 0 is uniformly distributed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Main Results</head><p>Now we show that, the convex program (6) is able to perfectly recover the low-rank and sparse components. We define n (1) = max(n 1 ,n 2 ) and n (2) =min(n 1 ,n 2 ). Theorem 3.1 Suppose L 0 ∈ R n×n×n3 obeys (13)- <ref type="bibr" target="#b14">(15)</ref>. Fix any n × n × n 3 tensor M of signs. Suppose that the support set Ω of S 0 is uniformly distributed among all sets of cardinality m, and that sgn (</p><formula xml:id="formula_23">[S 0 ] ijk )=[ M] ijk for all (i, j, k) ∈ Ω.</formula><p>Then, there exist universal constants c 1 ,c 2 &gt; 0 such that with probability at least 1 − c 1 n −c2 (over the choice of support of S 0 ), {L 0 , S 0 } is the unique minimizer to (6) with λ =1/ √ nn 3 , provided that rank t (L 0 ) ≤ ρ r n μ(log(nn 3 )) 2 and m ≤ ρ s n 2 n 3 ,</p><p>where ρ r and ρ s are positive constants. If L 0 ∈ R n1×n2×n3 has rectangular frontal slices, TRPCA with λ =1/ √ n (1) n 3 succeeds with probability at least 1 − c 1 n −c2 <ref type="bibr" target="#b0">(1)</ref> , provided that rank t (L 0 ) ≤ ρrn <ref type="bibr" target="#b1">(2)</ref> μ(log(n (1) n3)) 2 and m ≤ ρ s n 1 n 2 n 3 .</p><p>The above result shows that for incoherent L 0 , the perfect recovery is guaranteed with high probability for rank t (L 0 ) on the order of n/(μ(log nn 3 ) 2 ) and a number of nonzero entries in S 0 on the order of n 2 n 3 .F o rS 0 , we make only an assumption on the random location distribution, but no assumption about the magnitudes or signs of the nonzero entries. Also TRPCA is parameter free. Moreover, since the t-product of 3-way tensors reduces to the standard matrixmatrix product when the third dimension is 1, the tensor nuclear norm reduces to the matrix nuclear norm. Thus, RPCA is a special case of TRPCA and the guarantee of R-PCA in Theorem 1.1 in <ref type="bibr" target="#b3">[4]</ref> is a special case of our Theorem 3.1. Both our model and theoretical guarantee are consistent with RPCA. Compared with the SNN model <ref type="bibr" target="#b9">[10]</ref>, our tensor extension of RPCA is much more simple and elegant.</p><p>It is worth mentioning that this work focuses on the analysis for 3-way tensors. But it may not be difficult to generalize our model in <ref type="bibr" target="#b5">(6)</ref> and results in Theorem 3.1 to the case of order-p (p ≥ 3) tensors, by using the t-SVD for order-p tensors in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization by ADMM</head><p>ADMM is the most widely used solver for RPCA and its related problems. The work <ref type="bibr" target="#b27">[28]</ref> also uses ADMM to solve a similar problem as <ref type="bibr" target="#b5">(6)</ref>. In this work, we also use AD-MM to solve <ref type="bibr" target="#b5">(6)</ref> and give the details here since the setting of some parameters are different but critical in the experiments. See Algorithm 1 for the optimization details. Note that both the updates of L k+1 and S k+1 have closed form solutions <ref type="bibr" target="#b27">[28]</ref>. It is easy to see that the main per-iteration cost lies in the update of L k+1 , which requires computing FFT and n 3 SVDs of n 1 × n 2 matrices. Thus the periteration complexity is O n 1 n 2 n 3 log(n 3 )+n (1) n 2 (2) n 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct numerical experiments to corroborate our main results. We first investigate the ability of TRPCA for recovering tensors of various tubal rank from noises of various sparsity. We then apply TRPCA for image denoising. Note that the choice of λ in <ref type="formula" target="#formula_5">(6)</ref> is critical for the recovery performance. To verify the correctness of our main results, we set λ =1 / √ n (1) n 3 in all the experiments. But note that it is possible to further improve the performance by tuning λ more carefully. The suggested value in theory provides a good guide in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Exact Recovery from Varying Fractions of Error</head><p>We first verify the correct recovery guarantee in Theorem 3.1 on random data with different fractions of error. For simplicity, we consider the tensors of size n × n × n, with varying dimension n =100, 200 and 300. We gen-Algorithm 1 Solve (6) by ADMM Input: tensor data X , parameter λ.</p><formula xml:id="formula_25">Initialize: L 0 = S 0 = Y 0 = 0, ρ =1 .1, μ 0 =1 e−3, μ max =1e10, =1e−8. while not converged do 1. Update L k+1 by L k+1 =argmin L L * + μ k 2 L + E k − X + Y k μ k 2 F ; 2. Update E k+1 by E k+1 =argmin E λ E 1 + μ k 2 L k+1 + E − X + Y k μ k 2 F ; 3. Y k+1 = Y k + μ k (L k+1 + E k+1 − X );</formula><p>4. Update μ k+1 by μ k+1 =min(ρμ k ,μ max );</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Check the convergence conditions</head><formula xml:id="formula_26">L k+1 − L k ∞ ≤ , E k+1 − E k ∞ ≤ , L k+1 + E k+1 − X ∞ ≤ .</formula><p>end while erate a rank t -r tensor L 0 = P * Q, where the entries of P ∈ R n×r×n and Q ∈ R r×n×n are independently sampled from an N (0, 1/n) distribution. The support set Ω (with size m)ofS 0 is chosen uniformly at random. For all</p><formula xml:id="formula_27">(i, j, k) ∈ Ω, let [S 0 ] ijk =[ M] ijk ,</formula><p>where M is a tensor with independent Bernoulli ±1 entries. We test on two settings. <ref type="table" target="#tab_1">Table 1</ref> (top) gives the results of the first scenario with setting r = rank t (L 0 )=0.1n and m = S 0 0 =0.1n 3 . <ref type="table" target="#tab_1">Table 1</ref> (bottom) gives the results for a more challenging scenario with r = rank t (L 0 )=0 .1n and m = S 0 0 =0 .2n 3 . It can be seen that our convex program <ref type="bibr" target="#b5">(6)</ref> gives the correct rank estimation of L 0 in all cases and also the relative errors L − L 0 F / L 0 F are small, less than 10 −5 . The sparsity estimation of S 0 is not exact as the rank estimation. The reason may be that the sparsity of errors is much larger than the rank of L 0 . But note that the relative errors Ŝ − S 0 F / S 0 F are all small, less than 10 −8 (actually much smaller than the relative errors of the recovered low-rank component). These results well verify the correct recovery phenomenon as claimed in Theorem 3.1 for (6) with the chosen λ in theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Phase Transition in Rank and Sparsity</head><p>The results in Theorem 3.1 shows the perfect recovery for incoherent tensor with rank t (L 0 ) on the order of n/(μ(log nn 3 ) 2 ) and the sparsity of S 0 on the order of    n 2 n 3 . Now we exam the recovery phenomenon with varying rank of L 0 and varying sparsity of S 0 . We consider two sizes of L 0 ∈ R n×n×n3 : (1) n = 100, n 3 =5 0 ; (2) n = 100, n 3 =5 0 . We generate L 0 = P * Q, where the entries of P ∈ R n×r×n and Q ∈ R r×n×n are independently sampled from an N (0, 1/n) distribution. For S 0 ,we consider a Bernoulli model for its support and random signs for its values:</p><formula xml:id="formula_28">r = rankt(L0)=0 .1n, m = S0 0 =0 .1n 3 n r m rankt(L) Ŝ 0 L −L 0 F L 0 F Ŝ −S 0 F S</formula><formula xml:id="formula_29">r = rankt(L0)=0 .1n, m = S0 0 =0 .2n 3 n r m rankt(L) Ŝ 0 L −L 0 F L 0 F Ŝ −S 0 F S 0</formula><formula xml:id="formula_30">(a) n1 = n2 =100, n3 =50 (b) n1 = n2 = 200, n3 =50</formula><formula xml:id="formula_31">[S 0 ] ijk = ⎧ ⎪ ⎨ ⎪ ⎩ 1, w.p. ρ s /2, 0, w.p. 1 − ρ s , −1, w.p. ρ s /2.<label>(17)</label></formula><p>We set r/n as all the choices in [0.01 : 0.01 : 0.5], and ρ s in [0.01 : 0.01 : 0.5]. For each (r, ρ s )-pair, we simulate 10 test instances and declare a trial to be successful if the recoveredL satisfies L − L 0 F / L 0 F ≤ 10 −3 . <ref type="figure" target="#fig_0">Figure  2</ref> plots the fraction of correct recovery for each pair (black = 0% and white = 100%). It can be seen that there is a large region in which the recovery is correct. These results are quite similar as that in RPCA, see <ref type="figure">Figure 1</ref> (a) in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">TRPCA for Image Recovery</head><p>We apply TRPCA for image recovery from the corrupted images with random noises. We will show that the recovery performance is still satisfied with the choice of λ =1/ √ n (1) n 3 on real data. We conduct two experiments. The first one is to recover face images (of the same person) with random noises as that in <ref type="bibr" target="#b7">[8]</ref>. Assume that we are given n 3 gray face images of size h × w. Then we can construct a 3-way tensor X ∈ R h×w×n3 , where each frontal slice is a face image 1 . An extreme situation is that all these n 3 face images are all the same. Then the tubal rank of X will be 1, which is very low. However, the real face images often violate such lowrank tensor assumption (the same observation for low-rank matrix assumption when the images are vectorized), due to different noises. <ref type="figure" target="#fig_3">Figure 4</ref> (a) shows an example image taken from the Extended Yale B database <ref type="bibr" target="#b6">[7]</ref>. Each subject of this dataset has 64 images, and each has resolution 192 × 168. We simply select 32 images with different illuminations per subject, and construct a 3-way tensor X ∈ R 192×168×32 . Then, for each image, we randomly select a fraction of pixels to be corrupted with random pixel values. Then we solve TRPCA with λ =1 / √ n (1) n 3 to recover the face images.</p><formula xml:id="formula_32">Figure (4) (b)-(d)</formula><p>shows the recovered images from different proportions of corruption. It can be seen that it successfully removes the random noises. This also verifies the effectiveness of our choice of λ in theory. The second experiment is to apply TRPCA for image denoising. Different from the above face recovery problem which has many samples of a same person, this experiment is tested on the color image with one sample of 3 channels. A color image of size n 1 × n 2 , is a 3-way tensor X ∈ R n1×n2×3 in nature. Each frontal slice of X is corresponding to a channel of the color image. Actually, each channel of a color image may not be of low-rank. But it is observed that their top singular values dominate the main information. Thus, the image can be approximately recovered by a low-rank matrix <ref type="bibr" target="#b16">[17]</ref>. When regarding a color image as a tensor, it can be also well reconstructed by a low tubal rank tensor. The reason is that t-SVD is capable for compression as SVD, see Theorem 4.3 in <ref type="bibr" target="#b13">[14]</ref>. So we can apply TRPCA for image denoising. We compare our method with RPCA and the SNN model (4) <ref type="bibr" target="#b9">[10]</ref> which also own the theoretical guarantee.</p><p>We randomly select 50 color images from the Berkeley Segmentation Dataset <ref type="bibr" target="#b19">[20]</ref> for the test. For each image, we randomly set 10% of pixels to random values in [0, 255]. Note that this is a challenging problem since at most there are 30% of pixels corrupted and the positions of the corrupted pixels are unknown. We use our TRPCA with λ =1 / √ n (1) n 3 . For RPCA (1), we apply it on each channel independently with λ =1 / √ n <ref type="bibr" target="#b0">(1)</ref> . For SNN (4), we find that its performance is very bad when λ i 's are set to the values suggested in theory <ref type="bibr" target="#b9">[10]</ref>. We empirically set λ 1 = λ 2 =1 5and λ 3 =1 .5 which make SNN perform well in most cases. For the recovered image, we evaluate its quality by the Peak Signal-to-Noise Ratio (PSNR) value. The higher PSNR value indicates better recovery performance. <ref type="figure" target="#fig_4">Figure 5</ref> shows the PSNR values of the compared methods on all 50 images. Some examples are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. It can be seen that our TRPCA obtains the best recovery performance. The two tensor methods, TRPCA and SNN, also perform much better than RPCA. The reason is that RPCA, which performs the matrix recovery on each channel independently, is not able to use the information across channels, while the tensor methods improve the performance by taking the advantage of the multi-dimensional structure of data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this work, we study the Tensor Robust Principal Component (TRPCA) problem which aims to recover a low tubal rank tensor and a sparse tensor from their sum. We show that the exact recovery can be obtained by solving a tractable convex program which does not have any free parameter. We establish a theoretical bound for the exact recovery as RPCA. Benefit from the "good" property of t-SVD, both our model and theoretical guarantee are natural extension of RPCA. Numerical experiments verify our theory and the applications for image denoising shows its superiority over previous methods.</p><p>This work verifies the remarkable ability of convex optimizations for low-rank tensors and sparse errors recovery. This suggests to use these tools of tensor analysis for other applications, e.g., image/video processing, web data analysis, and bioinformatics. Also, consider that the real data usually are of high dimension, the computational cost will be high. Thus developing the fast solver for low-rank tensor analysis is an important direction. It is also interesting to consider nonconvex low-rank tensor models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the t-SVD of an n 1 ×n 2 ×n 3 tensor<ref type="bibr" target="#b12">[13]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>F n3 denotes the n 3 × n 3 Discrete Fourier Transform (DFT) matrix and ⊗ denotes the Kronecker product. Then one can perform the matrix SVD on each frontal slice ofĀ, i.e.,Ā (i) =Ū (i)S(i)V (i) * , whereŪ (i) ,S (i) andV (i) are frontal slices ofŪ ,S andV, respectively. Or equivalently, A =ŪSV * . After performing ifft along the 3-rd dimension, we obtain U = ifft(Ū , [], 3), S = ifft(S, [], 3), and V = ifft(V, [], 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Correct recovery for varying rank and sparsity. Fraction of correct recoveries across 10 trials, as a function of rankt(L0) (xaxis) and sparsity of S0 (y-axis). The experiments are test on two different sizes of L0 ∈ R n 1 ×n 2 ×n 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Removing random noises from face images. (a) Original image; (b)-(d): Top: noisy images with 10%, 15% and 20% pixels corrupted; Middle: the low-rank component recovered by TRPCA; Bottom: the sparse component recovered by TRPCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of the PSNR values of RPCA, SNN and TRPCA for image denoising on 50 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of Image recovery. (a) Original image; (b) Noisy image; (c)-(e) Recovered images by RPCA, SNN and TRPCA, respectively. Best viewed in ×2 sized color pdf file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Correct recovery for random problems of varying size.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also test TRPCA based on different ways of tensor data construction and find that the results are similar.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trend-s® in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Third-order tensors as linear operators on a space of matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Braman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1241" to="1253" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incoherence-optimal matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2909" to="2923" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tensor completion and low-n-rank tensor recovery via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gandy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25010</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust low-rank tensor recovery: Models and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="225" to="253" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Most tensor problems are NPhard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hillar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Provable lowrank tensor recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization-Online</title>
		<imprint>
			<biblScope unit="volume">4252</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust video restoration by joint sparse and low rank matrix approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1122" to="1142" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<editor>Wiley Online Library</editor>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Third-order tensors as operators on matrices: A theoretical and computational framework with applications in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Braman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hoover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="148" to="172" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Factorization strategies for third-order tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="641" to="658" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tensor completion for estimating missing values in visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Musialski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="220" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized nonconvex nonsmooth low-rank minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4130" to="4137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalized singular value thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An order-p tensor factorization with applications in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Larue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="474" to="490" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Square deal: Lower bounds and improved relaxations for tensor recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.5870</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2233" to="2246" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new convex relaxation for tensor completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2967" to="2975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tensorbased formulation and nuclear norm regularization for multienergy computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Semerci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1678" to="1693" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning with tensors: a framework based on convex optimization and spectral regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Signoretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">De</forename><surname>Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="351" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Estimation of low-rank tensors via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1010.0789</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aeron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04689</idno>
		<title level="m">Exact tensor completion using t-SVD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Novel methods for multilinear data completion and de-noising based on tensor-SVD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3842" to="3849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
