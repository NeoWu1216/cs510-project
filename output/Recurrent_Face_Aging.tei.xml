<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Face Aging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Research Center for Learning Science</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">360 Artificial Intelligence Institute</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<email>niculae.sebe@unitn.itelefjia</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Face Aging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modeling the aging process of human face is important for cross-age face verification and recognition. In this paper, we introduce a recurrent face aging (RFA) framework based on a recurrent neural network which can identify the ages of people from 0 to 80. Due to the lack of labeled face data of the same person captured in a long range of ages, traditional face aging models usually split the ages into discrete groups and learn a one-step face feature transformation for each pair of adjacent age groups. However, those methods neglect the in-between evolving states between the adjacent age groups and the synthesized faces often suffer from severe ghosting artifacts. Since human face aging is a smooth progression, it is more appropriate to age the face by going through smooth transition states. In this way, the ghosting artifacts can be effectively eliminated and the intermediate aged faces between two discrete age groups can also be obtained. Towards this target, we employ a twolayer gated recurrent unit as the basic recurrent module whose bottom layer encodes a young face to a latent representation and the top layer decodes the representation to a corresponding older face. The experimental results demonstrate our proposed RFA provides better aging faces over other state-of-the-art age progression methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face aging, also known as age progression, is attracting more and more research interest. It has wide applications in various domains including cross-age face verification <ref type="bibr" target="#b21">[22]</ref> and finding lost children. In recent years, face aging has witnessed various breakthroughs and a number of face ag- * J. Feng is supported by NUS startup grant R-263-000-C08-133. ing models have been proposed <ref type="bibr" target="#b6">[7]</ref>. Face aging, however, is still a very challenging task in practice for various reasons. First, faces may have many different expressions and lighting conditions, which pose great challenges to modeling the aging patterns. Besides, the training data are usually very limited and the face images for the same person only cover a narrow range of ages. Moreover, the face aging process also depends on the environment and genes which are hard to model. Generally, face aging follows some common patterns of the human aging process. For kids, the main appearance change is the shape change caused by cranium growth. For adults, the appearance change is mainly reflected by wrinkles <ref type="bibr" target="#b29">[30]</ref>. Various face aging approaches were proposed to model such dynamic aging patterns, which can be roughly divided into two types <ref type="bibr" target="#b29">[30]</ref>, namely, prototype approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref> and physical model approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>. The physical model approaches employ parametric models to simulate face aging by modeling the aging mechanisms of muscles, skins, or cranium. However, those approaches are very complex and computationally expensive, and they require a large number of face sequences of the same person covering a wide range of ages. However, few of the current face aging datasets can provide sufficient data. In contrast, the prototype approach <ref type="bibr" target="#b16">[17]</ref> does not require face sequences of the same person with continuous ages. The prototype approach models face aging using a non-parametric model. First, all the available faces are divided into discrete age groups, and an average face within each age group is computed as a prior. The difference between the average faces is treated as the aging pattern and the pattern is transferred to each individual face to produce an aged face. However, the prototype approaches totally discard the personalized information and all the people share the same aging pattern. Moreover, regardless of the model type, all those methods perform a one-step transformation from one age group to another by learning a single mapping function. Thus, the one-step mapping function typically fails to capture the dynamics of the in-between face sequence between adjacent age groups.</p><p>To model the complex yet smooth dynamics of face aging, we propose a recurrent face aging (RFA) framework. Our RFA is based on a recurrent neural network (RNN), and it transforms a face smoothly across different ages by modeling the intermediate transition states. Different from the one-step mapping function used in the previous prototype approaches, our RFA framework can generate the fine-grained in-between faces. Similar to the prototype approaches, our model only requires the short-term faces of the same person which can cover two adjacent groups. This setting effectively alleviates the issue caused by data insufficiency of the long-term face sequences. Similar to the prototype approaches, we divide the faces of each gender into 9 age groups (e.g., 1−5, 5−10, ...), as shown in <ref type="figure">Fig. 2</ref>. We adopt a recurrent neural network (RNN) to learn the aging pattern for every two adjacent age groups. Then all the RNNs are concatenated to form the complete face aging framework. One of the most attractive advantage of RNN <ref type="bibr" target="#b25">[26]</ref> is its ability to memorize the previous states and allows information to persist. Thus, the RFA framework can age the face gradually while preserving the identity of the face by memorizing the previous faces.</p><p>The collected face images, however, usually have various expressions. Mild expressions can have a drastic effect in face analysis methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>, specifically in the position of the landmarks. Thus, we need to normalize the faces before aging them. Another concern in aging real faces is the presence of various lighting conditions. Such lighting inconsistency can manipulate a large portion of pixels. It is likely for a face aging model to learn the lighting change instead of the aging patterns if we learn the pixelto-pixel mappings. Besides, there is possibly much noise in an image. For example, it is very common that forehead is occluded by hair. The shapes of the wrinkles are also diverse for different people. Thus, the noise (e.g., the small and detailed wrinkles) should be filtered out while the regular shading (e.g., the shading around the mouth) and texture information should be kept in the training phase. The eigenfaces <ref type="bibr" target="#b36">[37]</ref> are very robust to noise as the they capture the global structure information. Thus, after obtaining the normalized images, we project the images to the egienface space and take their low rank coefficients as the image representation input to the RFA framework. After obtaining the predicted low rank aged face, we synthesize the textures by transferring the textures from its nearest neighbor in the eigenface space. The textures of the in-between faces are synthesized by combining the textures of the young face and the textures of the nearest neighbour. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we can generate realistic old faces with detailed textures.</p><p>To summarize, our paper makes following contributions: (1) We propose a smooth face aging process between each neighbouring groups with RNN network; (2) Our method can generate smooth intermediate faces and it handles the ghosting artifacts properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Face aging</head><p>Face aging models can be roughly divided into prototype approaches and physical model approaches <ref type="bibr" target="#b6">[7]</ref>. The prototype approaches <ref type="bibr" target="#b33">[34]</ref> aim at constructing an average face as prototypes for the young and old groups, and transferring the texture difference between the prototypes to the test image. The state-of-the-art prototype method <ref type="bibr" target="#b16">[17]</ref> improves the result by replacing the average face with a relighted average face whose lighting condition can be tuned to be the same with the input. Apart from the lighting considerations, the geometry transformation is implemented using optical flow.</p><p>However, the limitation of the prototype models still exists: face texture changes are the same for different inputs. Besides, the detailed texture information (e.g., wrinkles) is averaged out. Recently, a coupled dictionary learning (CDL) model <ref type="bibr" target="#b26">[27]</ref> was proposed. Similar to <ref type="bibr" target="#b38">[39]</ref>, the CDL model learns a dictionary for each age group. This model assumes that the sparse coefficients of the same person remain the same across the dictionaries. Thus, the aging patterns are encoded by the dictionary bases. Every two neighbouring dictionaries are learned jointly. Besides, the reconstruction error is regarded as the personalized information and it is added into the synthesized aged face directly. However, this method still has ghost artifacts as the reconstruction residual does not evolve over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recurrent neural network</head><p>Traditional RNN can learn complex dynamics by mapping the input sequence to a sequence of hidden variables. By passing the hidden variables recursively to the repeat- ing module in the network, RNN is able to memorize the previous information. Thus, RNN performs well in dealing with sequential data which have dependencies. In the past few years, RNN has been successfully applied to a variety of natural language processing and image processing tasks, such as speech recognition <ref type="bibr" target="#b8">[9]</ref>, machine translation <ref type="bibr" target="#b0">[1]</ref>, hand-writing recognition <ref type="bibr" target="#b9">[10]</ref>, image caption generation <ref type="bibr" target="#b13">[14]</ref> and video to text description <ref type="bibr" target="#b37">[38]</ref>. As a special type of RNN, Long Short Term Memory (LSTM) networks are explicitly designed to tackle the longterm dependency problem. LSTM <ref type="bibr" target="#b7">[8]</ref> was firstly introduced for speech recognition problem <ref type="bibr" target="#b11">[12]</ref>, where the memory cells enable LSTM network to process sequential data with dependencies. The key idea behind LSTM is its memory state and four gates which can control the information flow inside the unit adaptively. Given the success of LSTM, many LSTM variants are explored, such as the Gated Recurrent Unit (GRU) introduced by Cho et al. <ref type="bibr" target="#b4">[5]</ref>. As shown in <ref type="figure">Fig. 5</ref>, GRU is a simplified version of LSTM. Given the various RNN variants, Greff et al. conducted thorough search in order to find out the optimal RNN structure <ref type="bibr" target="#b10">[11]</ref>. The research revealed that GRU outperforms LSTM on almost all the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Recurrent Face Aging</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Our model conducts face aging in the following two steps. The first step is face normalization and the second step is aging pattern learning. We crawl the face images from the Web, as well as the available databases. The details of image collection are in Section 4.1. As the face images are collected in the wild, they have various expressions.</p><p>There are various ways to normalize the faces, such as warping a face to the average face by matching the detected landmarks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40]</ref> through interpolation, or utilizing optical flow <ref type="bibr" target="#b15">[16]</ref>. As shown in <ref type="figure">Fig. 3</ref>, aligning a face to the VS <ref type="figure">Figure 3</ref>. Landmark matching method VS optical flow method <ref type="bibr" target="#b15">[16]</ref> average position of the landmarks via interpolation twist the face. To avoid this undesirable effect, we use optical flow for face normalization, which preserves wrinkles well as shown in <ref type="figure">Fig. 3</ref>. In the eigenface space, the face images can be normalized as desired and important details can be well preserved. More details of face normalization are given in Section 3.2. Based on the learned eigenfaces, a robust image representation can be obtained by projecting the image to the eigenfaces. These image representations are then fed into our RFA framework.</p><p>We exploit RNN to learn the aging patterns between the neighbouring groups. Although many models can be used to learn the smooth transformation, such as LSTM <ref type="bibr" target="#b7">[8]</ref>, GRU <ref type="bibr" target="#b10">[11]</ref>, as well as their variants, we employ GRU to learn the aging patterns because of its simple structure and superior performance <ref type="bibr" target="#b10">[11]</ref>. As shown in <ref type="figure">Fig. 5</ref>, a two-layer GRU is built as the basic recurrent module. The top GRU has the same structure as the bottom GRU with the difference in the dimensions of the hidden state h t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Intrinsic face normalization</head><p>The most important factor to be considered in face normalization is to preserve the intrinsic age information, such that one can normalize the face group-wisely by leveraging the faces within the same age group. In this way, the age-specific information can be maintained. For instance, the eyes of children are usually larger than the eyes of old people, as shown in <ref type="figure">Fig. 2</ref>. These characteristics could be well preserved by the eigenfaces. Another factor that needs to be considered is the smoothness of age progression between the adjacent age groups. Instead of normalizing each face group independently, we normalize the faces from every two adjacent age groups jointly. A shared egienface space can be learned for each pair of groups. Then a smooth transformation can be learned in the shared eigenface space.  We optimize the eigenfaces and optical flow estimation iteratively. First, we stack the images column-wisely into the matrix M=[I 1 , ..., I n ]. Here I denotes an image. Then we implement singular value decomposition on M: M=USV T . We keep the top k eigenvectors in U and denote them as H=U(:, 1:k). We reconstruct image I in the low rank eigenface space as I ′ =H(H T I) where H T I means projecting the image I to the eigenface space H. Then the optical flow from I ′ to I can be calculated, and we can get I ′ by warping I to I ′ reversely using the optical flow. As the optical flow can not recover the images perfectly, I ′ and I ′ are not exactly the same and I ′ has ghost artifacts. To remove the ghost artifacts, we reset M=[ I 1 , ..., I n ] and repeat the process above until convergence. In each new face normalization process, we progressively increase the number of eigenvectors.</p><p>We start the process from k=4 and terminate the process when k=80. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Problem formulation</head><p>Let H (k) represent the shared egienfaces for age group k and k+1, where each column in H (k) denotes one eigenface. The columns in H (k) are unit vectors and they are orthogonal to each other. Let Λ k =(λ 1 , λ 2 , ..., λ n ) T denote the eigenvalues of the eigenfaces. Let I y be the low rank young face, I o be the low rank image of the ground truth of the old face, I ′ o be the predicted low rank old face, and x y ,</p><formula xml:id="formula_0">x o , x ′</formula><p>o be their coefficients in the eigenface space. We expect the predicted image to be as similar as possible to the ground truth image. Therefore we define the following loss function:</p><formula xml:id="formula_1">I o −I ′ o 2 F = H (k) x o −H (k) x ′ o 2 F = x o −x ′ o 2 F .</formula><p>(1) During the face normalization process, we observe that the first 4 eigenvalues occupy more than 60% of the total energy. The previous studies <ref type="bibr" target="#b15">[16]</ref> revealed that the first 4 eigenfaces correspond to the lighting effect of the face while the others correspond to the texture. Thus, in order to keep the illumination consistency between the source and target images, we transfer the first 4 coefficients directly from the young image to the predicted old image. The high rank coefficients mainly preserve the texture information. We rely on the high rank coefficients to learn the aging patterns. We visualize the high rank eigenfaces and find that these eigenfaces capture different texture information (e.g., beard, open mouth with teeth and closed mouth). Here we normalize the distribution over these eigenfaces and propose the following loss function:</p><formula xml:id="formula_2">J = (x o − x ′ o ) ⊙ ( 1 λ 1 , ..., 1 λ n ) T 2 F .<label>(2)</label></formula><p>To optimize the objective function, we adopt RNN to learn the aging patterns as follows:</p><formula xml:id="formula_3">x y ⊙ ( 1 λ 1 , ..., 1 λ n ) T RN N −−−→ x ′ o ⊙ ( 1 λ 1 , ..., 1 λ n ) T .<label>(3)</label></formula><p>The ⊙ in Eq. 2 and Eq. 3 is an element-wise multiplication to scale the samples to the interval [−1, 1] by dividing the i-th element of x by λ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Recurrent age progression</head><p>The two-layer GRU is more flexible compared with the single GRU. As shown in <ref type="figure">Fig. 5</ref>, the bottom GRU first encodes the input face to a hidden high dimensional variable. Then the top GRU decodes the hidden high dimensional state to an aged face. The hidden states are initialized with zeros. Using high dimension could boost its capability to encode complex high dimensional signals. The difference between the output and the ground truth aged face is calculated as the loss. Different weights are assigned for the loss.  <ref type="figure">Figure 5</ref>. Recurrent face aging framework with two-layer gated recurrent unit</p><formula xml:id="formula_4">1− σ tanh × × σ + h t−1 b h t x t b h t−1 × 1− σ tanh × × σ + z t c t × 1− σ tanh × × σ + h t+1 × 1− σ tanh × × σ + r t+1 b h t+1 h t Neural Network Layer Pointwise Operation Vector Transfer Copy Concatenate r t z t+1 c t+1 b r t b z t b c t+1 b c t b r t+1 b z t+1 b h t b h t+1 x t+1 b z t = σ( c W zh b h t−1 + c W zx h t + b b z ) b r t = σ( c W rh b h t−1 + c W rx h t + b b r ) b c t = tanh( c W ch b r t b h t−1 + c W cx h t + b b c ) b h t =(1− b z t ) b h t−1 + b z t b c t z t = σ(W zh h t−1 + W zx x t + b z ) r t = σ(W rh h t−1 + W rx x t + b r ) c t = tanh(W ch r t h t−1 + W cx x t + b c ) h t =(1− z t ) h t−1 + z t c t … …</formula><p>The loss in the last recurrence step has the largest weights as we take the output of the last step as the aged face. Smaller weights are set for the first several recurrence steps. These losses will guide the system to age the face slowly. The equation shown in <ref type="figure">Fig. 5</ref> is the model of the two-layer GRU, where σ is the logistic sigmoid function.</p><p>[W z , W r , W] are weight matrices and [b z , b r , b c ] are bias terms which need to be learned. Each GRU has two gates and one hidden state.</p><p>The reset gate r t decides whether the information of previous faces should be ignored. If r t is close to 0, the previous face information is forced to be discarded, and the unit will focus on the current input face only. This gate allows the unit to remember or drop the irrelevant face information.</p><p>The update gate z t controls the amount of face information that could be transferred from the previous state to the current state. This update gate works like the forget and input gates. Instead of calculating the value of forget and input gates separately like LSTM, the update gate in GRU calculates them together with z t and 1 − z t . This setting means that the unit only accepts the new input face when it forgets something of the previous faces. The update gate acts similarly to the memory cell in the LSTM. c t is the new face candidate created by a tanh layer that could be added to the current face. Then the face candidate is merged with previous face information to form a new face (hidden state) with the wights generated by the update gate:</p><formula xml:id="formula_5">h t =(1−z t )⊙h t−1 +z t ⊙c t .<label>(4)</label></formula><p>The system has short-term memory and ignores the previous faces if the reset gate is activated all the time. If the update gate is always inactivated, the system can have longterm memory and all the previous faces will be memorized.</p><p>In our RFA framework, RNN acts as a refinement pro-cess which transforms the young face slowly to the aged face. In our settings, each basic unit will iterate for 3 times.</p><p>The input series [x 1 , ..., x n ] is the replicates of the young face. The loss is calculated after each recurrence. We expect to age the face gradually. In other words, the in-between faces should become more similar to the target face after each iteration. Thus, the loss between the in-between faces and the target faces should become smaller gradually in the training process. To meet this requirement, we set a series of weights w=[0.1, 1, 10] for the loss. The weight increases monotonically as we expect that the faces could be transformed to the target face gradually. We assign the largest weight for the loss of the output face. For the transformation from group k to k+1, we obtain the following loss function:</p><formula xml:id="formula_6">J = n t=1 w t (x k+1 − h t ) ⊙ (1/Λ k ) 2 F ,<label>(5)</label></formula><p>where x k+1 represents the target image in group k +1, h t is the predicted in-between states during the recurrent training process, and w t is the weight for recurrence step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Follow-up operations</head><p>For each pair of neighbouring RNNs, their corresponding low-rank egienface spaces are different. Thus, the output of the previous RNN can not be used as input to the following RNN directly. We rely on the following formula to transform the output (x k+1 ) of k-th RNN to the input (x k+1 ) of (k+1)-th RNN.</p><p>x k+1 =U k+1 (U k x k+1 )=(U k+1 U k )x k+1 =Ux k+1 (6) where x k+1 is a column vector and it is the output of the kth RNN. U k x k+1 is its low rank image. Then the operation U k+1 (U k x k+1 ) reprojects the image to the eigenface space of k+1-th RNN. This transformation can be integrated into the RNN framework. As shown in <ref type="figure">Fig. 5</ref>, the term Wx t in the first three equations can be transformed as following:</p><p>Wx t = W(Ux k )=(WU)x k = Wx k . (7) After obtaining the low rank face I ′ . The next step is to transfer the detailed features from its nearest neighbour. We find the nearest neighbour of I ′ in the eigenface space which is denoted as J. Feature transfer from its nearest neighbour J to I ′ is illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref>. First, the input face is warped to the coordinates of I ′ , and obtain I. Then we replace the low rank face of the I with I ′ . Thus, the detailed texture of the young face is preserved. After replacing the low rank face of J with I ′ , the aged face can be synthesized by linearly combining the two faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first describe the implementation details of data collection and image pre-processing, followed by introducing the implementation details of the RFA framework. Then we will show the qualitative experimental results, as well as the quantitative evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data collection</head><p>We collect face images according to a celebrity list which contains 3,561 celebrities from the dataset of Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b12">[13]</ref>. We collect 163,810 images from Google and Bing image search engine where 3,240 celebrities have the photos which cover different age groups. We also use the images from the Morph Aging Dataset <ref type="bibr" target="#b22">[23]</ref> and Cross-Age Celebrity Dataset (CACD) <ref type="bibr" target="#b3">[4]</ref>. The Morph Aging Dataset contains 13,000 people with 55,134 images. The CACD dataset contains 163,446 photos of 2,000 people. Both datasets contain multiple images for each person which cover different age groups. In order to ensure the high quality of the data, we remove the images which have large poses (greater than 30 degrees in yaw and pitch angles). For each crawled image, the groundtruth of its age is estimated by an off-the-shell age estimator <ref type="bibr" target="#b18">[19]</ref>. Then we manually checked the accuracy of the estimated age.</p><p>Finally, we have 4,371 photos for male and 6,264 photos for female in total. After dividing the data into 9 age groups for both male and female: 0-5, 6-10, 11-15, 16-20, 21-30, 31-40, 41-50, 51-60, 61-80, we obtain 2,611 image pairs which covers two neighbour age groups for male and 3,821 pairs for female in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>We follow a similar pipeline as <ref type="bibr" target="#b14">[15]</ref> for image preprocessing which includes face landmark detection, pose estimation, and masking the images. After detecting the 66 facial landmarks using the model provided in <ref type="bibr" target="#b39">[40]</ref>, the faces are aligned according to the centers of eyes and mouth. In the intrinsic face normalization process, we revise the optical flow implementation introduced by Liu <ref type="bibr" target="#b19">[20]</ref> to calculate the flow because of its superior performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. We follow <ref type="bibr" target="#b15">[16]</ref> to set the parameters of the flow algorithm.</p><p>After performing face normalization, we calculate 80 low rank eigenfaces for every two neighbouring age groups with respect to each channel of RGB. Then we concatenate the coefficients and get d=240 dimensional representations as the input for the two-layer GRU. As shown in <ref type="figure">Fig. 5</ref>, the dimension of the hidden unit of the bottom GRU can be set to different values (e.g., d×k) from the input dimension d.</p><p>To strengthen the encoding capability for various faces, we set k=15, which can generate satisfied in-between faces and also consumes less training time (around half an hour for each RNN). The hidden unit dimension of top GRU is set to be the same dimension as the input since it needs to decode the signal to the same dimension of the input signal. The RNN is implemented in Theano <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative comparison</head><p>We compare the performance of our method with another two face aging models, which are the coupled dictionary learning (CDL) model <ref type="bibr" target="#b26">[27]</ref> and Face Transformer (FT) demo (http://cherry.dcs.aber.ac. uk/Transformer/). CDL defines the same 9 age groups as ours. The FT Demo has fewer age groups: Baby, Child, Teenage, Young Adult and Older Adult. For fair comparison with FT Demo, we select the pairs from our dataset which have large age gaps such that the ages of the images can be consistent with the age groups in FT Demo. Some experimental results are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. We further compare our method with CDL on fine-grained age groups as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. The images from FG-NET database <ref type="bibr" target="#b17">[18]</ref> are used as the test data. In <ref type="figure" target="#fig_6">Fig. 7</ref>, the aged faces in the green boxes are generated by our method. One can observe that the images generated by CDL and FT Demo suffer from the ghost artifacts. Our method can generate images with more realistic appearance. There are also some failure cases for our method. The images in the red boxes show the cases where the better results are generated by the other methods.</p><p>Our RFA remembers all the previous faces and generates the aged face by referring to its memory. After each iteration, the shape of the face changes slightly. This change    <ref type="bibr" target="#b26">[27]</ref> and our RFA method . Here we plot the aged results of 9 people. Each person has three images: the masked image which is the input, aged face generated with RFA, and aged face generated with CDL. The number shown below each image is the age or target age range of the person. For example, for the first group, the input image is of age 40, and the target age range is 41-50. We can observe that the aged face generated by our method matches the characteristics of the target age group well. For instance, the aged face in the first group (row 1, column1) gets some wrinkles, and his eyes become smaller during the aging process. But for some cases our aged faces are not so clear as the ones generated by CDL, such as the examples in the red boxes.  with well-controlled magnitude will result in a high-quality flow which makes the image look more realistic (getting rid of ghost artifacts). For the one-step method (e.g., linear regression method), the shape and texture change is too dramatic. This leads to unnatural images with severe artifacts, as shown in <ref type="figure" target="#fig_10">Fig. 9</ref> (a). We also compare our method with two popular Convolution Neural Networks (VGG-S <ref type="bibr" target="#b2">[3]</ref>, Super-Resolution CNN <ref type="bibr" target="#b5">[6]</ref>). For VGG-S, we remove all the pooling layers and set the output of the last convolution layer to the aged face. However, these methods failed learning the wrinkles in the training phase, and their output are smoothed faces, as shown in <ref type="figure" target="#fig_10">Fig. 9</ref> (b)(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative comparison with prior works</head><p>Several prior works released their best face aging results <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Shu et al. <ref type="bibr" target="#b26">[27]</ref> summarized all the posted images, and found that there were 246 aged faces with 72 input images in total. We synthesize the aged face with the same age range of these methods for each input image. Similar to prior works, we evaluate our results through user study.</p><p>In user study, each subject views three images: the young image C, and the aged images B &amp; A which are generated by other methods and our method respectively. We set two metrics for the evaluation, age accuracy and identity accuracy. Each subject is asked to evaluate the images based on these two metrics. Three types of scores are provided. If A is better, it gets a score of 1. If B is better, A gets a score of 0. If A and B are similar, then both of them get the score of 0.5. We invite 40 people to evaluate our results and get 9840 scores in total. The statistics of the scores are as follows: 58.67% of the votings think our result is better, 10.40% think the these results are equivalently good, and 30.92% think other results are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation on cross-age face verification</head><p>Many groups have made breakthroughs for face verification <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>. We employ the deep Convo- To evaluate the performance of our method for the cross-age face verification, we exploit FG-NET dataset which consists of 1,002 photos of 82 people as our input. We select the image pair whose the age gap is larger than 20 years. 916 image pairs are obtained in total. We further select 916 image pairs randomly from different people as negative pairs. We name these pairs as "Original Pairs". For each image pair, we synthesize the aged face of the younger one. Thus we get our synthesized image pair by replacing the young face with our synthesized aged face. We name these pairs as "RFA Pairs". We also synthesize the aged faces with CDL <ref type="bibr" target="#b26">[27]</ref> method and illumination aware age progression (IAAP) method <ref type="bibr" target="#b16">[17]</ref>, and we name the synthesized pairs as "CDL Pairs" and "IAAP Pairs" respectively. The false acceptance rate-false rejection rate (FAR-FRR) curve is available in <ref type="figure" target="#fig_0">Fig. 10</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, our method has competitive performance compared with CDL and outperforms the other two methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we proposed a recurrent framework for face aging. By going through the smooth intermediate faces, the shape of the face evolves slowly and this leads to a highquality optical flow. Then the synthesized faces derived from the optical flow are more realistic compared with the one-step methods. We exploit a very powerful two-layer GRU as our recurrent module. The bottom layer works as an encoder which can project the image to a high-dimension space and the top layer works as a decoder which decode the hidden variables to an aged face. This powerful structure can model very complex dynamic appearance changes. However, during the testing phase, the system requires the age of the input face which might be unavailable. In the future, we will integrate the age estimation model into our framework. Another consideration is that the edge of the generated aged face is not very sharp. We will explore other texture synthesizing methods to generate clear textures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Exemplars of face aging from teenager to adult generated by our model. The left most column shows the input, and the other columns show the generated aged faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Intrinsic face normalization. (a) Examples of input images. (b) Masked images. (c) Estimated flow for face normalization. (d) Normalized faces with the estimated optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 4shows the face normalization results. It is worth noting that the expressions of the 4 people are normalized (e.g., the eyeballs of the first image come to the middle; the mouth of the second image becomes horizontal; the wide-open eyes of the third image become smaller, and the closed eyes of the forth image are open).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>×</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Feature transfer from the nearest neighbour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Face aging results comparison between FT Demo, CDL and our RFA method. The images in the green boxes are aged faces generated by our method. The images in the red boxes are the examples which are better than our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Comparison between Coupled Dictionary Learning (CDL) method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>shape change will result in a low-quality optical flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Comparison with one-step method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>False acceptance rate vs false rejection rate curve lutional Neural Network model introduced in [28] for face verification.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-age reference coding for age-invariant face recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Age synthesis and estimation via faces: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1955" to="1976" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6645" to="6649" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04069</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Face reconstruction in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collection flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Illumination-aware age progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward automatic simulation of aging effects on face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning ordinal discriminative features for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Beyond pixels: exploring new representations and applications for motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face recognition with temporal invariance: A 3d aging model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prediction of individual non-linear aging trajectories of faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sunkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Personalized age progression with aging dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<title level="m">Deepid3: Face recognition with very deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A concatenational graph evolution aging model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2083" to="2096" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A compositional and dynamic model for face aging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="401" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facial aging simulator considering geometry and patch-tiled texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tazoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maejima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2012 Posters</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prototyping and transforming facial textures for perception research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tiddeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="42" to="50" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-Adaptive Matrix Completion for Heart Rate Estimation from Face Videos under Realistic Condition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regressing a 3d face shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Category specific dictionary learning for attribute specific feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1465" to="1478" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
