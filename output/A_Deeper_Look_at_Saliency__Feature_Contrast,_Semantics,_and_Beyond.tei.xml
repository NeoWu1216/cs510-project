<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
							<email>bruce@cs.umanitoba.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Manitoba Winnipeg</orgName>
								<address>
									<region>MB</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Catton</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Manitoba Winnipeg</orgName>
								<address>
									<region>MB</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Janjic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Manitoba Winnipeg</orgName>
								<address>
									<region>MB</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we consider the problem of visual saliency modeling, including both human gaze prediction and salient object segmentation. The overarching goal of the paper is to identify high level considerations relevant to deriving more sophisticated visual saliency models. A deep learning model based on fully convolutional networks (FCNs) is presented, which shows very favorable performance across a wide variety of benchmarks relative to existing proposals. We also demonstrate that the manner in which training data is selected, and ground truth treated is critical to resulting model behaviour. Recent efforts have explored the relationship between human gaze and salient objects, and we also examine this point further in the context of FCNs. Close examination of the proposed and alternative models serves as a vehicle for identifying problems important to developing more comprehensive models going forward.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The past decade has seen numerous developments in modeling visual saliency, including introduction of a wide variety of models, many new and larger datasets, and a better understanding of metrics that are most appropriate for measuring model performance. Additionally, the notion of saliency has broadened such that several distinct problems are treated as problems of modeling saliency, including gaze prediction, salient object segmentation, and objectness measures <ref type="bibr" target="#b1">[2]</ref> respectively. Many domains of computer vision have benefitted from deep learning, and it is natural to consider that value of deep learning models in saliency prediction. Preliminary efforts of this variety suggest that there may also be significant benefits to deep learning for visual saliency prediction <ref type="bibr" target="#b18">[19]</ref>. One contribution of this paper is a deep learning model for visual saliency prediction based on fully convolutional networks <ref type="bibr" target="#b22">[23]</ref>. While the principal focus of this work is on gaze prediction, we also consider the problem of salient object segmentation and further explore the relationship between salient objects and human gaze patterns.</p><p>Given the vast array of developments in visual saliency prediction of late, there is value in taking account of where models currently stand in their capabilities, and where improvements might be made. This consideration is especially timely given that the choice of data is critical to how deep learning models will ultimately behave, and their capabilities will be determined in part by the relative balance of different types of patterns and concepts expressed in the data. Aside from the proposed deep learning model that is proposed, in our view this paper carries two additional and important contributions: 1. Data and ground truth: The manner in which data is selected, ground truth created, and performance assessed (e.g. loss function) are critical to the problem. We therefore place a heavy emphasis on examining this issue, and reveal nuances of the problem critical to treating gaze prediction using neural networks. This problem is introduced in section 2 and explored in detail in section 3. Supporting benchmark results are presented in section 4 of the paper. 2. Failures and shortcomings: Equally important to revealing what models grounded in deep learning can do well, is revealing what they cannot do well. To this end, we dedicate a significant portion of our analysis and discussion to revealing current challenges and suggestions for overcoming these challenges. These issues are explored in section 5.</p><p>Overall, the experimentation that is included reveals a number of issues important to any work involving the intersection of saliency with deep learning, and remaining challenges and possible paths forward are summarized in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>In this section, we describe the models we have considered and evaluation methodology. This includes the struc-  <ref type="figure">Figure 1</ref>. A high-level overview of the FUCOS architecture. Parameters in the legend are as follows: K corresponds to kernel dimensions, S to stride, N to number of outputs, % to the percent of dropout units. ture of FCNs used in experimentation and considerations relevant to benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Models</head><p>In recent years, there has been tremendous progress in certain areas of computer vision built on the success of deep learning. Problems that prescribe a pixel level labeling at the output such as semantic segmentation have shared in this success <ref type="bibr" target="#b11">[12]</ref>. In the paper, we explore the extent to which saliency prediction may benefit from a similar treatment. This is inspired in part by the apparent benefits of leveraging semantics in the context of saliency prediction. To this end, we present a model deemed Fully Convolutional Saliency (FUCOS), that is applied to either gaze, or salient object  <ref type="figure">Figure 2</ref>. Area under ROC curve for the FUCOS model corresponding to different ground truth thresholds, and with (N) or without (S) center-biased based normalization.</p><p>prediction. For some experiments involving salient object segmentation, we also consider an alternative model based on deconvolutional neural networks <ref type="bibr" target="#b25">[26]</ref> for comparison.</p><p>FUCOS shares a similar structure to models that have had success in addressing the problem of semantic segmentation including the family of FCN networks presented by Long et al. <ref type="bibr" target="#b22">[23]</ref>. A simple schematic of the FUCOS network employed for most of the experimentation in the paper is presented in <ref type="figure">Figure 1</ref>.</p><p>Model weights for layers matching the Long et al. architecture <ref type="bibr" target="#b22">[23]</ref> were initialized to those derived from training an 8 pixel prediction stride FCN on PASCAL-Context. Remaining layers were assigned random weights subject to Xavier initialization <ref type="bibr" target="#b13">[14]</ref>. The initial learning rate was set to 10 −14 , with weights decreasing by a step γ = 0.1 every 10k iterations. 80k total iterations were used to train each version of the model appearing in results and benchmark metrics. Note that versions specific to salient object segmentation (and not gaze) are introduced later on in section 4. These are referred to as SAL-FCN and SAL-DC to distinguish from the gaze based cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation</head><p>The problem of evaluating saliency models is challenging in itself which has contributed to fragmentation among benchmarks that are used. In light of these considerations, the specific evaluation we have applied aims to remove any advantages of one algorithm over another due to bias independent of image signal, and variation due to gain/contrast associated with algorithm output. We have restricted our analysis to gaze data to avoid challenges that may arise from the additional variability introduced by data collection methods that approximate gaze data on a larger scale <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30]</ref>. The evaluation is largely focused on modeling gaze prediction, however we also consider salient object segmentation. Given all of these considerations, benchmarking for both fixation data and salient object segmentation is based on the methods described by Li et al. <ref type="bibr" target="#b21">[22]</ref>.</p><p>We have compared our output with several saliency and segmentation algorithms including: ITTI <ref type="bibr" target="#b17">[18]</ref>, AIM <ref type="bibr" target="#b4">[5]</ref>, GBVS <ref type="bibr" target="#b14">[15]</ref>, DVA <ref type="bibr" target="#b16">[17]</ref>, SUN <ref type="bibr" target="#b36">[37]</ref>, SIG <ref type="bibr" target="#b15">[16]</ref>, AWS <ref type="bibr" target="#b12">[13]</ref>, FT <ref type="bibr" target="#b0">[1]</ref>, GC <ref type="bibr" target="#b9">[10]</ref>, SF <ref type="bibr" target="#b26">[27]</ref>, and PCAS <ref type="bibr" target="#b23">[24]</ref>. We have also compared to one alternative deep learning model SALICON <ref type="bibr" target="#b18">[19]</ref> that performs well in standard benchmarks. Model performance for gaze data is therefore based on the area under the ROC curve (auROC) corresponding to a shuffled-ROC metric <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b21">22]</ref>. Parameters relevant to a fair comparison of capabilities of different methods (e.g. post-processing blur) are also optimized in this process. Model performance for salient object segmentation is based on Precision-Recall (PR) analysis. Optimal F-Scores along the precision-recall curves are also taken as an additional measure of algorithm performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Devil in the Data</head><p>The outcome of learning is dependent on the specific nature of the image data used in training, and corresponding ground truth. Analysis presented in this paper therefore includes detailed consideration of these factors, as discussed in what follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Choice of Data</head><p>Models based on very deep neural networks typically require a significant amount of data for training. To overcome this challenge, recent efforts have sought to develop alternative strategies for deriving ground truth data that scale to much larger data sets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30]</ref>. In lieu of this strategy, we instead leverage a combination of different traditional eye tracking data sets to derive a suitably sized data set for training. Specifically, we have used a subset of the Bruce <ref type="bibr" target="#b4">[5]</ref>, Judd <ref type="bibr" target="#b19">[20]</ref>, PASCAL <ref type="bibr" target="#b21">[22]</ref>, and Imgsal <ref type="bibr" target="#b20">[21]</ref> datasets. The {training/testing} division for each of the datasets is as follows: {60/60}, {300/703}, {325/525}, {117/118}. This reflects a trade-off between the size of the set of training examples, and the desire to balance the contribution of samples from each distinct dataset. The intent of this latter consideration is towards diminishing bias specific to individual datasets. It is also the case then, that all output and benchmark results for different source datasets correspond to the same model, and not different models trained for each dataset individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ground Truth</head><p>Ground truth forms the basis for evaluation, but also serves an important role in its relation to the loss function used in training the neural network. This is complicated by the fact that most metrics employed for performance evaluation don't lend themselves to a form that can be directly optimized, or treated as a loss function. This obscures the connection between the ground truth, and the behavior of the resultant model that is trained. In the case of gaze data, one typically has a sparse sampling of discrete pixel locations within each image. These might be assumed to be observations that may be treated as samples from a latent distribution. One can convert discrete gaze points to a nonparametric density via convolution with a Gaussian kernel. This strategy has been employed to consider the approximate visual acuity of sampling corresponding to a set of fixation points <ref type="bibr" target="#b4">[5]</ref>, and blurring of the fixation map is often used to generate human fixation data derived saliency map that can be treated as a classifier.</p><p>For a variety of common loss functions, direct use of the discrete fixated locations may not be the best choice: For example, the sparsity of fixated pixels might imply that a zero output nearly everywhere is optimal in the eyes of a Euclidian loss function, without additional constraints. Moreover, a mix of different datasets results in some heterogeneity in the statistical properties of the combined dataset. In considering treatment of the ground truth data, we have identified a number of important considerations:</p><p>1. Intuitively, one might hope to maximize the signal associated with image content driving gaze, while minimizing noise corresponding to non-gaze related factors and low sample density in measured fixation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A continuous representation derived from raw gaze coordinates is a sensible means of producing dense ground truth.</p><p>3. Noise factors may include noise intrinsic to the gaze tracking instrument, but also inherent bias related to gaze behavior. The issue of center bias in gaze data has been much discussed <ref type="bibr" target="#b30">[31]</ref>, in particular for its role in evaluation. However, one might also treat this bias as a noise variable, since the salience of local samples is less related to image content when spatial bias is present in the data. This is especially important if convolutional layers have only a local view of the image, without any more global influence.</p><p>4. Gaze tracking data is heterogeneous. Factors that vary include total viewing times, number of viewers, image size, viewing distance, and bias in the nature of images. For these reasons, the spatial distribution and contrast of derived density maps can vary substantially. This may be problematic in combining ground truth samples drawn from different data sets, as their associated statistics may differ.</p><p>Given the aforementioned considerations, we have adopted the following procedure for producing ground truth data:</p><p>1. Fixation maps are first convolved with a Gaussian kernel, with σ corresponding to 1 degree visual angle. This implies that density is a good match with sampling density  <ref type="bibr" target="#b19">[20]</ref>. Each frame shows output corresponding to: (a) the original image, (b) DVA <ref type="bibr" target="#b16">[17]</ref>, (c) AIM <ref type="bibr" target="#b4">[5]</ref>, (d) SIG <ref type="bibr" target="#b15">[16]</ref>, (e) AWS <ref type="bibr" target="#b12">[13]</ref>, (f) SALICON <ref type="bibr" target="#b18">[19]</ref>, (g) 80N, (h) 95N, and (i) fixation density corresponding to the human data.</p><p>accounting for differences between foveal and peripheral visual acuity. Differences in contrast remain, due to differing numbers of viewers or viewing time per image.</p><p>2. Fixation density maps are averaged over all ground truth samples within each individual data set. This is preparation for the optional step that follows.</p><p>3.</p><p>[optional] The average density map for each dataset is subtracted from each individual density map from the same dataset. This step is intended to factor out the gaze signal driven by center bias. It is worth noting that more sophisticated methods might be used at this stage (e.g. factoring out context <ref type="bibr" target="#b5">[6]</ref>  Results are discussed in detail in the section that follows, but we present one set of results here that is germane to the points made in this section. <ref type="figure">Figure 2</ref> demonstrates performance of FUCOS for fixation prediction for the PASCAL dataset, in following the above steps for varying values of p. It is evident that the normalization for center bias is important to the results that are achieved. Note that given the dependence of the FUCOS model on data, we refer to individual instantiations of FUCOS according to shorthand that reflects the nature of the training data. In particular, each of the models is numbered according to the percentile threshold used, with N indicating normalization to compensate for center bias, and S indicating no such normalization. (e.g. 85S indicates training based on binary outputs corresponding to a threshold based on the 85th percentile value in the non-normalized fixation density maps)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we present qualitative examples of output from various algorithms corresponding to the 4 datasets, and results from performance evaluation benchmarks. <ref type="figure">Figure 3</ref> demonstrates the output of a variety of algorithms, with the correspondence to dataset and algorithm noted in the figure caption. Visualization is based on superimposing a heatmap corresponding to algorithm output (saliency) over the original image. The problem of visu-alizing saliency map output appropriately can be a challenge (e.g. see <ref type="bibr" target="#b6">[7]</ref>). To ensure that observed differences due to perceptual considerations are minimized, output saliency maps are histogram equalized to ensure matching contrast. Pixels covered by the superimposed heatmap correspond to the top 20% of output values in each saliency map, with colormap values mapped linearly to the equalized heatmap values. This provides an effective means of comparing the predictions of most salient regions across different algorithms.</p><p>As mentioned in the methods section, performance is quantified according to the area under the ROC curve, treating the output of algorithms as a binary classifier. Evaluation corresponds to the shuffled AUC score that is common in saliency evaluation. It is also important to note that benchmarking is performed on the raw gaze data as is typical, and the earlier discussion of binarized ground truth maps applied only to produce data used in training. Results for a variety of algorithms, and for the 4 datasets are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. It is evident that FUCOS significantly outperforms existing approaches to the problem. We have also compared the output to the SALICON model, which is a deep learning model. Interestingly, our solution performs significantly better for 2 of the 4 datasets, slightly better for 1 and significantly worse for the other. It is worth noting that the case where FUCOS is not the top performing model may be due to training of the competing model (SALICON) on images that comprise the test data. Overall, this analysis hints at strengths and weaknesses of deep </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Salient Object Segmentation</head><p>As discussed in the introduction, the notion of saliency has become fragmented, and encompasses both gaze prediction and salient object segmentation, among other tasks. As such, we have also considered the efficacy of fully convolutional network models to predict salient object regions (as defined by ground truth object masks). In addition to the FUCOS model considered for gaze prediction, we have also tested an additional model (DeconvNet <ref type="bibr" target="#b25">[26]</ref>) based on the expectation that its structure may allow more precise recovery of spatial information corresponding to boundaries of salient objects. For this line of experimentation, training was based on ground truth corresponding to the binary salient object masks.</p><p>Sample output is shown in <ref type="figure" target="#fig_2">Figure 5</ref>. Corresponding benchmark performance is characterized in <ref type="figure" target="#fig_3">Figure 6</ref>. It is interesting to note the efficacy of these models relative to existing solutions for the problem of salient object segmentation. Also of note, is the difference between SAL-FCN and SAL-DC models. Both of these produce a similar maximal F-Score, however SAL-DC reveals a higher precision at lower recall values, perhaps reflecting a more precise adherence to spatial boundaries of segmented salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Are Objects Salient?</head><p>Much of the emphasis of this paper is on saliency as it relates to gaze prediction. This section considers the relationship between objects and gaze prediction. More specifically, we seek to examine to what extent gaze is driven by the presence of objects rather than alternative factors. In this section, we explore this point in detail. This includes analysis of different factors that contribute to gaze behaviour and the extent to which these are reflected in traditional saliency models and those based on deep learning. We also highlight other potentially important points that are not addressed by existing models. There has been much debate about the relative importance of objects and saliency defined by feature contrast. Some studies have claimed that objects better predict gaze patterns than low level saliency models <ref type="bibr" target="#b10">[11]</ref>, and others have sought to rebut this claim <ref type="bibr" target="#b2">[3]</ref>. The most likely scenario is that objects play a role in driving gaze, but that feature contrast is also an important factor. Moreover, the behaviour that manifests depends critically on other factors including task and context <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>We have also examined the relationship between objects and gaze patterns directly in the context of this paper, in examining the performance of models intended for salient object segmentation in their ability to predict fixation patterns. For the Pascal dataset, the SAL-FCN and SAL-DC models trained for object segmentation have also been tested in their ability to predict gaze points. This is revealed in <ref type="figure" target="#fig_1">Figure  4</ref>. A whole-object centric model (SAL-FCN in particular) performs comparable to some of the better classic contrast based saliency algorithms. Differences between SAL-FCN and SAL-DC in this evaluation hint at how the two differ in the relative uniformity of their output, or distribution of confidence as a function of typically gazed at features on objects.</p><p>It is clear that some fixations are directed at objects, while others depend on local feature contrast that is relatively detached from semantics. Relevant to this discussion is the performance of the SALICON model relative to FU-COS. SALICON is notably strong on datasets that by some accounts are characterized as having the strongest semantically relevant content <ref type="bibr" target="#b3">[4]</ref>. In contrast, it's performance may be weaker for images in which feature contrast is a stronger driving factor, or where objects and semantics are IMAGE SALICON 80N CONTRAST <ref type="figure">Figure 7</ref>. Examples for which contrast is important to determining saliency, or contrast based saliency is in conflict with semantics. Shown are the source image, output of SALICON, FUCOS (80N), and a recent contrast driven saliency algorithm. <ref type="bibr" target="#b27">[28]</ref> less prevalent in the data. It is evident that a challenge moving forward, lies in balancing bottom-up purely stimulus driven factors, and top-down influences including prior knowledge in how models behave. It is also important to consider the extent to which these models are learning templates (e.g. for semantically relevant patterns) as opposed to more generalized feature contrast. To shed some light on this point, we present a variety of examples that are indicative of weaknesses of deep learning solutions in their capacity to generalize with respect to feature contrast. These examples are shown in <ref type="figure">Figure 7</ref>.</p><p>Having touched on the notion of prior knowledge and top-down influences, the balance of this section discusses a variety of considerations that are absent from current models but that may have relevance to future efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Affordance and Action</head><p>While objects are evidently important, there is also much prior knowledge tied to actions that presumably influences gaze behavior. This includes assumptions concerning how objects may be used, and ways in which the viewer might interact with objects or people. To consider the potential impact on models of gaze behavior, we have carefully examined model output from different datasets and compared this with human fixation densities. There appear to be a variety of cases where action, social, or affordance based considerations play a role in gaze behavior. Some examples of these  <ref type="figure">Figure 8</ref>. Images for which gaze patterns may be driven by prior knowledge that transcends object labels. Columns correspond to (Left to Right) the original image, DVA <ref type="bibr" target="#b16">[17]</ref>, AIM <ref type="bibr" target="#b4">[5]</ref>, SIG <ref type="bibr" target="#b15">[16]</ref>, AWS <ref type="bibr" target="#b12">[13]</ref>, SALICON <ref type="bibr" target="#b18">[19]</ref>, 80N, 95N and fixation density corresponding to the human data.</p><p>are presented in <ref type="figure">Figure 8</ref>. Some of the observed human gaze patterns seem to reflect a stronger emphasis in gaze on items in peripersonal space of people in the scene, items being manipulated, environment or object locations relevant to actions being performed, gazed at locations by humans in the scene (joint attention), and subregions of larger objects that have relevance according to their function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Visual Acuity and Memory</head><p>The need to gaze is driven largely by foveation. Higher resolution input from the fovea serves to provide disambiguating information about the scene possibly relevant to a task or specific target <ref type="bibr" target="#b24">[25]</ref>. For this reason, gaze is influenced by the quality of representation that is possible as a function of fixations made and prior knowledge. One phenomenon that seems to occur frequently is an unexpectedly large degree of fixation density on more distant targets. This is conceivably due to the need for more careful scrutiny to determine the identity of the object. This observation seems to be related to the notion of a horizon effect <ref type="bibr" target="#b33">[34]</ref>, albeit the role of distance and scene composition seem to play an important role in driving gaze in a more general sense.</p><p>Related to this are higher contrast items that receive relatively little attention. Some of these seem to correspond to items that are iconic, or have a common appearance (e.g. flags). It is also conceivable that strong prior knowledge concerning object characteristics precludes the need for direct foveation to completely disambiguate their characteristics. These observations all lend support to the notion that models that include more prior knowledge about objects, scene composition, actions, social interaction, and human behavior will help to bridge the gap between the state of the art in gaze prediction, and recorded human gaze data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper, we present a deep learning model for saliency prediction, and demonstrate the effectiveness of this approach for both gaze prediction and salient object segmentation. A variety of important observations follow from this analysis:</p><p>1. Choice of image data: The behaviour of FUCOS and related models evidently depends on the choice of input data. The variable nature of performance for the SALICON model across different datasets suggests that the source data used in training may match some of the datasets tested better than others. In contrast, a more varied selection of input data seems to result in strong performance across varied datasets. The role of efforts to crowdsource data collection that approximates gaze data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30]</ref> will no doubt help in these efforts. Nevertheless, training with more data, and more varied data is not necessarily the most effective solution. This is elaborated on in some of the points that follow.</p><p>2. Choice of ground truth: There are instances where the manner for treating ground truth is well-defined, especially when there is a direct relationship between the loss function being optimized in training, and the metric used for performance evaluation. This is generally not the case for gaze prediction, and results presented in this paper reveal the importance of carefully considering how human gaze data is translated into a form that is suitable training. Even in cases that have a clearer correspondence between ground truth and problem objective (e.g. salient object segmentation), sensitivity to perturbations of ground truth is not well established. Hand labeled regions may be relatively coarse grained, or follow specific form and details of the objects. These differences might conceivably have a similar importance in the model behavior following training.</p><p>3. Role of Semantics: Incorporating knowledge of semantics in gaze prediction is evidently important. This has been clear from efforts that reveal the importance of faces and text in driving fixations <ref type="bibr" target="#b8">[9]</ref>. It is also clear that people and animals are important, including a particular emphasis on both human and animal faces. One target for future work is the need to understand the relative im-portance of semantics beyond some of these established categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">On Feature Contrast:</head><p>A filter sensitive to edges, or a local Difference of Gaussians operator may each be viewed as a detector for a certain type of contrast. This consideration extends to more complex patterns including discontinuities in texture and other mid-level features. One might assume that an appropriate response to all patterns may be encoded in a feedforward manner, with an appropriate network configuration and training data. However, this presents a rather rigid way of encoding feature contrast. A more likely candidate is mechanisms that involve recurrence. Local divisive normalization is sufficiently ubiquitous in the human visual system that this has been deemed to be a canonical operation in visual and other sensory computation <ref type="bibr" target="#b7">[8]</ref>. It is likely that highly flexible models of visual saliency computation are possible with a much simpler network in implementing similar computation alongside semantically driven selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Affordances and more:</head><p>We have provided examples of cases that may be challenging for current models of gaze prediction. Some of these are anecdotal or based on the authors' assumptions concerning visual routines <ref type="bibr" target="#b34">[35]</ref> attached to these scenes. Nevertheless, it is well established that phenomena such as joint attention are important to driving viewing behaviour. Moreover, the role of the eye as a sensory instrument is largely lacking from existing models, but presumably plays a significant role in how visual content is parsed.</p><p>It is clear that deep learning models may be highly capable in visual saliency prediction. However, this capability may be bolstered significantly by careful decisions in how models are trained. As richer levels of scene and action understanding are developed, this will also allow models for gaze prediction to address scenarios that are currently out of reach. Feature contrast as a general phenomenon is one central component to saliency driven fixation behavior that may be poorly captured by deep learning models. This can likely be rectified by appropriate adjustments to the neural architecture used, including by incorporating local mechanisms for gain control and normalization. This is a consideration that may also have important implications for all deep learning models, including those that target problems outside of saliency or gaze modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Benchmark performance for gaze prediction for 4 datasets. Left column: Curves depicting area under ROC curve as a function of post-processing blurring kernel σ. Curves are labeled according to Legend. Right: Bar plots showing optimal area under ROC values for the shuffled-ROC metric. Note for the pascal dataset, two algorithms trained for salient object prediction are also included. the optional step 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Predicted salient object regions from the Pascal dataset. Output corresponds to algorithms intended for salient object segmentation (Left to Right): GC, SF, PCAS, MCG+GBVS, SAL-FCN, SAL-DC learning solutions as a function of the nature of the different datasets. This matter is discussed in much greater detail in section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Precision-Recall curves for salient object detection corresponding to a variety of algorithms (See Legend). Points on curves correspond to maximal F-Score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Figure 3. Heatmaps superimposed on images from the 4 datasets considered. Top Left: Bruce<ref type="bibr" target="#b4">[5]</ref>, Top Right: Pascal<ref type="bibr" target="#b21">[22]</ref>, Bottom Left: ImgSal<ref type="bibr" target="#b20">[21]</ref>, Bottom Right: Judd</figDesc><table>ABCDE 

FGH 
I 
ABCDE 
FGH 
I 

ABCDE 
FGH 
I 
ABCDE 
FGH 
I 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>The authors gratefully acknowledge financial support from the Natural Sciences and Engineering Research Council Canada (NSERC) Discovery Grants program and ONR grant #N00178-14-Q-4583.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemamiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2009 Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Objects do not predict fixations better than early saliency: A re-analysis of einhäuser et al.&apos;s data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of scores, datasets, and models in visual saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="921" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2005</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards fine-grained fixation analysis: distilling out context dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the symposium on eye tracking research and applications</title>
		<meeting>the symposium on eye tracking research and applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="99" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tsotsos. On computational modeling of visual saliency: Examining whats right, and whats left. Vision research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wloka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Normalization as a canonical neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="62" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Faces and text attract gaze independent of the task: Experimental data and computer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cerf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Frady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Objects predict fixations better than early saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Einhäuser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the relationship between optical variability, visual saliency, and eye fixations: A computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebor N</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fdez-Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">545</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image signature: Highlighting sparse salient regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dynamic visual attention: Searching for coding length increments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T PAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<title level="m">Saliency in context. CVPR 2015</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1072" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2009 IEEE 12th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual saliency based on scale-space analysis in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<title level="m">What makes a patch distinct? CVPR 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eye movement statistics in humans are consistent with an optimal search strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Najemnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04366</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency, scale and information: Towards a unifying theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2179" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Task and context determine where you look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Rothkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Hayhoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Crowdsourcing gaze data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rudoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Collective Intelligence (CI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The central fixation bias in scene viewing: Selecting an optimal viewing position independently of motor biases and image feature distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Current understanding of eye guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="777" to="789" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual correlates of fixation selection: effects of scale and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Baddeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Gilchrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="643" to="659" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Henderson. Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">766</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual routines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="159" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06755</idno>
		<title level="m">Turkergaze: Crowdsourcing saliency with webcam based eye tracking</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun: A bayesian framework for saliency using natural statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
