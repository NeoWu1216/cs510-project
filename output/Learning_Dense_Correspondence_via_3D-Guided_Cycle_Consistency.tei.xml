<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Dense Correspondence via 3D-guided Cycle Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enpc</forename><surname>Paristech</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tti-Chicago</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Dense Correspondence via 3D-guided Cycle Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminative deep learning approaches have shown</head><p>impressive results for problems where human-labeled ground truth is plentiful, but what about tasks where labels are difficult or impossible to obtain? This paper tackles one such problem: establishing dense visual correspondence across different object instances. For this task, although we do not know what the ground-truth is, we know it should be consistent across instances of that category. We exploit this consistency as a supervisory signal to train a convolutional neural network to predict cross-instance correspondences between pairs of images depicting objects of the same category. For each pair of training images we find an appropriate 3D CAD model and render two synthetic views to link in with the pair, establishing a correspondence flow 4-cycle. We use ground-truth synthetic-to-synthetic correspondences, provided by the rendering engine, to train a ConvNet to predict synthetic-to-real, real-to-real and realto-synthetic  correspondences that are cycle-consistent with the ground-truth. At test time, no CAD models are required. We demonstrate that our end-to-end trained Con-vNet supervised by cycle-consistency outperforms stateof-the-art pairwise matching methods in correspondencerelated tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consistency is all I ask!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOM STOPPARD</head><p>In the past couple of years, deep learning has swept though computer vision like wildfire. One needs only to buy a GPU, arm oneself with enough training data, and turn the crank to see head-spinning improvements on most computer vision benchmarks. So it is all the more curious to consider tasks for which deep learning has not made much inroad, typically due to the lack of easily obtainable training data. One such task is dense visual correspondencethe problem of estimating a pixel-wise correspondence field between images depicting visually similar objects or scenes. Not only is this a key ingredient for optical flow and stereo synthetic s <ref type="bibr" target="#b0">1</ref> synthetic s 2 real r 1 real r 2</p><formula xml:id="formula_0">F s1,s2 F s1,r1 F r1,r2 F r2,s2</formula><p>TRAINING TIME <ref type="figure">Figure 1</ref>. Estimating a dense correspondence flow field Fr 1 ,r 2 between two images r1 and r2 -essentially, where do pixels of r1 need to go to bring them into correspondence with r2 -is very difficult. There is a large viewpoint change and the physical differences between the cars are substantial. We propose to learn to do this task by training a ConvNet using the concept of cycle consistency in lieu of ground truth. At training time, we find an appropriate 3D CAD model and establish a correspondence 4cycle, training to minimize the discrepancy betweenFs 1 ,s 2 and Fs 1 ,r 1 •Fr 1 ,r 2 •Fr 2 ,s 2 , whereFs 1 ,s 2 is known by construction. At test time, no CAD models are used.</p><p>matching, but many other computer vision tasks, including recognition, segmentation, depth estimation, etc. could be posed as finding correspondences in a large visual database followed by label transfer. In cases where the images depict the same physical object/scene across varying viewpoints, such as in stereo matching, there is exciting new work that aims to use the commonality of the scene structure as supervision to learn deep features for correspondence <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">39]</ref>. But for computing correspondence across different object/scene instances, no learning method to date has managed to seriously challenge SIFT flow <ref type="bibr" target="#b25">[26]</ref>, the dominant approach for this task.</p><p>How can we get supervision for dense correspondence between images depicting different object instances, such as images r 1 and r 2 in <ref type="figure">Figure 1</ref>? Our strategy in this paper is to learn the things we don't know by linking them up to the things we do know. In particular, at training time, we use a large dataset of 3D CAD models <ref type="bibr" target="#b0">[1]</ref> to find one that could link the two images, as shown in <ref type="figure">Figure 1</ref>. Here the dense correspondence between the two views of the same 3D model s 1 and s 2 can serve as our ground truth supervision (as we know precisely where each shape point goes when rendered in a different viewpoint), but the challenge is to use this information to train a network that can produce correspondence between two real images at test time.</p><p>A naive strategy is to train a network to estimate correspondence between the rendered views of the same 3D model, and then hope that the network could generalize to real images as well. Unfortunately, this does not work in practice (see <ref type="table">Table 1</ref>), likely due to 1) the large visual difference between synthetic and real images and 2) the lack of cross-instance ground truth correspondence for training. Instead, in this paper we utilize the concept of cycle consistency of correspondence flows <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref> -the notion that the composition of flow fields for any circular path through the image set should have a zero combined flow. Here, cycle consistency serves as a way to link the correspondence between real images and the rendered views into a single 4cycle chain. We can then train our correspondence network using cycle consistency as the supervisory signal. The idea is to take advantage of the known synthetic-to-synthetic correspondence as ground-truth anchors that allow cycle consistency to propagate the correct correspondence information from synthetic to real images, without diverging or falling into a trivial solution. Here we could interpret the cycle consistency as a kind of "meta-supervision" that operates not on the data directly, but rather on how the data should behave. As we show later, such 3D-guided consistency supervision allows the network to learn crossinstance correspondence that potentially overcomes some of the major difficulties (e.g. significant viewpoint and appearance variations) of previous pairwise matching methods like SIFT flow <ref type="bibr" target="#b25">[26]</ref>. Our approach could also be thought of as an extension and a reformulation of FlowWeb <ref type="bibr" target="#b40">[40]</ref> as a learning problem, where the image collection is stored implicitly in the network representation.</p><p>The main contributions of this paper are: 1) We propose a general learning framework for tasks without direct labels through cycle consistency as an example of "metasupervision"; 2) We present the first end-to-end trained deep network for dense cross-instance correspondence; 3) We demonstrate that the widely available 3D CAD models can be used for learning correspondence between 2D images of different object instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Cross-instance pairwise correspondence The classic SIFT Flow approach <ref type="bibr" target="#b25">[26]</ref> proposes an energy minimization framework that computes dense correspondence between different scenes by matching SIFT features <ref type="bibr" target="#b27">[28]</ref> regularized by smoothness and small displacement priors. Deformable Spatial Pyramid (DSP) Matching <ref type="bibr" target="#b21">[22]</ref>, a recent follow-up to SIFT Flow, greatly speeds up the inference while modestly improving the matching accuracy. Barnes et al. <ref type="bibr" target="#b4">[5]</ref> extend the original PatchMatch <ref type="bibr" target="#b3">[4]</ref> algorithm to allow more general-purpose (including cross-instance) matching. Bristow et al. <ref type="bibr" target="#b5">[6]</ref> build an exemplar-LDA classifier around each pixel, and aggregate the matching responses over all classifiers with additional smoothness priors to obtain dense correspondence estimation. In these same proceedings, Ham et al. <ref type="bibr" target="#b13">[14]</ref> take advantage of recent developments in object proposals, and utilize local and geometric consistency constraints among object proposals to establish dense semantic correspondence.</p><p>Collection correspondence Traditionally, correspondence has been defined in a pairwise manner, but recent works have tried to pose correspondence as the problem of joint image-set alignment. The classic like on work on Congealing <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16]</ref> uses sequential optimization to gradually lower the entropy of the intensity distribution of the entire image set by continuously warping each image via a parametric transformation (e.g. affine). RASL <ref type="bibr" target="#b31">[31]</ref>, Collection Flow <ref type="bibr" target="#b20">[21]</ref> and Mobahi et al. <ref type="bibr" target="#b28">[29]</ref> first estimate a low-rank subspace of the image collection, and then perform joint alignment among images projected onto the subspace. FlowWeb <ref type="bibr" target="#b40">[40]</ref> builds a fully-connected graph for the image collection with images as nodes and pairwise flow fields as edges, and establishes globally-consistent dense correspondences by maximizing the cycle consistency among all edges. While achieving state-of-the-art performance, FlowWeb is overly dependent on the initialization quality, and scales poorly with the size of the image collection. Similar to a recent work on joint 3D shape alignment <ref type="bibr" target="#b17">[18]</ref>, Zhou et al. <ref type="bibr" target="#b41">[41]</ref> tackle the problem by jointly optimizing feature matching and cycle consistency, but formulate it as a low-rank matrix recovery which they solve with a fast alternating minimization method. Virtual View Networks <ref type="bibr" target="#b6">[7]</ref> leverage annotated keypoints to infer dense correspondence between images connected in a viewpoint graph, and use this graph to align a query image to all the reference images in order to perform single-view 3D reconstruction. Cho et al. <ref type="bibr" target="#b8">[9]</ref> uses correspondence consistency among selective search windows in a diverse image collection to perform unsupervised object discovery.</p><p>Deep learning for correspondence Recently, several works have applied convolutional neural networks to learn same-instance dense correspondence. FlowNet <ref type="bibr" target="#b10">[11]</ref> learns an optical flow CNN with a synthetic Flying Chairs dataset that generalizes well to existing benchmark datasets, yet still falls a bit short of state-of-the-art optical methods like DeepFlow <ref type="bibr" target="#b36">[36]</ref> and EpicFlow <ref type="bibr" target="#b32">[32]</ref>. Several recent works have also used supervision from reconstructed 3D scene and stereo pairs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b1">2]</ref>. However all these approaches are inherently limited to matching images of the same physical object/scene. Long et al. <ref type="bibr" target="#b26">[27]</ref> used deep features learned from large-scale object classification tasks to perform intra-class image alignment, but found it to perform similarly to SIFT flow.</p><p>Image-shape correspondence Our work is partially motivated by recent progress in image-shape alignment that allows establishing correspondence between images through intermediate 3D shapes. Aubry et al. <ref type="bibr" target="#b2">[3]</ref> learns discriminative patches for matching 2D images to their corresponding 3D CAD models, while Peng et al. <ref type="bibr" target="#b30">[30]</ref> utilizes CAD models to train object detectors with few shots of labeled real images. In cases where depth data is available, deep learning methods have recently been applied to 3D object recognition and alignment between CAD models and RGB-D images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37]</ref>. Other works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">34]</ref> leverage image and shape collections for joint pose estimation and refining image-shape alignment, which are further applied to single-view object reconstruction and depth estimation. Although our approach requires 3D CAD models for constructing the training set, the image-shape alignment is jointly learned with image-image alignment, and no CAD models are required at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to predict a dense flow (or correspondence) field F a,b : R 2 → R 2 between pairs of images a and b. The flow field F a,b (p) = (p x −q x , p y −q y ) computes the relative offset from each point p in image a to a corresponding point q in image b. Given that pairwise correspondence might not always be well-defined (e.g. a side-view car and a frontalview car do not have many visible parts in common), we additionally compute a matchability map M a,b :</p><formula xml:id="formula_1">R 2 → [0, 1] predicting if a correspondence exists M a,b (p) = 1 or not M a,b (p) = 0.</formula><p>We learn both the flow field and the matchability prediction with a convolutional neural network. Both functions are differentiable with respect to the network parameters, which could be directly learned if we had dense annotations for F a,b and M a,b on a large set of real image pairs. However, in practice it is infeasible to obtain those annotations at scale as they are either too time-consuming or ambiguous to annotate.</p><p>We instead choose a different route, and learn both functions by placing the supervision on the desired properties of the ground-truth, i.e. while we do not know what the ground-truth is, we know how it should behave. In this paper, we use cycle consistency with 3D CAD models as the desired property that will be our supervisory signal. Specifically, for each pair of real training images r 1 and r 2 , we find a 3D CAD model of the same category, and render two synthetic views s 1 and s 2 in similar viewpoint as r 1 and r 2 , respectively (see Section 4.1 for more details). For each training quartet &lt; s 1 , s 2 , r 1 , r 2 &gt; we learn to predict flows from s 1 to r 1 (F s1,r1 ) to r 2 (F r1,r2 ) to s 2 (F r2,s2 ) that are cycle-consistent with the ground-truth flow from s 1 to s 2 (F s1,s2 ) provided by the rendering engine (similarly for the matchability prediction). By constructing consistency supervision through 3D CAD models, we aim to learn 2D image correspondences that potentially captures the 3D semantic appearance of the query objects. Furthermore, makingF s1,s2 be ground-truth by construction prevents the cycle-consistency optimization from producing trivial solutions, such as identity flows.</p><p>Sections 3.1 and 3.2 formally define our training objective for learning correspondence F and matchability M , respectively. Section 3.3 demonstrates how to obtain continuous approximation of discrete maps that allows end-to-end training. Section 3.4 describes our network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning dense correspondence</head><p>Given a set of training quartets {&lt; s 1 , s 2 , r 1 , r 2 &gt;}, we train the CNN to minimize the following objective:</p><formula xml:id="formula_2">&lt;s1,s2,r1,r2&gt; L f low F s1,s2 , F s1,r1 •F r1,r2 •F r2,s2 ,<label>(1)</label></formula><p>whereF s1,s2 refers to the ground-truth flow between two synthetic views, F s1,r1 , F r1,r2 and F r2,s2 are predictions made by the CNN along the transitive path. The transitive flow compositionF a,c = F a,b • F b,c is defined as</p><formula xml:id="formula_3">F a,c (p) = F a,b (p) + F b,c (p + F a,b (p)) ,<label>(2)</label></formula><p>which is differentiable as long as F a,b and F b,c are differentiable. L f low (F s1,s2 ,F s1,s2 ) denotes the truncated Euclidean loss defined as</p><formula xml:id="formula_4">L f low (F s1,s2 ,F s1,s2 ) = p|Ms 1 ,s 2 (p)=1 min( F s1,s2 (p) −F s1,s2 (p) 2 , T 2 ) ,</formula><p>whereM s1,s2 (p) is the ground-truth matchability map provided by the rendering engine (M s1,s2 (p) = 0 when p is either a background pixel or not visible in s 2 ), and T = 15 (pixels) for all our experiments. In practice, we found the truncated loss to be more robust to spurious outliers for training, especially during the early stage when the network output tends to be highly noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning dense matchability</head><p>Our training objective for matchability prediction also utilizes the cycle consistency signal: &lt;s1,s2,r1,r2&gt; Flow field Matchability <ref type="figure">Figure 2</ref>. Overview of our network architecture, which consists of three major components: 1) feature encoder on both input images, 2) flow decoder predicting the dense flow field from the source to the target image and 3) matchability decoder that outputs a probability map indicating whether each pixel in the source image has a correspondence in the target. See Section 3.4 for more details.</p><formula xml:id="formula_5">L mat M s1,s2 , M s1,r1 •M r1,r2 •M r2,s2 ,<label>(3)</label></formula><p>whereM s1,s2 refers to the ground-truth matchability map between the two synthetic views, M s1,r1 , M r1,r2 and M r2,s2 are CNN predictions along the transitive path, and L mat denotes per-pixel cross-entropy loss. The matchability map composition is defined as</p><formula xml:id="formula_6">M a,c (p) = M a,b (p)M b,c (p + F a,b (p)) ,<label>(4)</label></formula><p>where the composition depends on both the matchability as well as the flow field. Due to the multiplicative nature in matchability composition (as opposed to additive in flow composition), we found that training with objective 3 directly results in the network exploiting the clean background in synthetic images, which helps predict a perfect segmentation of the synthetic object in M s1,r1 . Once M s1,r1 predicts zero values for background points, the network has no incentive to correctly predict the matchability for background points in M r1,r2 , as the multiplicative composition has zero values regardless of the transitive predictions along M r1,r2 and M r2,s2 . To address this, we fix M s1,r1 = 1 and M r2,s2 = 1, and only train the CNN to infer M r1,r2 . This assumes that every pixel in s 1 (s 2 ) is matchable in r 1 (r 2 ), and allows the matchability learning to happen between real images. Note that this is still different from directly usingM s1,s2 as supervision for M r1,r2 as the matchability composition depends on the predicted flow field along the transitive path.</p><p>The matchability objective 3 is jointly optimized with the flow objective 1 during training, and our final objective can be written as &lt;s1,s2,r1,r2&gt; L f low + λL mat with λ = 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Continuous approximation of discrete maps</head><p>An implicit assumption made in our derivation of the transitive composition (Eq. 2 and 4) is that F and M are differentiable functions over continuous input, while images inherently consist of discrete pixel grids. To allow end-toend training with stochastic gradient descent (SGD), we ob-tain continuous approximation of the full flow field and the matchability map with bilinear interpolation over the CNN predictions on discrete pixel locations. Specifically, for each discrete pixel locationp ∈ {1, . . . , W } × {1, . . . , H}, the network predicts a flow vector F a,b (p) as well as a matchability score M a,b (p), and the approximation over all</p><formula xml:id="formula_7">continuous points p ∈ [1, W ] × [1, H] is obtained by: F a,b (p) = p∈Np (1 − |p x −p x |)(1 − |p y −p y |)F a,b (p) M a,b (p) = p∈Np (1 − |p x −p x |)(1 − |p y −p y |)M a,b (p) ,</formula><p>where N p denotes the four-neighbor pixels (top-left, topright, bottom-left, bottom-right) of point p, or just p if it is one of the discrete pixels. This is equivalent to the differentiable image sampling with a bilinear kernel proposed in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network architecture</head><p>Our network architecture (see <ref type="figure">Figure 2</ref>) follows the encoder-decoder design principle with three major components: 1) feature encoder of 8 convolution layers that extracts relevant features from both input images with shared network weights; 2) flow decoder of 9 fractionallystrided/up-sampling convolution (uconv) layers that assembles features from both input images, and outputs a dense flow field; 3) matchability decoder of 9 uconv layers that assembles features from both input images, and outputs a probability map indicating whether each pixel in the source image has a correspondence in the target.</p><p>All conv/uconv layers are followed by rectified linear units (ReLUs) except for the last uconv layer of either decoder, and the filter size is fixed to 3 × 3 throughout the whole network. No pooling layer is used, and the stride is 2 when increasing/decreasing the spatial dimension of the feature maps. The output of the matchability decoder is further passed to a sigmoid layer for normalization.</p><p>During training, we apply the same network to three different input pairs along the cycle (s 1 → r 1 , r 1 , → r 2 , and r 2 → s 2 ), and composite the output to optimize the consistency objectives 1 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>In this section, we describe the details of our network training procedure, and evaluate the performance of our network on correspondence and matchability tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training set construction</head><p>The 3D CAD models we used for constructing training quartets come from the ShapeNet database <ref type="bibr" target="#b0">[1]</ref>, while the real images are from the PASCAL3D+ dataset <ref type="bibr" target="#b38">[38]</ref>. For each object instance (cropped from the bounding box and rescaled to 128 × 128) in the train split of PASCAL3D+, we render all 3D models under the same camera viewpoint (provided by PASCAL3D+), and only use K = 20 nearest models as matches to the object instance based on the HOG <ref type="bibr" target="#b9">[10]</ref> Euclidean distance. We then construct training quartets each consisting of two real images (r 1 and r 2 ) matched to the same 3D model and their corresponding rendered views (s 1 and s 2 ). On average, the number of valid training quartets for each category is about 80, 000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network training</head><p>We train the network in a category-agnostic manner (i.e. a single network for all categories). We first initialize the network (feature encoder + flow decoder pathway) to mimic SIFT flow by randomly sampling image pairs from the training quartets and training the network to minimize the Euclidean loss between the network prediction and the SIFT flow output on the sampled pair 1 . Then we fine-tune the whole network end-to-end to minimize the consistency loss defined in Eq. 1 and 3. We use the ADAM solver <ref type="bibr" target="#b22">[23]</ref> with β 1 = 0.9, β 2 = 0.999, initial learning rate of 0.001, step size of 50, 000, step multiplier of 0.5 for 200, 000 iterations. We train with mini-batches of 40 image pairs during initialization and 10 quartets during fine-tuning.</p><p>We visualize the effect of our cycle-consistency training in <ref type="figure">Figure 3</ref>, where we sample some random points in the synthetic image s 1 , and plot their predicted correspondences along the cycle s 1 → r 1 → r 2 → s 2 to compare with the ground-truth in s 2 . One can see that the transitive trajectories become more and more cycle-consistent with more iterations of training, while individual correspondences along each edge of the cycle also tend to become more semantically plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature visualization</head><p>We visualize the features learned by the network using the t-SNE algorithm <ref type="bibr" target="#b35">[35]</ref>. Specifically, we extract conv-9 features (i.e. the output of the last encoder layer) from the entire set of car instances in the PASCAL3D+ dataset, and embed them in 2-D with the t-SNE algorithm. <ref type="figure">Figure 4</ref> visualizes the embedding. Interestingly, while our network is not explicitly trained to perform viewpoint estimation, the embedding layout appears to be viewpoint-sensitive, which implies that the network might implicitly learn that viewpoint is an important cue for correspondence/matchability tasks through our consistency training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Keypoint transfer</head><p>We evaluate the quality of our correspondence output using the keypoint transfer task on the 12 categories from PASCAL3D+ <ref type="bibr" target="#b38">[38]</ref>. For each category, we exhaustively sample all image pairs from the val split (not seen during training), and determine if a keypoint in the source image is transferred correctly by measuring the Euclidean distance between our correspondence prediction and the annotated ground-truth (if exists) in the target image. A correct transfer means the prediction falls within α · max(H, W ) pixels of the ground-truth with H and H being the image height and width, respectively (both are 128 pixels in our case). We compute the percentage of correct keypoint transfer (PCK) over all image pairs as the metric, and provide performance comparison for the following methods in <ref type="table">Table 1</ref>:</p><p>• SIFT flow <ref type="bibr" target="#b25">[26]</ref> -A classic method for dense correspondence using SIFT feature descriptors and handdesigned smoothness and large-displacement priors.</p><p>We also ran preliminary evaluation on a more recent follow-up based on deformable spatial pyramids <ref type="bibr" target="#b21">[22]</ref>, and found it to perform similarly to SIFT flow.</p><p>• Long et al. <ref type="bibr" target="#b26">[27]</ref> -Similar MRF energy minimization framework as SIFT flow but with deep features learned from the ImageNet classification task.</p><p>• CNN I2S -Our network trained on real image pairs with correspondence inferred by compositing the output of an off-the-shelf image-to-shape alignment algorithm <ref type="bibr" target="#b16">[17]</ref> and the ground-truth synthetic correspondence (i.e. obtaining direct supervision for F r1,r2 through F r1,s1 •F s1,s2 •F s2,r2 , where F r1,s1 and F s2,r2 are inferred from <ref type="bibr" target="#b16">[17]</ref>).</p><p>• CNN init -Our network trained to mimic SIFT flow.</p><p>• CNN init + Synthetic ft. -fine-tuning on synthetic image pairs with ground-truth correspondence after initialization with SIFT flow. <ref type="figure">Figure 3</ref>. Visualizing the effects of consistency training on the network output. The randomly sampled ground-truth correspondences between synthetic images are marked in solid lines, and the correspondence predictions along the cycle (synthetic to real, real to real and real to synthetic) made by our network are marked in dashed lines. One can see that the transitive composition of our network output becomes more and more consistent with the ground-truth as training progresses, while individual correspondences along each edge of the cycle also tend to become more semantically plausible. <ref type="figure">Figure 4</ref>. Conv-9 feature embedding for cars visualized by t-SNE <ref type="bibr" target="#b35">[35]</ref>. Interestingly, the overall layout seems to be mainly clustered based on the camera viewpoint, while the network is not explicitly trained to perform viewpoint estimation. This suggests that the network might implicitly learn that viewpoint is an important cue for the correspondence/matchability tasks through our consistency training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training iterations</head><p>• CNN init + Consistency ft. -fine-tuning with our objectives 1 and 3 after initialization with SIFT flow.</p><p>Overall, our consistency-supervised network significantly outperforms all other methods (except on "bicycle" and "motorbike" where SIFT flow has a slight advantage).</p><p>Notice the significant improvement over the initial network after consistency fine-tuning. The performance gap between the last two rows of <ref type="table">Table 1</ref> suggests that consistency supervision is much more effective in adapting to the real image domain than direct supervision from synthetic Source Target SIFTflow Ours init.</p><p>Ours final <ref type="figure">Figure 5</ref>. Comparison of keypoint transfer performance for different methods on example test image pairs. Overall, our consistencysupervised network (second-to-last row) is able to produce more accurate keypoint transfer results than the baselines. The last column shows a case when SIFT flow performs better than ours. outperforms all baselines (except on "bicycle" and "motorbike"). Notice the performance gap between our initialization (CNNinit) and the final network, which highlights the improvement made by cycle-consistency training.</p><p>ground-truth. <ref type="figure">Figure 5</ref> compares sample keypoint transfer results using different methods. In general, our final prediction tends to match the ground-truth much better than the other baselines, and could sometimes overcome substantial viewpoint and appearance variation where previous methods, like SIFT flow, are notoriously error-prone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Matchability prediction</head><p>We evaluate the performance of matchability prediction using the PASCAL-Part dataset <ref type="bibr" target="#b7">[8]</ref>, which provides humanannotated part segment labeling 2 . For each test image pair, a pixel in the source image is deemed matchable if there exists another pixel in the target image that shares the same part label, and all background pixels are unmatchable. We measure the performance by computing the percentage of pixels being classified correctly. For our method, we classify a pixel as matchable if its probability is &gt; 0.5 according to the network prediction. To obtain matchability prediction for SIFT flow, we compute the L 1 norm of the SIFT feature matching error for each source pixel after the alignment, and a pixel is predicted to be matchable if the error is below a certain threshold (we did grid search on the training set to determine the threshold, and found 1, 000 to perform the best). <ref type="table">Table 2</ref> compares the classification accuracy between our method and SIFT flow prediction (chance performance is 50%). Our method significantly outperforms SIFT flow on all categories except "bicycle" and "motorbike" (67.8% vs. 57.1% mean accuracy).</p><p>We visualize some examples of our matchability prediction in <ref type="figure">Figure 6</ref>. Notice how the prediction varies when the target image changes with the source image being the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Shape-to-image segmentation transfer</head><p>Although in this paper we are mostly interested in finding correspondence between real images, a nice byproduct of our consistency training is that the network also implicitly learns cross-domain, shape-to-image correspondence, which allows us to transfer per-pixel labels (e.g. surface normals, segmentation masks, etc.) from shapes to real images. As a proof of concept, we ran a toy experiment on the Source Our prediction Ground-truth Target <ref type="figure">Figure 6</ref>. Sample visualization of our matchability prediction. Notice how the prediction varies for the same source image when changing only the target image. The last two columns demonstrate a typical failure mode of our network having trouble localizing the fine boundaries of the matchable regions.  <ref type="figure">Figure 7</ref>. Visual comparison among different segmentation methods. From left to right: input query image, segmentation by <ref type="bibr" target="#b23">[24]</ref>, segmentation transferred using SIFT flow, segmentation transferred using our flow and the retrieved shape whose segmentation is used for transferring. See Section 4.6 for more details.</p><p>task of segmentation transfer. Specifically, we construct a shape database of about 200 shapes per category, with each shape being rendered in 8 canonical viewpoints. Given a query real image, we apply our network to predict the correspondence between the query and each rendered view of the same category, and warp the query image according to the predicted flow field. Then we compare the HOG Euclidean distance between the warped query and the rendered views, and retrieve the rendered view with minimum error whose correspondence to the query image on the foreground region is used for segmentation transfer. <ref type="figure">Figure 7</ref> shows sample segmentation using different methods. We can see that our learned flows tend to produce more accurate segmentation transfer than SIFT flow using the same pipeline. In some cases our output can even segment challenging parts such as the bars and wheels of the chairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this paper, we used cycle-consistency as a supervisory signal to learn dense cross-instance correspondences. Not only did we find that this kind of supervision is surprisingly effective, but also that the idea of learning with cycle-consistency could potentially be fairly general. One could apply the same idea to construct other training scenarios, as long as the ground-truth of one or more edges along the cycle is known. We hope that this work will inspire more efforts to tackle tasks with little or no direct labels by exploiting cycle consistency or other types of indirect or "meta-supervision".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1. Keypoint transfer accuracy measured in PCK (α = 0.1) on the PASCAL3D+ categories. Overall, our final network (last row)</figDesc><table>aero bike boat bottle 
bus 
car 
chair table mbike sofa train 
tv 
mean 

SIFT flow [26] 
9.8 
23.3 
8.9 
28.3 
28.6 22.4 10.8 13.2 17.9 
14.2 14.4 42.9 
19.6 
Long et al. [27] 
10.4 22.8 
7.6 
30.8 
28.4 21.1 10.2 12.7 
13.5 
12.9 12.6 38.5 
18.5 

CNN I2S 
9.1 
14.7 
5.2 
25.9 
25.4 23.7 11.9 11.3 
13.4 
16.8 11.3 45.2 
17.8 
CNN init . 
8.6 
20.3 
8.5 
29.4 
24.3 20.1 
9.9 
11.6 
15.4 
11.6 12.5 40.2 
17.7 
CNN init + Synthetic ft. 
10.2 22.2 
8.7 
30.4 
24.5 21.3 10.2 12.1 
15.7 
12.0 12.8 40.5 
18.4 
CNN init + Consistency ft. 11.3 22.3 10.1 40.3 40.3 33.3 15.0 13.2 
17.2 
17.4 16.7 51.1 24.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 2. Performance comparison of matchability prediction between SIFT flow and our method (higher is better). See Section 4.5 for more details on the experiment setup.</figDesc><table>aero bike boat bottle 
bus 
car 
chair table mbike sofa train 
tv 
mean 

SIFT flow [26] 66.2 62.7 49.5 
50.5 
52.0 64.5 50.7 50.5 
80.6 
49.6 58.5 50.2 
57.1 
Ours 
75.8 61.0 66.7 67.1 67.3 72.0 66.1 68.4 
68.0 
71.2 64.4 65.1 67.8 

Query 
Dense CRF 
SIFTflow 
Ours 
Ret. Shape 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also experimented with other initialization strategies (e.g. predicting ground-truth flows between synthetic images), and found that initializing with SIFT flow output works the best.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For categories without part labels, including boat, chair, table and sofa, we use the foreground segmentation mask instead.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Leonidas Guibas, Shubham Tulsiani, and Saurabh Gupta for helpful discussions. This work was sponsored in part by NSF/Intel VEC 1539099, ONR MURI N000141010934, and a hardware donation by NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.shapenet.org.1" />
		<title level="m">Shapenet</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Seeing 3d chairs: Exemplar part-based 2d-3d alignment using a large dataset of cad models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;14</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;14<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3762" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics-TOG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The generalized patchmatch correspondence algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dense semantic correspondence where every pixel is a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bristow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Virtual view networks for object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06825</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aligning 3D models to RGB-D images of cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised joint alignment of complex images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single-view reconstruction via joint analysis of image and shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Consistent shape maps via semidefinite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collection flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1792" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deformable spatial pyramid matching for fast dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<title level="m">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials. NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learned-Miller. Data driven image models through continuous joint alignment. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="250" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A compositional model for low-dimensional image set representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1322" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep object detectors from 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rasl: Robust alignment by sparse and low-rank decomposition for linearly correlated images. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2233" to="2246" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimating image depth using shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (Special issue of SIGGRAPH</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing highdimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2014 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Žbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.05970</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flowweb: Joint image set alignment by weaving consistent, pixel-wise correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-image matching via fast alternating minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
