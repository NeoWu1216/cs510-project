<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Action Recognition from Novel Viewpoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
							<email>hossein@csse.uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
							<email>ajmal.mian@uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Action Recognition from Novel Viewpoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a human pose representation model that transfers human poses acquired from different unknown views to a view-invariant high-level space. The model is a deep convolutional neural network and requires a large corpus of multiview training data which is very expensive to acquire. Therefore, we propose a method to generate this data by fitting synthetic 3D human models to real motion capture data and rendering the human poses from numerous viewpoints. While learning the CNN model, we do not use action labels but only the pose labels after clustering all training poses into k clusters. The proposed model is able to generalize to real depth images of unseen poses without the need for re-training or fine-tuning. Real depth videos are passed through the model frame-wise to extract viewinvariant features. For spatio-temporal representation, we propose group sparse Fourier Temporal Pyramid which robustly encodes the action specific most discriminative output features of the proposed human pose model. Experiments on two multiview and three single-view benchmark datasets show that the proposed method dramatically outperforms existing state-of-the-art in action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video based human action recognition is challenging because significant intra-action variations exist due to changes in viewpoint, illumination, visual appearance (such as color and texture of clothing), scale (due to different human body sizes or distances from the camera), background and speed of performing an action. Some challenges have been simplified by the use of real-time depth cameras (e.g. Kinect) that capture the texture and illumination invariant human body shape and simplify human segmentation. However, variations due to viewpoint remains a major challenge and is explicitly addressed in this paper.</p><p>Many methods <ref type="bibr">[23, 31, 34, 36-38, 44, 47, 48, 55-57, 63, 67, 68, 74]</ref> have been proposed which achieve impressive action recognition results when videos are acquired from a common viewpoint. However, their performance degrades sharply under viewpoint changes <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b58">60]</ref>. This is because the same human pose appears quite different when observed from different viewpoints. To cope with this problem, view-invariant approaches <ref type="bibr">[17, 39-42, 54, 59, 60]</ref> have been recently proposed for action recognition in videos acquired from novel views. Most of these methods operate on RGB videos <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b58">60]</ref> or skeleton data <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b57">59]</ref>. Joints extraction methods are inaccurate and sometimes fail when subject is not in the upright or frontal view position <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b62">64]</ref>. Moreover, view-invariant information can be more reliably extracted from depth videos <ref type="bibr" target="#b37">[39]</ref>. For instance, <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b38">40]</ref> have achieved higher accuracy by extracting view-invariant local spatio-temporal features from depth videos. However, their performance is limited by the discriminative power of the local features <ref type="bibr" target="#b58">[60]</ref>.</p><p>To overcome these drawbacks, we propose a depth video based cross-view action recognition method that consists of two main steps: (1) learning a general view-invariant human pose model from synthetic depth images, and (2) modeling the temporal action variations. The former is a deep CNN which represents different human body shapes and poses observed from numerous viewpoints in a viewinvariant high-level space. However, learning such a model requires a large corpus of training data containing a large number of human body poses observed from many viewpoints. Such data is not publicly available and is very expensive to acquire and label. Our solution is to generate the training data synthetically but in the most realistically possible way. To achieve this, we fit realistic synthetic 3D human models to real mocap data [2] and then render each pose from a large number of viewpoints as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>We learn a single model for all poses and views without using action labels and show that our model generalizes to real depth images of unseen human poses acquired from novel views without re-training or fine-tuning. Our learned model operates on a frame by frame basis transferring human pose in each frame to a high-level view-invariant representation. Our motivation for using a frame based CNN model comes from the findings <ref type="bibr" target="#b19">[21]</ref> that a single frame model performs equally well as the multiframe CNN model. Since actions are performed over a period of time, modeling the temporal structure of videos is performed in the next stage. Many methods <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b58">60]</ref> model the temporal variations of videos using optical flow. However, optical flow is not reliable in the presence of noise and lack of texture <ref type="bibr" target="#b32">[34]</ref> which is especially the case for depth videos <ref type="bibr" target="#b37">[39]</ref>. Moreover, in spatio-temporal matching, temporal misalignments can also become a source of errors. We propose a representation which is robust to depth noise and temporal misalignments. Our representation is a group sparse Fourier Temporal Pyramid that extracts features from the view-invariant high-level representation layer of the proposed CNN model. We capitalize on the fact that the output of different neurons in the CNN representation layer contributes differently to each human pose and hence each action. Thus, we learn action specific sparse neurons-sets for accurate classification. New action classes can be efficiently added to our framework as it requires retraining the action classifier only while using the same learned CNN model. Experiments on two benchmark multiview human action datasets i.e. Northwestern-UCLA Multiview Action3D <ref type="bibr" target="#b58">[60]</ref> and UWA3D Multiview Activity II <ref type="bibr" target="#b38">[40]</ref>, and comparison with state-of-the-art show that our method achieves 12% and 13% higher accuracies respectively than the nearest competitor (Section 6). To show that our method performs equally good in the single/known view case, we provide comparative results on three single-view benchmark human action datasets, MSR Gesture3D <ref type="bibr" target="#b55">[57]</ref>, MSR Action Pairs3D <ref type="bibr" target="#b32">[34]</ref> and MSR Daily Activity3D <ref type="bibr" target="#b56">[58]</ref> in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action recognition methods can be divided into three categories based on the type of video data i.e. RGB, skeleton or depth. This section discusses related work in each category as well as deep learning based methods.</p><p>RGB Videos: Some methods use view-invariant spatiotemporal features <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b60">62]</ref> and others infer the 3D scene structure through geometric transformations to achieve view invariance in RGB videos <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b67">69]</ref>. Recently, knowledge transfer based methods <ref type="bibr">[11, 12, 17, 27-29, 41, 60, 61, 66, 72, 73]</ref> have become popular that find a set of transformations in feature space such that the features extracted from different views are comparable. For example, Wang et al. <ref type="bibr" target="#b58">[60]</ref> proposed a cross-view video action representation by discovering the compositional structure in spatio-temporal patterns and geometrical relations among different views. They trained a spatio-temporal AND-OR graph structure by learning a separate linear transformation for each body part between different views. Thus, for action recognition from a novel view, all learned transformations are used for exhaustive matching and the results are combined with an AND-OR Graph. Skeleton Videos: Skeleton-based methods <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b63">65]</ref> generally use the human joint positions, extracted by the OpenNI tracking framework <ref type="bibr" target="#b48">[50]</ref>, as interest points. For example, Wang et al. <ref type="bibr" target="#b57">[59]</ref> proposed the histogram of occupancy pattern of a fixed region around each joint in each frame. They also proposed a data mining technique to discover the most discriminative joints for each action class. Vemulapalli et al. <ref type="bibr" target="#b52">[54]</ref> proposed a body part-based skeleton representation to model their relative geometry and modeled human actions as curves in the Lie group. For robustness to viewpoint variations, they rotate the skeletons such that the ground plane projection of the vector from left hip to right hip is parallel to the global x-axis. It is important to note that the human joints extraction methods (such as <ref type="bibr" target="#b48">[50]</ref>) are not accurate and sometimes fail when the human is not in the upright or frontal view position <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b62">64</ref>].</p><p>Depth Videos: Action recognition from depth videos has recently become more popular due to the availability of real-time cost-effective sensors. For instance, Oreifej and Liu <ref type="bibr" target="#b32">[34]</ref> proposed a histogram of oriented 4D normals (HON4D) for action recognition. Yang and Tian <ref type="bibr" target="#b65">[67]</ref> extended HON4D by concatenating the 4D normals in the local neighbourhood of each pixel as its descriptor. However, these descriptors must be extracted from interest points, e.g. joint positions, when the subjects significantly change their locations. To overcome this problem, Xia and Aggarwal <ref type="bibr" target="#b62">[64]</ref> proposed a method to filter the depth sensor noise and extract more reliable spatio-temporal interest points. However, their approach is sensitive to the speed of performing actions <ref type="bibr" target="#b37">[39]</ref>. Although, these methods achieve impressive accuracies for action recognition from a fixed view (mostly frontal), their performance drops sharply when recognition is performed on videos acquired from novel views <ref type="bibr" target="#b37">[39]</ref>. More recently, Rahmani et al. <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b38">40]</ref> proposed Histogram of Oriented Principal Components (HOPC) to first detect and then describe spatio-temporal interest points which are repeatable and robust to viewpoint variations. This method directly processes the 3D pointclouds and calculates the HOPC descriptor at every point.</p><p>Deep Learning Methods: Due to the impressive results of deep learning on image classification <ref type="bibr" target="#b23">[25]</ref> and object detection <ref type="bibr" target="#b12">[14]</ref>, several attempts have been recently made to train deep networks for action recognition <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b49">51]</ref>. Ji et al. <ref type="bibr" target="#b17">[19]</ref> proposed a deep 3D convolutional neural network (CNN) where convolutions are performed in 3D feature maps from spatial and temporal dimensions. However, Karapathy et al. <ref type="bibr" target="#b19">[21]</ref> show that the single-frame model performs equally well as the multiframes model. Simonyan and Zisserman <ref type="bibr" target="#b49">[51]</ref> trained two CNNs, one for RGB images and one for optical flow, to learn spatio-temporal features. Gkioxari and Malik <ref type="bibr" target="#b13">[15]</ref> extended this approach for action localization. Donahue et al. <ref type="bibr" target="#b7">[9]</ref> proposed an end-to-end trainable recurrent convolutional network which processes video frames with a CNN, whose outputs are passed through a recurrent neural network. These methods are not designed for cross-view action recognition in videos acquired from novel views. For crossview action recognition, Rahmani and Mian <ref type="bibr" target="#b39">[41]</ref> proposed a deep network which learns a set of non-linear transformations from multiple source views to a single canonical view. However, this method uses a fixed canonical view (frontal) as target view and learns the transfer model from hand-crafted features i.e. motion trajectories.</p><p>All the above deep models are designed for RGB videos and learning these models requires a large corpus of action video training data which is unavailable in the case of depth videos. Furthermore, motion trajectory and optical flow features, besides being hand crafted, are unreliable in the case of depth videos <ref type="bibr" target="#b32">[34]</ref>. These limitations motivate us to propose methods for learning a view-invariant human pose model and for reliable encoding of the temporal structure of depth videos for cross-view action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generating synthetic training data</head><p>We propose a pipeline (see <ref type="figure" target="#fig_0">Fig. 1</ref>) for generating synthetic depth images of different human body shapes in a large number of poses rendered from numerous viewing directions. Details of each step are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Building a pose dictionary</head><p>The set of all possible human body poses is extremely large. Therefore, we build a dictionary that contains the most representative ones. We use the CMU Motion Capture database [2] which contains over 2600 mocap sequences (over 200K poses) of subjects performing a variety of actions. The 3D joint positions in the dataset are quite accurate as they were captured using a high-precision camera array and body joint markers. However, many poses look similar. Using the skeletal distance function <ref type="bibr" target="#b47">[49]</ref>, we apply k-means clustering to 50K randomly selected mocap poses and select 339 representative ones to form a pose dictionary which is later used to generate synthetic depth images and train the CNN. Note that we do not use the action labels provided with the CMU mocap data [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Full 3D human body models</head><p>Bogo et al. <ref type="bibr" target="#b4">[6]</ref> developed the FAUST dataset containing full 3D human body scans of 10 individuals in 30 poses. However, skeleton data is not provided for the scans. Another way to generate 3D human model is to use the open source MakeHuman software <ref type="bibr" target="#b1">[3]</ref> which can generate different synthetic human shapes in a predefined pose and provide the joint positions which can be used for changing the human pose. We use this technique for generating the 3D human body models in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fitting 3D human models to mocap data</head><p>Several methods <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b28">30]</ref> have been proposed to fit a human model to motion capture skeleton data of a person. For instance, the SCAPE method <ref type="bibr" target="#b2">[4]</ref> learns pose and bodyshape deformation models from scans of different human bodies in a few poses. Given a set of markers, SCAPE constructs a full mesh which is consistent with the SCAPE models and best matches with the given markers. These methods aim to generate fine-grain human bodies in a variety of poses. However, real-time depth cameras generally have low resolution. Therefore, we use the open source Blender package <ref type="bibr" target="#b0">[1]</ref> to fit 3D human models to mocap data. Given a 3D human model generated by the MakeHuman software and a mocap frame, Blender normalizes the mo-cap skeleton with respect to the skeleton data of the human model and then fits the model to the normalized mocap data. This process results in a synthetic full 3D human body pose corresponding to the given mocap skeleton ( <ref type="figure" target="#fig_0">Fig. 1-(b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Rendering from multiple viewpoints</head><p>We deploy a total of 180 synthetic cameras (at distinct latitudes and longitudes) on a hemisphere surrounding the subject as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. For each camera, we remove self-occluded points. First, we perform back-face culling by removing points whose normals face away from the camera and then perform hidden point removal <ref type="bibr" target="#b20">[22]</ref> on the remaining points. <ref type="figure" target="#fig_0">Figure 1-(c)</ref> shows the full human model from two different views and <ref type="figure" target="#fig_0">Fig. 1-(d)</ref> shows the corresponding 3D pointclouds after removing the hidden points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Surface fitting</head><p>So far, we have generated 3D pointclouds of different 3D human models in different poses. To generate their corresponding depth images, we fit a surface of the form z♣x, yq to each 3D pointcloud using gridfit <ref type="bibr" target="#b6">[8]</ref> which approximates the 3D pointcloud as closely as possible. <ref type="figure" target="#fig_0">Figure 1</ref>-(e) shows two surfaces constructed using gridfit for two views of a human pose. The extrapolated points that do not belong to the human body are set to zero using a neighborhood test with the pointcloud that was used for surface fitting. The z values are normalized in the 0✁255 range to get the final depth image. <ref type="figure" target="#fig_0">Figure 1</ref>-(f) shows two depth images corresponding to the surfaces in <ref type="figure" target="#fig_0">Fig. 1</ref>-(e). It is worth mentioning that surface fitting is not required for real data at test time as real data is already in the form of depth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">View-invariant human pose representation</head><p>Realistic action videos lie on non-linear manifolds, especially when actions are captured from different views. However, most cross-view action recognition methods <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b70">72]</ref> represent the connection between action videos captured from two different views as a sequence of linear transformations of action descriptors. Moreover, such methods do not scale well to new action classes because they must repeat the computationally expensive model learning process. To overcome these problems, we propose a general view-invariant human pose representation model that learns to transfer human poses from any view to a shared view-invariant high-level space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model architecture and learning</head><p>Our proposed model is a deep convolutional neural network (CNN) whose architecture is similar to <ref type="bibr" target="#b16">[18]</ref> except that we replace the last fully-connected layer with a 339neurons layer. Let C♣k, n, sq denote a convolutional layer with kernel size k ✂ k, n filters and a stride of s, P ♣k, sq a max pooling layer of kernel size k ✂ k and stride s, N a normalization layer, RL a rectified linear unit, F C♣nq a fully connected layer with n filters and D♣rq a dropout layer with dropout ratio r. The architecture of our CNN follows:</p><formula xml:id="formula_0">C♣11, 96, 4q Ñ RL Ñ P ♣3, 2q Ñ N Ñ C♣5, 256, 1q Ñ RL Ñ P ♣3, 2q Ñ N Ñ C♣3, 384, 1q Ñ RL Ñ C♣3, 384, 1q Ñ RL Ñ C♣3, 256, 1q Ñ RL Ñ P ♣3, 2q Ñ F C♣4096q Ñ RL Ñ D♣0.5q Ñ F C♣4096q Ñ RL Ñ D♣0.5q Ñ F C♣339q.</formula><p>We refer to the fully-connected layers as f c 6 , f c 7 , and f c 8 , respectively. During learning, a softmax loss layer is added at the end of the network.</p><p>For each pose i ✏ 1, ☎ ☎ ☎ , 339 in the dictionary, the corresponding synthetic depth images from all 180 synthetic cameras are generated using our proposed pipeline and assigned the same class label i. Thus, our training dataset consists of 339 human pose classes. We use the synthetic depth images from 162 randomly selected cameras as the training set and those from the remaining 18 cameras as the validation set. Proper initialization is a key for successful training of CNNs and for avoiding over-fitting. We initialize the CNN with a model that was trained on approximately 1.2 million RGB images from the 2012 ImageNet challenge and then fine-tuned on depth images from NYUD2 <ref type="bibr" target="#b16">[18]</ref>. We train our CNN with back-propagation and use an initial learning rate of 0.01 for the convolution layers and 0.01 for the fully-connected layers. We use a momentum of 0.9 and a weight decay of 0.0005. We train the network for 21K iterations. During training, the input images are flipped horizontally with a probability of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inference</head><p>So far, we have learned a deep CNN model whose input is a human pose depth image and output is the corresponding pose class. The proposed CNN is able to classify only 339 pose classes which do not cover all possible human poses. However, the fully-connected layers (e.g. f c 6 and f c 7 ) of the learned model encode the view-invariant highlevel representation of human poses. To use this model for extracting view-invariant features from real depth videos, we perform the following two steps. Pre-processing: The synthetic depth dataset used for training the proposed CNN model contains depth images of only human body poses. Therefore, for extracting features from a real depth image, we pass the segmented human body image through our learned model. Fortunately, the Kinect camera is able to discern the human body from the rest of the scene and provide a segmented image (i.e. human body) in real-time (see <ref type="figure" target="#fig_5">Fig. 4</ref>). The segmented depth image is then cropped to the bounds of the region of interest i.e. human body, and converted to a form that is compatible with the learned CNN model. More precisely, the depth values of the region of interest are normalized in the range 0 ✁255 and the image is resized to 227✂227. The average depth image calculated from training images is then subtracted from it. In case human body segmentations are not available, our method still achieves state-of-the-art recognition accuracy e.g. see our result on the MSR Action Pairs3D <ref type="bibr" target="#b32">[34]</ref> dataset in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Temporal modeling and classification</head><p>To represent an action sequence with our CNN model, we feed forward the depth images sequentially through the network and temporally align the f c 7 layer features. Recall that no surface fitting is required for real depth images. Depth images captured by low cost real-time cameras have high levels of noise <ref type="bibr" target="#b62">[64]</ref>. Moreover, the correct region of interest, containing only the human, extracted in real-time by Kinect cameras is not always accurate and may contain some parts of the background as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. Finally, to match two video segments, they must be temporally aligned. Therefore, we need a representation that is robust to noisy depth images, inaccurate segmentations and temporal misalignments between different video segments.</p><p>The Fourier Temporal Pyramid (FTP) <ref type="bibr" target="#b57">[59]</ref> is shown to be successful for encoding temporal variations of noisy data. We employ the FTP representation since it is robust to noise and temporal misalignments. In addition to the global Fourier coefficients, we recursively partition the actions into a pyramid, and use the short time Fourier transform for all the segments to better capture the temporal structure of the action videos. The final action video descriptor is the concatenation of the Fourier coefficients from all the segments. We apply the Short Fourier Transform <ref type="bibr" target="#b31">[33]</ref> to B i j ✏ rB i j,1 B i j,2 ☎ ☎ ☎ B i j,f s and keep the first q low frequency coefficients. Next, we divide B i j into two segments and apply the Short Fourier Transform again to each individual segment to obtain its low frequency coefficients. We repeat this process l times and compute a Fourier Temporal Pyramid descriptor, A i j , for each neuron j by concatenating the low-frequency coefficients at all levels of the pyramid.</p><p>Thus A j R γ where γ ✏ 2 l ✂ q. We refer to the concatenated descriptor A i ✏ rA i</p><formula xml:id="formula_1">1 A i 2 ☎ ☎ ☎ A i j ☎ ☎ ☎ A i m s ❏ as the</formula><p>spatio-temporal features for the i-th video sample. Each neuron in the fully-connected f c 7 layer contributes differently to different pose classes and hence, different actions. This is because each neuron in the f c 7 layer is connected to the penultimate layer, f c 8 , with different weights. We define a neurons-set as a conjunction of neurons whose outputs are more discriminative for a particular action. If a neuron is considered for a particular action, then all its output FTP features must be selected and if a neuron is not selected then all its output features must be discarded. We discover the discriminative neurons-sets by solving an ℓ 1 ④ℓ 2 -norm regularized least squares problem <ref type="bibr" target="#b68">[70]</ref>:</p><formula xml:id="formula_2">min X 1 2 ⑤⑤AX ✁ Y ⑤⑤ 2 2 λ m ➳ j✏1 ⑤⑤X Gj ⑤⑤ 2 ,<label>(1)</label></formula><formula xml:id="formula_3">where A ✏ rA 1 A 2 ☎ ☎ ☎ A n s ❏ R n✂v , Y R n✂1 , X R v✂1 is divided into m non-overlapping groups X G1 , X G2 , ☎ ☎ ☎ , X Gm , and v ✏ γ ✂ 4096 denotes the di-</formula><p>mension of feature vector of each video sample. Such a solution incorporates a grouping structure by inducing sparsity at the neuron level and smoothness in the individual neuron output feature vector A j . We solve this optimization function using the one-vs-all strategy for all action classes which gives us a sparse discriminative neurons-set for each action class. This process also reduces the complexity of the action specific classifiers and leads to better generalization of the learning <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b30">32]</ref>. <ref type="figure" target="#fig_4">Figure 3</ref> shows an overview of the proposed temporal modeling and classification method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluated our proposed algorithm on two multiview and three single-view benchmark datasets. The former includes the Northwestern-UCLA Multiview Action3D <ref type="bibr" target="#b58">[60]</ref> and UWA3D Multiview Activity II <ref type="bibr" target="#b38">[40]</ref> datasets whereas the latter includes MSR Action Pairs3D <ref type="bibr" target="#b32">[34]</ref>, MSR Daily Activity3D <ref type="bibr" target="#b56">[58]</ref>, and MSR Gesture3D <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b55">57]</ref> datasets. The results on single-view datasets are provided in the supplementary material.</p><p>We report action recognition results of our method for unseen (novel) and unknown views, i.e. we assume that no videos, labels or correspondences from the target view are available at training time. More importantly, we use the same CNN model, learned from synthetic data, for all five datasets to show the generalization strength of our model and to show that our model can be applied to any depth action video without the need for re-training or fine-tuning. We used the MatConvNet toolbox <ref type="bibr" target="#b51">[53]</ref> for implementing convolutional neural networks. In our experiments, we set the number of Fourier Pyramid levels l ✏ 3, and the number  From here on, we refer to our proposed view-invariant human pose representation model (Section 4) and our proposed temporal modeling (Section 5) as HPM and TM, respectively. In addition to other compared methods, we report the accuracy of our defined baseline method which uses a similar approach to <ref type="bibr" target="#b13">[15]</ref> but with the CNN model that was fine-tuned on depth images from NYUD2 <ref type="bibr" target="#b16">[18]</ref>. We report the recognition accuracy of our method in two different settings: (1) HPM where we apply average pooling on the CNN features of all frames of a video to obtain its representation, and (2) HPM+TM where we employ the proposed temporal modeling approach on the CNN features to capture the temporal structure of the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Northwestern-UCLA dataset [60]</head><p>This dataset contains RGB, depth and human skeleton data captured simultaneously by 3 Kinect cameras from different views. It consists of 10 action classes including: (1) pick up with one hand, (2) pick up with two hands, (3) drop trash, (4) walk around, (5) sit down, (6) stand up, (7) donning, (8) doffing, (9) throw, and (10) carry. Each action was performed by 10 subjects 1 to 6 times. <ref type="figure" target="#fig_6">Fig. 5</ref> shows sample depth images of four actions captured by the three cameras.</p><p>We follow <ref type="bibr" target="#b58">[60]</ref> and use the samples from the first two cameras for training and the samples from the third camera 1 http://www.csse.uwa.edu.au/✒ajmal/code.html for testing. Comparative results are shown in <ref type="table" target="#tab_0">Table 1</ref>. The recognition accuracy of the proposed method in the first setting (HPM) significantly outperforms our defined baseline and all existing methods excluding HOPC <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b38">40]</ref>. This demonstrates the effectiveness of the proposed training approach. However, average pooling is unable to fully encode the temporal structure of actions. Combining our temporal modeling algorithm with the HPM significantly improves the recognition accuracy by 14% and achieves 92.0% accuracy. Moreover, it dramatically outperforms state-of-the-art methods irrespective of the modality they use. <ref type="figure" target="#fig_7">Figure 6</ref> compares the action specific recognition accuracies of our method in the two settings. The proposed temporal modeling (HPM+TM) achieves significantly higher accuracies than average pooling (HPM) for most action classes. The recognition accuracies of the stand up and sit down actions significantly improve, because these actions result in similar descriptors through average pooling.</p><p>It is important to emphasize that the proposed viewinvariant pose model was learned from synthetic depth images generated from a small number of human poses, i.e. size of the pose dictionary was 339. A search for many human poses such as drop trash, donning and doffing from the Northwestern-UCLA dataset returns no results in the pose dictionary or mocap data. Moreover, some activities in this dataset (e.g. donning, doffing, carry) involve human-object interactions. Yet, the proposed model is able to achieve high recognition accuracies for these actions. Input: RGB images AOG <ref type="bibr" target="#b58">[60]</ref> 73.3 Action Tube <ref type="bibr" target="#b13">[15]</ref> 61.5 LRCN <ref type="bibr" target="#b7">[9]</ref> 64.7 NKTM <ref type="bibr" target="#b39">[41]</ref> 75.8 Input: Depth images+Skeleton data Actionlet <ref type="bibr" target="#b57">[59]</ref> 76.0 LARP <ref type="bibr" target="#b52">[54]</ref> 74.2 Input: Depth images CCD <ref type="bibr" target="#b5">[7]</ref> 34.4 DVV <ref type="bibr" target="#b25">[27]</ref> 52.1 CVP <ref type="bibr" target="#b70">[72]</ref> 53.5 HON4D <ref type="bibr" target="#b32">[34]</ref> 39.9 SNV <ref type="bibr" target="#b65">[67]</ref> 42.8 HOPC <ref type="bibr" target="#b37">[39]</ref> 80 6.2. UWA3DII dataset <ref type="bibr" target="#b38">[40]</ref> This dataset consists of 30 human actions performed by 10 subjects with different scales: (1) one hand waving, (2) one hand Punching, (3) two hand waving, (4) two hand punching, (5) sitting down, (6) standing up, (7) vibrating, <ref type="bibr" target="#b6">(8)</ref> falling down, <ref type="bibr" target="#b7">(9)</ref> holding chest, (10) holding head, <ref type="bibr" target="#b9">(11)</ref> holding back, <ref type="bibr" target="#b10">(12)</ref> walking, (13) irregular walking, <ref type="bibr" target="#b12">(14)</ref> lying down, <ref type="bibr" target="#b13">(15)</ref> turning around, <ref type="bibr" target="#b14">(16)</ref> drinking, <ref type="bibr" target="#b15">(17)</ref> phone answering, <ref type="bibr" target="#b16">(18)</ref> bending, <ref type="bibr" target="#b17">(19)</ref> jumping jack, <ref type="bibr" target="#b18">(20)</ref> running, <ref type="bibr" target="#b19">(21)</ref> picking up, <ref type="bibr" target="#b20">(22)</ref> putting down, <ref type="bibr" target="#b21">(23)</ref> kicking, <ref type="bibr" target="#b22">(24)</ref> jumping, <ref type="bibr" target="#b23">(25)</ref> dancing, <ref type="bibr" target="#b24">(26)</ref> moping floor, <ref type="bibr" target="#b25">(27)</ref> sneezing, (28) sitting down (chair), <ref type="bibr" target="#b27">(29)</ref> squatting, and (30) coughing. Each subject performed 30 actions 4 times. Each time the action was captured from a different viewpoint (front, top, left and right). This dataset is challenging because the videos were acquired at different times from varying viewpoints and the data contains self-occlusions, more action classes and high similarity across action classes. Moreover, in the top view, the lower part of the body was not properly captured because of occlusion. <ref type="figure" target="#fig_8">Figure 7</ref> shows sample depth images of four actions observed from the four viewpoints.</p><p>We follow <ref type="bibr" target="#b38">[40]</ref> and use videos from two views for training and videos from the remaining views as test data. <ref type="table" target="#tab_1">Table 2</ref> summarizes our results. Our HPM significantly outperforms the state-of-the-art methods excluding NKTM <ref type="bibr" target="#b39">[41]</ref> on all view pairs. However, NKTM <ref type="bibr" target="#b39">[41]</ref> must extract hand-crafted dense motion trajectories prior to using the model. The combination of HPM and our proposed temporal modeling (HPM+TM) dramatically improves the average recognition accuracy to 76.9% which is over 13.4% higher than the nearest competitor (NKTM). It is interesting to note that our method achieves 76.5% average recognition accuracy when view 4 is used as the test view. As shown in <ref type="figure" target="#fig_8">Fig. 7</ref>, view 4 is the top view where the lower part of the body was not properly captured by the videos. <ref type="figure" target="#fig_9">Figure 8</ref> compares the class specific action recognition accuracies of our proposed method in the two settings. HPM+TM achieves significantly higher accuracies than using average pooling for most action classes. The recognition accuracies of the stand up and sit down actions dramatically improve which again demonstrates the effectiveness of our proposed temporal modeling method.</p><p>It is important to emphasize that for many human poses in the UWA3DII dataset such as two hand waving, holding chest, holding head, holding back, sneezing and coughing, a similar pose does not exist in the CMU mocap data and hence the pose dictionary used to learn our model. However, our method still achieves high recognition accuracies for these actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Computation time</head><p>Our model can be used in real-time applications as it does not involve complex feature processing or computationally expensive training and testing phases. With a Matlab implementation, our method can process 25 frames per second on a 3.4GHz machine with 24GB RAM. The nearest competitor, in terms of accuracy, HOPC <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b38">40]</ref> is 50 times slower than our method. <ref type="table" target="#tab_2">Table 3</ref> compares the speed of our  method to the nearest competitors from each modality.</p><p>It is interesting to note that our technique outperforms the current state-of-the-art on both cross-view datasets while using the same CNN model learned from synthetic data. This shows the generalization ability of our CNN model and its ability to be deployed for online action recognition because the cost of adding a new action class is equal to training the action specific classifiers. On the other hand, adding more action classes is computationally expensive for existing techniques <ref type="bibr">[17, 39-41, 54, 59, 60]</ref>. NKTM <ref type="bibr" target="#b39">[41]</ref> must extract computationally expensive motion trajectories. LARP <ref type="bibr" target="#b58">[60]</ref> requires to compute a nominal curve for the new action and warp all the training curves to this nominal curve using DTW. Similarly, HOPC <ref type="bibr" target="#b58">[60]</ref> computes computationally expensive spatio-temporal features.  <ref type="bibr" target="#b52">[54]</ref> 0.1 fps 10 fps HOPC <ref type="bibr" target="#b37">[39]</ref> 0.04 fps 0.5 fps Ours 22 fps 25 fps</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed a deep CNN model that represents depth images of different human poses acquired from multiple views in a view-invariant high-level space. To train the model, we proposed a framework for generating a large corpus of training data synthetically by fitting realistic human models to real mocap data and rendering it from multiple viewpoints. We also introduced a temporal modeling and classification method which encodes the temporal structures of actions and discovers a discriminative set of neurons corresponding to each action class. The proposed method is scalable as it requires to be trained only once using synthetic depth images and generalizes well to real data. Experiments on benchmark multiview datasets show that the proposed approach outperforms existing state-of-the-art. Our method performs equally well on single-view benchmark datasets (see supplementary material) and generalizes to hand gestures even though the CNN model was trained on full human body poses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Proposed pipeline for generating synthetic depth images. (a)-(b) A 3D human body is fitted to each mocap skeleton, (c) rendered from 180 different viewing directions (seeFig. 2), (d) processed for hidden point removal, (e) fitted with smooth surfaces<ref type="bibr" target="#b6">[8]</ref> and finally (f) processed for removal of extrapolated points and normalized in the 0 ✁ 255 range to generate depth images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Each point on the hemisphere corresponds to a virtual camera looking towards the center of the sphere.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Feature extraction: For each depth video frame, viewinvariant features are computed by forward propagating the mean-subtracted 227 ✂ 227 depth image through the CNN and the outputs of the f c 7 layer are used as the viewinvariant frame descriptor. Our experiments show that using the outputs of this layer achieves better recognition accuracy than f c 6 (see supplementary material for results).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Let f denote the number of frames in a given action video and m ✏ 4096 the number of neurons in the fullyconnected f c 7 layer of the proposed model. Let us denote each neuron output of the i-th video sample by B i j,t , where j ✏ 1 . . . m is the neuron number and t ✏ 1 . . . f is the frame number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the proposed temporal modeling and classification. Video frames are individually passed through the CNN model and the Fourier Temporal Pyramid features are extracted from the time series of each neuron output of the CNN representation layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualizing the data and segmentation noise. Column wise: Raw depth images from Kinect; after background removal by Kinect (the green pixels); after background removal; the normalized depth image which feeds to our proposed model. of low frequency Fourier coefficients q ✏ 4 using crossvalidation on training samples. The learned CNN model and MATLAB code of our method are freely available 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Sample depth images from the Northwestern-UCLA dataset [60] captured simultaneously by 3 Kinect cameras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Class specific action recognition accuracies of our proposed method in two settings: 1) HPM and 2) HPM+TM on the Northwestern-UCLA Multiview Action3D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Sample depth images from the UWA3D Multiview Ac-tivityII dataset<ref type="bibr" target="#b38">[40]</ref> captured by one camera from 4 different views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Per class recognition accuracy of the proposed HPM and HPM+TM on the UWA3D Multiview ActivityII dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Comparison of action recognition accuracy (%) on the</figDesc><table>Northwestern-UCLA Multiview Action3D dataset. 
Method 
Recognition accuracy(%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison of action recognition accuracy (%) on the UWA3D Multiview ActivityII dataset. Each time two views are used for training and the remaining two views are individually used for testing. Ours (HPM+TM) 80.6 80.5 75.2 82.0 65.4 72.0 77.3 67.0 83.6 81.0 83.6 74.1 76.9</figDesc><table>Training views 
V1 &amp; V2 
V1 &amp; V3 
V1 &amp; V4 
V2 &amp; V3 
V2 &amp; V4 
V3 &amp; V4 
Mean 
Test view 
V3 
V4 
V2 
V4 
V2 
V3 
V1 
V4 
V1 
V3 
V1 
V2 

Input: RGB images 
AOG [60] 
47.3 39.7 43.0 30.5 35.0 42.2 50.7 28.6 51.0 43.2 51.6 44.2 
42.3 
Action Tube [15] 
49.1 18.2 39.6 17.8 35.1 39.0 52.0 15.2 47.2 44.6 49.1 36.9 
37.0 
LRCN [9] 
53.9 20.6 43.6 18.6 37.2 43.6 56.0 20.0 50.5 44.8 53.3 41.6 
40.3 
NKTM [41] 
60.1 61.3 57.1 65.1 61.6 66.8 70.6 59.5 73.2 59.3 72.5 54.5 
63.5 
Input: Depth images+Skeleton data 
Actionlet [59] 
45.0 40.4 35.1 36.9 34.7 36.0 49.5 29.3 57.1 35.4 49.0 29.3 
39.8 
LARP [54] 
49.4 42.8 34.6 39.7 38.1 44.8 53.3 33.5 53.6 41.2 56.7 32.6 
43.4 
Input: Depth images 
CCD [7] 
10.5 13.6 10.3 12.8 11.1 
8.3 
10.0 
7.7 
13.1 13.0 12.9 10.8 
11.2 
DVV [27] 
23.5 25.9 23.6 26.9 22.3 20.2 22.1 24.5 24.9 23.1 28.3 23.8 
24.1 
CVP [72] 
25.0 25.6 25.5 28.2 24.7 24.0 23.0 24.5 26.6 23.3 30.3 26.8 
25.6 
HON4D [34] 
31.1 23.0 21.9 10.0 36.6 32.6 47.0 22.7 36.6 16.5 41.4 26.8 
28.9 
SNV [67] 
31.9 25.7 23.0 13.1 38.4 34.0 43.3 24.2 36.9 20.3 38.6 29.0 
29.9 
HOPC [39] 
52.7 51.8 59.0 57.5 42.8 44.2 58.1 38.4 63.2 43.8 66.3 48.0 
52.2 
baseline 
53.1 47.3 50.1 49.2 35.5 42.3 52.2 31.6 65.2 51.6 67.8 50.9 
49.7 
Ours (HPM) 
71.3 58.4 58.3 64.4 38.7 51.5 58.0 42.7 69.5 64.6 71.7 57.1 
58.9 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Average computation speed (fps: frames per second).On-line training speed is that of adding a new action class.</figDesc><table>Method 
On-line training 
Testing 

NKTM [41] 
12 fps 
16 fps 
LARP </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment We thank the authors of <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b65">67]</ref> for making their codes publicly available. We thank NVIDIA for their K40 GPU donation. This research was supported by ARC grant DP110102399 and DP160101458.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Blender: a 3D modelling and rendering package</title>
		<ptr target="http://www.blender.org/.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">MakeHuman: an open source 3D computer graphics software</title>
		<ptr target="http://www.makehuman.org/.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SCAPE: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FAUST: Dataset and evaluation for 3D mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human daily action analysis with multi-view and color-depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Surface fitting using gridfit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>D&amp;apos;errico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MATLAB Central File Exchange</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize activities from the wrong view point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tabrizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A latent model of discriminative aspect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tabrizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D model-based tracking of humans in action: a multi-view approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D pose from motion for cross-view action recognition via nonlinear circulant temporal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">View-independent action recognition from temporal selfsimilarities. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Direct visibility of point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinear heterogeneous information machine for RGB-D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse multinomial logistic regression: fast algorithms and generalization bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A real time system for dynamic hand gesture recognition with a depth sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUSIPCO</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative virtual views for crossview action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via view knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via view knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipersy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MoSh: motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structured sparsity and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Discrete time signal processing. Prentice Hall Signal Processing Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Buck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">HON4D: histogram of oriented 4D normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">View invariance for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminative human action classification using localityconstrained linear coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action classification with locality-constrained linear coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real time action recognition using histograms of depth gradients and random decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">HOPC: Histogram of oriented principal components of 3D pointclouds for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Histogram of oriented principal components for cross-view action recognition. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning a deep model for human action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.00828</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">View-invariant representation and recognition of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep multimodal feature analysis for action recognition in RGB+D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multimodal multipart learning for action recognition in depth videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-modal feature fusion for action recognition in RGB-D sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Communications, Control and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning Task-Specific Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action recognition from arbitrary views using 3D exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">MatConvNet -Convolutional Neural Networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3D skeletons as points in a Lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Robust 3D action recognition with random occupancy patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning actionlet ensemble for 3D human action recognition. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Making action recognition robust to occlusions and viewpoint changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Özuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Free viewpoint action recognition using motion history volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Action recognition in videos acquired by a moving camera using motion decomposition of Lagrangian particle trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Spatio-temporal depth cuboid similarity feature for activity recongition using depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning 4D action feature models for arbitrary view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Super normal vector for activity recognition using depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Recognizing actions using depth motion maps-based histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Action sketch: a novel action representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via a continuous virtual path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Crossview action recognition via a transferable dictionary pair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Interaction part mining: A mid-level approach for fine-grained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
