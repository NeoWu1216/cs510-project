<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Occlusion-free Face Alignment: Deep Regression Networks Coupled with De-corrupt AutoEncoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
							<email>jie.zhang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
							<email>meina.kan@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<email>shiguang.shan@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xilin.chen@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Occlusion-free Face Alignment: Deep Regression Networks Coupled with De-corrupt AutoEncoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face alignment or facial landmark detection plays an important role in many computer vision applications, e.g., face recognition, facial expression recognition, face animation, etc. However, the performance of face alignment system degenerates severely when occlusions occur. In this work, we propose a novel face alignment method, which cascades several Deep Regression networks coupled with De-corrupt Autoencoders (denoted as DRDA) to explicitly handle partial occlusion problem. Different from the previous works that can only detect occlusions and discard the occluded parts, our proposed de-corrupt autoencoder network can automatically recover the genuine appearance for the occluded parts and the recovered parts can be leveraged together with those non-occluded parts for more accurate alignment. By coupling de-corrupt autoencoders with deep regression networks, a deep alignment model robust to partial occlusions is achieved. Besides, our method can localize occluded regions rather than merely predict whether the landmarks are occluded. Experiments on two challenging occluded face datasets demonstrate that our method significantly outperforms the state-of-the-art methods.</p><p>Although the above methods have a certain tolerance of partial occlusion, the occlusion problem is not essentially solved. In this work, we propose a novel Deep Regression network coupled with De-corrupt Autoencoders (DRDA) to explicitly tackle the partial occlusion problem. As illustrat-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face alignment or facial landmark detection, as an fundamental problem in computer vision, is widely used in face recognition, 3D face modeling, face animation, etc. For decades, many efforts are devoted to exploring robust alignment methods and great progresses have been achieved on face alignment under control condition or even in the wild <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref>. However, it is still a challenging problem due to various variations in appearance and shape, e.g., pose, expression, especially partial occlusions. The alignment system usually degenerates severely under partial occlusions, and this problem is hard to tackle as any part of the face can be occluded by arbitrary objects.</p><p>Typical alignment models like Active Shape Models (ASMs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> and Active Appearance Models (AAMs) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref> employ Principal Component Analysis (PCA) to build a shape model or simultaneously establish shape and appearance models. These methods are sensitive to partial occlusions although the shape deformations are restricted by a PCA-based shape model.</p><p>Regression based methods have achieved impressive results on face alignment in the wild <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>. Different from the parametric models ASM and AAM, these methods directly model the mapping from local features to the face shape with linear <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b23">24]</ref> or deep architecture <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Especially, deep models (e.g., Convolutional Neural Networks, Auto-encoders and Restricted Boltzmann Machines) make great progresses benefited from their favorable ability of modeling nonlinearity. Sun et al. <ref type="bibr" target="#b27">[28]</ref> conduct a deep convolutional neural network to regress facial points by taking the holistic face as input and then cascade several convolutional neural networks to further refine the detection results within each local patch, which is extracted around the current face shape. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> design deep auto-encoder networks with joint shape-indexed features to regress the face shape. These methods achieve promising results on LFPW <ref type="bibr" target="#b2">[3]</ref> and HE-LEN <ref type="bibr" target="#b19">[20]</ref>. Benefited from the local features, these methods are somewhat robust to occlusions. However, they may still fail when severe partial occlusion occurs as they do not exclusively consider the occlusion problem. <ref type="figure">Figure 1</ref>. Overview of our DRDA for occlusion-free face alignment. F denotes the deep regression network which characterize the nonliner mapping from shape-indexed feature ϕ(x,S) to a face shape, x is an input face andS is the initial face shape. Gi denotes the de-corrupt autoencoder for recovering from the occluded parts to achieve de-corrupted face imagex, which can be leveraged in the following deep alignment model. Several stages are cascaded for both deep regression networks and de-corrupt autoencoders. DR is the abbreviation of deep regression networks and DA is the abbreviation of de-corrupt autoencoders. ed in <ref type="figure">Figure 1</ref>, several deep regression networks are cascaded to characterize the complex nonlinear mapping from appearance to shape. The de-corrupt autoencoders are exploited to recover the occluded parts automatically. To recover a face with more appearance details under various poses and expressions, we partition the face into several components and establish one de-corrupt autoencoder for each of them. The de-corrupted image is then fed to the following deep regression networks for alignment task. By jointly learning de-corrupt autoencoders and deep regression networks, a deep alignment model robust to partial occlusions is attained. Moreover, different from previous works which focus on predicting whether the landmarks are occluded, our method can localize occluded regions rather than merely predict whether the landmarks are occluded.</p><p>The contributions are summarized as follows:</p><p>1. We present a novel face alignment method which can explicitly handle partial occlusion problem.</p><p>2. Different from the previous works that can only detect occlusions and discard the occluded parts, our proposed de-corrupt autoencoder network can automatically recover the occluded parts and the recovered parts can be leveraged together with those non-occluded parts, which leads to better alignment results.</p><p>3. Our method achieves the state-of-the-art results on the challenging datasets OCFW and COFW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Partial occlusions is a great challenge for face alignment. To improve the robustness to occlusions, many researches focus on implicitly or explicitly handling occlusions problem.</p><p>Most existing alignment methods achieve robustness to occlusions either by introducing shape constrains <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref> or improving appearance representations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>. For the first category, shape models are usually built to restrict the shape deformations so as to relieve the influence of occlusion. Huang et al. <ref type="bibr" target="#b16">[17]</ref> integrate markov network inducing shape constrains to improve the robustness to occlusion. For the other category, part-based appearance representations are widely used to improve the robustness to occlusion. CLM <ref type="bibr" target="#b8">[9]</ref> performs PCA on the concatenated local patches to build the appearance model. SDM <ref type="bibr" target="#b33">[34]</ref> concatenates SIFT features extracted around landmarks and directly learns linear mapping from SIFT features to shape deviations. Differently, LBF <ref type="bibr" target="#b23">[24]</ref> learns the linear mapping based on learnt local binary features of each facial point rather than the handcrafted SIFT, which achieves better performance than SD-M. Local features have some tolerance to occlusion and the concentrated features can support each other to get reasonable results even if some patterns are occluded. Besides, Hu et al. <ref type="bibr" target="#b14">[15]</ref> employ an active wavelet network (AWN) representation to replace the PCA-based appearance model in AAM. Under occlusion, the reconstruction error spreads over the whole face in PCA-based appearance model while it remains local in wavelet network representation, leading to improved performance regarding partial occlusion. Liu et al. <ref type="bibr" target="#b20">[21]</ref> design boosted appearance model (BAM) to substitute the appearance model in AAM and demonstrate the superiority for alignment under occlusion. To some extend, these methods may reduce the influence of occlusions. However, the occlusion problem is not settled in essence.</p><p>Recently, there are several methods attempting to explicitly handle occlusions problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref>. Burgos-Artizzu et al. <ref type="bibr" target="#b4">[5]</ref> present a Robust Cascaded Pose Regression (RCPR) method to predict the landmarks as well as the occlusions. The face is divided into 3 × 3 grid and for each time, only one non-occluded block is utilized to predict the landmarks. It achieves impressive results on challenging dataset COFW which contains various occluded faces. However, the grid partition is not flexible and the features used for each regressor are limited within one block. Besides, training RCPR models needs occlusion annotations, which are expensive to achieve. Yu et al. <ref type="bibr" target="#b36">[37]</ref> propose a occlusion-robust method recorded as Cor, which uses a Bayesian model to fuse the prediction results from multiple occlusion-specific regressors. Each regressor is trained by omitting some pre-defined parts in a face to tackle one type of occlusions. However, it is hard or even impossible to define occlusion-specific regressors to cover all types of occlusions considering the combinational explosion problem. Moreover, it may be time-consuming as there are several regressors for predicting a face shape. Golnaz et al. <ref type="bibr" target="#b12">[13]</ref> explicitly model occlusions of parts into a hierarchical deformable part model to detect facial points and occlusions simultaneously. If a keypoint is occluded, the corresponding feature is discarded to avoid the influence of occlusions. It achieves better results than RCPR on COFW dataset. However it suffers from computational problem (about 30 seconds per image). Xing et al. <ref type="bibr" target="#b32">[33]</ref> jointly learn an occlusion dictionary within a rational dictionary to capture different kinds of appearance under partial occlusions and get promising results on the occluded face dataset OCFW.</p><p>Overall, all these methods explicitly process occlusion problem either by discarding the occluded parts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref> or building an occlusion dictionary to capture the appearance variations due to occlusions <ref type="bibr" target="#b32">[33]</ref>. Different from the existing methods, we explicitly tackle the occlusion problem by automatically recovering the occluded parts and then the de-corrupted parts are leveraged together with nonoccluded parts for robust face alignment under occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we will firstly illustrate the overview of the proposed DRDA. Then we will demonstrate the details about deep regression networks for face alignment and de-corrupt autoencoders for recovering occluded face separately, followed by the learning of deep regression networks coupled with de-corrupt autoencoders under a cascade structure. Finally we will give a discussion about the differences with existing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method Overview</head><p>This work attempts to learn a deep alignment model that is robust to partial occlusions. To this end, we propose a schema of coupling deep regression networks with decorrupt autoencoders as shown in <ref type="figure">Figure 1</ref>. Firstly a deep regression network is employed to predict a face shape. After that, the de-corrupt autoencoders are designed to recover the occluded appearance around current shape. Then the de-corrupted face is taken as the input of successive deep regression network to further refine the face shape. As the occluded appearance is recovered in the de-corrupted face, more informative context rather than the worthless occlusion can be leveraged, leading to a better regression network for alignment. Furthermore, both deep regression networks and de-corrupt autoencoders can be conducted under a cascade structure, so as to attain a better and better decorrupted face and face shape.</p><p>Suppose we have a training set</p><formula xml:id="formula_0">{(x i , S i )} N i=1</formula><p>, which consists of N face images x i and its corresponding p facial landmarks S i ∈ R 2p . The deep regression network is a nonlinear model that maps the image x i to its corresponding face shape S i by optimization the following objective:</p><formula xml:id="formula_1">F * = arg min F N i=1 ||∆S i − F(ϕ(x i ,S))|| 2 2 ,<label>(1)</label></formula><formula xml:id="formula_2">∆S i = S i −S,<label>(2)</label></formula><p>where ϕ is a feature extraction function, andS ∈ R 2p is an initial shape. ∆S i denotes the shape deviation between the groundtruth S i and the initial shapeS.</p><p>In real-world scenarios, the input image x i may be corrupted by occlusions. The corrupted features ϕ(x i ,S) will significantly impact the learning of deep regression networks. As stated in <ref type="bibr" target="#b30">[31]</ref>, the corrupted parts can be recovered from partial reservations for images with redundance, such as face image. Inspired by this characteristic, we propose a de-corrupt autoencoder G to repair the destroyed patterns, which can be formulated as follows:</p><formula xml:id="formula_3">G * = arg min G N i=1 ||x i − G(x i )|| 2 2 .<label>(3)</label></formula><p>Here, x i is a corrupted face andx i denotes the "pure" images without occlusions. Considering the the powerful recovering ability of autoencoders, G is modeled as an autoencoder-like neural network, denoted as De-corrupt Autoencoder.</p><p>With the "repaired" faces, the following deep regression networks can be relieved from suffering of occlusions. In the following, we will illustrate the deep regression networks F for shape prediction and the de-corrupt autoencoders neural networks G for recovering the occluded parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Regression Network for Shape Prediction</head><p>The deep regression network F aims at characterizing the nonlinear mapping from appearance to shape. For a deep network with m − 1 hidden layers, it can be formulated as optimizing the following objective function:</p><formula xml:id="formula_4">F * = arg min F N i=1 ||∆S i − f m (f m−1 (...f 1 (ϕ(x i ,S))))|| 2 2 + λ m i=1 ||W i || 2 F ,<label>(4)</label></formula><formula xml:id="formula_5">a q f q (a q−1 ) = σ(W q a q−1 + b q ), q ∈ [1, m − 1], (5) f m (a m−1 ) = W m a m−1 + b m ,<label>(6)</label></formula><p>where <ref type="bibr" target="#b21">[22]</ref> features extracted around the shapeS. σ is the nonlinear activation functions, e.g., sigmoid or tanh function. a q denotes the response of hidden layer q. For the last layer m, a linear regression is utilized to project the feature representation a m−1 to the corresponding face shape devia-</p><formula xml:id="formula_6">F consists of f 1 , f 2 , ..., f m . f q is the nonlinear func- tion of qth layer in the deep network parameterized with W q and b q , q ∈ [1, m − 1]. ϕ(x i ,S) denotes the shape-indexed SIFT</formula><formula xml:id="formula_7">tion ∆S i . A regularization term m i=1 ||W i || 2</formula><p>F is introduced to prevent over-fitting. Eq. (4) can be optimized by using the algorithm of L-BFGS <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">De-corrupt Autoencoders for Recovering the Occluded Face</head><p>Usually, the deep regression model can achieve an accurate face shape. However, under the presence of partial occlusions, the performance may suffer from degenerating. To tackle the partial occlusions, we design an autoencoderlike neural network to explicitly recover the occluded parts, denoted as De-corrupt Autoencoder.</p><p>Auto-Encoder. A conventional auto-encoder network consists of two components, i.e., encoder and decoder <ref type="bibr" target="#b3">[4]</ref>. It is trained by minimizing the reconstruction error of the input, which is purely unsupervised. The encoder function g is exploited to map the input vector x ∈ R d to a hidden representation y ∈ R r , where r is the number of hidden units. The mapping function g is illustrated as follows:</p><formula xml:id="formula_8">y = g(x) = σ(W x + b),<label>(7)</label></formula><p>where W is a r × d weight matrix and b ∈ R r×1 is a bias term. σ is a sigmoid function or tanh function, which induces the nonlinearity. The decoder function h attempts to map the representation y back to x:</p><formula xml:id="formula_9">h(y) = σ(W ′ y + b ′ ),<label>(8)</label></formula><p>where W ′ ∈ R d×r , b ′ ∈ R d×1 . W ′ is optionally be constrained by W ′ = W T , in which it calls as tied weights. Given a training set with N samples {x i } N i=1 , the parameters of encoder and decoder functions are optimized by minimizing the reconstruction error as follows:</p><formula xml:id="formula_10">{W * , b * , W ′ * , b ′ * } = arg min W,b,W ′ ,b ′ N i=1 ||x i − h(g(x i ))|| 2 2 ,<label>(9)</label></formula><p>De-corrupt Autoencoder. Inspired by the favorable ability of autoencoder for reconstruction, the proposed decorrupt autoencoder is designed to recover the genuine appearance from the occlusions. Assume we have a training set with N samples</p><formula xml:id="formula_11">{(x i ,x i ,Ŝ i )} N i=1 ,</formula><p>x i is a occluded face with the shapeŜ i andx i is the corresponding genuine face without occlusions. Here,Ŝ i is the predicted shape from the anterior deep regression network. A de-corrupt autoencoder network G is designed to reconstruct the genuine facê x i from the occluded one x i as below:</p><formula xml:id="formula_12">{W * , b * , W ′ * , b ′ * } = arg min W,b,W ′ ,b ′ N i=1 ||x i (Ŝ i ) − h(g(x i (Ŝ i )))|| 2 2 ,<label>(10)</label></formula><p>where x i (Ŝ i ) denotes the appearance of x i around the shapê S i , andx i (Ŝ i ) denotes the appearance ofx i around the shapeŜ i . Generally, the encoder function g and deocoder function h can be designed with multiple layers.</p><p>Considering that the face appearance varies under different poses and expressions, it is nontrivial to design one decorrupt autoencoder network to reconstruct the details of the whole face. To recover a face with vivid appearance under various poses and expressions, we partition the face image x i into J components according to face shapeŜ i and design several independent de-corrupt autoencoder networks G j , one for each component. Each of the J components is denoted as x i (Ŝ ij ) withŜ ij representing those facial points belonging to the jth component, j ∈ [1, J]. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, 68 facial points are partitioned into seven components. The region size of each component is calculated as the rectangular hull around the relevant facial points.</p><p>To attain de-corrupt autoencoder networks, we need to build a training set consisting of x i and the genuine version x i . However, occluders have thousands of appearance variations and occlusion may occur anywhere. It is difficult or even impossible to collect real-world images with a variety of possible occlusions. Fortunately, it is easy to collect imagesx i without occlusions. And a large scale of face images with occlusions can be obtained by randomly putting occluders on it. Specifically, for each component j, we randomly pick patches from nature images which contain no face, and then randomly place the patch anywhere within this component, inducing an occluded face component.</p><p>With the de-corrupt autoencoder network G, a corrupted face image x i can be recovered as G(x i ). Instead of directly using the recovered G(x i ), we only leverage the recovered appearance for those occluded parts, but original appearance for those non-occluded parts, forming the de-corrupted imagex i used for further refining the shape. Specifically, the occluded parts are determined as follows: firstly, compute the difference between x i and G(x i ); then simply regard those pixels with the difference larger than a threshold τ (τ =30 in this work) as occluded pixels; finally those components with the proportion of occluded pixels more than 30% are determined as occluded parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Cascade Deep Regression with De-corrupt Autoencoder</head><p>Face shape S 1 can be attained from the network F stated above. However, it may not approach the ground truth due to partial occlusion, pose, expression, etc. To achieve finer alignment results, several successive deep regression networks coupled with de-corrupt autoencoders are cascaded. Specifically, for the kth stage, the de-corrupt autoencoder network G k j is constructed for the jth component based on current shape prediction S k−1 , formulated as follows:</p><formula xml:id="formula_13">G k * j = arg min G k j N i=1 ||x i (S k−1 ij ) − h k j (g k j (x i (S k−1 ij )))|| 2 2 .</formula><p>(11) With G k * j | J j=1 , a de-corrupted facex k−1 i is achieved. By takingx k−1 i as input, the following deep regression network of stage k is designed to further refine the shape by predicting the current shape deviation ∆S k = S − S k−1 :</p><formula xml:id="formula_14">F k * = arg min F k N i=1 ||f k m (f k m−1 (...f k 1 (ϕ(x k−1 i , S k−1 i )))) − ∆S k i || 2 2 + λ m i=1 ||W k i || 2 F ,<label>(12)</label></formula><p>where ϕ(x k−1 i , S k−1 i ) denotes the shape-indexed feature extracted from the de-corrupted face imagex k−1 i with the current shape S k−1 . With Eq. (12), the shape can be refined as S k = S k−1 + ∆S k which is further utilized for learning the following de-corrupt autoencoder networks.</p><p>By learning de-corrupt autoencoder networks and deep regression networks under a cascade structure, they can benefit from each other. On the one hand, with more accurate face shape, the appearance variations within each component becomes more consistent, leading to more compact de-corrupt autoencoder networks for better de-corrupted face images. On the other hand, the deep regression networks that are robust to occlusions can be attained by leveraging better de-corrupted faces. The final shape prediction S K can be achieved by summing the outputs of each stage:</p><formula xml:id="formula_15">S K =S + K k=1 ∆S k , ∆S k = F k (ϕ(x k−1 , S k−1 )</formula><p>). Besides, the occluded regions (i.e., the occluders) can be localized by comparing the difference between the final decorrupted imagex K−1 and the origin x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussions</head><p>In this section, we give a brief discussion about the differences between our method and some existing methods which also explicitly tackle occlusions.</p><p>Differences from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref>. All these methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref> handle partial occlusion problem by learning occlusionaware face alignment model, i.e., explicitly considering the occlusion state when building alignment models, as well as ours. However our method differ from them in two aspects: 1) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref> discard the occluded parts to achieve the face alignment model robust to occlusions, while our proposed de-corrupt autoencoder network automatically recovers the occluded parts which can be further utilized together with those non-occluded parts for predicting face shape, leading to better alignment results. 2) The existing methods can only predict whether the landmarks are occluded while ours can easily localize the exact occluded regions with the de-corrupt autoencoder as shown in <ref type="figure" target="#fig_3">Figure 5</ref>, see details in Sec. 4.3.</p><p>Differences from <ref type="bibr" target="#b32">[33]</ref>. Xing et al. <ref type="bibr" target="#b32">[33]</ref> propose a novel method named OSRD to deal with occlusions by learning an occlusion dictionary and a rational dictionary. However, it may be hard for a linear model to well capture more complex variations. In contrast, our de-corrupt autoencoder networks can well undo the challenging corruption, attributed to the favorable ability for modeling the nonlinearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We firstly introduce the evaluation settings including the datasets and methods for comparison, and then illustrate the implementation details of our method. Finally, we compare the proposed approach with the state-of-the-art methods on three challenging datasets under various occlusions, i.e., OCFW <ref type="bibr" target="#b32">[33]</ref>, COFW <ref type="bibr" target="#b4">[5]</ref> and IBUG <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Methods for Comparison</head><p>To evaluate the effectiveness of the proposed method, we employ two challenging datasets with varied occluded faces: OCFW <ref type="bibr" target="#b32">[33]</ref> and COFW <ref type="bibr" target="#b4">[5]</ref>. OCFW contains 2591 images for training and 1246 images for testing, which comes from LFPW <ref type="bibr" target="#b2">[3]</ref>, HELEN <ref type="bibr" target="#b19">[20]</ref>, AFW <ref type="bibr" target="#b40">[41]</ref> and IBUG <ref type="bibr" target="#b24">[25]</ref>. The images are collected in the wild, which contain large variations in pose, expression, partial occlusion, etc. All training samples are without occlusions while all testing samples are partially occluded, which formulates a challenging scenario of face alignment under occlusions. Annotations of 68 landmarks are published in website <ref type="bibr" target="#b0">[1]</ref>. COFW is another occluded face dataset in the wild, which is published by Burgos-Artizzu et al. <ref type="bibr" target="#b4">[5]</ref>. Its training set consists of 845 faces from LFPW training set and extra 500 faces with heavy occlusions. The test set contains 507 faces which are also heavily occluded. The faces have large variations in varying degrees of occlusions, together with different poses and facial expressions. Both 29 landmarks and their occluded/unoccluded state are released in <ref type="bibr" target="#b4">[5]</ref>. Besides, we also do comparisons on 135 images from IBUG dataset which formulates a more general "in the wild" scenario.</p><p>We compare our method with a few state-of-the-art, i.e., SDM <ref type="bibr" target="#b33">[34]</ref>, RCPR <ref type="bibr" target="#b4">[5]</ref>, OSRD <ref type="bibr" target="#b32">[33]</ref>. For RCPR, we use the off-the-shelf codes released by the original authors. Since the released SDM model only predicts the inner 49 landmarks, we re-train SDM to predict 68 landmarks. For OS-RD, we quote results from <ref type="bibr" target="#b32">[33]</ref>. We cascade several deep regression networks without de-corrupt autoencoders as a baseline, denoted as CDRN. The normalized root mean squared error (NRMSE) is calculated with respect to the interocular distance. The cumulative distribution function (CDF) of the normalized root mean squared error (NRMSE) is employed for performance evaluation and the NRMSE is calculated with respect to interocular distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our proposed deep alignment model consists of 3 stages. For the first stage, the regression network has three layers including two non-linear hidden layers and one linear layer. The numbers of hidden neurons are set as 784 and 400 respectively. For the following two stages, the regression network is deeper, which contains three hidden layers with 1296, 784 and 400 hidden neurons respectively, and  one linear regression layer. The network of the first stage takes face of 80 × 80 as input and for the last two stages, face image of higher resolution (i.e., 160 × 160) is used for finer shape prediction. The initial shapeS is a mean shape based on face detection results. For all stages, The weight decay parameter α is set as 0.001, which balances the sum-of-squares error term and the weight decay term. We divide the face shape of 68 landmarks into seven components, and for each component, a de-corrupt autoencoder network with one hidden layer is utilized to recover the occluded parts. The number of hidden units is set as 576 for all de-corrupt autoencoders. Sigmoid function is used to induce nonlinearity for both deep regression networks and de-corrupt autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluations on OCFW Dataset</head><p>Firstly, we evaluate our method and the existing approaches on OCFW dataset. All methods are trained on OCFW training set and then are evaluated on OCFW test set in terms of 68 facial landmarks. <ref type="figure" target="#fig_1">Figure 3</ref> shows the cumulative error distribution curves of all methods. As seen, RCPR performs better than S-DM. It is possibly because RCPR designs the interpolated shape-indexed features which are robust to large pose, and applies smart restart strategy to relieve the sensitivity of shape initialization. Furthermore, CDRN performs slightly better than RCPR when the NRMSE is above 0.065, which can be attributed to the favorable ability of modeling nonlinear mapping from appearance to shape. Benefited from simultaneously learning an occlusion dictionary and a rational dictionary, OSRD achieves better results than both RCPR and CDRN. Furthermore, our method outperforms OSRD, with an improvement up to 10% when NRMSE is 0.05. This significant improvement can be attributed to the schema of effectively coupling deep alignment networks with de-corrupt autoencoders. <ref type="figure" target="#fig_2">Figure 4</ref> shows the landmark detection results of RCPR, CDRN and our DRDA on some extremely challenging samples. It can be observed that our method improves the robustness of face alignment to partial occlusions under varied situations. Besides the face alignment, our proposed method can roughly localize the occluded region by comparing the de-corrupted image with the origin occluded image, as stated in Sec. 3.3. Some de-corrupted images and occlusion localization results are illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. As seen, our method can well recover the genuine appearance from the occlusions as well as localize the occluded parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluations on COFW Dataset</head><p>We further evaluate the detection accuracy of all methods on Caltech Occluded Faces in the Wild (COFW). COFW is another challenging dataset with real-world faces occluded to different degrees. And it also contains large shape and appearance variations due to pose and expressions. All methods are evaluated on COFW test set in terms of 29 facial landmarks according to the official protocol. For R-CPR, we use the model released by authors for evaluations, which is trained with occlusion annotations for robust face alignment under occlusions and achieves promising results on COFW test set <ref type="bibr" target="#b4">[5]</ref>. For the rest methods, including ours, we directly use the model trained on OCFW training set for evaluation. Since the model trained on OCFW training set is to predict 68 landmarks, we follow <ref type="bibr" target="#b12">[13]</ref> to learn a linear mapping from the predictions to the 29 facial points.</p><p>The performances of all methods are illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>. Simliar conculusions can be achieved. As seen, R-CPR performs better than SDM and CDRN when the N-RMSE is lower than 0.08. RCPR explicitly predicts occlusions and utilizes the occlusion information to help shape prediction, so as to achieve robustness to partial occlusions. Benefited from the de-corrup autoencoder networks, Our method performs better than RCPR and CDRN, with an improvement up to 6% when NRMSE is 0.10. Besides, we compare our method with methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref> in terms of the mean error. In <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>, they achieves 7.46(×10 −2 ) and 7.30(×10 −2 ) respectively on COFW testset and ours achieves a much better results of 6.46(×10 −2 ), which also demonstrates the superiority of our effective alignment framework for dealing with occlusions. Some alignment results of our method are shown in <ref type="figure" target="#fig_5">Figure 7</ref>. The first row shows the alignment results under occlusions simultaneously with varying poses. The second row exhibits exemplars under simultaneous occlusions and expressions. Samples with a variety of occluders (e.g., sunglasses, respirator, cameras, etc.) are illustrated in the last row. As seen, our method is robust to different types of occlusions under various poses and expressions.</p><p>Since COFW consists of occlusion annotations, we do quantitative evaluation of occlusion detection on COFW and compare with RCPR <ref type="bibr" target="#b4">[5]</ref>, OC <ref type="bibr" target="#b12">[13]</ref>, CoR <ref type="bibr" target="#b36">[37]</ref>. The occlusion detection precision/recall curves are shown in <ref type="figure">Fig. 8</ref>. As can be seen, our method significantly outperforms the others. The visualized results of occlusion detection are shown in <ref type="figure">Fig. 9</ref>. Our method can not only detect occlusions accurately but also well recover occluded parts.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluations on IBUG Dataset</head><p>Besides, we conduct experiments on IBUG dataset, which formulates a more general scenario containing large variations due to pose, expression, occlusion, illumination, etc. We compare our method with more state-of-the-art works, including Dantone et al. <ref type="bibr" target="#b9">[10]</ref>, Zhu et al. <ref type="bibr" target="#b40">[41]</ref>, Yu et al. <ref type="bibr" target="#b35">[36]</ref>, DRMF <ref type="bibr" target="#b1">[2]</ref>, SDM <ref type="bibr" target="#b33">[34]</ref>, RCPR <ref type="bibr" target="#b4">[5]</ref>. All methods are trained with LFPW trainset, HELEN trainset and AFW and evaluated on IBUG dataset in terms of 68 landmarks. <ref type="figure" target="#fig_7">Fig. 10</ref> shows the comparison results of all methods with respect to cumulative error distribution curves. The NRMSE is normalized by the face size for better exhibition. As shown in <ref type="figure" target="#fig_7">Fig. 10</ref>, our method achieves better results than the state-of-the-art methods and also outperforms the CDRN, which demonstrates the effectiveness of inducing de-corrupt autoencoder networks. Besides, we compare ours with LBF <ref type="bibr" target="#b23">[24]</ref>. The mean error of LBF is 11.98% while our method achieves a better result of 10.79%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Works</head><p>In this work, we present a novel Deep Regression networks coupled with De-corrupt Autoencoders (DRDA) for face alignment under partial occlusions. Aiming at explicitly tackling the occlusion problem, we design the de-corrupt autoencoder networks to automatically recover the genuine appearance for the occluded parts and leverage the recovered parts together with the non-occluded parts in the deep regression network to get an accurate shape prediction under occlusions. By learning the de-corrupt autoencoders and deep alignment networks under a cascade architecture, the de-corrupt anutoencoder network becomes more and more compact with better shape predictions, and the alignment model becomes more and more robust with improved de-corrupted faces, leading to accurate face alignment results. Our method achieves the state-of-the-art performance on three challenging datasets consisting of various occlusions together with large pose and expression variations, which demonstrates the effectiveness of our DRDA for face alignment under occlusions. In the future, we will explore other types of deep architectures for recovering the genuine appearance for occluded parts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Component partition of 68 facial points. The 68 facial landmarks are divided into seven components, i.e., left-eyeeyebrow, right-eye-eyebrow, nose, mouth, left-face-contour, rightface-contour and bottom-face-contour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Evaluations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualized results of RCPR, CDRN and our proposed DRDA on OCFW. The first row shows the results of RCPR. The second row shows the results of CDRN and the results of our proposed DRDA are illustrated in the last row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Exemplar results of de-corrupted face images and occluded parts localizations on OCFW. The first row shows the origin occluded faces. The de-corrupted images are shown in the second row. The last row shows the occlusion localization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Evaluations on COFW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Exemplar results on COFW. The first row: occluded images under various poses; the seconde row: images under occlusions together with different expressions; the last row: images occluded by a variety of objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Occlusion detection precision/recall curves on COFW.ction Visualized results of occlusion detection and reconstruction on COFW. The first row shows the occlusion detection results, where the red dots denotes occlusion and the green dosts denotes non-occlusion. The corresponding de-corrupted faces are shown in the last row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Evaluations on IBUG.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">300 faces in-the-wild challenge</title>
		<ptr target="http://ibug.doc.ic.ac.uk/resources/300-W/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Active shape models-their training and application. Computer Vision and Image Understanding (CVIU)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time facial feature detection using conditional regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2578" to="2585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random cascaded-regression copse for robust facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A generative shape regularization model for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active wavelet networks for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time view-based face alignment using active wavelet networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Analysis and Modeling of Faces and Gestures</title>
		<imprint>
			<publisher>AMFGW</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A component-based framework for generalized face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On optimization methods for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prochnow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generic face alignment using boosted appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Active appearance models revisit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face alignment through subspace constrained mean-shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonparametric context modeling of local appearance for pose-and expression-robust facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimization problems for fast aam fitting in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gauss-newton deformable part models for face alignment in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facial feature tracking under varying facial expressions and face poses based on restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards multiview and partially-occluded face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Posefree facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Consensus of regression for occlusion-robust facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
