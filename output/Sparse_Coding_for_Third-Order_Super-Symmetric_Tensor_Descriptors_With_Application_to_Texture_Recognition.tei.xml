<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Coding for Third-order Super-symmetric Tensor Descriptors with Application to Texture Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
							<email>piotr.koniusz@nicta.com.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Canberra Research Laboratory</orgName>
								<orgName type="laboratory" key="lab2">ARC Centre of Excellence for Robotic Vision</orgName>
								<orgName type="institution" key="instit1">National ICT Australia (NICTA)</orgName>
								<orgName type="institution" key="instit2">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
							<email>anoop.cherian@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Canberra Research Laboratory</orgName>
								<orgName type="laboratory" key="lab2">ARC Centre of Excellence for Robotic Vision</orgName>
								<orgName type="institution" key="instit1">National ICT Australia (NICTA)</orgName>
								<orgName type="institution" key="instit2">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Coding for Third-order Super-symmetric Tensor Descriptors with Application to Texture Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Super-symmetric tensors -a higher-order extension of scatter matrices -are becoming increasingly popular in machine learning and computer vision for modeling data statistics, co-occurrences, or even as visual descriptors. They were shown recently to outperform second-order approaches <ref type="bibr" target="#b17">[18]</ref>, however, the size of these tensors are exponential in the data dimensionality, which is a significant concern. In this paper, we study third-order supersymmetric tensor descriptors in the context of dictionary learning and sparse coding. For this purpose, we propose a novel non-linear third-order texture descriptor. Our goal is to approximate these tensors as sparse conic combinations of atoms from a learned dictionary. Apart from the significant benefits to tensor compression that this framework offers, our experiments demonstrate that the sparse coefficients produced by this scheme lead to better aggregation of high-dimensional data and showcase superior performance on two common computer vision tasks compared to the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent times have witnessed an increasing trend in several machine learning and computer vision applications to use rich representations that capture the inherent structure and statistics of the data. A few notable such representations are histograms, strings, covariances, trees, and graphs. The goal of this paper is to study a new class of structured data descriptors -third-order super-symmetric tensors -in the context of sparse coding and dictionary learning. Tensors are often used to capture higher-order moments of data distributions such as the covariance, skewness, or kurtosis and have been used as data descriptors in several computer vision applications. In region covariances <ref type="bibr" target="#b35">[36]</ref>, a covariance matrix -computed on multi-modal features from an image region -is used as a descriptor for the region and is useful for object tracking, retrieval, texture recognition, and video analysis <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10]</ref>. Given bag-of-words histograms or local descriptor vectors from an image, a second-order co-occurrence pooling of these vectors cap-tures the occurrences of two features together in an image and is recently shown to provide superior performance in semantic segmentation and visual concept detection, compared to their first-order counterparts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>. A natural extension of the idea is to use higher-order pooling operators, an extensive experimental analysis of which is provided in <ref type="bibr" target="#b17">[18]</ref>. Their paper shows that pooling using third-order super-symmetric tensors can significantly improve upon the second-order descriptors, e.g., by more than 7% MAP on the challenging PASCAL VOC07 dataset.</p><p>However, given that the size of the tensors increases exponentially against the dimensionality of their first-order counterpart, efficient representations are extremely important for applications that use these higher-order descriptors. To this end, the goal of this paper is to study these descriptors in the classical dictionary learning and sparse coding setting <ref type="bibr" target="#b26">[27]</ref>. Using the properties of super-symmetric tensors, we formulate a novel optimization objective (Sections 6, 7) in which each third-order data tensor is approximated by a sparse non-negative linear combination of positive semi-definite matrices. Although our objective is nonconvex -typical to several dictionary learning algorithms -we show that our objective is convex in each variable, thus allowing a block-coordinate descent scheme for the optimization. Experiments (Section 8) on the PASCAL VOC07 dataset show that the compressed coefficient vectors produced by our sparse coding preserve the discriminative properties of the original tensors and provide robust tensor compression and aggregation. Inspired by the merits of third-order pooling proposed in <ref type="bibr" target="#b17">[18]</ref>, we further introduce a novel tensor descriptor for texture recognition via the linearization of explicitly defined RBF kernels, and show that sparse coding of these novel tensors performs better than the state-of-the-art descriptors used for texture recognition.</p><p>In summary, our contributions are: i) a novel third-order texture descriptor 1 , ii) novel third-order tensor dictionary learning and sparse coding, and iii) theoretical guarantees on the existence of the sparse decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Dictionary learning and sparse coding <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref> methods have significantly contributed to improving the performance of numerous applications in computer vision and image processing. While these algorithms assume a Euclidean vectorial representation of the data, there have been extensions to other data descriptors such as tensors, especially symmetric positive definite matrices <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>. These extensions typically use non-Euclidean geometries defined by a similarity measure. Popular choices of such measures for positive definite matrices are the log-determinant divergence <ref type="bibr" target="#b33">[34]</ref>, the log-Euclidean metric <ref type="bibr" target="#b0">[1]</ref>, and the affine invariant Riemannian metric <ref type="bibr" target="#b27">[28]</ref>. However, the third-order tensors considered in this paper are neither positive definite nor there are any standard similarity measures known, apart from the Euclidean distance. Thus, extending these prior methods to our setting is infeasible, demanding novel formulations.</p><p>Third-order tensors have been used for various tasks. Spatio-temporal third-order tensor on video data for activity analysis and gesture recognition is proposed in <ref type="bibr" target="#b14">[15]</ref> . Non-negative factorization is applied to third-order tensors in <ref type="bibr" target="#b32">[33]</ref> for image denoising. Multi-linear algebraic methods for tensor subspace learning are surveyed in <ref type="bibr" target="#b23">[24]</ref>. Tensor textures are used in the context of texture rendering in computer vision applications in <ref type="bibr" target="#b37">[38]</ref>. Similar to eigenfaces for face recognition, multi-linear algebra based techniques for face recognition use third-order tensors in <ref type="bibr" target="#b36">[37]</ref>. However, these applications work with a single tensor; the objective of this paper is to learn the underlying structure of a large collection of such tensors generated from visual data using the framework of dictionary learning and sparse coding, which to the best of our knowledge is a novel proposition.</p><p>In addition to the dictionary learning framework, we also introduce an image descriptor for textures. While we are not aware of any previous works that propose third-order tensors as image region descriptors, the most similar methods to our approach are i) third-order probability matrices for image splicing detection <ref type="bibr" target="#b41">[42]</ref> and ii) third-order global image descriptors that aggregates SIFT vectors into autocorrelation tensors <ref type="bibr" target="#b17">[18]</ref>. In contrast, our descriptor, is assembled from elementary signals such as intensity and its first-and second-order derivatives, analogous to covariance descriptors <ref type="bibr" target="#b35">[36]</ref>, but demonstrating superior accuracy. The formulation chosen by us for texture recognition also differs from prior works such as kernel descriptors <ref type="bibr" target="#b1">[2]</ref> and convolutional kernel networks <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Notations</head><p>Before we proceed, we briefly review our notations next. We use bold-face upper-case calligraphic and regular letters for third-order tensors and matrices, respectively, bold-face lower-case letters for vectors and normal fonts for scalars.</p><p>Each second-order tensor along the third mode of a thirdorder tensor is called a slice. Using Matlab style notation, the s-th slice of X is given by X :,:,s . The operation ↑ ⊗ stands for an outer product of a second-order tensor with a vector. For example, Y = Y ↑ ⊗ y produces a third-order tensor Y ∈ R d1×d2×d3 from a matrix Y ∈ R d1×d2 and a vector y ∈ R d3 , where the s-th slice of Y is given by Yy s , y s being the s-th dimension of y. I N stands for the index set {1, 2, ..., N }. We denote the space of d × d positive semi-definite matrices as S d + ⊂ R d×d , the space of supersymmetric tensors of dimension d × d × d as S d ⊂ R d×d×d and the space of tensors R d×...×d with r modes as R ×rd . Going by the standard terminology in higher-order tensor factorization literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>, we define a core tensor as the analogous of the singular value matrix produced by SVD in the second-order case. A core tensor need not be diagonal in general and depends on the chosen decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Background</head><p>In this section, we review super-symmetric tensors and their properties, followed by a brief exposition of the method described in <ref type="bibr" target="#b17">[18]</ref> for generating tensor descriptors for an image. The latter will come useful when introducing our new texture descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Third-Order Super-Symmetric Tensors</head><p>We define a super-symmetric tensor descriptor as follows: Definition 1. Suppose x n ∈ R d + , ∀n ∈ I N represents data vectors from an image, then a third-order super-symmetric tensor (TOSST) descriptor X ∈ S d of these data vectors is given by:</p><formula xml:id="formula_0">X = 1 N N n=1 (x n x T n ) ↑ ⊗ x n .<label>(1)</label></formula><p>In an object recognition setting, the data vectors are usually SIFT descriptors, Gabor filters, or other primitives extracted from the image, while the tensor descriptor is obtained as the third-order autocorrelation tensor via applying (1).</p><p>The following properties of the TOSST descriptor are useful in the sequel. Proposition 1. For a TOSST descriptor X ∈ S d , we have:</p><p>1. Super-Symmetry: X i,j,k = X Π(i,j,k) for indexes i, j, k and their permutation given by Π, ∀Π.</p><p>2. Every slice is positive semi-definite, that is, X :,:,s ∈ S d + , ∀s ∈ I d .</p><p>3. Indefiniteness, i.e., under a CP decomposition <ref type="bibr" target="#b19">[20]</ref>, it can have positive, negative, or zero entries in its coretensor -equivalent of eigenvalues in matrix case.</p><p>Proof. The first two properties can be proved directly from <ref type="bibr" target="#b0">(1)</ref>. To prove the last property, note that for a TOSST tensor X and some z ∈ R d , z = 0, we have</p><formula xml:id="formula_1">((X ⊗ 1 z) ⊗ 2 z) ⊗ 3 z = d s=1 z s (z T X s z)</formula><p>where X s is the s-th slice of X . While z T X s z ≥ 0, the tensor product could be negative for z &lt; 0. In the above, ⊗ i denotes tensor product in the i-th mode <ref type="bibr" target="#b15">[16]</ref>.</p><p>Due to the indefiniteness of the tensors, we cannot use some of the well-known distances on the manifold of SPD matrices <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>. Thus, we restrict to using the Euclidean distance for the derivations and experiments in this paper.</p><p>Among several properties of tensors, one that is important and is typically preserved by tensor decompositions is the tensor rank defined below.</p><p>Definition 2 (Tensor Rank). Given a tensor X ∈ S d , its tensor rank TRank(X ) is defined as the minimum p such that X :,:,</p><formula xml:id="formula_2">s ∈ Span (M 1 , M 2 , ..., M p ) for all s ∈ I d , where each M i ∈ S d + is rank-one.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Third-Order Global Image Descriptor</head><p>In what follows, we outline a global image descriptor for the object category recognition from <ref type="bibr" target="#b17">[18]</ref> used in our experiments on the PASCAL VOC07 dataset. This descriptor is a specific example of the TOSST approach. To the unfamiliar reader, we also illustrate below, some preliminary results for this descriptor to highlight its robustness. The simplicity of the approach and good results it produces, motivate us to develop a third-order descriptor for textures (Section 5) as a natural extension of second-order region covariance descriptors (RCD), which are very popular for texture recognition. Furthermore, the high dimensionality of TOSST descriptors provides a strong motivation to extend the classical dictionary learning and sparse coding algorithm to generate compact tensor representations (Section 6).</p><p>The Third-order Global Image Descriptor is based on SIFT descriptors <ref type="bibr" target="#b22">[23]</ref> aggregated into a third-order autocorrelation tensor. This step precedes applying Higher Order Singular Value Decomposition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16]</ref> for whitening via Power Normalisation <ref type="bibr" target="#b17">[18]</ref> performed on the core tensor. The following steps represent this tensor descriptor:</p><formula xml:id="formula_3">V i = ↑ ⊗ r v i , ∀i ∈ I (2) V = Avg i∈I (V i ) (3) (E; A) = HOSVD(V) (4) E = Sgn(E) |E| γe (5) V = ((Ê ⊗ 1 A) ...) ⊗ r A (6) X = Sgn(V) |V| γc<label>(7)</label></formula><p>Equations (2, 3) assemble a higher-order autocorrelation tensorV per image. First, the outer product ↑ ⊗ r of order r <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> is applied to the local image descriptors v i ∈ R d from an index set I i.e., indexes of SIFT vectors from an image. This results in |I| autocorrelation matrices</p><formula xml:id="formula_4">(↑ ⊗ 2 v i v i v T i ) or third-order tensors V i ∈ S d : V i = (↑ ⊗ 3 v i (v i v T i ) ↑ ⊗ v i )</formula><p>of rank one if r = 2 or r = 3, respectively. These rank-one tensors are then averaged by Avg, resulting in tensorV ∈ S d that describes globally the image and could be used as a training sample. However, practical image representations have to deal with the so-called burstiness which is "the property that a given visual element appears more times in an image than a statistically independent model would predict" <ref type="bibr" target="#b12">[13]</ref>.</p><p>Power Normalization (PN) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13]</ref> is known to suppress burstiness. Therefore, equations (4-6) and <ref type="formula" target="#formula_3">(7)</ref> apply two-stage pooling with the eigenvalue-and coefficientwise PN respectively. In equation (4), operator HOSVD : S d → R ×rd ; R d×d decomposes tensorV into a core tensor E ∈ R ×rd with eigenvalues and an orthonormal factor matrix A ∈ R d×d , which can be interpreted as the principal components in r modes. PN is then applied element-wise by equation <ref type="formula">(5)</ref> to the eigenvalues of E to even out their contributions. TensorV ∈ R ×rd is then assembled in equation <ref type="formula">(6)</ref> by the tensor product ⊗ i in the i-th mode <ref type="bibr" target="#b15">[16]</ref>. Then, the coefficient-wise PN acting onV produces tensor X ∈ R ×rd in equation <ref type="formula" target="#formula_3">(7)</ref>.</p><p>In our experiments, we use r = 3 and i) X =V when PN is disabled, e.g., γ e = γ c = 1, or ii) apply the above HOSVD approach if 0 &lt; γ e &lt; 1 ∧ 0 &lt; γ c ≤ 1. Therefore, the final descriptor X in equation <ref type="formula" target="#formula_3">(7)</ref> represents an image by a TOSST which consists entirely of SPD matrix slices, if no PN is used. Preliminary results in <ref type="table">Table 1</ref> show that the third-order descriptor outperforms the second-order approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL VOC07</head><p>Caltech 101 Flowers 102 second-order 54.0% 78.5% 83.1% third-order 62.7% 83.9% 89.0% <ref type="table">Table 1</ref>: Evaluation of the Third-order Global Image Descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">TOSST Texture Descriptors</head><p>Many region descriptors for texture recognition are described in the literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref>. Their construction generally consists of the following steps: i) For an image I and a region R in it, extract feature statistics from the region. If (x, y) represent pixel coordinates in R, then a feature vector from R at (x, y) is given as:</p><formula xml:id="formula_5">φ xy (I)= x−x0 w−1 , y−y0 h−1 , I xy , ∂Ixy ∂x , ∂Ixy ∂y , ∂ 2 Ixy ∂x 2 , ∂ 2 Ixy ∂y 2 T ,<label>(8)</label></formula><p>where (x 0 , y 0 ), w and h are the coordinate origin, width, and height of R, respectively.</p><p>ii) Given feature vectors φ xy , compute a covariance matrix</p><formula xml:id="formula_6">Φ(I, R) = 1 |R|−1 (x,y)∈R φ xy (I)−φ φ xy (I)−φ T ,</formula><p>whereφ is the mean over φ xy (I) from region R <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21]</ref>. Alternatively, one can compute an autocorrelation matrix Φ(I, R)= 1 |R| (x,y)∈R φ xy (I)φ xy (I) T as proposed in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In this work, we make modifications to the steps listed above. Instead of using linear representations for φ xy (I), we use RBF kernels that are known to improve classification performance. The concatenated statistics in step (i) can be seen as linear features forming linear kernel K lin :</p><formula xml:id="formula_7">K lin ((x, y, I a ), (x ′ , y ′ , I b )) = φ xy (I a ) T φ x ′ y ′ (I b ). (9)</formula><p>We go a step further and define the following sum kernel K rbf composed of τ = 7 RBF kernels G:</p><formula xml:id="formula_8">K rbf (x, y, I a ), x ′ , y ′ , I b = τ i=1 G σi (φ i xy (I a )−φ i x ′ y ′ (I b )),<label>(10)</label></formula><formula xml:id="formula_9">where φ i xy (I) is the i-th feature in equation (8) while G σi (u − u ′ ) = exp(− u−u ′ 2 /2σ 2 i )</formula><p>is the so-called relative compatibility kernel that measures the compatibility between features of type i in each of the image regions.</p><p>The next step involves linearization of Gaussian kernels G σi for i = 1, ..., τ to obtain feature maps that expresses the kernel K rbf by the dot-product. In what follows, we use a fast approximation method based on probability product kernels <ref type="bibr" target="#b11">[12]</ref>. Specifically, we employ the inner product for the d ′ -dimensional isotropic Gaussian distribution:</p><formula xml:id="formula_10">G σ u−u ′ = 2 πσ 2 d ′ 2 ζ∈R d ′ G σ/ √ 2 (u−ζ) G σ/ √ 2 (u ′ −ζ) dζ.<label>(11)</label></formula><p>Equation <ref type="formula" target="#formula_0">(11)</ref> is then approximated by replacing the integral with the sum over Z pivots ζ 1 , ..., ζ Z :</p><formula xml:id="formula_11">G σ (u − u ′ ) ≈ √ wφ(u), √ wφ(u ′ ) , φ(u) = G σ/ √ 2 (u − ζ 1 ), ..., G σ/ √ 2 (u − ζ Z ) T .<label>(12)</label></formula><p>A weighting constant w relates to the width of the rectangle in the numerical integration and is factored out by the ℓ 2 normalization:</p><formula xml:id="formula_12">G σ (u − u ′ ) = G σ (u−u ′ ) G σ (u−u)G σ (u ′ −u ′ ) ≈ (13) √ wφ(u) √ wφ(u) 2 , √ wφ(u ′ ) √ wφ(u ′ ) 2 = φ(u) φ(u) 2 , φ(u ′ ) φ(u ′ ) 2 .</formula><p>The task of linearizing each G σi in equation <ref type="formula" target="#formula_0">(10)</ref> is trivial as these kernels use one-dimensional variables (d ′ = 1). Thus, we uniformly sample domains of each variable and use Z=5. With this tool at hand, we rewrite kernel K rbf as:</p><formula xml:id="formula_13">K rbf ((x, y, I a ), (x ′ , y ′ , I b )) ≈ v a xy , v b x ′ y ′ ,<label>(14)</label></formula><p>where vector v a xy is composed by τ sub-vectors φ i xy,σ i φ i xy,σ i 2 and each sub-vector i = 1, ..., τ is a result of linearization by equations <ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref>. We use a third-order aggregation of v according to equation (1) to generate our TOSST descriptor for textures. For comparisons, we also aggregate φ from equation <ref type="formula" target="#formula_5">(8)</ref> into a third-order linear descriptor. See Appendix A for the derivation of the third-order aggregation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Problem Formulation</head><p>Suppose we are given data tensors X n , n ∈ I N , each X n ∈ S d . We want to learn a tensor dictionary B with atoms B 1 , B 2 , ..., B K , where each B k ∈ S d consists of dslices. Let the s-th slice of the k-th atom be given by B s k where s ∈ I d , k ∈ I K and each B s k ∈ S d + . Then, the problem of dictionary learning and sparse coding can be formulated as follows for sparse coefficient vectors α n ∈ R K , n ∈ I N :</p><formula xml:id="formula_14">arg min B α 1 ,...,α N N n=1 X n − K k=1 B k α n k 2 F + λ α n 1 .<label>(15)</label></formula><p>Note that in the above formulation, third-order dictionary atoms B k are multiplied by scalars α n k . For K atoms, each of size d 3 , there are Kd 3 parameters to be learned in the dictionary learning process. An obvious consequence of this reconstruction process is that we require N ≫ K to prevent overfitting. To circumvent this bottleneck and work in the regime dN ≫ K, the reconstruction process is amended such that every dictionary atom is represented by an outer product of a second-order symmetric positive semi-definite matrix B k and a vector b k . Such a decomposition/approximation will reduce the number of parameters from Kd 3 to K(d 2 + d). Using this strategy, we re-define dictionary B to be represented by atom pairs</p><formula xml:id="formula_15">B ≡ {(B k , b k )} K k=1 such that B k = B k ↑ ⊗ b k .</formula><p>Note that the tensor rank of B k under this new representation is equal to the Rank(B k ) as formally stated below. Proof. Rank of B is the smallest p satisfying (B↑ ⊗b) :,:,s = B · b s ∈ Span (M 1 , M 2 , ..., M p ) , ∀s ∈ I d , where each M i ∈ S d + is rank-one. The smallest p satisfying B ∈ Span (M 1 , M 2 , ..., M p ) is the same, since multiplication of a matrix by any non-zero scalar (B · b s , b s = 0) will not change its eigenvectors, which constitute the spanning set.</p><p>Using the above idea, we can rewrite (15) as follows:</p><formula xml:id="formula_16">arg min B α 1 ,...,α N N n=1 X n − K k=1 (B k ↑ ⊗ b k ) α n k 2 F + λ α n 1 .</formula><p>(16) Introducing optimization variables β n k and rewriting the loss function by taking the slices out, we rewrite (16) as:</p><formula xml:id="formula_17">arg min B α 1 ,...,α N N n=1 S s=1 X s n − K k=1 B k β s,n k 2 F + λ α n 1 ,</formula><p>subject to β n k = b k α n k , ∀k ∈ I K and n ∈ I N . <ref type="formula" target="#formula_0">(17)</ref> We may rewrite the equality constraints in (17) as a proximity constraint in the objective function (using a regularization parameter γ), and constrain the sparse coefficients in α n to be non-negative, as each slice is positive semidefinite. This is a standard technique used in optimization, commonly referred to as variable splitting. We can also normalize the dictionary atoms B k to be positive semi-definite and of unit-trace for better numerical stability. In that case, vector atoms b k can be constrained to be non-negative (or even better, β n k ≥ 0 instead of α n ≥ 0 and b k ≥ 0). Introducing chosen constraints, we can rewrite our tensor dictionary learning and sparse coding formulation: In the above equation, operator ⊙ is an element-wise multiplication between vectors. Coefficients γ and λ control the quality of the proximity and sparsity of α n , respectively. The formulation <ref type="bibr" target="#b17">(18)</ref> is convex in each variable and can be solved efficiently using block coordinate descent. Note that, we may use a richer characterization of the loss by replacing the Frobenius norm with one based on the Riemannian geometry of positive definite matrices <ref type="bibr" target="#b5">[6]</ref>; however, in that case the convexity in each variable will be lost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 1 (Non-symmetric Tensors).</head><p>Note that nonsymmetric X n ∈ R d1×d2×d3 can be coded if the positive semi-definite constraint on B k in <ref type="bibr" target="#b17">(18)</ref> is dropped, such that B k ∈ R d1×d2 and b k ∈ R d3 . Other non-negative constraints can also be removed. While this case is a straightforward extension of our formulation, a thorough investigation into it is left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization</head><p>We propose a block-coordinate descent scheme to solve <ref type="bibr" target="#b17">(18)</ref>, in which each variable is solved alternately, while keeping other variables fixed. Due to the convexity Algorithm 1: Third-order Dictionary Learning and Sparse Coding.</p><p>Data: N data tensors X ≡ {X 1, ..., X N }, proximity and sparsity constants γ and λ, stepsize η, K dictionary</p><formula xml:id="formula_18">atoms B ≡ {(B k , b k )} K k=1 if coding, otherwise LearnDict = true Result: N sparse coeffs. α ≡ {α 1 , ..., α N }, K atoms B ≡ {(B k , b k )} K k=1 if LearnDict = true Initialization: if LearnDict then</formula><p>• Uniformly sample slices from X and fill B k , ∀k ∈ IK • Uniformly sample values in − 1 K ; 1 K to init. b k , ∀k ∈ IK end • Vectorize slices X s n , ∀s ∈ IS, ∀n ∈ IN , and atoms B k , ∀k ∈ IK , solve Lasso problem to fill β s,n , ∀s ∈ IS, ∀n ∈ IN • Uniformly sample values in − 1</p><formula xml:id="formula_19">Kλ ; 1 Kλ to fill α n , ∀n ∈ IN • Objective (0) = 0, t = 1 Main loop: while ¬ Converged do if LearnDict then • B (t+1) k = ΠSP D B (t) k −η ∂f ∂ B k B (t) k , ∀k ∈ IK, where f is the cost from (18), projection ΠSP D (B) = B * Tr(B * ) , B * = U max(λ * , 0)V T for (U , λ * , V ) = SV D(B) • b (t+1) k = Π+ b (t) k − η ∂f ∂ b k b (t) k</formula><p>, ∀k ∈ IK , and</p><formula xml:id="formula_20">Π+(b) = max(b, 0) if Π+ is used end • β s,n,(t+1) = Π+ β s,n,(t) − η ∂f ∂ β β s,n,(t) k , ∀s ∈ IS, ∀n ∈ IN , use of Π+ is optional • α (t+1) n = Π+ α (t) n − η ∂f ∂ α α (t) n , ∀n ∈ IN , use of Π+ is optional • Objective (t) = f X , B (t) , b (t) , α (t) , where B (t) and b (t)</formula><p>are K matrix and vector atoms • Converged = EvaluateStoppingCriteria t, Objective (t) , Objective (t−1) end of each sub-problem, we observe convergence to a local minimum empirically, but a theoretical convergence analysis is difficult due to the non-smoothness of the problem. Initial values of B k , b k , and α n are chosen randomly from the uniform distribution within a prespecified range. Vectors β s,n are initialized with the Lasso algorithm. Next, we solve four separate convex sub-problems resulting in updates of B k , b k , β s,n , and α n . For learning second-order matrices B k , we employ the PQN solver <ref type="bibr" target="#b31">[32]</ref> which lets us implement and apply projections into the SPD cone, handle the trace norm or even the rank of matrices (if this constraint is used). Similarly, the projected gradient is used to keep b k inside the simplex b k 1 ≤ 1. For the remaining sub-problems, we use the L-BFGS-B solver <ref type="bibr" target="#b3">[4]</ref> which enables us to handle simple box constraints e.g., we split coding for α n into two non-negative sub-problems α n + and α n − (unless α n ≥ 0) by applying box constraints α n + ≥ 0 and α n − ≥ 0. Finally, we obtain α n = α n + − α n − after minimization. For the sparse coding phase (no dictionary learning), we fix {(B k , b k )} K k=1 . The algorithm proceeds as above, but without updates of B k and b k . Algorithm 1 shows the important steps of our dictionary learning and sparse coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Theory</head><p>In this section, we analyze the theoretical properties of our sparse coding formulation. We establish that under an approximation setting, a sparse coding of the data tensor exists in some dictionary for every data point. We also provide bounds on the quality of this approximation and its tensor rank. Note that the exact sparse recovery is not mandatory in image classification <ref type="bibr" target="#b40">[41]</ref>. Thus, we skip this line of investigation. The following lemma comes useful in the sequel.</p><formula xml:id="formula_21">Lemma 1 ([35]). Let Y = m i=1 φ i φ T i , where each φ i ∈ R d and m = Ω(d 2 )</formula><p>. Then, there exist an α ∈ R m + and ǫ ∈ (0, 1) such that:</p><formula xml:id="formula_22">Y m i=1 α i φ i φ T i (1 + ǫ)Y,<label>(19)</label></formula><p>where α has O(d log(d)/ǫ 2 ) non-zeros.</p><formula xml:id="formula_23">Theorem 1. Let X = m i=1 (φ i φ T i ) ↑ ⊗ φ i , then</formula><p>there exist a second-order dictionary B with md atoms and a sparse coefficient vector β ∈ R md + with O(d 2 log(d)/ǫ 2 ) non-zeros, such that for ǫ ∈ (0, 1),</p><formula xml:id="formula_24">I) X = m i=1 (φ i φ T i )↑ ⊗β i and II) X − X F ≤ ǫ d s=1 X s F ,</formula><p>where X s is the s-th slice of X andβ i ∈ R d .</p><p>Proof. To prove I:</p><formula xml:id="formula_25">each slice X s = m i=1 (φ i φ T i )φ s i , where φ s i is the s-th dimension of φ i . Let φ ′ i = φ i φ s i , then X s = m i=1 φ ′ i φ ′ T i .</formula><p>If the slices are to be treated independently (as in <ref type="formula" target="#formula_0">(18)</ref>), then to each slice we can apply Lemma 1 that results in sparse coefficient vectorsβ i having O(d log(d)/ǫ 2 ) non-zeros. Extending this result to all the d-slices, we obtain the result. To prove II: substituting X with its sparse approximation and using the upper-bound in <ref type="bibr" target="#b18">(19)</ref> for each slice, the result follows.</p><p>Theorem 2 (Approximation quality via Tensor Rank). Let X be the sparse approximation to X ∈ S d obtained by solving (17) using a dictionary B and if Rank(B) represent the maximum of the rank of any atom, then: where Supp(X s ) is the support set produced by <ref type="bibr" target="#b16">(17)</ref> for slice X s .</p><formula xml:id="formula_26">TRank( X ) ≤ min d s=1 Supp(X s ) Rank(B), d 2 ,<label>(20)</label></formula><formula xml:id="formula_27">(a) (b) (c) (d) (e)</formula><p>Proof. Rewriting X in terms of its sparse tensor approximation, followed by applying Proposition 2 and Definition 2, the proof directly follows. Note that the maximum tensor rank of X is d 2 .</p><p>Theorem 2 gives a bound on the rank of approximation of a tensor X by X (which is a linear combination of atoms). Knowing rank of X and X helps assess the quality of approximation (beyond the Euclidean norm between X and X ). This is useful in experiments with imposed Rank-R constraints (R = 1, 2, ...) on atoms B k and helps measure how rank constraints on B k impact the approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiments</head><p>In this section, we present experiments demonstrating the usefulness of our framework. As second-order region covariance descriptors (RCD) are the most similar tensor descriptors to our TOSST descriptor, we decided to evaluate our performance on applications of RCDs, specifically for texture recognition. In the sequel, we evaluate our novel TOSST texture descriptor and compare to the state of the art on two texture benchmarks, namely the Brodatz textures <ref type="bibr" target="#b25">[26]</ref> and the UIUC materials <ref type="bibr" target="#b21">[22]</ref>. Moreover, experiments illustrating behaviour of our dictionary learning are provided. We also demonstrate the adequacy of our framework to compression of third-order global image descriptors <ref type="bibr" target="#b17">[18]</ref> on the challenging PASCAL VOC07 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Datasets</head><p>The Brodatz dataset illustrated in <ref type="figure" target="#fig_2">Figures 1(a)-1(b)</ref> contains 111 different textures each one represented by a single 640×640 image. We follow the standard protocol, i.e., each texture image is subdivided into 49 overlapping blocks of size 160×160. For training, 24 randomly selected blocks are used per class, while the rest is used for testing. We use 10 data splits. The UIUC materials dataset illustrated in <ref type="figure" target="#fig_2">Figures 1</ref>  in the wild from four general categories e.g., bark, fabric, construction materials, and outer coat of animals. Each subcategory contains 12 images taken at various scales. We apply a leave-one-out evaluation protocol <ref type="bibr" target="#b21">[22]</ref>. The PAS-CAL VOC07 set consists of 5011 images for training and 4952 for testing, 20 classes of objects of varied nature e.g., human, cat, chair, train, bottle, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Experimental Setup</head><p>TOSST descriptors. We evaluate the following variants of the TOSST descriptor on the Brodatz dataset: linear descriptor with first-and -second order derivatives as in <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b8">9)</ref> (i) but without luminance, and ii) with luminance. RBF descriptor as in (8, 10) (iii) without luminance, and iv) with luminance. Moreover, v) is a variant based on (iv) that uses the opponent color cues for evaluations on the UIUC materials set <ref type="bibr" target="#b21">[22]</ref>. Removing luminance for (i) and (iii) helps emphasize the benefit of RBF over the linear formulation. First though, we investigate our dictionary learning. Dictionary learning. We evaluate several variants of our dictionary learning approach on the Brodatz dataset. We use the RBF descriptor without the luminance cue (iii) to prevent saturation in performance. We apply patch size 40 with stride 20 to further lower computational complexity, sacrificing performance slightly. This results in 49 TOSST descriptors per block. We do not apply any whitening, thus preserving the SPD structure of the TOSST slices. <ref type="figure" target="#fig_0">Figures 2(a)</ref> and 2(b) plot the accuracy and the dictionary learning objective against an increasing size K of the dictionary. We analyze three different regularization schemes on the second-order dictionary atoms in <ref type="bibr" target="#b17">(18)</ref>, namely (1) with only SPD constraints, (2) with SPD and lowrank constraints, and (3) without SPD but TRank(B k ) = Rank(B k ) ≤ d. When we enforce Rank(B k ) ≤ R &lt; d for k = 1, ..., K to obtain low-rank atoms B k by scheme (2), the convexity w.r.t. B k is lost because the optimization domain in this case is the non-convex boundary of the PD cone. However, the formulation (18) for schemes <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula">(3)</ref> is convex w.r.t. B k . The plots highlight that for lower dictionary sizes, variants (1) and (2) perform worse than <ref type="formula">(3)</ref> which exhibits the best performance due to a very low objective. However, the effects of overfitting start emerging for larger K. The SPD constraint appears to offer the best trade-off resulting in a good performance for both small and large K. In <ref type="figure" target="#fig_0">Figure 2</ref>(c), we further evaluate the performance for a Rank-R constraint given K = 20 atoms. The plot highlights that imposing the low-rank constraint on atoms may benefit classification. Note that for 15 ≤ R ≤ 30, the classification accuracy fluctuates up to 2%, which is beyond the standard deviation error (≤ 0.4%) despite any significant fluctuation to the objective in this range. This suggests that imposing some structural constraints on dictionary atoms is an important step in dictionary learning. Lastly, <ref type="figure" target="#fig_0">Figure 2</ref>(d) shows the classification accuracy for the coding and pooling steps w.r.t. λ controlling sparsity and suggests that the texture classification favors low sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Comparison to the State of the Art on Textures</head><p>In this experiment, descriptor variants were whitened as in <ref type="bibr" target="#b17">[18]</ref>. In each block, we extract 225 TOSST descriptors with patch size 20 and stride 10, apply our dictionary learning given K = 2000, followed by the sparse coding, and perform pooling as in <ref type="bibr" target="#b18">[19]</ref> prior to classification with SVM. Table 2 demonstrates our results which highlight that the RBF variant outperforms the linear descriptor. This is even more notable when the luminance cue is deliberately removed from descriptors to degrade their expressiveness. <ref type="table" target="#tab_2">Table 2</ref> also demonstrates state-of-the-art results. With 99.9±0.08% accuracy, our method is the strongest performer. Additionally, <ref type="table" target="#tab_2">Table 2</ref> provides our results on the UIUC materials obtained with descriptor variant (v) (described above) and K = 4000 dictionary atoms. We used TOSST descriptors <ref type="bibr">(</ref>   with patch size 40, stride 20 and obtained 58.0 ± 4.3% 2 which outperforms other methods. <ref type="table" target="#tab_4">Table 3</ref> shows the compression rates achieved with our coding prior to pooling of sparse features <ref type="bibr" target="#b18">[19]</ref>, which could not be performed directly on e.g., Product Quantized codes <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Signature Compression on PASCAL VOC07</head><p>In this section, we evaluate our method on the PAS-CAL VOC07 dataset. We use the setup in <ref type="bibr" target="#b17">[18]</ref> to generate Third-order Global Image Descriptors detailed in Section 4.2 with the goal of compressing them by our sparse coding framework. In detail, we use SIFT vectors extracted from gray-scale images with radii 12, 16, 24, 32, and stride 4, 6, 8, 10, reduce SIFT size to 90D, append SPM codes of size 11D <ref type="bibr" target="#b17">[18]</ref>, and obtain the global image signatures with the baseline score of 61.3% MAP. Next, we learn dictionaries using our setup as in equation <ref type="formula" target="#formula_0">(18)</ref> and apply our sparse coding to reduce signature size from 176851 (upper simplex of TOSST) to sizes from 2K to 25K. The goal is to regain the baseline score. <ref type="figure" target="#fig_4">Figure 3(a)</ref> shows that a learned dictionary (DL) of size 25K yields 61.2% MAP at 7× compression rate. A random dictionary is about 4.3% worse. For completeness, <ref type="figure" target="#fig_4">Figure 3</ref>(b) shows results for Product Quantization (PQ) <ref type="bibr" target="#b13">[14]</ref>. PQ compressor requires partial decompression (e.g., in SGD-based SVM) unlike sparse codes. In the extreme case, it reduces the bit-rate of each dimension . It is complementary to our dictionary learning. Combined approach (DL+PQ) outperforms PQ by up to 3% MAP. Complexity. On a 4-core CPU and using Matlab, our sparse coding takes about 3.2s using d = 30 and a dictionary with 2K atoms. Dictionary Learning converges in about 50 iterations. We have observed that about 9× speedup is possible for SGD. The sparse coding sub-problem includes solving for the variables β and α alternately -which are nonnegative (ℓ 1 constrained) least squares problems; each alternating iteration of which costs O(S(K 2 S + K 3 )) time for K atoms and S slices. As for dictionary learning, each dictionary update takes O(KS 3 ) time. <ref type="bibr">Excellence</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Proposition 2 (</head><label>2</label><figDesc>Atom Rank). Let B = B ↑ ⊗ b and b 1 = 0, then TRank(B) = Rank(B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>subject to B k 0, Tr(B k ) ≤ 1, b k 1 ≤ 1, k ∈ I K . (18)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Examples of textures in 1(a) and 1(b) show 4 images from 4 different classes of the Brodatz dataset. Note the high inerclass similarity between the top and bottom images. Figures 1(c)-1(e) show 2 samples per class per column to illustrate UIUC materials. Note high interclass variations (in contrast to Brodatz).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Parameter sensitivity on Brodatz textures: 2(a) and 2(b) show classification accuracy and objective values against various dictionary sizes respectively, 2(c) and 2(d) show accuracy with fixed K = 20, but varying the atom rank R, and sparsity regularization λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Impact of K in the signature compression on PASCAL VOC07. Dictionary learning (DL) in 3(a) outperforms (Random Dict.) formed by atoms sampled from a distribution of DL. In 3(b), DL merged with Product Quantization (DL+PQ) is compared to Product Quantization (PQ) w.r.t. compressed signature size K * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>i) linear, no lum. (ii) linear, lum. (iii) RBF, no lum. (iv) RBF, lum. (v) RBF, lum., opp. ECM 97.9% [21] CDL 52.3±4.3% [40]RC 97.7%<ref type="bibr" target="#b35">[36]</ref> RSR 52.8±5.1%<ref type="bibr" target="#b10">[11]</ref> </figDesc><table>dataset 
Brodatz 
Brodatz 
Brodatz 
Brodatz 
UIUC materials 
ten. size d 
6 
7 
30 
35 
45 
accuracy 
93.9±0.2% 
99.4±0.1% 
99.4±0.2% 
99.9±0.08% 58.0±4.3% 2 

Brodatz 
UIUC materials 
ELBCM 98.72% [31] 
SD 43.5% [22] 
L 2 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Evaluations of the proposed TOSST descriptor (left) and comparisons to the state of the art (right).</figDesc><table>K 

MAP (%) 

1K 5K 9K 13K 17K 21K 25K 
46 

48 

50 

52 

54 

56 

58 

60 

62 

DL 
Rand. Dict. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>for Robotic Vision (project number CE140100016).</figDesc><table>Brodatz 
UIUC 
PASCAL VOC07 
d 
30 
35 
45 
101 
sig. size 
4960 
7770 
16215 
176851 
K 
100-2000 
2000 
4000 
25000 
compr. 
49.6×-2.48× 
3.88× 
4.05× 
7.07× 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Compression rates (sig. size -number of unique coefficients per tensor, d -side dimension, K -vocabulary size).</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that, third-order descriptors can be easily aggregated from CNN features similar to<ref type="bibr" target="#b8">[9]</ref>; however, the choice of input features is complementary to our main goal -theoretical foundations of third-order tensor coding.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that we use for simplicity a single scale/size descriptor. With multiple scales/sizes, this result will improve further as UIUC materials exhibit large intraclass scale variations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the ARC through the ICT Centre of Excellence program. AC is funded by the Australian Research Council Centre of</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions</head><p>We presented a novel formulation and an efficient algorithm for sparse coding third-order tensors using a learned dictionary consisting of both first-and second-order atoms. Our experiments demonstrate that our scheme leads to significant compression of the input tensors, while not sacrificing accuracy. Further, we proposed a novel tensor descriptor for texture recognition, which when sparse-coded by our approach, achieves state-of-the-art accuracy on two benchmark datasets. Our approach is general and useful for a variety of other applications, e.g., action recognition <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Derivation of the Third-order Aggregation.</p><p>Rising K rbf to the power r and applying the outer product ↑ ⊗ r of order r to v xy yield a higher-order tensor of rank one denoted as V xy :</p><p>where i ∈ {a, b}. Next, tensors V xy are aggregated over image regions R a and R b as:</p><p>and (x ′ , y</p><p>The aggregation step in <ref type="formula">(21)</ref> is analogous to (2-3) and step (ii) in Section 5. We shift the origin x a 0 to x b 0 and obtain x ′ in R b corresponding to x in R a . Next, higher-order autocorrelation tensorsV are formed, which are whitened using (4-7). Our experiments use r = 3. Using dictionary learning and sparse coding, we generate mid-level features, one per region. Given a test image and a set of overlapping regions, we perform pooling <ref type="bibr" target="#b18">[19]</ref> over such mid-level features to obtain a descriptor that is then used in a classifier.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Log-Euclidean metrics for fast and simple calculus on diffusion tensors. Magnetic resonance in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object recognition with hierarchical kernel descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalized histogram intersection kernel for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Riemannian sparse coding for positive definite matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tensors: a brief introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Action recognition from video using feature covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse coding and dictionary learning for symmetric positive definite matrices: A kernel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability product kernels. JMLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="844" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the burstiness of visual elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tensor canonical correlation analysis for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1604.00239" />
		<title level="m">Tensor representations via kernel linearization for action recognition from 3D skeletons. ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Higherorder occurrence pooling for bags-of-words: Visual concept detection. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparison of midlevel feature coding approaches and pooling strategies in visual concept detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A multilinear singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Local log-Euclidean covariance matrix (L 2 ECM) for image representation and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Non-parametric filtering for geometric detail extraction and material representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey of multilinear subspace learning for tensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Venetsanopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1540" to="1551" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<title level="m">Convolutional kernel networks. NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Riemannian framework for tensor computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<title level="m">Covariance tracker. CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhanced local binary covariance matrices (ELBCM) for texture analysis and object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Terán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gouiffès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lacassagne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIRAGE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Friedlander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIS-TATS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-negative tensor factorization with applications to statistics and computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tensor sparse coding for region covariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sivalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Morellas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph sparsification by effective resistances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1913" to="1926" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Region covariance: A fast descriptor for detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<title level="m">Multilinear analysis of image ensembles: Tensorfaces. ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tensortextures: multilinear image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="336" to="342" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tracking by third-order tensor representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="385" to="396" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Covariance discriminative learning: A natural and efficient approach to image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A comprehensive study on third order statistical features for image splicing detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Forensics and Watermarking</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="256" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
