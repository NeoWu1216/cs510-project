<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Monocular Depth Estimation in Complex Dynamic Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Monocular Depth Estimation in Complex Dynamic Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach to dense depth estimation from a single monocular camera that is moving through a dynamic scene. The approach produces a dense depth map from two consecutive frames. Moving objects are reconstructed along with the surrounding environment. We provide a novel motion segmentation algorithm that segments the optical flow field into a set of motion models, each with its own epipolar geometry. We then show that the scene can be reconstructed based on these motion models by optimizing a convex program. The optimization jointly reasons about the scales of different objects and assembles the scene in a common coordinate frame, determined up to a global scale. Experimental results demonstrate that the presented approach outperforms prior methods for monocular depth estimation in dynamic scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Can mobile monocular systems densely estimate the spatial layout of complex dynamic scenes? Can a mobile robot, UAV, or wearable device equipped with a single video camera see complex dynamic environments in three dimensions? In static scenes, dense depth can be recovered from a single video captured by a moving camera using the established theory of multiple view geometry <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9]</ref>. Can this theory be extended to reconstruct complex scenes from monocular video when both the camera and the scene are in motion?</p><p>To support challenging field applications, a monocular depth reconstruction approach should have a number of characteristics. It must provide complete dense reconstructions, in order to support dense mapping and detailed geometric reasoning. It must natively handle complex scenes, with dozens of objects moving independently in a complex environment. It must accommodate non-rigid motion, so as to properly perceive people, animals, and articulated struc- <ref type="figure">Figure 1</ref>: Given two frames from a monocular video of a dynamic scene captured by a single moving camera, our approach computes a dense depth map that reproduces the spatial layout of the scene, including the moving objects. Top: input frames. The white vehicle is approaching the camera, while the camera itself undergoes forward translation and in-plane rotation. Bottom: the estimated depth map.</p><p>tures. And it must accommodate realistic camera models, including perspective projection.</p><p>In this paper, we present a monocular depth estimation approach that has all of these characteristics. The approach densely estimates depth throughout the visual field, including both static and dynamic parts of the environment. Multiple moving objects, complex geometry, and non-rigid motion are accommodated. The approach works with perspective cameras and yields metric reconstructions.</p><p>Our approach comprises two stages. The first stage performs motion segmentation. This stage segments the dynamic scene into a set of motion models, each described by its own epipolar geometry. This enables reconstruction of each component of the scene up to an unknown scale. We propose a novel motion segmentation algorithm that is based on a convex relaxation of the Potts model <ref type="bibr" target="#b4">[5]</ref>. Our algorithm supports dense segmentation of complex dynamic scenes into possibly dozens of independently moving components.</p><p>The second stage assembles the scene in a common met-ric frame by jointly reasoning about the scales of different components and their location relative to the camera. The main insight is that moving objects do not exist in a vacuum, but fulfill intrinsic occluder-ocludee relationships with respect to each other and the static environment. This can be used to reason about the placement of different objects in the scene. We formulate this reconstruction problem as continuous optimization over scales and depths and introduce ordering and connectivity constraints to assemble the scene. The result is a reconstruction of the dynamic scene from only two frames, determined up to a single global scale. We evaluate the presented approach on complex dynamic sequences from the challenging Sintel and KITTI datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>. In all cases, the input is monocular video: we do not use stereo or depth input. Our approach outperforms prior depth estimation techniques by a significant margin. <ref type="figure">Figure 1</ref> shows a reconstruction produced by the presented approach on a dynamic scene from the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>Three significant families of approaches have been proposed for estimating dynamic scene geometry from monocular video: multibody structure-from-motion, non-rigid structure-from-motion, and non-parametric depth transfer. We briefly review each approach in turn.</p><p>Multibody structure-from-motion is the most direct extension of classical multi-view geometry to dynamic environments <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. This approach is based on the assumption that the environment consists of multiple rigidly moving objects. The basic idea is to cluster feature tracks and fit rigid motion models to each cluster. Since each cluster is assumed to be rigid, traditional multi-view techniques can be applied to estimate its motion, assuming proper segmentation and a sufficient number of tracks. This approach typically assumes a small set of rigid objects in the scene and has not produced detailed reconstructions of complex scenes with non-rigidly moving objects. We contribute new robust formulations that accommodate significantly more general objects and environments.</p><p>The second family of approaches for three-dimensional reconstruction of dynamic scenes from monocular video is non-rigid structure-from-motion <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. The elegant mathematical formulations employed by these approaches hinge on strong assumptions: typically, object shape or motion trajectory matrices are assumed to be low-rank and the camera model is assumed to be orthographic. This severely restricts the applicability of these techniques. While recent work has sought to relax some of the constraints of earlier formulations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11]</ref>, significant limitations remain. For example, Garg et al. <ref type="bibr" target="#b11">[12]</ref> reconstruct a single foreground object that is assumed to be manually presegmented. Russell et al. <ref type="bibr" target="#b26">[27]</ref> deal with scenes dominated by a single foreground object and demonstrate reconstruction results qualitatively on three videos. Fragkiadaki et al. <ref type="bibr" target="#b10">[11]</ref> produce non-metric reconstructions of track clusters in separate coordinate systems and do not estimate the layout of the scene. In contrast, our approach estimates dense depth for complex dynamic scenes over the entire visual field. All objects are reconstructed jointly, yielding consistent reconstructions of complete scenes.</p><p>The third family of approaches for monocular depth estimation in dynamic scenes is non-parametric depth transfer <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. This approach relies on the availability of a dataset of color-depth image pairs at test time. The dataset is assumed to contain scenes that have a similar geometric layout and similar appearance to the test scene. For a given test video, similar images are retrieved from the dataset for every frame, corresponding depth images are warped to fit the test frames, and the resulting depth estimates are spatiotemporally regularized. This approach requires the availability of an appropriate dataset with ground-truth depth data at test time. It is limited to environments that are compatible with the available training data. In contrast, we present a geometric method that does not require a training dataset and naturally applies to novel environments.</p><p>Motion and epipolar models can also be used to improve optical flow estimation. Hornacek et al. <ref type="bibr" target="#b16">[17]</ref> used an overparametrization approach to estimate optical flow, which also explicitly reasons about the depth and rigid body motion at each pixel in the image. They recover depth only up to an unknown scale for each rigid object, since their main goal is to use epipolar models to guide optical flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview and Preliminaries</head><p>The proposed pipeline consists of two major stages. First, the scene is segmented into a set of epipolar motion models. The segmentation is performed on optical flow and is formulated as a variational labeling problem. (Note that segmentation of the optical flow field has been explored in the past <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref>.) The second stage performs triangulation and joint reconstruction of all objects. The key assumption in the second stage is that the scene consists of objects that are connected in space. In particular, we assume that dynamic objects are connected to the surrounding environment. This assumption is true for many scenes likely to be encountered by a mobile vision system, such as a robot or a wearable device. In particular, vehicles and people are generally supported by surrounding structures. Note that we do not make narrow assumptions about the supporting structures, say by estimating the ground plane, but infer the point of attachment flexibly by reasoning about the scene as a whole.</p><p>Let M be the number of pixels in the image. We index integer positions on the image grid using the superscript i: for example, (x i , y i ) refers to the x and y coordinates of the pixel indexed by i. We use the standard operator ∇ : R M → R 2M to denote the linear operator corresponding to the discrete forward differences in the x and y directions. We denote the standard Euclidean norm by · and use subscripts whenever a different norm is used. Specifically, we will make use of the following norm:</p><formula xml:id="formula_0">p 2,1 = M i=1 (p i ) 2 + (p i+M ) 2 , p ∈ R 2M . (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Motion Segmentation</head><p>The task of the motion segmentation stage is to decompose the dynamic scene into a set of independent rigid motions, each described by a fundamental matrix, together with a per-pixel assignment to these motion models. Note that this approach automatically oversegments non-rigid objects into approximately rigid parts. We estimate the number of independent rigid motions as part of the global optimization to ensure that non-rigid motions are approximated well. To generate metric reconstructions, we assume that the intrinsic camera parameters are known. In this form, the motion segmentation problem is an instance of the more general multiple-model fitting problem. Existing state-ofthe-art approaches typically assume sparse correspondences <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref> and are thus unable to process dense correspondence fields in reasonable time. We propose a new approach that efficiently handles dense correspondence fields, as produced by dense optical flow estimation. Our approach is most closely related to the discrete energy-based multiplemodel fitting approach of Isack and Boykov <ref type="bibr" target="#b17">[18]</ref>, but operates on soft assignments and models the data association as a continuous convex problem. This allows us to leverage recent advances in convex optimization <ref type="bibr" target="#b5">[6]</ref> and enables an efficient GPU-based implementation.</p><p>The motion segmentation takes as input a dense optical flow field f = (f x , f y ) : f x , f y ∈ R M between images I 1 and I 2 , and produces a soft assignment u l ∈ [0, 1] M of each pixel to either one of L distinct motion models F l or an additional outlier label L + 1. We formulate this as a joint labeling and estimation problem, where we additionally exploit the fact that nearby pixels are likely to belong to the same motion model:</p><formula xml:id="formula_1">(u * l , F * l ) = arg min u l ,F l L+1 l=1 u l · g(F l ) + W l ∇u l 2,1 (2) subject to L+1 l=1 u i l = 1, u i l ≥ 0 (SPX) ∀l. rank(F l ) = 2.<label>(EPI)</label></formula><p>To measure the fitting error of the motion models with respect to the observed correspondences, we compute the symmetric distance to the epipolar lines <ref type="bibr" target="#b15">[16]</ref> for each model l ∈ {1 . . . L}:</p><formula xml:id="formula_2">g i (F l ) = d(x i 1 , F l x i 2 ) 2 + d(x i 2 , F ⊤ l x i 1 ) 2 ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">x i 1 = [x i , y i , 1] ⊤ are homogeneous coordinates in the first image, x i 2 = [x i − f i x , y i − f i y , 1</formula><p>] ⊤ denote their corresponding homogeneous coordinates in the second image, and d denotes the Euclidean point-to-line distance. With a slight abuse of notation, we assign a fixed cost g(F L+1 ) = γ to the outlier label. We further estimate occlusions and gross errors in the optical flow using a forwardbackward consistency check and fix the assignment of occluded pixels to the outlier label. The smoothness term W l ∇u l 2,1 reflects the fact that nearby correspondences are likely to belong to the same motion model. We use a diagonal weighting matrix W l to enable edge-preserving regularization based on the reference image I 1 :</p><formula xml:id="formula_4">W l = diag exp − β ∇I 1 2 .<label>(4)</label></formula><p>The simplex constraint (SPX) ensures that the soft assignments sum to one at each pixel, thus u i l ∈ [0, 1] can be interpreted as the probability that pixel i belongs to the motion model F l . The matrices F l ∈ R 3×3 encapsulate the epipolar geometry of the pixels belonging to segment l. The rank constraints (EPI) ensure that each F l is a fundamental matrix <ref type="bibr" target="#b15">[16]</ref>.</p><p>Energy <ref type="formula">(2)</ref> is a joint optimization problem in the unknown motion models F l and the pixel-to-motion-model assignments u l . The energy is non-convex due to the complex dependence on the fundamental matrices F l . Even worse, the number of independent motion models, L, is unknown a priori. For a fixed set of motion models, however, energy (2) is convex <ref type="bibr" target="#b4">[5]</ref>. We will exploit this property to derive an iterative algorithm to approximately minimize (2).</p><p>Let us first consider a simplified example, where the number of independent motions is known a priori. Thus the number of labels (L + 1) is fixed. Energy (2) can be approximately optimized using a block coordinate descent strategy that iterates over the following two steps. First, fix the motion models F l and optimize for the assignment probabilities u. Second, fix the assignment probabilities u and optimize F l . We use a recently proposed variant of the primaldual algorithm <ref type="bibr" target="#b5">[6]</ref> that employs entropy proximal terms to implicitly represent the simplex constraints (SPX) in order to efficiently solve for the labeling. The re-estimation of the fundamental matrices F l can be decomposed over the individual models and solved for all L models in parallel. In particular, we exploit the soft assignments u i to reweigh individual correspondences:</p><formula xml:id="formula_5">F * l = arg min F l M i=1 u i l (x i 1 ) ⊤ F l (x i 2 ) 2 subject to rank(F l ) = 2.<label>(5)</label></formula><p>These subproblems can be approximately solved using a reweighted version of the normalized 8-point algorithm <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. It is important to note that we do not perform a hard assignment of correspondences to models. Instead, each correspondence (x i 1 , x i 2 ) contributes to the estimation of every model F l according to its inlier probability u i l . In order to discover the number of motion models we opt for a simple greedy bootstrapping strategy, where we make extensive use of the outlier label to mine potential funda- repeat ⊲ Data association <ref type="bibr">12:</ref> Minimize (2) for u l to get (u l ) n+1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Update F l by solving <ref type="formula" target="#formula_5">(5)</ref> 14:</p><p>until no decrease in energy <ref type="formula">(2)</ref> 15: 16:</p><p>Recover hard assignmentû l using <ref type="formula" target="#formula_7">(6)</ref> 17:</p><p>for each l = 1 . . . L + 1 do ⊲ Model discovery <ref type="bibr">18:</ref> Splitû l into connected components C j</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>for each j = {1 . . . J} with |C j | &gt; T do <ref type="bibr">20:</ref> Find F using LMedS on (x 1 , x 2 ) ∈ C j 21:</p><formula xml:id="formula_6">F ← F ∪ {F } 22:</formula><p>end for <ref type="bibr">23:</ref> end for 24: until mental matrices from the data. We start by mining a small set of candidate motions by iteratively applying the normalized 8-point algorithm in a robust least-median-of-squares (LMedS) framework <ref type="bibr" target="#b31">[32]</ref>. Based on this initialization, we solve energy (2) using the previously described alternating minimization approach until no further decrease in energy can be made. We then expand the pool of candidate motions. New models are added by robustly estimating motion models from pixels that have the outlier label as their most probable assignment. Specifically, we robustly fit a motion model to each connected component of the pixels assigned to the outlier label. We further expand the pool by splitting labels with disconnected regions and fitting motion models to these regions if the size of the region is larger than a threshold T . (Note that we do not remove the original models from the set of models.) We again perform alternating minimization based on this new label set and repeat this process until no further decrease in energy can be made. We found that this strategy is generally able to discover the number of models, the models themselves, and their perpixel assignments within 10 iterations. A summary of the algorithm can be found in Algorithm 1.</p><p>The result of the motion segmentation stage is a set of epipolar geometries F * l as well as membership probabilities u * l for each pixel. We obtain the final pixel-to-model associations by extracting the label with maximum probability from u * l to getû l : <ref type="figure" target="#fig_0">Figure 2</ref> shows an example result of the motion segmentation stage.</p><formula xml:id="formula_7">u i l = 1 if l = max l∈{1,...,L+1} (u * l ) i 0 else.<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Reconstruction</head><p>While the results of modern optical flow algorithms are reliable for many scenes, they still exhibit artifacts in most cases. The optical flow may be noisy due to properties of the model (e.g., staircasing artifacts in models leveraging firstorder smoothness assumptions), motion boundaries are often badly localized (edge bleeding), and some regions might be completely wrong. A reconstruction pipeline that relies on optical flow needs to be robust to these errors. We use a superpixel-based formulation in order to robustly reconstruct the dynamic scene from optical flow correspondences and the epipolar models estimated in Section 4.</p><p>We begin by triangulating each correspondence that was not labeled an outlier by the motion segmentation stage using its associated motion model F * l . This yields a set of depth estimates z l ∈ R M . Note that each depth estimate is only valid for pixel i withû i l = 1. We set pixels that belong to the segment with largest support as environment pixels and fix their scales to 1.</p><p>We now estimate the relative scales between all segments. This cannot be done without additional prior assumptions as the problem is ill-posed in general. For example, when a plane is seen in the sky, it is generally impossible to tell how large it is or how far it is: it could be a Boeing 737 at a certain distance or a larger 747 that is farther away. To resolve scale ambiguities and assemble the scene, we use a prior assumption that is often appropriate in daily life: objects are supported by their environment. We model this assumption using a combination of two constraints:</p><p>1. An ordering constraint, which captures the assumption that dynamic objects occlude the static environment. This can be expressed by requiring the inverse depth of segments belonging to the dynamic objects to be larger or equal to the inverse depth of the environment in their immediate vicinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A smoothness constraint, which states that jumps in inverse depth between dynamic objects and segments belonging to the environment should be minimized. This constraint connects the dynamic objects with the environment, subject to the ordering constraint.</p><p>In order to be robust to outliers in the input data, we formulate these constraints as an energy minimization problem defined on a superpixel graph. Consider a superpixel segmentation of the reference image into K segments. That is, each pixel i is assigned to one of K superpixels. We formally write i ∈ P k for the set of pixels belonging to superpixel k and denote the edges in the superpixel graph by E. We use Quickshift <ref type="bibr" target="#b33">[34]</ref> to produce a superpixel segmentation and break up superpixels that straddle boundaries in the motion segmentation.</p><p>Our goal is to estimate a plane for each superpixel k with parameters θ k = [θ 1 k , θ 2 k , θ 3 k ] ⊤ and scales s = [1, s 2 , . . . , s L ] ⊤ ∈ R L + for all independently moving objects, subject to the previously described constraints. This is formulated as a convex optimization problem with the following objective:</p><formula xml:id="formula_8">E(s, θ) = E ord (θ) + E sm (θ) + E fit (s, θ).<label>(7)</label></formula><p>The following paragraphs define the three terms in this objective.</p><p>Ordering constraint. Let E d ⊂ E denote all pairs of edges in the superpixel graph that connect the static environment to dynamic objects. That is, (k, h) ∈ E d if k is part of the environment and h is part of a dynamic object. Let A P k ∈ R |P k |×3 be the matrix that results from vertically stacking all (x i 1 ) ⊤ belonging to segment k. We enforce a hard constraint on the planar reconstructions of these pixels that encapsulates the desired ordering:</p><formula xml:id="formula_9">E ord (θ) = (k,h)∈E d E loc (θ, k, h) E loc (θ, k, h) = 0 if max(A P k θ k ) ≤ A P h θ h ∞ else.<label>(8)</label></formula><p>Note that this term is convex as it can be represented as a set of linear inequality constraints of the form</p><formula xml:id="formula_10">θ ⊤ k x i 1 ≤ A P h θ h , ∀i ∈ P k<label>(9)</label></formula><p>Smoothness term. We impose smoothness by requiring that planes of neighboring superpixels coincide at their boundary B:</p><formula xml:id="formula_11">E sm (θ) = λ 2 (k,h)∈E (i,j)∈B k,h w kh θ ⊤ k x i 1 − θ ⊤ h x j 1 2 .<label>(10)</label></formula><p>The parameter λ &gt; 0 controls the overall smoothness of the solution and w k,h steers the smoothness according to superpixel appearance:</p><formula xml:id="formula_12">w k,h = exp −κ m k − m h 2 ,<label>(11)</label></formula><p>where m k and m h denote the average color of superpixels k and h, respectively.</p><p>Fitting term. The fitting term performs a plane fit to the scaled inverse depth values:</p><formula xml:id="formula_13">E fit (s, θ) = K k=1 i∈P k w i θ ⊤ k x i 1 − L l=1û i l s l z i l 2 .<label>(12)</label></formula><p>The interpretation of this term is as follows. For each pixel i, the inverse depth is scaled by the factor s l . Furthermore, the indicator variableû i l ensures that only a single reconstruction is active at a given pixel. By summing over all epipolar models we arrive at a reconstruction over the complete image, where individual parts are scaled by s l . θ ⊤ k x i 1 provides the inverse depth value at pixel i according to the Input GT DT <ref type="bibr" target="#b18">[19]</ref> Ours <ref type="figure">Figure 3</ref>: Results on three frames from the KITTI dataset. For each frame, the figure shows the input color image, the ground-truth depth (GT, inpainted for visualization), results produced by depth transfer (DT) <ref type="bibr" target="#b18">[19]</ref>, and results produced by our approach.</p><p>plane parameters θ k . Note that the fitting term in isolation is underconstrained, since it does not provide any information on the scales s. Arbitrary scales s can lead to the same minimal fitting energy, as the parameters θ can just be scaled accordingly. The weight w i reweighs the contribution of each pixel according to its residual error:</p><formula xml:id="formula_14">w i = exp − γ L l=1û i l g i l ifû i L+1 = 0 0 else.<label>(13)</label></formula><p>Energy <ref type="formula" target="#formula_8">(7)</ref> is convex and poses an optimization problem of moderate size. We use CVX for optimization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>, together with an efficient conic solver <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>We evaluate the presented approach quantitatively and qualitatively on two datasets that depict complex and realistic dynamic scenes: the MPI Sintel dataset <ref type="bibr" target="#b3">[4]</ref> and the KITTI dataset <ref type="bibr" target="#b12">[13]</ref>. To the best of our knowledge, this is the first quantitative evaluation of monocular reconstruction on dynamic scenes of this complexity.</p><p>The accuracy of the presented approach is compared to two state-of-the-art techniques for monocular depth estimation from video. The first is the depth transfer approach of Karsch et al. <ref type="bibr" target="#b18">[19]</ref>, a nonparametric method that relies on training data with ground-truth depth. Due to its nonparametric nature, this approach can be expected to perform well only if the test images are sufficiently similar to images in the training database.</p><p>The second approach we compare to is the nonrigid structure-from-motion formulation of Fragkiadaki et al. <ref type="bibr" target="#b10">[11]</ref>. This formulation was shown to outperform prior non-rigid structure-from-motion techniques. Unlike our approach, this method produces depth estimates for disconnected tracks, rather than for all pixels. The generated point clouds also lack absolute depth: all depth estimates lie in [−1, 1]. To maximize the accuracy reported for this approach, we only measure error along the tracks, rather than over all pixels in the input images. The generated point cloud is scaled to the range of the ground-truth depth map in each frame.</p><p>Accuracy is reported using three standard measures. The first is mean relative error (MRE). Let z be the estimated depth and let z gt be the ground-truth depth. MRE is defined as</p><formula xml:id="formula_15">MRE(z) = 1 M M i=1 |z i − z i gt | z i gt .<label>(14)</label></formula><p>This measures the relative per-pixel error: an error of 0.1m at a depth of 1m is penalized equally to an error of 1m at a depth of 10m. MRE is closely related to the depth contrast measure used to evaluate the effectiveness of different depth cues in human vision <ref type="bibr" target="#b7">[8]</ref>. For completeness, we also report the root mean square error (RMSE), defined as i (z i − z i gt ) 2 /M , and the log 10 error, defined as i | log 10 (z i ) − log 10 (z i gt )|. Only pixels with groundtruth depth within 20 meters are used for evaluation. In order to allow for a comparison in terms of absolute depth, we fit a global scale for each method and each frame such that the MRE is minimized.</p><p>The runtime of our approach is dominated by the motion segmentation stage, which is in turn dependent on the complexity of the motion in the scene. We implemented the labeling step on the GPU. All other parts are implemented in Matlab, which results in an execution time that is on the order of 1 minute per frame.</p><p>KITTI. We first evaluate the presented approach on the KITTI dataset. Specifically, we use the KITTI odometry set <ref type="bibr" target="#b12">[13]</ref>. The dataset provides sparse ground-truth depth measurements acquired by a LiDAR scanner. These are used for quantitative evaluation. We compared the presented approach to the prior approaches introduced above. Unfortunately, the implementation of Fragkiadaki et al. <ref type="bibr" target="#b10">[11]</ref> crashes on all sequences in this dataset. We thus report results for depth transfer (DT) <ref type="bibr" target="#b18">[19]</ref>. We make the 11 training sequences available to DT at test time. <ref type="figure">Figure 3</ref> shows qualitative results. Quantitative results are provided in <ref type="table">Table 1</ref>. DT performs well on this dataset. This can be attributed to the significant resemblance of scenes in the KITTI test set to images in the training data. In particular, the geometric layout of many frames in the KITTI dataset is almost identical. Nevertheless, our approach achieves a higher accuracy than DT on all reported metrics, without relying on any training data. MPI Sintel. The MPI Sintel dataset consists of complex computer graphics sequences. It was constructed for thorough evaluation of optical flow techniques, but has also been used for evaluating other low-level vision algorithms. The advantage of using computer graphics is the availability of precise ground truth. We use monocular sequences of color images as input and report results on the challenging 'final' rendering pass of this benchmark. To maximize the accuracy reported for depth transfer <ref type="bibr" target="#b18">[19]</ref>, we performed cross-validation such that for each test sequence all other sequences are made available as the training database. We exclude sequences from the evaluation that show no or only insignificant camera motion (alley 1, bandage 1, bandage 2, shaman 2). <ref type="table" target="#tab_3">Table 2</ref> provides the results of a quantitative evaluation on this dataset. To assess the sensitivity of our approach to the input optical flow, we have evaluated the approach when the input flow fields are computed by LDOF <ref type="bibr" target="#b2">[3]</ref>, EpicFlow <ref type="bibr" target="#b24">[25]</ref>, and FlowFields <ref type="bibr" target="#b1">[2]</ref>, respectively. The results demonstrate that the presented approach substantially outperforms the prior work with any of these input flows. With input flows provided by FlowFields <ref type="bibr" target="#b1">[2]</ref>, the presented MRE log 10 RMSE Depth Transfer <ref type="bibr" target="#b18">[19]</ref>   approach reduces the MRE by 40% relative to DT <ref type="bibr" target="#b18">[19]</ref> and by 30% relative to NR-SfM <ref type="bibr" target="#b10">[11]</ref>. The poor performance of DT on this dataset can be explained by the diversity of the sequences. A qualitative comparison is shown in <ref type="figure">Figure 5</ref>.</p><p>Limitations. The presented formulation is motivated in part by impressive recent advances in optical flow estimation. These advances are ongoing and are expected to continue <ref type="bibr" target="#b6">[7]</ref>. Our method can directly benefit from novel optical flow algorithms. On the other hand, if optical flow estimation fails, the presented approach will fail. A number of other limitations are inherent in the purely geometric nature of our approach, which does not use prior information about object shapes and sizes. In particular, the presented formulation will not yield accurate results for objects that are disconnected from their environment, such as birds in flight. <ref type="figure" target="#fig_1">Figure 4</ref> shows two failure cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented an approach to dense depth estimation from monocular video. Our approach leverages optical flow to segment a dynamic scene into a set of independently moving objects. We reason about the layout of the environment and the placement of the moving objects in it. The Input GT depth Input GT Depth DT <ref type="bibr" target="#b18">[19]</ref> DT <ref type="bibr" target="#b18">[19]</ref> NR <ref type="bibr" target="#b10">[11]</ref> NR <ref type="bibr" target="#b10">[11]</ref> Ours Ours  <ref type="figure">Figure 5</ref>: Results on four frames from the MPI Sintel dataset. For each frame, the figure shows the input color image, the ground-truth depth, and results produced by three techniques: depth transfer (DT) <ref type="bibr" target="#b18">[19]</ref>, non-rigid SfM (NR) <ref type="bibr" target="#b10">[11]</ref>, and our approach. We dilate the results of NR for visualization. For each technique, the estimated depth map is visualized on the right and per-pixel relative error is visualized on the left.</p><p>approach produces dense depth maps of complex dynamic scenes purely from geometric principles.</p><p>An important direction for future work is the incorporation of additional prior knowledge into our geometric framework. Nonparametric or learning-based approaches can be leveraged to improve the reconstruction and can also be used to estimate the absolute scale of the scene. We believe that combining these complementary techniques with our geometric approach can lead to a powerful general framework for monocular depth estimation from video. Other opportunities for future work are to couple the optical flow estimation and multiple model fitting and to enforce temporal consistency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example result of the motion segmentation stage. (a)-(b) The input image and the input optical flow [25]. (c) Segmentation result. Black pixels are assigned to the outlier label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Failure cases. (a) The flying dragon is pushed to the background, which overestimates its depth. (b) Failure due to erroneous input flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>MRE log 10 RMSE Depth Transfer<ref type="bibr" target="#b18">[19]</ref> 0.171 0.076 2.830Table 1: Quantitative evaluation on the KITTI dataset.</figDesc><table>Ours 
0.148 0.065 2.408 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation on the MPI Sintel dataset. We evaluate the accuracy of our approach when different optical flow estimation algorithms are used to produce the input flow. Results are reported on the challenging 'final' rendering pass.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Trajectory space: A dual representation for nonrigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A convex approach to minimal partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the ergodic convergence rates of a first-order primal-dual algorithm. Mathematical Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceiving layout and knowing distances: The integration, relative potency, and contextual use of different information about depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Vishton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Perception of Space and Motion</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autonomous, vision-based flight and live dense 3D mapping with a quadrotor micro aerial vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fontana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pizzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multibody structure and motion: 3-D reconstruction of independently moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grouping-based low-rank trajectory completion and 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense variational reconstruction of non-rigid surfaces from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The KITTI dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph implementations for nonsmooth convex programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Learning and Control</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">CVX: Matlab software for disciplined convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<ptr target="http://cvxr.com/cvx" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Highly overparameterized optical flow using PatchMatch belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hornacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Energy-based geometric multimodel fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Isack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<idno>2012. 3</idno>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Intrinsic depth: Improving depth transfer with intrinsic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Realtime multibody visual SLAM with a smoothly moving monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">T-linkage: A continuous relaxation of j-linkage for multi-model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Magri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fusiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conic optimization via operator splitting and homogeneous self-dual embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multibody structure-from-motion in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Ozden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dense multibody motion estimation and reconstruction from a handheld camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video pop-up: Monocular 3D reconstruction of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deformable Surface 3D Reconstruction from Monocular Images. Synthesis Lectures on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint SFM and detection cues for monocular 3D localization in road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A fully-connected layered model of foreground and background flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-rigid structure from locally-rigid motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geometric motion segmentation and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno>1998. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<date type="published" when="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Nonrigid structure-from-motion: Estimating shape and motion with hierarchical priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quick shift and kernel methods for mode seeking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video-based, real-time multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representing moving images with layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient sparse-to-dense optical flow estimation using a learned basis and layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simultaneous multi-body stereo and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Determining the epipolar geometry and its uncertainty: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>1998. 4</idno>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
