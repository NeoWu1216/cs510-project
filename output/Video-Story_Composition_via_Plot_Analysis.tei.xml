<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video-Story Composition via Plot Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
							<email>jschoi@rcv.kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video-Story Composition via Plot Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of composing a story out of multiple short video clips taken by a person during an activity or experience. Inspired by plot analysis of written stories, our method generates a sequence of video clips ordered in such a way that it reflects plot dynamics and content coherency. That is, given a set of multiple video clips, our method composes a video which we call a video-story. We define metrics on scene dynamics and coherency by dense optical flow features and a patch matching algorithm. Using these metrics, we define an objective function for the video-story. To efficiently search for the best video-story, we introduce a novel Branch-and-Bound algorithm which guarantees the global optimum. We collect the dataset consisting of 23 video sets from the web, resulting in a total of 236 individual video clips. With the acquired dataset, we perform extensive user studies involving 30 human subjects by which the effectiveness of our approach is quantitatively and qualitatively verified.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>People have the natural desire to capture and store personal experiences and memories. Today, we are able to record our activities more easily with decreasing cost of cameras and media storages. Moreover, with the success of smart phones and applications, photos and videos have become omnipresent in our daily lives. Consequently, people tend to capture photos and record videos without a limited storage burden and process them later. Unfortunately, manual post-processing of these contents is usually tedious, and thus a need for an automatic summarization of contents has arisen leading to many research on this topic <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>. This need has arisen due to the people's tendency to preserve only meaningful contents. More recently, rather than simply extracting summaries, many people choose to generate meaningful stories out of photos <ref type="bibr" target="#b14">[15]</ref> and videos <ref type="bibr" target="#b19">[20]</ref>, and many applications (e.g. 1 Second Everyday, Roadmovies, Snapmovie) attempt to provide this type of media. Basically, works on photo story generation deal with <ref type="figure">Figure 1</ref>: Story plot analysis. A written story consists of an exposition, rising action, climax, and resolution. Given a collection of independently captured video clips, we aim to build a video sequence with a story plot. This figure shows our results with River-Surfing short video clips. The sequence starts the exposition with a clip showing a stationary scene of the environment. The following scenes portray consecutive rising action and eventually show the actual river surfing at the climax leading to similar scenes until resolution. Notice the adjacent scenes are visually coherent as well which do not interrupt the flow of the story. aligning multiple photos supposedly into a temporal order. Similar works such as photo sequencing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> take a photo sequence input of the same scene or action and produce a temporally ordered photo sequence. In video summarization, a long video is summarized into a shorter version while maintaining the overall story <ref type="bibr" target="#b19">[20]</ref>. Mobile applications recently gaining much popularity like 1 Second Everyday and Roadmovies allow the user to capture short video clips with a mobile camera and then concatenate the short video clips taken one after another to produce a single video.</p><p>We address the problem of composing an ordered video clip sequence which we call video-stories out of multiple video clips taken by a person during an activity or experience. When a person captures separate video clips later to be concatenated, that person may not be so much concerned about the overall structure of the resulting video. For example, let's say a person captures separate video clips while on a surfing trip. The person may start off recording multiple clips of the actual surfing. Then, the person may decide to appreciate the environment of the scene and record the surrounding environment. Next, the person may choose to capture family and friends. Then, the person may again resume capturing the actual surfing. The video clips captured may well contain all of the aspects the person intended to capture during the surfing trip. However, when the videos are concatenated together in temporal order, it may not reflect a sense of structure, but rather simply a series of individual experiences on the same day. Thus, our goal is to take separate video clips and produce a video reflecting plot structure and sense of story, namely a video-story.</p><p>Our work is inspired by the notion of plot analysis for written stories. <ref type="figure">Fig. 1</ref> shows the notion of a plot diagram and its component notions along with actual results obtained by our method. A typical story contains an exposition where it introduces the beginning and setting of the story. The rising action phase represents the intermediate events between the beginning and the climax of the story which typically involves building up action and dynamics. The climax represents the main event, and resolution marks the ending of the story. Simply, a story gradually increases in action dynamics and reaches its peak at the climax and gradually reaches the end of the story. Although the dynamics present in the resolution varies among stories, most will have an ending with more activity than its beginning. Also, stories will tend to be coherent in its contents, meaning it will not have abrupt changes in scenes nor abrupt revisitation of scenes. Our goal is to structure the individual video clips into a video-story following this general plot structure while maintaining coherent story transitions.</p><p>An overview of our approach is as follows. First, given multiple short video clips, we measure the amount of activity in the individual clips via a dynamicity measure which we define. We also measure coherency between the clips based on a patch matching algorithm. Next, we design an objective function that scores video sequences depending on how well it represents a story plot structure and how smooth and coherent the clip transitions are. The dynamicity measure is used to evaluate how well the candidate video sequence follows the story plot structure. Similarly, the dissimilarity measure is utilized in evaluating the overall smoothness in clip transitions. Finally, we find the optimal solution via Branch-and-Bound algorithm.</p><p>Our main contribution is the idea of composing a structured video out of multiple short videos that has story-like qualities. We propose a general framework that achieves this goal. To the best of our knowledge, our work is the first to address the problem of automatically composing a story sequence with multiple video clips separately captured. In order to accomplish this, (1) we define a dynamicity metric based on optical flow features to reflect activity in video clips. (2) We introduce a reliable bidirectional patch match-ing algorithm to measure the dissimilarity between clips.</p><p>(3) We design an objective function that returns the best chain of clips representing sense of story and coherency. <ref type="bibr" target="#b3">(4)</ref> We introduce a Branch-and-Bound algorithm to efficiently find the optimal solution. (5) We construct a dataset of 23 video sets (total of 236 individual clips) and conduct extensive experiments involving 30 subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent research on generating a story form of media is mainly dealt in video summarization and image sequencing which we review in this section.</p><p>Video summarization. Works on generating a summary of a long video can take different representations. Keyframe-based methods represent the video summary as a sequence of keyframes selected from the video. Wolf et al. <ref type="bibr" target="#b27">[28]</ref> used optical flow features and Liu et al. <ref type="bibr" target="#b18">[19]</ref> used object tracks to select the set of keyframes. Methods including mosaic-based representation <ref type="bibr" target="#b0">[1]</ref> have been explored to efficiently cluster scenes into physical settings, and user interaction based approaches <ref type="bibr" target="#b9">[10]</ref> have been proposed to render action summary layouts. Lee et al. <ref type="bibr" target="#b16">[17]</ref> proposed to find important people and objects from regional cues for egocentric video summarization. Also, some optimization approaches include works that represent a video as a high dimensional trajectory curve and analyze via binary curve splitting algorithm <ref type="bibr" target="#b5">[6]</ref>. Apart from keyframe representation, some works represent summaries via skims or subshots. Naturally, some works address spatio-temporal features <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> for subshot selection. Ngo et al. <ref type="bibr" target="#b20">[21]</ref> proposed a motion attention model based on human perception to compute subshot quality. Feldman et al. <ref type="bibr" target="#b7">[8]</ref> proposed a novel core-set algorithm for k-segmentation of streaming data. On the other hand, a supervised learning approach <ref type="bibr" target="#b10">[11]</ref> has also been conducted for selecting appropriate subshots. In addition to feature based approaches, Lu et al. <ref type="bibr" target="#b19">[20]</ref> proposed to measure influence between subshots based on visual objects in egocentric videos. In robotics, Volkov et al. <ref type="bibr" target="#b25">[26]</ref> proposed a feature-based core-set algorithm for summarizing video data. Video summarization focuses on representing the summary of a single long video, whereas our approach addresses generating a video-story from multiple video clips.</p><p>Image sequencing. Many works on image sequencing have attempted to align the order of images that lack temporal ordering. Basha et al. <ref type="bibr" target="#b2">[3]</ref> proposed to detect static and dynamic features and then conduct an epipolar geometry based approach to find temporally ordered image sequences. Feature based methods including motion signature based synchronization <ref type="bibr" target="#b6">[7]</ref> have been explored as well. Wang et al. <ref type="bibr" target="#b26">[27]</ref> jointly utilized image and text information to generate image storylines. Also, works have addressed using geolocation and path cues <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> to generate image sequences illustrating the tourist's experience in temporal order. Apart from temporally aligning photo sequences, Averbuch-Elor et al. <ref type="bibr" target="#b1">[2]</ref> introduced a spectral technique for recovering the spatial order of photos taken by a group of people around the same event. Works on storyline graphs have been introduced for large-scale web images <ref type="bibr" target="#b14">[15]</ref>, personal photos <ref type="bibr" target="#b21">[22]</ref>, and outdoor activity classes <ref type="bibr" target="#b13">[14]</ref>. Image sequencing and storyline graphs aim to identify the temporal ordering of images. We aim to instill a sense of story into our proposed video-story generation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach -Composing the Video-Story</head><p>Our approach addresses the problem of making a videostory out of multiple video clips. It is our job to find the best way to order these clips such that the resulting videostory (1) follows a story plot structure, and <ref type="formula" target="#formula_1">(2)</ref> is coherent in presenting the subsequent clips one-by-one.</p><p>Consider we are given N video clips denoted as C = {c 1 , ..., c N }. Let s ⊂ P denote an ordered sequence, where P denotes the set of all possible permutations of C. Our goal is to find the optimally ordered sequence:</p><formula xml:id="formula_0">s * = arg min s⊂P Q(s),<label>(1)</label></formula><p>where Q(s) is an objective function:</p><formula xml:id="formula_1">Q(s) = αP(s) + (1 − α)D(s).<label>(2)</label></formula><p>The Plot Penalty term P(s) denotes the penalty given to a candidate sequence of clips depending on how poorly it is structured as a story. The Dissimilarity term D(s) denotes the dissimilarity present in adjacent clips given to a candidate sequence of clips. We provide detailed explanations of these terms in Sec. 3.1.</p><p>Directly selecting the best permutation of clips through an exhaustive search is N P -hard. Thus, we need an efficient algorithm to find the optimal solution. We introduce a novel Branch-and-Bound algorithm that efficiently finds the best video-story while guaranteeing global optimum. Detailed explanations are provided in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Story Scores for Candidate Video-Stories</head><p>We provide detailed illustrations on the terms introduced in Eq. (2) and their importance in evaluating the story quality of a candidate video-story sequence.</p><p>Plot dynamics of the overall video. The Plot Penalty term P(s) indicates how much a candidate sequence of video clips represents a poor story structure and is crucial to the novelty of our approach. Specifically, this term penalizes for not following the general story plot as illustrated in <ref type="figure">Fig. 1</ref>. Thus, a good video-story will in general contain the essential aspects of a story plot such as the presence of a gradual rising action phase, a climax and a resolution. Consequently, a video-story that follows the general plot structure will return a small Plot Penalty score. Now, the components of a plot (i.e. exposition, rising action, climax, and resolution) are based on the notion of amount of activity. In other words, plot components are structured depending on how dynamic each scenes are. We define a dynamicity measure based on dense optical flow features <ref type="bibr" target="#b17">[18]</ref> to represent the amount of activity present in a video clip. Recent methods using dense optical flow have shown efficient video representation for action recognition tasks and have achieved state-of-the-art results. Given a video clip c l with length L(c l ) and displacement vectors ∆T t = T t+1 − T t at time t, we define the dynamicity as</p><formula xml:id="formula_2">D(c l ) = L(c l ) j=1 ∆T j 2 L(c l ) .<label>(3)</label></formula><p>This measure represents the amount of activity contained in clip c l normalized by the clip length L(c l ). Camera motion caused by the user however cannot be thought of as a dynamic component of the clip, because it usually has little to do with the actual dynamics of the scene depicted. Thus, we preprocess the displacement vectors ∆T t by taking the camera motion into account. To estimate camera motion, we extract SURF <ref type="bibr" target="#b3">[4]</ref> descriptors in each frame and compute homographies with RANSAC <ref type="bibr" target="#b8">[9]</ref> for consecutive frames. We use the homographies to cancel out the camera motion to produce displacement vectors ∆T t containing pure dynamics of the scene.</p><p>Based on the dynamicity measure, given a sequence s with N video clips, we define the Plot Penalty term P(s) as</p><formula xml:id="formula_3">P(s) = N −1 i=1P (s i , s i+1 ),<label>(4)</label></formula><p>wherẽ</p><formula xml:id="formula_4">P (s i , s i+1 ) = D(s i ) − D(s i+1 ) if D(s i+1 ) &lt; D(s i ), 0 if D(s i+1 ) ≥ D(s i ).<label>(5)</label></formula><p>For clarification, the P(s) term penalizes candidate videostories on decreasing dynamicity of adjacent clip pairs. This simple formulation in fact provides an elegant representation of all component properties of a story plot. First of all, the exposition (first clip of the video-story) would tend to start off with low dynamics as suspected. Also, the overall video-story would most likely follow the rising action phase due to how the penalty measureP (s i , s i+1 ) is defined. Consequently, the climax may come after the rising action phase. Since most stories typically end with higher dynamics than its exposition, this formulation implicitly allows the resolution to end with high dynamics. Furthermore, notice that the magnitude of penalization is equal to the dynamicity difference. This is backed with the intuition that a significant drop in dynamics must be penalized more than a subtle drop in dynamics. <ref type="figure" target="#fig_0">Fig. 2</ref> shows a comparison of plot dynamics of a temporally ordered videostory segment and our proposed video-story segment.</p><p>Coherency of story contents. The Dissimilarity term D(s) indicates how much a candidate sequence of video clips contains dissimilar clips adjacent to each other. A good story usually reflects smooth transitions of events. For instance, a scene of the ocean would likely be followed by other scenes showing the ocean rather than a cascade of unrelated scenes. In this sense, a video-story that presents better coherency in contents with smoother transitions between clips will return a smaller Dissimilarity score.</p><p>We first introduce how to measure dissimilarity between two clips by modifying the bidirectional similarity <ref type="bibr" target="#b23">[24]</ref> measure which is based on a patch matching algorithm. We choose to define our dissimilarity measure in this way in order to take advantage of its property: robustness to local changes between video frames. Given two clips c 1 and c 2 , let G and H each denote a set of patches from c 1 and c 2 , respectively. The dissimilarity measure is defined as </p><p>where Dist(G, H) is obtained by the Sum of Squared Distance (SSD), measured in CIE L * a * b * color space and nor- malized by the patch size. The original implementation of the dissimilarity measure allows more than one patch from a clip to correspond to the same patch in the other clip. However, upon computing the dissimilarity measure, we enforce a one-to-one correspondence requirement on sets G and H. That is, each and every patch taken from clips c 1 and c 2 must have exclusive correspondences. Consequently, this imposes an emphasis on global dissimilarity between clips as a whole. The original bidirectional similarity is mainly used in spacial and/or temporal summarization of images or videos. That is, bidirectional similarity is used to make smaller and/or shorter versions of an input image or video, which naturally emphasizes the need to find local similarities. Since our approach requires to find the overall similarity (or dissimilarity) across clips, an emphasis on finding the global similarity better fits our needs, and thus the one-to-one correspondence requirement is enforced. An illustration of this aspect is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Given N video clips and a clip sequence s, we define the Dissimilarity term D(s) as follows.</p><formula xml:id="formula_6">D(s) = N −1 i=1 d(s i , s i+1 ),<label>(7)</label></formula><p>which is simply the sum of all dissimilarity measures between adjacent clips. This term accounts for the smooth scene transitions illustrated in <ref type="figure">Fig. 1</ref> and <ref type="figure" target="#fig_0">Fig. 2(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Searching for the Optimal Video-Story</head><p>Exhaustively searching for the optimal story sequence from all possible permutations of video clips is N P -hard. In this section, we provide an efficient way to find the global optimum based on the Branch-and-Bound algorithm <ref type="bibr" target="#b11">[12]</ref> with breadth-first-search. We start by a brief introduction to Branch-and-Bound, then illustrate how the bounds are defined, and finally present our search procedure.</p><p>Introduction to Branch-and-Bound. The basic idea of Branch-and-Bound (BnB) is to divide the search space into smaller subspaces and discard subspaces that cannot contain a better solution than the current one. The discard decision is made by a rejection test based on the bounds of the subspace. If a subspace passes the rejection test, then it is again partitioned into smaller subspaces. The size of the subspaces iteratively decreases and the current solution converges to the global optimum. It is important to define tight intervals between the lower and upper-bounds because it affects the algorithm speed. If the intervals are tighter, early rejections of subspaces will occur more frequently. Thus, the number of branches (subspace subdivisions) are reduced, leading to a faster search procedure. In order to apply BnB to our problem, we specify the subspace subdivision scheme (i.e. how to divide subspaces into smaller subspaces), and define how to obtain the lower and upper-bounds of a subspace. This leads us to develop the first video-story search algorithm whose global optimality is guaranteed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defining lower and upper bounds.</head><p>Defining how the bounds are computed for a subspace is important since it forms the basis of the rejection test which affects the over-all algorithm efficiency. Before we explain how the bounds are defined, we briefly describe how the subspace branches are defined. Given N video clips, we define the search subspace I n ∈ N N in the form of a sequence of natural numbers with length N . Let the first n entries of subspace I n be specified by clip indices, then the remaining entries are left blank to define the search area of the subspace. With each branching, the next entry (i.e. (n + 1)-th entry) of the preceding subspace branch I n is specified by a clip index (i.e. generating I (n+1) ), and consequently converges iteratively to a single optimum sequence of clips (i.e. our best video-story sequence).</p><p>For a subspace I n , where the first n entries are fixed, the lower-bound of a subspace is acquired by first obtaining the lower-bound Plot Penalty scoreP by Eq. (4) only up to the n-th entry. The lower-bound Dissimilarity scoreD is partially obtained by Eq. <ref type="formula" target="#formula_6">(7)</ref> up to the n-th entry, and then must be completed as described as follows. Here, let us define the dissimilarity matrixD as the symmetric matrix where its elements are the dissimilarity measures between clip pairs as</p><formula xml:id="formula_7">D =      d(c 1 , c 1 ) d(c 1 , c 2 ) · · · d(c 1 , c N ) d(c 2 , c 1 ) d(c 2 , c 2 ) · · · d(c 2 , c N ) . . . . . . . . . . . . d(c N , c 1 ) d(c N , c 2 ) · · · d(c N , c N )      .<label>(8)</label></formula><p>For each of the remaining entries after the n-th entry in I n , we add the minimum element ofD excluding the elements already used to partially calculateD. The lowerbound score is obtained by a weighted sum ofP andD in the same way as described in Eq. <ref type="bibr" target="#b1">(2)</ref>. A detailed algorithm for computing the lower-bound score is shown in Alg. 1. Notice that no sequence in the subspace I n can possibly have a score lower than the lower-bound of its subspace, which indicates that the lower-bound definition is suitable. The upper-bound score of a subspace can be obtained by finding an arbitrary sequence within the subspace and taking its score. We can define the upper-bound score as such because the score of any sequence within a subspace is always greater than or equal to the lowest score possible in that subspace. Although selecting any sequence at random would suffice as the upper-bound, we would like to make a tight interval. Thus, we would like to find a sequence with a low score, but would also like to find it fast (for sufficient algorithm speed). We define our upper-bound as the score of a sequence in the subspace found by a sequential search method. For a subspace I n where the first n entries are fixed, we define a sequenceŝ with the same n entries. We assign the next entry (i.e. (n + 1)-th entry) with a clip index returning the lowest score, and repeat until the sequenceŝ is complete. This is done by defining vectorsP j and d j whose elements represent all pairwise penalty measures and dissimilarity measures withŝ j respectively, where Algorithm 3 Branch-and-Bound for optimal video-story 1: Input: Search space I 0 ∈ N N (i.e. Initial search space with no fixed entry) 2: for n = 0 : N − 1 do 3:</p><p>Subdivide I n by assigning remaining clip indices to the (n + 1)-th entry <ref type="bibr">4:</ref> Store subdivided subspaces in LI 5:</p><p>Compute lower and upper-bound scores: L b and U b , and store in L b 6:</p><formula xml:id="formula_8">U b * = min U b ∈ L b 7:</formula><p>Remove all subdivided subspaces from LI whose L b &gt; U b * 8: end for 9: Return: I N (i.e. The subspace I N last to remain in LI is the only remaining subspace and represents the global optimal video-story sequence s * ) s j is the j-th element ofŝ. Obviously, already selected clip indices cannot be selected again. The upper-bound score is obtained by a weighted sum of P(ŝ) and D(ŝ) in the same way as Eq. (2). A detailed algorithm is shown in Alg. 2.</p><p>Search procedure. An example illustration of the search procedure is provided in <ref type="figure">Fig. 4</ref>. Ultimately, the goal is to find the global optimal story sequence s * ∈ N N from a search space I 0 ∈ N N (which has no entries fixed with video clip indices). The BnB algorithm iteratively subdivides the search space by fixing the entries of I 0 with clip indices one-by-one. The subdivided subspaces are stored in the subspace list L I . Also in each iteration, the associated bounds are computed and stored in list L b . Since our problem is a minimization problem, the rejection test decides to discard a subspace when its corresponding lower bound is greater than the current minimum upper bound. We are safe to remove these subspaces from L I since it signifies that the best solutions drawn from these subspaces are worse than any solution drawn from the current best subspace, and thus the optimal solution cannot be within these subspaces. The procedure stops when the last entry (i.e. N -th entry) is assigned leaving only one subspace I N which represents the global optimal story sequence s * . Notice that the lower and upper-bounds of I N is equal to each other and thus signifies the algorithm's convergence to the optimal solution. The detailed algorithm of the optimal video-story search procedure is shown in Alg. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We now analyze and evaluate our method. Since evaluating the quality of video-stories is a subjective task, we conduct extensive user studies to quantitatively evaluate our method. We provide detailed explanations on the evaluation settings, evaluation tasks via user studies, and quantitative and qualitative results. <ref type="figure">Figure 4</ref>: Branch-and-Bound search procedure. This shows an example illustration of BnB regarding subspaces with 3 fixed entries. The subspace I 3 2 has a lower-bound larger than the minimum upper-bound, thus the subspace I 3 2 is removed from the subspace list LI . This means that any story sequence starting with the clip indices (1, 2, 4) cannot be the optimal story sequence. Dataset. We collect 23 user-made video-stories from YouTube. Each video-story consists of 8-12 video clips which are 2-3 seconds long. This gives a total of 236 video clips for our dataset. The contents of our dataset include many activities (e.g. sightseeing, skateboarding, walking, surfing, shopping, driving, swimming, etc.) at various locations (e.g. river, park, ocean, streets, mall, landmarks, museum, marketplace, garden, beach, etc.).</p><p>Methods for comparison. We provide evaluation results on the following methods.</p><p>• Plot Analysis refers to our method described in Sec. 3. The weight parameter α in Eq. (2) is set to 0.5 in order to equally emphasize plot dynamics and coherency of story contents. We stress that this parameter was not tuned, although tuning this parameter via cross-validation could improve the results. The resulting video-story is found by the BnB algorithm and <ref type="figure" target="#fig_4">Fig. 5</ref> shows an illustration of the lower and upper-bound convergence and search space volume convergence to prove that the bounds are valid.</p><p>• Shortest-path: We construct a graph connecting all pairs of clips weighted by the dissimilarity measures. We select the shortest-path sequence from the graph. It is natural to think that smooth transitions with similar clips next to each are enough to make a sufficient video-story. There- <ref type="figure" target="#fig_6">Figure 6</ref>: Pairwise preference scores. The scores represent pairwise preference scores normalized by the number of subjects. The leftmost bar set shows the average preference scores. The labels indicate the contents of the video set. Since the preferences are recorded in a pairwise manner, the score should be at least higher than 1 to validate that our method is superior to other baselines. fore, this baseline provides a comparison for our method to verify whether coherency is enough for sufficient videostory composition or if plot analysis (i.e. coherency as well as plot dynamics) is indeed superior.</p><p>• Temporal: This baseline is the original video-story composed by the actual user. The separate video clips are taken in temporal order and sequenced together to make a video-story. This baseline is used to verify whether simply ordering clips temporally is indeed the best way to compose a video-story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on Overall Video-Story Quality</head><p>This task involves showing subjects the video-stories each generated from the aforementioned baselines. For each set of video-stories, the contents are identical, but the sense of story differs according to each baseline. We ask the subjects to evaluate the sense of story present in the videostories. The evaluation is done in the form of selecting the better story in a pairwise manner. The selected video-story in the pairwise comparison is given 1 point, thus the maximum score a video-story can get is 2 in a video set. We do not reveal which is which, and the presentation order of the video-stories is random.</p><p>Since our dataset consists of 23 videos, there are a total of 23 video sets. A total of 30 subjects (age range from 21-55 years old, and about half have no background in computer vision) participated in this task and were asked to evaluate at least 10 video sets. This gives at least 30 (subjects) × 10 (video sets) × 3 (pairwise comparisons) = 900 tasks done by our subjects. We estimate each pairwise comparison to take 3 minutes to complete, resulting in at least 45 hours of user study. To the best of our knowledge, this is one of the most extensive user studies carried out in video composition evaluation. We would like to note that similar evaluation structures are performed in the data mining community (e.g. text analysis <ref type="bibr" target="#b22">[23]</ref>) and the computer vision community <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref> implying that the evaluation we perform is not designed to conveniently return desired results.  We find that our method composes video-stories with better sense of story when the contents have at least the slightest relevance between constituent video clips. For example, the video set WU involves walking in the city. Each video clip seems to be taken independently without any consideration of a story structure, but still contains the slightest relevance among them due to the same urban context. Our plot analysis approach manages to group similar clips into smooth story transitions and arrange the clip dynamics into a plot structure. The shortest-path method on the other hand, sufficiently links individual video clips with similar contents, however fails to compose a story plot. Lastly, the temporal baseline shows inconsistency in quality of videostories. This suggests that composing video-stories in temporal order does not always guarantee high quality stories. In some cases involving uneventful or uncorrelated contents (e.g. video set FM: uncorrelated scenes and activities in a marketplace), our method shows less advantage over other baselines. Since the order of presenting uneventful or uncorrelated video clips does not greatly affect the story, it is reflected in our evaluation results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation via Story Components</head><p>In this task we evaluate video-stories by its constituent components. Since we have shown the overall evaluation of video-stories as a whole in Sec. 4.1, we now evaluate on a smaller scale: constituent story components in a videostory. The idea is to obtain ground truth video-story sequences and evaluate our method and baselines with them in a component-wise manner. However, it is difficult to obtain ground truth sequences since it is highly subjective. To avoid this difficulty, we perform an experiment as follows.</p><p>We show 4 workers all of the clips in each of the 23 video sets (total of 236 clips) and have them identify which two clips should be close together when composing a videostory for every video set. Instead of asking the workers to indicate a whole story sequence, asking to identify separate pairs of clips have several advantages. (1) It lessens the task burden on the workers. (2) Identifying pairs is less prone to subjectiveness and sets of pairs contain more concentrated information than a whole sequence. (3) It returns reliable ground truth information easier to statistically analyze.</p><p>We take the union of the output returned by the workers as ground truth. By thresholding the distance between clips in the sequence, we obtain average ROC curves for our method and baselines shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. Once again, we would like to point out that similar experiments are performed on various works in the data mining <ref type="bibr" target="#b22">[23]</ref> and computer vision community <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref> to emphasize that the evaluations are fair.</p><p>Qualitative result example. <ref type="figure" target="#fig_7">Fig. 8</ref> shows example videostories composed by our method and other baselines. Notice how our video-story starts with a low dynamic clip as the exposition. The clips that follow represent the gradual rising action phase leading to a climax. The video-story ends with a dynamic clip as the resolution. The contents of our result show coherency as well, grouping similar scenes together. On the contrary, the video-story composed via the shortest-path method lacks structure in plot dynamics. The original temporally ordered video-story not only lacks plot structure, but also content coherency of adjacent clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our work deals with composing a story out of multiple short videos, namely a video-story. For this goal, we have defined and developed the plot analysis approach. Specifically, we have shown how to incorporate plot dynamics into a sequence of video clips, while also preserving content coherency. This was done by developing a novel Branchand-Bound algorithm guaranteeing the globally optimal solution. Our extensive user study verifies the effectiveness of our approach. In the future, it would be interesting to take semantic information of the video clips into account for story composition. Now people can take video clips when they feel like it, without thinking of content order, and still expect a well-structured video-story in the end.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Plot dynamics. This figure shows story segments taken from (a) the original temporal sequence and (b) the video-story generated by our proposed method. In these video-stories, clips (shown as snapshots in the top rows) transition from left to right. The heat map (shown below each snapshot) shows the accumulation of dense optical flow magnitudes. Our proposed story reflects smooth rising dynamics with coherent scenes whereas the original sequence shows orderless dynamics with inconsistent scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Effect of one-to-one correspondence requirement. This figure shows top three similar (smallest dissimilarity) clips of a reference clip (leftmost column) using (a) the original bidirectional similarity and (b) the bidirectional similarity with oneto-one correspondence requirement. Notice that the addition of this requirement emphasizes global similarity between clips as a whole rather than local similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>j</head><label></label><figDesc>Lower-bound score: L b = αP + (1 − α)D Algorithm 2 Upper-bound score of a subspace 1: Input: Subspace I n ∈ N N 2: Letŝ denote a sequence in subspace I n whereŝ1:n = I n = [P (ŝj, c1), · · · ,P (ŝj, cN )] 6: d j = [d(ŝj, c1), · · · , d(ŝj, cN )] 11: end if 12: Upper-bound score: U b = αP(ŝ) + (1 − α)D(ŝ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Convergence of bounds and search space volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Component-wise comparison results. The curves represent average ROC curves for our method and baselines on story component evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6</head><label>6</label><figDesc>shows the results of the pairwise preference test of our method and baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Video-story example result. (a) Our Plot Analysis, (b) Shortest-path, and (c) Temporal baseline. The heat map shown below each clip snapshot is the accumulation of dense optical flow magnitudes representing the overall dynamics of each clip.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the Technology Innovation Program (No. 10048320), funded by the Korea government (MOTIE).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video summaries and crossreferencing through mosaic-based representation. Computer Vision and Image Understanding (CVIU)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aner-Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ringit: Ring-ordering casual photos of a temporal event</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photo sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Basha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clues from the beaten path: Location estimation with bursty sequences of tourist photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video summarization by curve simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dementhon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kobla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view synchronization of human actions and dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coresets for k-segmentation of streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Schematic storyboarding for video visualization and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG(SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video summarization by learning submodular mixtures of objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Global optimization: Deterministic approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tuy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image sequence geolocation with human travel priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vesselova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jointly aligning and segmenting multiple web photo streams for the inference of collective photo storylines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reconstructing storyline graphs for image recommendation from web community photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video summarization from spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laganire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hocevar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM TRECVid Video Summarization Workshop</title>
		<meeting>of ACM TRECVid Video Summarization Workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Humanassisted motion annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hierarchical visual model for video object summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic video summarization by graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supporting personal photo storytelling for social albums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Obrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Connecting the dots between news articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shahaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A spatiotemporal motion model for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coresets for visual summarization with applications to loop closure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generating pictorial storylines via minimum-weight connected dominating set approximation in multi-view graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Key frame selection by motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
