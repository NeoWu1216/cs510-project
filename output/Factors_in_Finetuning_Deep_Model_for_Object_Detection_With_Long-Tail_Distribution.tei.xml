<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wlouyang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
							<email>zhangcong0929@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
							<email>xkyang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finetuning from a pretrained deep model is found to yield state-of-the-art performance for many vision tasks. This paper investigates many factors that influence the performance in finetuning for object detection. There is a longtailed distribution of sample numbers for classes in object detection. Our analysis and empirical results show that classes with more samples have higher impact on the feature learning. And it is better to make the sample number more uniform across classes. Generic object detection can be considered as multiple equally important tasks. Detection of each class is a task. These classes/tasks have their individuality in discriminative visual appearance representation. Taking this individuality into account, we cluster objects into visually similar class groups and learn deep representations for these groups separately. A hierarchical feature learning scheme is proposed. In this scheme, the knowledge from the group with large number of classes is transferred for learning features in its subgroups. Finetuned on the GoogLeNet model, experimental results show 4.7% absolute mAP improvement of our approach on the ImageNet object detection dataset without increasing much computational cost at the testing stage. Code is available on www.ee.cuhk.edu.hk/Ëœwlouyang/ projects/ImageNetFactors/CVPR16.html</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Finetuning refers to the approach that initializes the model parameters for the target task from the parameters pretrained on another related task. Finetuning from the deep model pretrained on the large-scale ImageNet dataset is found to yield state-of-the-art performance for many vision tasks such as tracking <ref type="bibr" target="#b37">[38]</ref>, segmentation <ref type="bibr" target="#b11">[12]</ref>, object detection <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref>, action recognition <ref type="bibr" target="#b15">[16]</ref>, and human pose estimation <ref type="bibr" target="#b4">[5]</ref>. When finetuning the deep model for object detection, however, we have two observations.</p><p>The first is the long-tail property. The ImageNet image classification dataset is a well compiled dataset, in which objects of different classes have similar number of samples. In real applications, however, we will experience the long-  <ref type="figure">Figure 1</ref>. The number of samples in y-axis sorted in decreasing order for different classes in x-axis on different datasets (a) and the models obtained using different strategy (b). Long-tail property is observed for ImageNet and Pascal object detection dataset in (a). Models are visualized using the DeepDraw <ref type="bibr" target="#b0">[1]</ref>. Compared with the model on the left in (b) finetuned for all the 200 classes in Im-ageNet detection dataset, the model finetuned for specific classes on the right column in (b) is better in representing rabbit and squirrel. Best viewed in color.</p><p>tail phenomena, where small number of object classes appear very often while most of the others appear more rarely. For segmentation, pixel regions for certain classes appear more often than the regions for other classes. For object detection, some object classes such as person have much more samples than the other object classes like sheep for both PASCAL VOC <ref type="bibr" target="#b7">[8]</ref> and ImageNet <ref type="bibr" target="#b28">[29]</ref> object detection dataset, as shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. More examples and discussion on the long-tail property is given in a recent talk given by Bengio <ref type="bibr" target="#b2">[3]</ref>. For detection approaches using hand-crafted features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref>, feature extraction is separated from the classifier learning task. Therefore, the feature extraction is not influenced by the long-tail property. For deeply learned features, however, the feature learning will be dominated by the object classes with large number of samples so that the features are not good for object classes with fewer samples in the long tail. We analyze the influence of long tail in learning the deep model using the ImageNet object detection dataset as a study case. We find that even if around 40% positive samples are left out from this dataset for feature learning, the detection accuracy is improved a bit if the number of samples among different classes is more uniform.</p><p>The second is in learning specific feature representations for specific classes. The detection of multiple object classes is composed of multiple tasks. Detection of each class is a task. At the testing stage, detection scores of different object classes are independent. And evaluation of the results are also independent for these object classes. Existing deep learning methods consider all classes/tasks jointly and learn a single feature representation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>. Is this shared representation the best for all object classes? Objects of different classes have their own discriminative visual appearance. If the learned representation can focus on specific classes, e.g. mammals, the learned representation is better in describing these specific classes. For example, the model finetuned for 200 classes only focuses on the head of rabbit and squirrel, as shown in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. In comparison, if the model can focus on these mammals, the model can also learn representation for the body and ear shape of rabbit, and the tail and ear shape of squirrel, as highlighted by the blue ellipses in <ref type="figure" target="#fig_1">Fig.  2</ref> (b). In this paper, we propose a scheme that inherits the shared representation and learns the specific representation for the specific subset of classes, e.g. mammals.</p><p>The contribution of this paper is as follows: 1. Analysis and experimental investigation on the factors that influence the effectiveness of finetuning. The investigated factors include the influence of the pretraining and finetuning on different layers of the deep model, the influence of the long tail, the influence of the training sample number, the effectiveness of different subsets of object classes, and the influence from the subset of training data.</p><p>2. A cascaded hierarchical feature learning approach. In this approach, object classes are grouped. Different models are used for detecting object classes in different groups. The model gradually focuses on the specific group of object classes. The knowledge from the larger number of generic classes is transferred to the smaller number of specific classes through hierarchical finetuning. The cascade of the models saves the computational time and helps the models to focus on hard samples. Through cascade, each model only focuses on around 6 candidate regions per image. With the proposed feature learning approach, 4.7% absolute mAP increase is achieved on the ImageNet object detection dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The long-tail property is noticed by researchers working on scene parsing <ref type="bibr" target="#b43">[44]</ref> and zero-shot learning <ref type="bibr" target="#b19">[20]</ref>. Yang et al. <ref type="bibr" target="#b43">[44]</ref> expand the samples of rare classes and achieve more balanced superpixel classification results. Norouzi et al. <ref type="bibr" target="#b19">[20]</ref> use the semantically similar object classes to predict the unseen classes of images. Deep learning is considered as a good representation sharing approach in the battle against the long tail <ref type="bibr" target="#b2">[3]</ref>. The influence of long tail in deep learning, to our knowledge, is not investigated. We provide analysis and experimental investigation on the influence of the long tail in learning features. Our investigation provides knowledge for training data preparation in deep learning.</p><p>Deep learning is found to be effective in many vision tasks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Deep learning is applied for object detection in many works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b14">15]</ref>. Existing works mainly focus on developing new deep models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref> and better object detection pipeline <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref>. These works use one feature representation for all object classes. When using the hand-crafted features, the same feature extraction mechanism is used for all object classes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b9">10]</ref>. In our work, however, different object classes use different deep models so that the discriminative representations are specifically learned for these object classes.</p><p>Our work is also different from the model ensemble used in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b24">25]</ref>. In model ensemble, the detection score for an object class is from multiple deep models with different parameters or different network architectures. The detection score for an object class is from only one model in our approach. Therefore, our approach is complementary to model ensemble in further improving accuracy.</p><p>Cascade is used in many object detection works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b36">37]</ref>. We use cascade to speed-up the testing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Factors in finetuning for ImageNet object detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline model</head><p>Region proposal. The learned detector is used for classifying each candidate region as containing certain object class or not. In this paper we use the selective search <ref type="bibr" target="#b31">[32]</ref> for obtaining candidate regions. By default, we use the bounding box rejection approach in <ref type="bibr" target="#b24">[25]</ref> so that around 6% candidate regions from selective search are retained.</p><p>Training and Testing data. We use the large scale Ima-geNet detection dataset for training and testing. The Ima-geNet ILSVRC2013 detection dataset is split into three sets: train13 (395,918), val <ref type="bibr" target="#b19">(20,</ref><ref type="bibr">121)</ref>, and test <ref type="bibr" target="#b39">(40,</ref><ref type="bibr">152)</ref>, where the number of images in each set is in parentheses. Based on the ILSVRC2013 dataset, the extra train14 (60,658), is collected in the ILSVRC2014 dataset. There are 200 object classes for detection in this dataset. val and test splits are drawn from the same image distribution. To use val for both training and validation, val is split into val1 and val2 in <ref type="bibr" target="#b11">[12]</ref>. The split is copied from <ref type="bibr" target="#b11">[12]</ref> for our experiments. The test set is not available for extensive evaluation. Therefore, we have to resort to the val2 data, which con-tains around 10,000 images. If not specified, val2 images are used for evaluation, and the train13, val1, and train14 images are used for training. If not specified, we use the selective search to obtain negative and extra positive bounding boxes in val1 and the ground truth positive bounding boxes in train13 and train14.</p><p>Network, pretraining, and fintuning. The GoogLeNet <ref type="bibr" target="#b34">[35]</ref> is shown to be the state-of-the-art in many recent works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref> for ImageNet object detection. We use exactly the same model structure as that in <ref type="bibr" target="#b34">[35]</ref>. The pretrained GoogLeNet with bounding box annotations provided online 1 is used for finetuning. The mAP on val2 is 40.3% in our four of five trials, another trial has 40.4% mAP. Therefore, the pretrained model we use with 40.3% mAP after finetuning is better than that in <ref type="bibr" target="#b34">[35]</ref>, which is 38.8%. At the finetuning stage, aside from replacing the CNNs 1000-way pretrained classification layer with a randomly initialized (200 + 1)-way softmax classification layer (plus 1 for background), the CNN architecture is unchanged.</p><p>SVM learning. After the features are learned, one-vs-rest linear SVMs are learned for obtaining the detectors for each object class, the same as that in <ref type="bibr" target="#b11">[12]</ref>. Since this paper focuses on learning deep model, training data preparation for SVM is kept unchanged for all experiments, although we will investigate different training data preparation for deep model learning.</p><p>Summary. Pretrained with bounding box annotations, the baseline GoogLeNet has 40.3% mAP on val2 when trained using ILSVRC14 detection data with selective search for region proposal. This deep model is finetuned by 200+1 softmax loss and then linear SVM is used for learning the classifier based on the learned deep model.</p><p>For the experiments we conduct in this paper, we only change one of the factors while keeping others the same as the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Investigation on freezing the pretrained layer in finetuning</head><p>In this experiment, we investigate how finetuning specific layers influences the detection performance. Given the pretrained GoogLeNet, we freeze the parameter of certain layers and only finetune parameters of the remaining layers. The experimental results are shown in <ref type="table">Table 1</ref>. There are 11 modules in the GoogLeNet: two convolutional layers, i.e. conv1 and conv2, and nine inception modules, i.e. icp (3a), icp (3b), icp (4a), icp (4b), icp (4c), icp (4d), icp (4e), icp (5a), and icp (5b). If we freeze all the 11 modules and use the features learned from the pretrained model for learning SVM classification, the mAP is 33.0%, much worse than finetuning of all 11 modules that has mAP 40.3%. Finetuning all the 11 modules has the same mAP as freezing the conv1-icp(4a) during finetuning. These frozen modules are extracting general low level features, which have been well-learned by the pretrained model. Therefore, it is so not necessary to finetune these modules. The mAP only drops by 0.7% even if we fix the eight modules conv1-icp(4d), which takes 43% the number of parameters, and 80% the number of operations. As we freeze more and more modules to higher levels, the mAP decreases more and more rapidly. The upper layers are more responsible for discriminating semantic objects. Therefore, finetuning of the upper layers have more impact on the detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Investigation on training data preparation</head><p>In this section, we investigate the use of different training data for learning features. The same as the baseline setting in Section 3.1, train13, train14 and val1 are used for learning the SVM. In this way, only the learned features are the factors in influencing the detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Investigation on different subset of training data</head><p>As illustrated in Section 3.1, there are three different subsets of training data. The performance of single subset and leave-one-subset-out is shown in <ref type="table">Table 2</ref>. Experimental results show that train13 is not so effective in learning features when compared with train14 and val1. The val1, val2 and test images are scene-like. The train13 images are drawn from the ILSVRC2013 classification image distribution. It has a skew towards images of a single centered object. The mismatch in train13 and val leads to the lower mAP in using train13 only.</p><p>If positive samples are from train14, the model trained using negative samples from val1 has mAP 35.2%, while the model using the negative samples from train14 has mAP 39.6%. Therefore, for the same positive samples, it is better to use negative samples from the same image instead of from other images for learning the model.</p><p>If the positive samples and the negative samples are from the same images, val1 has mAP 39% and train14 has mAP 39.6%. There are 60,658 train14 images and 9,887 val1 images. The increase of training images by about 6 times only results in 0.6% mAP improvement. We find that train14, although claimed to be fully annotated, still has many objects not annotated. The unannotated objects are much less on val1 and val2. The noise in having potential objects not annotated is one of the reasons for the small increase in mAP with the large increase in training images. We will further investigate the relationship between the number of samples and mAP in Section 3.3.3.   <ref type="table">Table 2</ref>. Detection mAP (%) on val2 trained from different combination of training data. The performance of using train13+val1+train14 is 40.3%. (s) denote the augmentation of positive data by boxes from selective search. (g) denotes the use of only ground-truth data.  <ref type="formula" target="#formula_0">(1643)</ref>, where the number of samples for each class is in parentheses. In comparison, the three classes smallest in sample number are hamster <ref type="formula" target="#formula_0">(16)</ref>, lion <ref type="bibr" target="#b18">(19)</ref>, and centipede <ref type="bibr" target="#b18">(19)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">The long-tail property</head><p>provided with the test data annotations, it is reasonable to assume that the test data also has similar long-tail property.</p><p>In order to make the number of samples more uniformly distributed, the number of samples from train13 is constrained to be less than or equal to 1,000 in <ref type="bibr" target="#b11">[12]</ref>. With this constraint, 49.5% ground-truth samples are from 40 object classes with largest sample number when considering the train13, val1 and train14 data altogether. The long tail still exists. The softmax (cross entropy) loss used for learning the deep model is as follows:</p><formula xml:id="formula_0">L = âˆ’ N n=1 C c=1 t n,c log p n,c , where p n,c = e netn,c e C c=1 netn,c .<label>(1)</label></formula><p>t n,c denotes the target label and p n,c denotes the prediction for the nth sample and cth class. t n,c =1if the nth sample belongs to the cth class, t n,c =0otherwise. net n,c is the classification prediction from the neural network. Denote Î¸ as the parameters to be learned from the network, the derivative is as follows:</p><formula xml:id="formula_1">âˆ‚L âˆ‚Î¸ = n,c (pn,c âˆ’ tn,c) Â· âˆ‚netn,c âˆ‚Î¸ .<label>(2)</label></formula><p>It can seen from (2) that the gradient of the parameters is influenced by two factors. First, the accuracy of p n,c in predicting t n,c . The more accurate p n,c is, the smaller the gradient in back-propagation (BP) for the nth sample. Second, the number of samples belonging to class c. Suppose the prediction error (p n,c âˆ’ t n,c ) in <ref type="formula" target="#formula_1">(2)</ref> is of similar magnitude for all samples. If the class bird has 16,00 samples but the class hamster has only 16 samples, then the magnitude of the gradient from bird will be around 100 times of the magnitude of the gradient from hamster. In this case, although the network representation is shared by different classes, the network parameters will be dominated by the class bird which has much more samples. This is fine for applications where the importances of classes are determined by their sample number. For applications like object detection, however, each class is equally important. The features learned from deep model dominated by the class bird may not be good for the class hamster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Experimental results on the long-tail property</head><p>In this experiment, we use train13, train14 and val1 as the training data, which are supposed to have N + positive samples/bounding-boxes and N âˆ’ negative samples/bounding-boxes. We obtain subset from these data by the following three schemes: 1. Rand-pos. In this scheme, the N + positive boxes are reduced to be N â€² + boxes by random sampling so that N â€² + /N + = r = {2 âˆ’1 , 2 âˆ’2 , 2 âˆ’3 ,...}. r corresponds to the ratio of the remaining positive boxes. The negative boxes are kept unchanged. 2. Rand-all. In this scheme, the numbers of positive and negative boxes are reduced to be N â€² + and N â€² âˆ’ respectively by random sampling so that N â€² + /N + = N â€² âˆ’ /N âˆ’ = r. 3. Pseudo-uniform. In this scheme, the classes with samples larger than N max will be randomly sampled to have N max remaining samples. Classes with samples smaller than N max are untouched. We also require that the remaining samples divided by N + is r. Denote the number of positive boxes for class c by N +,c . After sampling, we have N â€² +,c positive boxes for class c. In this scheme, we have r =( c N â€² +,c )/N + ,N â€² +,c &lt;= N max . In the pseudo-uniform scheme, the number of positives samples for different classes becomes more uniform when N max is smaller. In the Rand-pos and Rand-all scheme, the long-tail property is preserved. <ref type="figure" target="#fig_4">Fig. 3</ref> shows the experimental results on the three different schemes. Using all negative samples, we can see that pseudo-uniform performs better than rand-pos if the same number of positive samples are used. In fact, when log 2 r = âˆ’1, âˆ’2, âˆ’3, âˆ’4, âˆ’5, pseudo-uniform requires only half the number of positive samples to achieve the  same mAP as rand-pos. For example, pseudo-uniform has mAP 39.9% when log 2 r = âˆ’3(r =1 2 .5%), and randpos has mAP 39.9% when log 2 r = âˆ’2(r = 25%). The baseline that uses all samples has mAP 40.3%. In the pseudo-uniform scheme, we observe small improvement (mAP 40.5%) when N max =3 , 000, in which around 40% positive samples are not used in the finetuning. If we keep all the training samples and enforce that the mini-batches in the stochastic gradient descent based BP should have uniform distribution in positive sample number, the mAP is 40.7%. The approaches in increasing mAP are not used in the other experimental results of this paper for fair comparison. Therefore, our empirical results show that it is better to have uniform number of samples per class than long-tailed samples for learning features.</p><p>If both positive and negative boxes are randomly sampled using the scheme rand-all, the performance drops by 0.4%-0.5% compared with the rand-pos scheme that only samples positive boxes.</p><p>When r =0.01, rand-all have mAP 35.5%. In this case, only around 34 positive boxes per class are used for finetuning. Finetuning (mAP 35.5%) still has observable increase in mAP (2.5%) compared with the model without fine-tuning (mAP 33%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Experimental study on subsets of object classes</head><p>There are 200 object classes in the ImageNet object detection challenge. In this section, we investigate using features learned from a subset of object classes for the other object classes. As analyzed in Section 3.3.2, both the number of samples in each class and the estimation accuracy determine the gradient in learning the deep model. We conduct experiments on using subsets of object classes that have largest/smallest number of samples and largest/smallest accuracy. The results are shown in <ref type="table">Table 3</ref>.</p><p>We can use the C = {50, 100, 150} object classes having the largest accuracy for finetuning, and then use this model for extracting features for all the 200 classes and learning their SVMs. In this way, the mAP is 37.9% when C = 50. Much better than the model without finetuning, which has mAP 33%. Therefore, finetuning on the 50 classes has learned representations that can be shared by the other 150 classes that are not used for finetuning the deep model. For example, a learned feature that is good at describing the dog is good for describing the tiger.</p><p>If we leave the 50 object classes with fewest samples out and use 150 object classes with most samples for finetuning, the mAP is 40.1%. If all the 200 classes are used for finetuning, the mAP is 40.3%. The inclusion of the 50 object classes with the fewest samples in finetuning only increases mAP by 0.2%.</p><p>It can be seen from the results that the number of object classes used for finetuning is the key factor in influencing the mAP. For example, the use of 50 object classes have only at most 37.9% mAP in <ref type="table">Table 3</ref>. Among the 200-class positive boxes used for finetuning, these 50-class boxes have around 50% samples. In comparison, if the 50% positive samples are randomly sampled from 200 classes for finetuning, the mAP is 40.1%, as shown in <ref type="figure" target="#fig_4">Fig 3.</ref> In fact, even if only 6.25% positive boxes are randomly sampled, the mAP is 38.6% and performs better than the use of only 50 classes. Among the four choices of subsets in <ref type="table">Table 3</ref>, the choice of the C least accurate object classes has the lowest mAP. Thus it is the worst choice in obtaining features that can be shared by other object classes.</p><p>This section investigates the use of C classes for 200 classes in finetuning. The next section investigates the use of C classes for C classes in finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Cascaded hierarchical feature learning for object detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Grouping objects into hierarchical clusters</head><p>The 200 object classes are grouped into hierarchical clusters. Our approach is not constrained to any clustering method. In Section 5.2.1, we will investigate different clustering methods, in which we find visual similarity to be the best in detection accuracy. Thus we use visual similarity as the example for illustration. The visual similarity between classes a and b is as follows:</p><formula xml:id="formula_2">Sim(a, b)= Ni i=1 Nj j=1 &lt; h a,i , h b,j &gt;/ N i N j ,<label>(3)</label></formula><p>where h a,i is the last GoogleNet hidden layer for the ith training sample of class a, h b,j is for the jth training sample of class b. &lt; h a,i , h b,j &gt; denotes the inner product between h a,i and h b,j . With the similarity between two classes defined, we use the approach in <ref type="bibr" target="#b45">[46]</ref> for grouping object classes into hierarchical clusters. At the hierarchical level l, denote the j l th group by S l,j l . In our implementation, l =1 ,..  <ref type="figure" target="#fig_7">Fig. 4</ref> by a few exemplar classes. In <ref type="figure" target="#fig_7">Fig. 4</ref>  <ref type="table">Table 3</ref>. Object detection accuracy in mAP when finetuned using different number of classes. Num. cls denotes the number of classes used for finetuning. pos num ratio denotes ratio, i.e. the number of positive samples for class subset choice divided by the number of the all positive samples. Largest/least accuracy denotes the use of the most/least accurate 50/100/150 classes for finetuning. Softmax accuracy of the training data is used for evaluating accuracy. Largest/least number denotes the use of the 50/100/150 classes with the largest/smallest training sample number for finetuning.  and S 2,1 = S 3,1 âˆª S 3,2 . In the hierarchical clustering results, the parent node par(l, j l ) and children set ch(l, j l ) of a node (l, j l ) are defined such that S l+1,j â€² âŠ‚ S l,j l , âˆ€(l +1,j â€² ) âˆˆ ch(l, j l ), S l,j l = âˆª (l+1,j â€² )âˆˆch(l,j l ) S l+1,j â€² , and S l,j l âŠ‚ S lâˆ’1,par(l,j l ) . Therefore, a hierarchical tree structure is defined as shown by examples in <ref type="figure" target="#fig_7">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Our approach at the Testing Stage</head><p>Our approach at the testing stage is described in Algorithm 1. In this approach, a testing sample is evaluated from root to leaves on the tree. At the node (l, j l ), the detection scores for the classes in group S l,j l are evaluated (line 6 in Agorithm 1). These detection scores are used for deciding if the children nodes ch(l, j l ) need to be evaluated (line 8 in Agorithm 1). For the child node (l +1,j â€² ) âˆˆ ch(l, j l ), if the maximum detection score among the classes in S l+1,j â€² is smaller than a threshold T l , this sample is not considered as a positive sample in class group S l+1,j â€² , and then the node (l+1,j â€² ) and its children nodes are not evaluated. T l chosen so that the recall on val1 is not influenced much and a large number of candidates can be rejected. For example, initially the detection scores for 200-classes {y c } câˆˆS1,1 are obtained at the node (1, 1) for a given sample of class bird. These 200-class scores are used for accepting this sample as an animal S 2,1 and rejecting this sample as ball S 2,2 , instrument S 2,3 or furniture S 2,4 . And then the scores {y c } câˆˆS2,1 of animals are used for accepting the bird sample as vertebrate and rejecting it as invertebrate. Therefore, each node focuses on rejecting the sample as not belonging to a group of object classes. Finally, only the groups that are not rejected have the SVM scores for their classes (line 13 in Agorithm 1).</p><p>Algorithm 1: Our Approach at the Testing Stage. Input: {x, the testing sample.</p><p>{S l,j l }, hierarchical clusters of object classes. M l,j l , the models. } Output: {y =[y 1 ,...,y C ], the detection score of x } 1 f 1,1 = true ; 2 f l,j l = false for l =2,...,L, j l =1,...,J l ; 3 for l =1toL do <ref type="bibr" target="#b3">4</ref> for j l =1toJ l do <ref type="bibr" target="#b4">5</ref> if f l,j l then <ref type="bibr" target="#b5">6</ref> Get scores {y c } câˆˆS l,j l of x using M l,j l ; 7 for (l +1,j â€² ) âˆˆ ch(j l ) do <ref type="bibr" target="#b7">8</ref> If max câˆˆS l+1,j â€² y c â‰¥ T l , then f l+1,j â€² = true ; 9 end 10 end 11 end 12 end 13 y c = s c,L for c âˆˆ S L,j L if f L,j L is true; y c = âˆ’âˆž otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hierarchical Feature Learning</head><p>The proposed feature learning approach is described in Algorithm 2. Each node (l, j l ) corresponds to a group of object classes S l,j l . For the node (l, j l ), a deep model M l,j l is finetuned using the model of its parent node M lâˆ’1,par(j l ) as initial point (lines 3-4 in algorithm 2). When finetuning the model M l,j l , the positive samples are constrained to have class labels in the group S l,j l (line 7 in Algorithm 2), and the negative samples are constrained to be accepted by the its parent node (line 8 in Algorithm 2). Therefore, only a subset of object classes are used for finetuning the model M l,j l . In this way, the model focuses on learning the representations for this subset of object classes. When learning the model M l,j l , we use the model in its parent node as the initial point so that the knowledge from Finetune M l,j l using X l,j l ,+ and X l,j l ,âˆ’ ; 5 for (l +1,j â€² ) âˆˆ ch(j l ) do <ref type="bibr" target="#b5">6</ref> Use M l,j l to obtain detection scores When finetuning the deep model, we use the multi-class cross-entropy loss to learn the feature representation. Then the 2-class hinge loss is used for learning the classifier based on the feature representation.</p><formula xml:id="formula_3">{y x =max câˆˆS l+1,j â€² y c (x)|x âˆˆ X l,j l ,âˆ’ } ; 7 X l+1,j â€² ,âˆ’ = {x|x âˆˆ x l,j l ,âˆ’ &amp; y x &gt;T l } ; 8 X l+1,j â€² ,+ = {x|x is a class in S l+1,j â€² } ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results on the Hierarchical</head><p>Feature Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with existing works</head><p>We compare with single-model results across state-ofthe-art methods. <ref type="table">Table 4</ref> summarizes the result for RCNN <ref type="bibr" target="#b11">[12]</ref> and the results from ILSVRC2014 object detection challenge. It includes the best results on the test data submitted to ILSVRC2014 from GoogLeNet, DeepID-Net, DeepInsignt, UvA-Euvision, and Berkeley Vision, NIN, SPP, which ranked top among all the teams participating in the challenge. Our model is based on the GoogLeNet model without adding any other layer and provided by the authors in <ref type="bibr" target="#b24">[25]</ref> online, which has mAP 40.3%. <ref type="bibr" target="#b1">2</ref> We also include the recent approach in <ref type="bibr" target="#b42">[43]</ref>. The approach in <ref type="bibr" target="#b42">[43]</ref> uses context (1.3% mAP increase), better region proposal (0.9% mAP increase in <ref type="bibr" target="#b42">[43]</ref>), pair wise term for bounding box relationship (3.5% mAP increase in <ref type="bibr" target="#b42">[43]</ref>), which are not used us but complementary to our all of implementations in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>The experiments in this section are only different in finetuining from the baseline introduced in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Investigation on different clustering methods</head><p>The experimental results that investigate different clustering methods are shown in <ref type="table">Table 5</ref>. In these experiments, we cluster the 200 object classes into 4 groups, which corresponds to the tree of level 2 in <ref type="figure" target="#fig_7">Fig. 4</ref>. The models for the 4 groups are finetuned from the model finetuned using 200 object classes, which is the baseline with mAP 40.3% introduced in Section 3.1. It can be seen from <ref type="table">Table 5</ref> that all the clustering approaches improved the detection accuracy except for the approach that randomly assigns the object classes into 4 groups, Random in <ref type="table">Table 5</ref>. The use of confusion matrix and the use of visual similarity perform better than the other clustering approaches. When the wordnet id (WNID) is used, we cluster the 200 object classes into the following 4 groups: 1) animals and person; 2) device and traffic light; 3) instrumentation that is not device; 4) other remaining artifacts e.g. food, substance.</p><p>We find that the clustering results obtained from visual similarity is very similar to the results obtained from wordnet id for animals. We also find many examples of exceptions for artifacts and person. Person is assigned to artifacts that frequently contact with person, e.g. accordion, baby bed. Bookshelf, which belongs to instrumentation, is grouped with refrigerator, which does not belong to instrumentation in wordnet id. Baseball, which belongs to instrumentation, is grouped with bathing cap, which does not belong to instrumentation.</p><p>Experimental results show that the confusion matrix and the visual similarity have similar performance. We also find that their clustering results are very similar. Visually similar objects of different classes often cause confusion. Therefore, both confusion matrix and the visual similarity are good choices for clustering for our approach. Since the empirical results show that the visual similarity performs better than the other approaches, we have adopted it for clustering in our final implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Investigation on the influence of hierarchy level</head><p>The experimental results evaluating the influence of level L in hierarchical feature learning is shown in <ref type="table">Table 6</ref>. Consistent mAP improvement is observed when the level increases. As the level increases from 1 to 4, the mAP increases from 40.3% to 45%. When the level increases, each model focuses on learning more specific feature representations. With the more specific representations, the features are more discriminative in distinguishing them from approach SPP * NIN * RCNN Berkeley UvA DeepInsight DeepID-Net GoogLeNet S-Pixels ours <ref type="bibr">[</ref>  <ref type="table">Table 5</ref>. Detection accuracy increase in mAP on ILSVRC val2 for different methods compared with the baseline model with mAP 40.3%. The classification accuracy of object classes in the training data as the descriptor for clustering for accuracy. The number of samples is used as the descriptor for Num. Sample. The average size of bounding boxes (measured by area of bounding box) is used as the descriptor for size. The hierarchy in wordnet id (WNID) is used for clustering for WNID. The confusion matrix of object classes is used as the similarity among classes for confusion. The visual similarity in <ref type="formula" target="#formula_1">(2)</ref>   <ref type="table">Table 6</ref>. Detection accuracy in mAP and other statistics on ILSVRC14 val2 for the cascaded hierarchical feature learning with different levels. N b,l denotes the average number of boxes per image evaluated per model for a given hierarchy level l. Nm denotes the number of models for a given tree depth.  <ref type="table">Table 7</ref>. Detection accuracy in mAP on ILSVRC val2 for different finetuning strategies. 0 denotes the pretrained model, l =1 , 2, 3 denotes the model at the tree level l. For example, 0 â‡’ 1 denotes finetuning 200 object classes from the 1000-class model. 0 â‡’ 2 denotes finetuning 4 class groups from the 1000-class model. the background. Therefore, these better features lead to better detection accuracy.</p><formula xml:id="formula_4">approach 0 â‡’ 1 0 â‡’ 2 0 â‡’ 1 â‡’ 2 0 â‡’ 1 â‡’ 2 â‡’ 3 0 â‡’ 1 â‡’ 3<label>mAP</label></formula><p>Since we have adopted the bounding box rejection approach in <ref type="bibr" target="#b24">[25]</ref> for the model with level 1, there are only 136 boxes per image left for the model with level 1. When the level is 4, there are 18 models to be evaluated for each bounding box. This seems to be a huge number. However, with the cascade, we can reject a large number of boxes for each model. On average, there are only 5.6 boxes per image evaluated for each model. Even if the 18 models are considered altogether, there are only around 100 boxes per image used for feature extraction and classification. Therefore, the use of multiple models that extract features for different object classes does not take much computational time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Investigation on the finetuning strategy</head><p>In our final implementation, the deep models at higher levels (larger l) are finetuned based on the deep model at lower levels. The experimental results in <ref type="table">Table 7</ref> shows the variations on the finetuning strategy.</p><p>If we use only one model for 200 classes and finetune this model from the 1000-class ImageNet pretrained model, the performance is 40.3%, which the baseline described in Section 3.1 and denoted by 0 â‡’ 1 in <ref type="table">Table 7</ref>.</p><p>If we directly finetune the 4 models at level 2 from the 1000-class pretrained model, the mAP decreases to 38.9%. In comparison, when learning 4 models by using the 200class finetuned model at level 1 as the initial point, the mAP increases to 41.3%. The 4 models at level 2 focuses on discriminating around 50 object classes from the background. Direct finetunng of the 4 models from the pretrained model does not use the knowledge of their correlated other 150 object classes. In comparison, finetuning from the model at level 1, which is finetuned using the 200 object classes, has used the knowledge from the 200 object classes. Therefore, improvement is observed when using the 200-class finetuned model as initial point. When finetuning the models at level 3, the use of the models at level 2 as initial point has mAP 42.5%, and the use of the models at level 1 as initial point has mAP 41.8%. The learning strategy that gradually focuses the model from 200 to 50 and then to 29 classes performs better than the abrupt jump from 200 classes to 29 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Results on the PASCAL VOC</head><p>We also observe 1.2% mAP improvement on PASCAL VOC 2007 when its object classes are clustered into 4 groups for GoogLeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper provides analysis and experimental results on the factors that influences finetuning on the object detection task. We find that it is better to have the number of samples uniform across different classes for feature learning. A cascaded hierarchical feature learning is proposed to improve the effectiveness of the learned features. 4.7% absolute mAP improvement is achieved using the proposed scheme without much increase in computational cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>shows the number of samples in val1 for the 200 object classes. It can be seen from Fig. 2 that the number of samples varies a lot for different classes. When the object classes are sorted by the number of samples, we observe the long-tail property. 59.5% ground-truth samples are from 20 object classes with largest sample number. Similar statistics are observed in the val2 data. Although we are not</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The number of annotated positive samples in val1 as a function of the object class index in ImageNet (left) and the number of samples for each class sorted in decreasing order (right). The three classes largest in sample number are person (6,007), dog (2,142) and bird</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>mAP on val2 as a function of the ratio r for remaining positive boxes for three different schemes -rand-pos, rand-all, and pseudo-uniform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>.,L, L =4 , j l = {1,...,J l }, J 1 =1 ,J 2 = 4,J 3 =7 ,J 4 =1 8 . Since there are 200 object classes in ILSVRC object detection, initially, S 1,1 = {1,...,200}. On average, there are 200 object classes per group at level 1, 50 classes per group at level 2, 29 classes per group at level 3, and 11 classes per group at level 4. The hierarchical cluster result is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Grouping object classes into hierarchical clusters S l,j l and finetuning them to obtain multiple models M l,j l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 2 :</head><label>2</label><figDesc>Hierarchical Learning of the models. Input: { Î¨={x} training samples. {S l,j l }, hierarchical clusters of object classes . X 0,1,+ , set of all positive samples. X 0,1,âˆ’ , set of all negative samples . M 0,1 , pretrained deep model. } Output: {M l,j l , the finetuned models.} 1 for l =1toL do 2 for j l =1toJ l do 3 M l,j l = M lâˆ’1,par(j l ) ;4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>the parent node is transferred to the current model. Since the root node is the pretrained for the 1000-class problem and finetuned for the 200+1 class problem, the model with larger level l have inherited the knowledge from both 1000class problem and the 200+1 class problem. Cascade is used for negative samples so that the model M l,j l focuses on hard examples that can not be handled well by the model M lâˆ’1,par(j l ) in the parent node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. Detection mAP (%) on val2 when freezing modules in GoogLeNet.</figDesc><table>Num. modules frozen 

0 
36789 
Modules frozen 
none conv1-icp(4a) conv1-icp(4d) conv1-icp(4e) conv1-icp(5a) conv1-icp(5b) 
mAP 
40.3 
40.3 
39.6 
38.8 
36.5 
33 
Table 1positive train13 val1(s) train14(g) train14(s) train14(g)+val1(s) train13+train14(g) train13+val1(s) 
negative val1 
val1 
val1 
train14 
val1 
val1 
val1 
mAP 
37.5 
39 
35.2 
39.6 
39.3 
37.2 
40.1 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>. Detection mAP (%) on ILSVRC2014 for top ranked approaches with single model. For fair comparison with<ref type="bibr" target="#b24">[25]</ref>, we use their learned GoogLeNet parameters provided online as our baseline. The methods marked with * do not use classification data for pre-training.</figDesc><table>14][ 18][ 12][ 12][ 32][ 42][ 25][ 35][ 43] 
ImageNet val2 n/a 35.6 31.0 
33.4 
n/a 
40.1 
40.3 
38.8 
44.8 45.0 
ImageNet test 31.8 n/a 31.4 
34.5 35.4 
40.2 
n/a 
38.0 
42.5 n/a 
Table 4Clustering method Random Accuracy Num Sample 
Size 
WNID Confusion Visual Sim 
Increase in mAP 
-1.1% 
0.48% 
0.56% 
0.66% 0.77% 
0.91% 
0.967% 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>is used as the similarity among classes for Visual Sim.</figDesc><table>Hierarchy level L 
1234 
#. groups (=Nm) 
147 
1 8 
avg #. classes per group 
200 
50 
29 
11 
N b,l 
136 
25.8 
15.2 
5.6 
N b,l Â· Nm 
136 
103.2 
106.4 100.8 
mAP 
40.3% 41.3% 42.5% 
45% 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.ee.cuhk.edu.hk/Ëœwlouyang/projects/ imagenetDeepId/index.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">There are results higher than 40.3% reported in<ref type="bibr" target="#b24">[25]</ref>, using additional layers, better region proposals, additional context and bounding box regression that we did not use but complementary to our approach. We use the 40.3% baseline result they provide online to be consistent with the baseline introduced in Section 3.1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepdraw</surname></persName>
		</author>
		<ptr target="/auduno/deepdraw.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object detection using stronglysupervised deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The battle against the long tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Talk on Workshop on Big Data and Statistical Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-task recurrent neural network for immediacy prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crosstalk cascades for frame-rate pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascade object detection with deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grishick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06981</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">R-cnn minus r. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep representation with large-scale attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning mutual visibility relationship for pedestrian detection with a deep model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4657" to="4666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Smeulders. Fisher and vlad with flair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, pages I:511-I:518</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3119" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stct: Sequentially training convolutional networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The fastest deformable part model for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeper vision and deep insight solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop on ILSVRC2014</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Object detection by labeling superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Context driven scene parsing with attention to rare classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Graph degree linkage: Agglomerative clustering on a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>ECCV. 2012. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep learning identity-preserving face space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
