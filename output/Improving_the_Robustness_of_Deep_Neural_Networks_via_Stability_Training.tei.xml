<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving the Robustness of Deep Neural Networks via Stability Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">Zheng</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caltech</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">Song</forename><surname>Google</surname></persName>
							<email>yangsong@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow Google</surname></persName>
						</author>
						<title level="a" type="main">Improving the Robustness of Deep Neural Networks via Stability Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the stateof-the-art Inception architecture <ref type="bibr" target="#b10">[11]</ref> against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on largescale near-duplicate detection, similar-image ranking, and classification on noisy datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks learn feature embeddings of the input data that enable state-of-the-art performance in a wide range of computer vision tasks, such as visual recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref> and similar-image ranking <ref type="bibr" target="#b12">[13]</ref>. Due to this success, neural networks are now routinely applied to vision tasks on large-scale un-curated visual datasets that, for instance, can be obtained from the Internet. Such un-curated visual datasets often contain small distortions that are undetectable to the human eye, due to the large diversity in formats, compression, and manual post-processing that are commonly applied to visual data in the wild. These lossy image processes do not change the correct ground truth labels and semantic content of the visual data, but can significantly confuse feature extractors, including deep neural networks. Namely, when presented with a pair of indistinguishable images, state-of-the-art feature extractors can produce two significantly different outputs.</p><p>In fact, current feature embeddings and class labels are not robust to a large class of small perturbations. Recently, it has become known that intentionally engineered imperceptible perturbations of the input can change the class label <ref type="figure">Figure 1</ref>: Near-duplicate images can confuse state-of-the-art neural networks due to feature embedding instability. Left and middle columns: near-duplicates with small (left) and large (middle) feature distance. Image A is the original, image B is a JPEG version at quality factor 50. Right column: a pair of dissimilar images. In each column we display the pixel-wise difference of image A and image B, and the feature distance D <ref type="bibr" target="#b12">[13]</ref>. Because the feature distances of the middle near-duplicate pair and the dissimilar image pair are comparable, near-duplicate detection using a threshold on the feature distance will confuse the two pairs. output by the model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>. A scientific contribution of this paper is the demonstration that these imperceptible perturbations can also occur without being contrived and widely occur due to compression, resizing, and cropping corruptions in visual input.</p><p>As such, output instability poses a significant challenge for the large-scale application of neural networks because high performance at large scale requires robust performance on noisy visual inputs. Feature instability complicates tasks such as near-duplicate detection, which is essential for large-scale image retrieval and other applications. In nearduplicate detection, the goal is to detect whether two given images are visually similar or not. When neural networks <ref type="figure">Figure 2</ref>: Visually similar video frames can confuse state-of-theart classifiers: two neighboring frames are visually indistinguishable, but can lead to very different class predictions. The class score for 'fox' is significantly different for the left frame (27%) and right frame (63%), which causes only the fox in the right image to be correctly recognized, using any reasonable confidence threshold (e.g. &gt; 50%). are applied to this task, there are many failure cases due to output instability. For instance, <ref type="figure">Figure 1</ref> shows a case where a state-of-the-art deep network cannot distinguish a pair of near-duplicates <ref type="bibr" target="#b12">[13]</ref> and a pair of dissimilar images.</p><p>Analogously, class label instability introduces many failure cases in large-scale classification and annotation. For example, unstable classifiers can classify neighboring video-frames inconsistently, as shown in <ref type="figure">Figure 2</ref>. In this setting, output instability can cause large changes in label scores of a state-of-the-art convolutional neural network on consecutive video-frames that are indistinguishable.</p><p>The goal of this paper is to propose a general approach to stabilize machine learning models, in particular deep neural networks, and make them more robust to visual perturbations. To this end, we introduce a fast and effective stability training technique that makes the output of neural networks significantly more robust, while maintaining or improving state-of-the-art performance on the original task. To do so, our method operates through two mechanisms: 1) introducing an additional stability training objective and 2) training on a large class of distorted copies of the input. The goal of this approach is to force the prediction function of the model to be more constant around the input data, while preventing underfitting on the original learning objective. In summary, our contributions are as follows:</p><p>• We propose stability training as a general technique that improves model output stability while maintaining or improving the original performance. Our method is fast in practice and can be used at a minimal additional computational cost.</p><p>• We validate our method by stabilizing state-of-the-art classification and ranking networks based on the Inception architecture <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>. We evaluate on three tasks: near-duplicate image detection, similar-image ranking, and image classification.</p><p>• We show the impact of stability training by visualizing what perturbations the model has become robust to.</p><p>• Finally, we show that stabilized networks offer robust performance and significantly outperform unstabilized models on noisy and corrupted data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Adversarial examples. Recently, several machine learning algorithms were found to have extreme instability against contrived input perturbations <ref type="bibr" target="#b11">[12]</ref> called adversarial examples. An open question remained as to whether such small perturbations that change the class label could occur without intentional human intervention. In this work, we document that they do in fact occur. Previous work has shown that training a classifier to resist adversarial perturbation can improve its performance on both the original data and on perturbed data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. We extend this approach by training our feature embeddings to resist the naturally occurring perturbations that are far more common in practice.</p><p>Furthermore, our work differs drastically from <ref type="bibr" target="#b6">[7]</ref>, which is about how a model responds to intentionally contrived inputs that don't resemble the original data at all. In contrast, in this paper we consider the stability to practically widely occurring perturbations.</p><p>Data augmentation. A natural strategy to improve label stability is to augment the training data with hard positives, which are examples that the prediction model does not classify correctly with high confidence, but that are visually similar to easy positives. Finding such hard positives in video data for data augmentation has been used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref> and has been found to improve predictive performance and consistency. As such, data augmentation with hard positives can confer output stability on the classes of perturbations that the hard positives represent. However, our work differs from data augmentation in two ways. Firstly, we take a general approach by proposing a method that intends to make model performance more robust to various types of natural perturbations. Secondly, our proposed method does not use the extra generated samples as training examples for the original prediction task, but only for the stability objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Stability training</head><p>We now present our stability training approach, and how it can be applied to learn robust feature embeddings and class label predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stability objective</head><p>Our goal is to stabilize the output f (x) ∈ R m of a neural network N against small natural perturbations to a natural image x ∈ [0, 1] w×h of size w × h, where we normalize all pixel values. Intuitively, this means that we want to formulate a training objective that flattens f in a small neighborhood of any natural image x: if a perturbed copy x ′ is close to x, we want f (x) to be close to f (x ′ ), that is</p><formula xml:id="formula_0">∀x ′ : d(x, x ′ ) small ⇔ D(f (x), f (x ′ )) small.<label>(1)</label></formula><p>Here d is the distance on [0, 1] w×h and D is an appropriate distance measure in feature space. Given a training objective L 0 for the original task (e.g. classification, ranking), a reference input x and a perturbed copy x ′ , we can implement the stability objective (1) as:</p><formula xml:id="formula_1">L(x, x ′ ; θ) = L 0 (x; θ) + αL stability (x, x ′ ; θ), (2) L stability (x, x ′ ; θ) = D(f (x), f (x ′ )),<label>(3)</label></formula><p>where α controls the strength of the stability term and θ denotes the weights of the model N . The stability objective L stability forces the output f (x) of the model to be similar between the original x and the distorted copy x ′ . Note that our approach differs from data augmentation: we do not evaluate the original loss L on the distorted inputs x ′ . This is required to achieve both output stability and performance on the original task, as we explain in 3.2. Given a training dataset D, stability training now proceeds by finding the optimal weights θ * for the training objective <ref type="bibr" target="#b1">(2)</ref>, that is, we solve</p><formula xml:id="formula_2">θ * = argmin θ xi∈D,d(xi,x ′ i )&lt;ǫ L(x i , x ′ i ; θ).<label>(4)</label></formula><p>To fully specify the optimization problem, we firstly need a mechanism to generate, for each training step, for each training sample x i , a random perturbed copy x ′ i . Secondly, we need to define the distance D, which is task-specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sampling perturbed images x ′</head><p>Sampling using Gaussian noise. During training, at every training step we need to generate perturbed versions x ′ of a clean image x to evaluate the stability objective (3).</p><p>A natural approach would be to augment the training data with examples with explicitly chosen classes of perturbation that the model should be robust against. However, it is hard to obtain general robustness in this way, as there are many classes of perturbations that cause output instability, and model robustness to one class of perturbations does not confer robustness to other classes of perturbations.</p><p>Therefore, we take a general approach and use a sampling mechanism that adds pixel-wise uncorrelated Gaussian noise ǫ to the visual input x. If k indexes the raw pixels, a new sample is given by:</p><formula xml:id="formula_3">x ′ k = x k + ǫ k , ǫ k ∼ N 0, σ 2 k , σ k &gt; 0,<label>(5)</label></formula><p>where σ 2 k is the variance of the Gaussian noise at pixel k. In this work, we use uniform sampling σ k = σ to produce unbiased samples of the neighborhood of x, using the variance σ 2 as a hyper-parameter to be optimized. Gaussian noise strength σ 0.0 0.1 0.2 Triplet ranking score @ top-30 7,312 6,300 5,065 <ref type="table">Table 1</ref>: Underfitting by data augmentation with Gaussian noise on an image ranking task (higher score is better), see section 5.2 for details. The entry with σ = 0.0 is the model without data augmentation.</p><p>Preventing underfitting. Augmenting the training data by adding uncorrelated Gaussian noise can potentially simulate many types of perturbations. Training on these extra samples could in principle lead to output robustness to many classes of perturbations. However, we found that training on a dataset augmented by Gaussian perturbation leads to underfitting, as shown in <ref type="table">Table 1</ref>. To prevent such underfitting, we do not evaluate the original loss L 0 on the perturbed images x ′ in the full training objective (2), but only evaluate the stability loss (3) on both x and x ′ . This approach differs from data augmentation, where one would evaluate L 0 on the extra training samples as well. It enables achieving both output stability and maintaining high performance on the original task, as we validate empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stability for feature embeddings</head><p>We now show how stability training can be used to obtain stable feature embeddings. In this work, we aim to learn feature embeddings for robust similar-image detection. To this end, we apply stability training in a ranking setting. The objective for similar-image ranking is to learn a feature representation f (x) that detects visual image similarity <ref type="bibr" target="#b12">[13]</ref>. This learning problem is modeled by considering a ranking triplet of images (q, p, n): a query image q, a positive image p that is visually similar to q, and a negative image n that is less similar to q than p is.</p><p>The objective is to learn a feature representation f that respects the triplet ranking relationship in feature space, that is,</p><formula xml:id="formula_4">D(f (q), f (p)) + g &lt; D(f (q), f (n)), g &gt; 0,<label>(6)</label></formula><p>where g is a margin and D is the distance. We can learn a model for this objective by using a hinge loss:</p><formula xml:id="formula_5">L 0 (q, p, n) = max(0, g + D(f (q), f (p)) − D(f (q), f (n))</formula><p>). <ref type="formula">(7)</ref> In this setting, a natural choice for the similarity metric D is the L 2 -distance. The stability loss is,</p><formula xml:id="formula_6">L stability (x, x ′ ) = ||f (x) − f (x ′ )|| 2 .<label>(8)</label></formula><p>To make the feature representation f stable using our approach, we sample triplet images (q ′ , p ′ , n ′ ) close to the reference (q, p, n), by applying <ref type="formula" target="#formula_3">(5)</ref> to each image in the triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Stability for classification</head><p>We also apply stability training in the classification setting to learn stable prediction labels for visual recognition. For this task, we model the likelihood P (y|x; θ) for a labeled dataset {(x i ,ŷ i )} i∈I , whereŷ represents a vector of ground truth binary class labels and i indexes the dataset. The training objective is then to minimize the standard cross-entropy loss</p><formula xml:id="formula_7">L 0 (x; θ) = − jŷ j log P (y j |x; θ),<label>(9)</label></formula><p>where the index j runs over classes. To apply stability training, we use the KL-divergence as the distance function D:</p><formula xml:id="formula_8">L stability (x, x ′ ; θ) = − j P (y j |x; θ) log P (y j |x ′ ; θ),<label>(10)</label></formula><p>which measures the correspondence between the likelihood on the natural and perturbed inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network</head><p>Base network. In our experiments, we use a state-ofthe-art convolutional neural network architecture, the Inception network <ref type="bibr" target="#b10">[11]</ref> as our base architecture. Inception is formed by a deep stack of composite layers, where each composite layer output is a concatenation of outputs of convolutional and pooling layers. This network is used for the classification task and as a main component in the triplet ranking network.</p><p>Triplet ranking network. Triplet ranking loss <ref type="formula">(7)</ref> is used train feature embeddings for image similarity and for near duplicate image detection, similar to <ref type="bibr" target="#b12">[13]</ref>. This network architecture uses an Inception module (while in <ref type="bibr" target="#b12">[13]</ref>, a network like <ref type="bibr" target="#b2">[3]</ref> is used) to process every input image x at full resolution and uses 2 additional low-resolution towers. The outputs of these towers map into a 64-dimensional L 2 -normalized embedding feature f (x). These features are used for the ranking task: for each triplet of images (q, p, n), we use the features (f (q), f (p), f (n)) to compute the ranking loss and train the entire architecture.</p><p>Stability training. It is straightforward to implement stability training for any given neural network by adding a Gaussian perturbation sampler to generate perturbed copies of the input image x and an additional stability objective layer. This setup is depicted in <ref type="figure" target="#fig_1">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Distortion types</head><p>To demonstrate the robustness of our models after stability training is deployed, we evaluate the ranking, nearduplicate detection and classification performance of our stabilized models on both the original and transformed copies of the evaluation datasets. To generate the transformed copies, we apply visual perturbations that widely occur in real-world visual data and that are a result of lossy image processes. JPEG compression. JPEG compression is a commonly used lossy compression method that introduces small artifacts in the image. The extent and intensity of these artifacts can be controlled by specifying a quality level q. In this work, we refer to this as JPEG-q.</p><p>Thumbnail resizing. Thumbnails are smaller versions of a reference image and obtained by downscaling the original image. Because convolutional neural networks use a fixed input size, both the original image and its thumbnail have to be rescaled to fit the input window. Downscaling and rescaling introduces small differences between the original and thumbnail versions of the network input. In this work we refer to this process as THUMB-A, where we downscale to a thumbnail with A pixels, preserving the aspect ratio.</p><p>Random cropping. We also evaluated the performance on perturbations coming from random crops of the original image. This means that we take large crops with window size w ′ × h ′ of the original image of size w × h, using an offset o &gt; 0 to define w ′ = w − o, h ′ = h − o. The crops are centered at random positions, with the constraint that the cropping window does not exceed the image boundaries. Due to the fixed network input size, resizing the cropped image and the original image to the input window introduces small perturbations in the visual input, analogous to thumbnail noise. We refer to this process as CROP-o, for crops with a window defined by offset o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimization</head><p>To perform stability training, we solved the optimization problem (2) by training the network using mini-batch stochastic gradient descent with momentum, dropout <ref type="bibr" target="#b9">[10]</ref>, RMSprop and batch normalization <ref type="bibr" target="#b1">[2]</ref>. To tune the hyperparameters, we used a grid search, where the search ranges are displayed in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>As stability training requires a distorted version of the original training example, it effectively doubles the training batch-size during the forward-pass, which introduces a significant extra computational cost. To avoid this over-  head, in our experiments we first trained the network on the original objective L 0 (x; θ) only and started stability training with L(x, x ′ ; θ) only in the fine-tuning phase. Additionally, when applying stability training, we only fine-tuned the final fully-connected layers of the network. Experiments indicate that this approach leads to the same model performance as applying stability training right from the beginning and training the whole network during stability training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Here we present experimental results to validate our stability training method and characterize stabilized models.</p><p>• Firstly, we evaluate stabilized features on nearduplicate detection and similar-image ranking tasks.</p><p>• Secondly, we validate our approach of stabilizing classifiers on the ImageNet classification task.</p><p>We use training data as in <ref type="bibr" target="#b12">[13]</ref> to train the feature embeddings for near-duplicate detection and similar-image ranking. For the classification task, training data from ImageNet are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Near-duplicate detection</head><p>Detection criterion. We used our stabilized ranking feature to perform near-duplicate detection. To do so, we define the detection criterion as follows: given an image pair (a, b), we say that a, b are near-duplicates ⇐⇒ ||f (a) − f (b)|| 2 &lt; T, <ref type="bibr" target="#b10">(11)</ref> where T is the near-duplicate detection threshold.</p><p>Near-duplicate evaluation dataset. For our experiments, we generated an image-pair dataset with two parts: one set of pairs of near-duplicate images (true positives) and a set of dissimilar images (true negatives).</p><p>We constructed the near-duplicate dataset by collecting 650,000 images from randomly chosen queries on Google Image Search. In this way, we obtained a representative sample of un-curated images. We then combined every image with a copy perturbed with the distortion(s) from section 4.2 to construct near-duplicate pairs. For the set of dissimilar images, we collected 900,000 random image pairs from the top 30 Image Search results for 900,000 random search queries, where the images in each pair come from the same search query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Experimental results</head><p>Precision-recall performance. To analyze the detection performance of the stabilized features, we report the nearduplicate precision-recall values by varying the detection threshold in <ref type="bibr" target="#b10">(11)</ref>. Our results are summarized in <ref type="figure">Figure 6</ref>. The stabilized deep ranking features outperform the baseline features for all three types of distortions, for all levels of fixed recall or fixed precision. Although the baseline features already offer very high performance in both precision and recall on the near-duplicate detection task, the stabilized features significantly improve precision across the board. For instance, recall increases by 1.0% at 99.5% precision for thumbnail near-duplicates, and increases by 3.0% at 98% precision for JPEG near-duplicates. This improved performance is due to the improved robustness of the stabilized features, which enables them to correctly detect nearduplicate pairs that were confused with dissimilar image pairs by the baseline features, as illustrated in <ref type="figure">Figure 1</ref>.</p><p>Feature distance distribution. To analyze the robustness of the stabilized features, we show the distribution of the feature distance D(f (x), f (x ′ )) for the near-duplicate evaluation dataset in <ref type="figure" target="#fig_3">Figure 7</ref>, for both the baseline and stabilized deep ranking feature. Stability training significantly increases the feature robustness, as the distribution of feature distances becomes more concentrated towards 0. For instance, for the original feature 76% of near-duplicate image pairs has feature distance smaller than 0.1, whereas this is 86% for the stabilized feature, i.e. the stabilized feature is significantly more similar for near-duplicate images. </p><formula xml:id="formula_9">tance D(f (xi), f (x ′ i )) = ||f (xi) − f (x ′ i )||2 for near-duplicate pairs (xi, x ′ i )</formula><p>. Red: baseline features, 76% of distribution &lt; 0.1. Green: stabilized features using stability training with α = 0.1, σ = 0.2, 86% of distribution &lt; 0.1. The feature distances are computed over a dataset of 650,000 near-duplicate image pairs (reference image and a JPEG-50 version). Applying stability training makes the distribution of D(f (x), f (x ′ )) more concentrated towards 0 and hence makes the feature f significantly more stable.</p><p>Stabilized feature distance. We also present our qualitative results to visualize the improvements of the stabilized features over the original features. In <ref type="figure">Figure 8</ref> we show pairs of images and their JPEG versions that were confusing for the un-stabilized features, i.e. that lay far apart in feature space, but whose stabilized features are significantly more close. This means that they are correctly detected as nearduplicates for much more aggressive, that is, lower detection thresholds by the stabilized feature, whereas the original feature easily confuses these as dissimilar images. Consistent with the intuition that Gaussian noise applies a wide range of types of perturbations, we see improved performance for a wide range of perturbation types. Importantly, this includes even localized, structured perturbations that do not resemble a typical Gaussian noise sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Similar image ranking</head><p>The stabilized deep ranking features (see section 3.3) are evaluated on the similar image ranking task. Hand-labeled triplets from <ref type="bibr" target="#b12">[13]</ref> 1 are used as evaluation data. There are 14,000 such triplets. The ranking score-at-top-K (K = 30) is used as evaluation metric. The ranking score-at-top-K is defined as ranking score @top-K = # correctly ranked triplets − # incorrectly ranked triplets, <ref type="bibr" target="#b11">(12)</ref> where only triplets whose positive or negative image occurs among the closest K results from the query image are con- sidered. This metric measures the ranking performance on the K most relevant results of the query image. We use this evaluation metric because it reflects better the performance of similarity models in practical image retrieval systems as users pay most of their attentions to the results on the first few pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Experimental results.</head><p>Our results for triplet ranking are displayed in <ref type="table" target="#tab_3">Table 3</ref>. The results show that applying stability training improves the ranking score on both the original and transformed versions of the evaluation dataset. The ranking performance of the baseline model degrades on all distorted versions of the original dataset, showing that it is not robust to the input distortions. In contrast, the stabilized network achieves ranking scores that are higher than the ranking score of the baseline model on the original dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image classification</head><p>In the classification setting, we validated stability training on the ImageNet classification task <ref type="bibr" target="#b8">[9]</ref>, using the Inception network <ref type="bibr" target="#b10">[11]</ref>. We used the full classification dataset, which covers 1,000 classes and contains 1.2 million images, where 50,000 are used for validation. We evaluated the classification precision on both the original and a JPEG-50 version of the validation set. Our benchmark results are in <ref type="table">Table 4</ref>.   <ref type="table">Table 4</ref>: Classification evaluation performance of Inception with stability training, evaluated on the original and JPEG versions of ImageNet. Both networks give similar state-of-the-art performance on the original evaluation dataset (note that the performance difference on the original dataset is within the statistical error of 0.3% <ref type="bibr" target="#b8">[9]</ref>). However, the stabilized network is significantly more robust and outperforms the baseline on the distorted data.</p><p>Applying stability training to the Inception network makes the class predictions of the network more robust to input distortions. On the original dataset, both the baseline and stabilized network achieve state-of-the-art performance. However, the stabilized model achieves higher precision on the distorted evaluation datasets, as the performance degrades more significantly for the baseline model than for the stabilized model. For high distortion levels, this gap grows to 5% to 6% in top-1 and top-5 precision.</p><p>Robust classification on noisy data. We also evaluated the effectiveness of stability training on the classification performance of Inception on the ImageNet evaluation dataset with increasing JPEG corruption. In this experiment, we collected the precision @top-1 scores at convergence for a range of the training hyper-parameters: the regularization coefficient α and noise standard deviation σ. A summary of these results is displayed in <ref type="figure">Figure 9</ref>.</p><p>At the highest JPEG quality level, the performance of the baseline and stabilized models are comparable, as the visual distortions are small. However, as the JPEG distortions become stronger, the stabilized model starts to significantly <ref type="figure">Figure 9</ref>: A comparison of the precision @ top-1 performance on the ImageNet classification task for different stability training hyper-parameters α, using JPEG compressed versions of the evaluation dataset at decreasing quality levels, using a fixed σ = 0.04. At the highest JPEG quality level, the baseline and stabilized models perform comparably. However, as the quality level decreases, the stabilized model starts to significantly outperform the baseline model. outperform the baseline model. This qualitative behavior is visible for a wide range of hyper-parameters, for instance, using α = 0.01 and σ = 0.04 results in better performance already below the 80% quality level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we proposed stability training as a lightweight and effective method to stabilize deep neural networks against natural distortions in the visual input. Stability training makes the output of a neural network more robust by training a model to be constant on images that are copies of the input image with small perturbations. As such, our method can enable higher performance on noisy visual data than a network without stability training. We demonstrated this by showing that our method makes neural networks more robust against common types of distortions coming from random cropping, JPEG compression and thumbnail resizing. Additionally, we showed that using our method, the performance of stabilized models is significantly more robust for near-duplicate detection, similarimage ranking and classification on noisy datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Examples of reference and distorted training images used for stability training. Left: an original image x. Right: a copy x ′ perturbed with pixel-wise uncorrelated Gaussian noise with σ = 0.06, in normalized pixel values. During stability training, we use dynamically sampled copies x ′ together with the stability loss (3) to flatten the prediction function f around the original image x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The architecture used to apply stability training to any given deep neural network. The arrows display the flow of information during the forward pass. For each input image I, a copy I ′ is perturbed with pixel-wise independent Gaussian noise ǫ. Both the original and perturbed version are then processed by the neural network. The task objective L0 is only evaluated on the output f (I) of the original image, while the stability loss Lstability uses the outputs of both versions. The gradients from both L0 and Lstability are then combined into the final loss L and propagated back through the network. For triplet ranking training, three images are processed to compute the triplet ranking objective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Examples of natural distortions that are introduced by common types of image processing. From left to right: original image (column 1 and 5), pixel-wise differences from the original after different forms of transformation: thumbnail downscaling to 225 × 225 (column 2 and 6), JPEG compression at quality level 50% (column 3 and 7) and random cropping with offset 10 (column 4 and 8). For clarity, the JPEG distortions have been up-scaled by 5×. Random cropping and thumbnail resizing introduce distortions that are structured and resemble the edge structure of the original image. In contrast, JPEG compression introduces more unstructured noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Cumulative distribution of the deep ranking feature dis-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 8 :</head><label>68</label><figDesc>Precision-recall performance for near-duplicate detection using feature distance thresholding on deep ranking features. We compare Inception-based deep ranking features (blue), and the same features with stability training applied (red). Every graph shows the performance using near-duplicates generated through different distortions. Left: THUMB-50k. Middle: JPEG-50. Right: CROP-10. Across the three near-duplicate tasks, the stabilized model significantly improves the near-duplicate detection precision over the baseline model. Examples of near-duplicate image pairs that are robustly recognized as near-duplicates by stabilized features (small feature distance), but easily confuse un-stabilized features (large feature distance). Left group: using JPEG-50 compression corruptions. Right group: random cropping CROP-10 corruptions. For each image pair, we display the reference image x, the difference with its corrupted copy x − x ′ , and the distance in feature space D(f (x), f (x ′ )) for the un-stabilized (red) and stabilized features (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Hyper-parameter search range for the stability training experiments.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Ranking score @top-30 for the deep ranking network with and without stability training (higher is better) on distorted image data. Stability training increases ranking performance over the baseline on all versions of the evaluation dataset. We do not report precision scores, as in<ref type="bibr" target="#b12">[13]</ref>, as the ranking score @top-30 agrees more with human perception of practical similar image retrieval.</figDesc><table>Precision @top-5 Original JPEG-50 JPEG-10 
Szegedy et al [11] 
93.3% 
Inception 
93.9% 
92.4% 
83.0% 
Stability training 
93.6% 
92.7% 
88.3% 

Precision @top-1 
Inception 
77.8% 
75.1% 
61.1% 
Stability training 
77.9% 
75.7% 
67.9% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/site/ imagesimilaritydata/.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Explaining and Harnessing Adversarial Examples</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Expanding object detector&apos;s horizon: Incremental learning framework for object detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Watch and learn: Semi-supervised learning for object detectors from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<title level="m">Distributional Smoothing with Virtual Adversarial Training</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
