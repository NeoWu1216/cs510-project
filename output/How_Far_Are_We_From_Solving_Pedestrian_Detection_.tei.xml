<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Far are We from Solving Pedestrian Detection?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How Far are We from Solving Pedestrian Detection?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Encouraged by the recent progress in pedestrian detection, we investigate the gap between current state-of-the-art methods and the "perfect single frame detector". We enable our analysis by creating a human baseline for pedestrian detection (over the Caltech dataset), and by manually clustering the recurrent errors of a top detector. Our results characterise both localisation and background-versusforeground errors.</p><p>To address localisation errors we study the impact of training annotation noise on the detector performance, and show that we can improve even with a small portion of sanitised training data. To address background/foreground discrimination, we study convnets for pedestrian detection, and discuss which factors affect their performance.</p><p>Other than our in-depth analysis, we report top performance on the Caltech dataset, and provide a new sanitised set of training and test annotations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection has received great attention during recent years. Pedestrian detection is a canonical sub-problem that remains a popular topic of research due to its diverse applications.</p><p>Despite the extensive research on pedestrian detection, recent papers still show significant improvements, suggesting that a saturation point has not yet been reached. In this paper we analyse the gap between the state of the art and a newly created human baseline (section 3.1). The results indicate that there is still a ten fold improvement to be made before reaching human performance. We aim to investigate which factors will help close this gap.</p><p>We analyse failure cases of top performing pedestrian detectors and diagnose what should be changed to further push performance. We show several different analysis, including human inspection, automated analysis of problem cases (e.g. blur, contrast), and oracle experiments (section 3.2). Our results indicate that localisation is an important source of high confidence false positives. We address this   <ref type="figure">Figure 1</ref>: Overview of the top results on the Caltech-USA pedestrian benchmark (CVPR2015 snapshot). At ∼ 95% recall, state-of-the-art detectors make ten times more errors than the human baseline.</p><p>aspect by improving the training set alignment quality, both by manually sanitising the Caltech training annotations and via algorithmic means for the remaining training samples (sections 3.3 and 4.1).</p><p>To address background versus foreground discrimination, we study convnets for pedestrian detection, and discuss which factors affect their performance (section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>In the last years, diverse efforts have been made to improve the performance of pedestrian detection. Following the success of integral channel feature detector (ICF) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>, many variants <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref> were proposed and showed significant improvement. A recent review of pedestrian detection <ref type="bibr" target="#b2">[3]</ref> concludes that improved features have been driving performance and are likely to continue doing so. It also shows that optical flow <ref type="bibr" target="#b17">[18]</ref> and context information <ref type="bibr" target="#b15">[16]</ref> are complementary to image features and can further boost detection accuracy.</p><p>By fine-tuning a model pre-trained on external data convolution neural networks (convnets) have also reached state-of-the-art performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Most of the recent papers focus on introducing novelty and better results, but neglect the analysis of the resulting system. Some analysis work can be found for general object detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>; in contrast, in the field of pedestrian detection, this kind of analysis is rarely done. In 2008, <ref type="bibr" target="#b19">[20]</ref> provided a failure analysis on the INRIA dataset, which is relatively small. The best method considered in the 2012 Caltech dataset survey <ref type="bibr" target="#b6">[7]</ref> had 10× more false positives at 20 % recall than the methods considered here, and no method had reached the 95 % mark.</p><p>Since pedestrian detection has improved significantly in recent years, a deeper and more comprehensive analysis based on state-of-the-art detectors is valuable to provide better understanding as to where future efforts would best be invested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>Our key contributions are as follows: (a) We provide a detailed analysis of a state-of-the-art pedestrian detector, providing insights into failure cases. (b) We provide a human baseline for the Caltech Pedestrian Benchmark; as well as a sanitised version of the annotations to serve as new, high quality ground truth for the training and test sets of the benchmark. This data is public 1 . (c) We analyse the effects of training data quality. More specifically we quantify how much better alignment and fewer annotation mistakes can improve performance. (d) Using the insights of the analysis, we explore variants of top performing methods: filtered channel feature detector <ref type="bibr" target="#b22">[23]</ref> and R-CNN detector <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, and show improvements over the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Before delving into our analysis, let us describe the datasets in use, their metrics, and our baseline detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Caltech-USA pedestrian detection benchmark</head><p>Amongst existing pedestrian datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>, KITTI <ref type="bibr" target="#b9">[10]</ref> and Caltech-USA are currently the most popular ones. In this work we focus on the Caltech-USA benchmark <ref type="bibr" target="#b6">[7]</ref> which consists of 2.5 hours of 30Hz video recorded from a vehicle traversing the streets of Los Angeles, USA. The video annotations amount to a total of 350 000 bounding boxes covering ∼ 2 300 unique pedestrians. Detection methods are evaluated on a test set consisting of 4 024 frames. The provided evaluation toolbox generates plots for different subsets of the test set based on annotation size, occlusion level and aspect ratio. The established procedure for training is to use every 30th video frame which results in a total of 4 250 frames with ∼ 1 600 pedestrian cut-  <ref type="table">Table 2</ref>: Detection quality gain of adding context <ref type="bibr" target="#b15">[16]</ref> and optical flow <ref type="bibr" target="#b17">[18]</ref>, as function of the base detector.</p><p>outs. More recently, methods which can leverage more data for training have resorted to a finer sampling of the videos <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, yielding up to 10× as much data for training than the standard "1×" setting. </p><formula xml:id="formula_0">MR O ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Filtered channel feature detectors</head><p>For the analysis in this paper we consider all methods published on the Caltech Pedestrian benchmark, up to the last major conference (CVPR2015). As shown in <ref type="figure">figure 1</ref>, the best method at the time is Checkerboards, and most of the top performing methods are of its same family.</p><p>The Checkerboards detector <ref type="bibr" target="#b22">[23]</ref> is a generalisation of the Integral Channels Feature detector (ICF) <ref type="bibr" target="#b5">[6]</ref>, which filters the HOG+LUV feature channels before feeding them into a boosted decision forest.</p><p>We compare the performance of several detectors from the ICF family in table 1, where we can see a big improvement from 44.2% to 18.5% MR O −2 by introducing filters over the feature channels and optimising the filter bank.</p><p>Current top performing convnets methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> are sensitive to the underlying detection proposals, thus we first focus on the proposals by optimising the filtered channel feature detectors (more on convnets in section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotated filters</head><p>For the experiments involving training new models (in section 4.1) we use our own re-implementation of Checkerboards <ref type="bibr" target="#b22">[23]</ref>, based on the LDCF <ref type="bibr" target="#b14">[15]</ref> codebase. To improve the training time we decrease the number of filters from 61 in the original Checkerboards down to 9 filters. Our so-called RotatedFilters are a simplified version of LDCF, applied at three different scales (in the same spirit as SquaresChnFtrs (SCF) <ref type="bibr" target="#b2">[3]</ref>). More details on the filters are given in the supplementary material.</p><p>As shown in table 1, RotatedFilters are significantly better than the original LDCF, and only 1 pp (percent point) worse than Checkerboards, yet run 6× faster at training and test time.</p><p>Additional cues The review <ref type="bibr" target="#b2">[3]</ref> showed that context and optical flow information can help improve detections. However, as the detector quality improves (table 1) the returns obtained from these additional cues erodes <ref type="table">(table 2)</ref>. Without re-engineering such cues, gains in detection must come from the core detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysing the state of the art</head><p>In this section we estimate a lower bound on the remaining progress available, analyse the mistakes of current pedestrian detectors, and propose new annotations to better measure future progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Are we reaching saturation?</head><p>Progress on pedestrian detection has been showing no sign of slowing in recent years <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3]</ref>, despite recent impressive gains in performance. How much progress can still be expected on current benchmarks? To answer this question, we propose to use a human baseline as lower bound. We asked domain experts to manually "detect" pedestrians in the Caltech-USA test set; machine detection algorithms should be able to at least reach human performance and, eventually, superhuman performance.</p><p>Human baseline protocol To ensure a fair comparison with existing detectors, most of which operate at test time over a single image, we focus on the single frame monocular detection setting. Frames are presented to annotators in random order, and without access to surrounding frames from the source videos. Annotators have to rely on pedestrian appearance and single-frame context rather than (longterm) motion cues.</p><p>The Caltech benchmark normalises the aspect ratio of all detection boxes <ref type="bibr" target="#b6">[7]</ref>. Thus our human annotations are done by drawing a line from the top of the head to the point between both feet. A bounding box is then automatically generated such that its centre coincides with the centre point of the manually-drawn axis, see illustration in figure 2. This procedure ensures the box is well centred on the subject (which is hard to achieve when marking a bounding box).</p><p>To check for consistency among the two annotators, we produced duplicate annotations for a subset of the test images (∼ 10%), and evaluated these separately. With a Intersection over Union (IoU) ≥ 0.5 matching criterion, the results were identical up to a single bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In <ref type="figure">figure 3</ref>, we compare our human baseline with other top performing methods on different subsets of the test data . We find that the human baseline widely outperforms state-of-the-art detectors in all settings 2 , indicating that there is still room for improvement for automatic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Failure analysis</head><p>Since there is room to grow for existing detectors, one might want to know: when do they fail? In this section we analyse detection mistakes of Checkerboards, which obtains top performance on most subsets of the test set (see <ref type="figure">figure 3</ref>). Since most top methods of figure 1 are of the ICF family, we expect a similar behaviour for them too. Methods using convnets with proposals based on ICF detectors will also be affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Error sources</head><p>There are two types of errors a detector can do: false positives (detections on background or poorly localised detections) and false negatives (low-scoring or missing pedestrian detections). In this analysis, we look into false positive and false negative detections at 0.1 false positives per image (FPPI, 1 false positive every 10 images), and manually cluster them (one to one mapping) into visually distinctive groups. A total of 402 false positive and 148 false negative detections (missing recall) are categorised by error type.</p><p>False positives After inspection, we end up having all false positives clustered in eleven categories, shown in figure 4a. These categories fall into three groups: localisation, background, and annotation errors. Localisation errors are defined as false detections overlapping with ground truth bounding boxes, while background errors have zero overlap with any ground truth annotation. Background errors are the most common ones, mainly vertical structures (e.g. figure 5b), tree leaves, and traffic lights. This indicates that the detectors need to be extended with a better vertical context, providing visibility over larger structures and a rough height estimate. Localisation errors are dominated by double detections  <ref type="figure">Figure 3</ref>: Detection quality (log-average miss rate) for different test set subsets. Each group shows the human baseline, the Checkerboards <ref type="bibr" target="#b22">[23]</ref> and RotatedFilters detectors, as well as the next top three (unspecified) methods (different for each setting). The corresponding curves are provided in the supplementary material.</p><p>(high scoring detections covering the same person, e.g. figure 5a). This indicates that improved detectors need to have more localised responses (peakier score maps) and/or a different non-maxima suppression strategy. In sections 3.3 and 4.1 we explore how to improve the detector localisation. The annotation errors are mainly missing ignore regions, and a few missing person annotations. In section 3.3 we revisit the Caltech annotations.</p><p>False negatives Our clustering results in <ref type="figure" target="#fig_2">figure 4b</ref> show the well known difficulty of detecting small and occluded objects. We hypothesise that low scoring side-view persons and cyclists may be due to a dataset bias, i.e. these cases are under-represented in the training set (most persons are non-cyclist walking on the side-walk, parallel to the car). Augmenting the training set with external images for these cases might be an effective strategy. To understand better the issue with small pedestrians, we measure size, blur, and contrast for each (true or false) detection. We observed that small persons are commonly saturated (over or under exposed) and blurry, and thus hypothesised that this might be an underlying factor for weak detection (other than simply having fewer pixels to make the decision). Our results indicate however that this is not the case. As figure 4c illustrates, there seems to be no correlation between low detection score and low contrast. This also holds for the blur case, detailed plots are in the supplementary material. We conclude that the small number of pixels is the true source of difficulty. Improving small objects detection thus need to rely on making proper use of all pixels available, both inside the window and in the surrounding context, as well as across time.</p><p>Conclusion Our analysis shows that false positive errors have well defined sources that can be specifically targeted with the strategies suggested above. A fraction of the false negatives are also addressable, albeit the small and occluded pedestrians remain a (hard and) significant problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Oracle test cases</head><p>The analysis of section 3.2.1 focused on errors counts. For area-under-the-curve metrics, such as the ones used in Caltech, high-scoring errors matter more than low-scoring ones. In this section we directly measure the impact of localisation and background-vs-foreground errors on the detec- tion quality metric (log-average miss-rate) by using oracle test cases.</p><p>In the oracle case for localisation, all false positives that overlap with ground truth are ignored for evaluation. In the oracle tests for background-vs-foreground, all false positives that do not overlap with ground truth are ignored. <ref type="figure">Figure 6a</ref> shows that fixing localisation mistakes improves performance in the low FPPI region; while fixing background mistakes improves results in the high FPPI region. Fixing both types of mistakes results zero errors, even though this is not immediately visible in the double log plot.</p><p>In <ref type="figure">figure 6b</ref> we show the gains to be obtained in MR O −4 terms by fixing localisation or background issues. When comparing the eight top performing methods we find that most methods would boost performance significantly by fixing either problem. Note that due to the log-log nature of the numbers, the sum of localisation and background deltas do not add up to the total miss-rate.</p><p>Conclusion For most top performing methods localisation and background-vs-foreground errors have equal impact on the detection quality. They are equally important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Improved Caltech-USA annotations</head><p>When evaluating our human baseline (and other methods) with a strict IoU ≥ 0.8 we notice in <ref type="figure">figure 3</ref> that the performance drops. The original annotation protocol is based on interpolating sparse annotations across multiple frames <ref type="bibr" target="#b6">[7]</ref>, and these sparse annotations are not necessarily located on the evaluated frames. After close inspection we notice that this interpolation generates a systematic offset in the annotations. Humans walk with a natural up and down oscillation that is not modelled by the linear interpolation used, thus in most frames have shifted bounding box annotations. This effect is not noticeable when using the forgiving IoU ≥ 0.5, however such noise in the annotations is a hurdle when aiming to improve object localisation.</p><p>This localisation issues together with the annotation errors detected in section 3.2.1 motivated us to create a new set of improved annotations for the Caltech pedestrians dataset. Our aim is two fold; on one side we want to provide a more accurate evaluation of the state of the art, in particular an evaluation suitable to close the "last 20%" of the problem. On the other side, we want to have training annotations   and evaluate how much improved annotations lead to better detections. We evaluate this second aspect in section 4.1.</p><p>New annotation protocol Our new annotations are done both on the test and training 1× set, and focus on high quality. The annotators are allowed to look at the full video to decide if a person is present or not, they are requested to mark ignore regions in areas covering crowds, human shapes that are not persons (posters, statues, etc.), and in areas that could not be decided as certainly not containing a person. Each person annotation is done by drawing a line from the top of the head to the point between both feet, the same as human baseline. The annotators must hallucinate head and feet if these are not visible. When the person is not fully visible, they must also annotate a rectangle around the largest visible region. This allows to estimate the occlusion level in a similar fashion as the original annotations.  The new annotations do share some bounding boxes with the human baseline (when no correction was needed), thus the human baseline cannot be used to do analysis across different IoU thresholds over the new test set. In summary, our new annotations differ from the human baseline in the following aspects: both training and test sets are annotated, ignore regions and occlusions are also annotated, full video data is used for decision, and multiple revisions of the same image are allowed. After creating a full independent set of annotations, we con- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Improving the state of the art</head><p>In this section we leverage the insights of the analysis, to improve localisation and background-versus-foreground  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Impact of training annotations</head><p>With new annotations at hand we want to understand what is the impact of annotation quality on detection quality. We will train ACF <ref type="bibr" target="#b4">[5]</ref> and RotatedFilters models (introduced in section 2.2) using different training sets and evaluate on both original and new annotations (i.e.</p><formula xml:id="formula_1">MR O −2 ,M R O −4 and MR N −2 ,M R N −4 )</formula><p>. Note that both detectors are trained via boosting and thus inherently sensitive to annotation noise. <ref type="table" target="#tab_7">Table 4</ref> shows results when training with original, new and pruned annotations (using a 5 /6+ 1 /6 training and validation split of the full training set). As expected, models trained on original/new and tested on original/new perform better than training and testing on different annotations. To understand better what the new annotations bring to the table, we build a hybrid set of annotations. Pruned annotations is a mid-point that allows to decouple the effects of removing errors and improving alignment. Pruned annotations are generated by matching new and original annotations (IoU ≥ 0.5), marking as ignore region any original annotation absent in the new ones, and adding any new annotation absent in the original ones. From original to pruned annotations the main change is removing annotation errors, from pruned to new, the main change is better alignment. From table 4 both ACF and RotatedFilters benefit from removing annotation errors, even in MR O −2 . This indicates that our new training set is better sanitised than the original one. We see in MR N −2 that the stronger detector benefits more from better data, and that the largest gain in detection quality comes from removing annotation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pruning benefits</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment benefits</head><p>The detectors from the ICF family benefit from training with increased training data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, using 10× data is better than 1× (see section 2.1). To leverage the 9× remaining data using the new 1× annotations we train a model over the new annotations and use this model 1× data  to re-align the original annotations over the 9× portion. Because the new annotations are better aligned, we expect this model to be able to recover slight position and scale errors in the original annotations. <ref type="figure" target="#fig_8">Figure 8</ref> shows example results of this process. See supplementary material for details. <ref type="table" target="#tab_9">Table 5</ref> reports results using the automatic alignment process, and a few degraded cases: using the original 10×, self-aligning the original 10× using a model trained over original 10×, and aligning the original 10× using only a fraction of the new annotations (without replacing the 1× portion). The results indicate that using a detector model to improve overall data alignment is indeed effective, and that better aligned training data leads to better detection quality (both in MR O and MR N ). This is in line with the analysis of section 3.2. Already using a model trained on 1 /2 of the new annotations for alignment, leads to a stronger model than obtained when using original annotations.</p><formula xml:id="formula_2">10× data aligned with MR O −2 (MR O −4 )M R N −2 (MR N<label>−</label></formula><p>We name the RotatedFilters model trained using the new annotations and the aligned 9× data, Rotated-Filters-New10×. This model also reaches high median true positives IoU in table 3, indicating that indeed it obtains more precise detections at test time.</p><p>Conclusion Using high quality annotations for training improves the overall detection quality, thanks both to improved alignment and to reduced annotation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Convnets for pedestrian detection</head><p>The results of section 3.2 indicate that there is room for improvement by focusing on the core background versus foreground discrimination task (the "classification part of object detection"). Recent work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> showed competitive performance with convolutional neural networks (con-  <ref type="table">Table 6</ref>: Detection quality of convnets with different proposals. Grey numbers indicate worse results than the input proposals. All numbers are MR N −2 on the Caltech test set. vnets) for pedestrian detection. We include convnets into our analysis, and explore to what extent performance is driven by the quality of the detection proposals.</p><p>AlexNet and VGG We consider two convnets. 1) The AlexNet from <ref type="bibr" target="#b13">[14]</ref>, and 2) The VGG16 model from <ref type="bibr" target="#b10">[11]</ref>. Both are pre-trained on ImageNet and fine-tuned over Caltech 10× (original annotations) using SquaresChnFtrs proposals. Both networks are based on open source, and both are instances of the R-CNN framework <ref type="bibr" target="#b11">[12]</ref>. Albeit their training/test time architectures are slightly different (R-CNN versus Fast R-CNN), we expect the result differences to be dominated by their respective discriminative power (VGG16 improves 8 pp in mAP over AlexNet in the Pascal detection task <ref type="bibr" target="#b11">[12]</ref>). <ref type="table">Table 6</ref> shows that as the quality of the detection proposals improves, AlexNet fails to provide a consistent gain, eventually worsening the results of our ICF detectors (similar observation in <ref type="bibr" target="#b13">[14]</ref>). Similarly VGG provides large gains for weaker proposals, but as the proposals improve, the gain from the convnet re-scoring eventually stalls.</p><p>After closer inspection of the resulting curves (see supplementary material), we notice that both AlexNet and VGG push background instances to lower scores, and at the    <ref type="bibr">[14,</ref> supp. material, <ref type="figure" target="#fig_9">fig. 9</ref>]), however convnets have difficulties giving low scores to these windows surrounding the true positives. In other words, despite their fine-tuning, the convnet score maps are "blurrier" than the proposal ones. We hypothesise this is an intrinsic limitation of the AlexNet and VGG architectures, due to their internal feature pooling. Obtaining "peakier" responses from a convnet most likely will require using rather different architectures, possibly more similar to the ones used for semantic labelling or boundaries estimation tasks which require pixel-accurate output. Fortunately, we can compensate for the lack of spatial resolution in the convnet scoring by using bounding box regression. Adding bounding regression over VGG, and applying a second round of non-maximum suppression (first NMS on the proposals, second on the regressed boxes), has the effect of "contracting the score maps". Neighbour proposals that before generated multiple strong false positives, now collapse into a single high scoring detection. We use the usual IoU ≥ 0.5 merging criterion for the second NMS.</p><p>The last column of table 6 shows that bounding box regression + NMS is effective at providing an additional gain over the input proposals, even for our best detector RotatedFilters-New10×. On the original annotations RotatedFilters-New10×+VGG reaches 14.2% MR O −2 , which improves over <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. Our best performing detector RotatedFilters-New10× runs on a 640 × 480 image for~3.5 seconds, including the ICF sliding window detection and VGG rescoring. Training times are counted 1~2 days for the RotatedFilters detector, and 1~2 days for VGG fine-tunning. <ref type="figure" target="#fig_9">Figure 9</ref> repeats the oracle tests of section 3.2.2 over our convnet results. One can see that VGG significantly cuts down the background errors, while at the same time slightly increases the localisation errors.</p><p>Conclusion Although convnets have strong results in image classification and general object detection, they seem to have limitations when producing well localised detection scores around small objects. Bounding box regression (and NMS) is a key ingredient to side-step this limitation with current architectures. Even after using a strong convnet, background-versus-foreground remains the main source of errors; suggesting that there is still room for improvement on the raw classification power of the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>In this paper, we make great efforts on analysing the failures for a top-performing detector on Caltech dataset. Via our human baseline we have quantified a lower bound on how much improvement there is to be expected. There is a 10× gap still to be closed. To better measure the next steps in detection progress, we have provided new sanitised Caltech train and test set annotations.</p><p>Our failure analysis of a top performing method has shown that most of its mistakes are well characterised. The error characteristics lead to specific suggestions on how to engineer better detectors (mentioned in section 3.2; e.g. data augmentation for person side views, or extending the detector receptive field in the vertical axis).</p><p>We have partially addressed some of the issues by measuring the impact of better annotations on localisation accuracy, and by investigating the use of convnets to improve the background to foreground discrimination. Our results indicate that significantly better alignment can be achieved with properly trained ICF detectors, and that, for pedestrian detection, convnet struggle with localisation issues, that can be partially addressed via bounding box regression. Both on original and new annotations, the described detection approach reaches top performance, see progress in table 7.</p><p>We hope the insights and data provided in this work will guide the path to close the gap between machines and humans in the pedestrian detection task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of bounding box generation for human baseline. The annotator only needs to draw a line from the top of the head to the central point between both feet, a tight bounding box is then automatically generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Errors analysis of Checkerboards [23] on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Example of analysed false positive cases (red box). Additional ones in supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>and two oracle curves for Checkerboards detector. Legend indicates MR O −2 MR O −4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Comparison of miss-rate gain (∆MR O −4 ) for top performing methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Oracle cases evaluation over Caltech test set. Both localisation and background-versus-foreground show important room for improvement. Examples of errors in original annotations. New annotations in green, original ones in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>solidated the new annotations by cross-validating with the old annotations. Any correct old annotation not accounted for in the new set, was added too. Our new annotations correct several types of errors in the existing annotations, such as misalignments (figure 7b), missing annotations (false negatives), false annotations (false positives, figure 7a), and the inconsistent use of "ignore" regions. More examples of "original versus new annotations" provided in the supplementary material, as well as a visualisation software to inspect them frame by frame. Better alignment In table 3 we show quantitative evidence that our new annotations are at least more precisely localised than the original ones. We summarise the alignment quality of a detector via the median IoU between true positive detections and a given set of annotations. When evaluating with the original annotations ("median IoU O " column in table 3), only the model trained with original annotations has good localisation. However, when evaluating with the new annotations ("median IoU N " column) both the model trained on INRIA data, and on the new annotations reach high localisation accuracy. This indicates that our new annotations are indeed better aligned, just as INRIA annotations are better aligned than Caltech.Detailed IoU curves for multiple detectors are provided in the supplementary material. Section 4.1 describes the RotatedFilters-New10× entry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Examples of automatically aligned ground truth annotations. Left/right→ before/after alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Oracle case analysis of proposals + convnets (after second NMS). Miss-rate gain, ∆MR O −4 . The convnet significantly improves background errors, while slightly increasing localisation ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>1 http://www.mpi-inf.mpg.de/pedestrian_detection_cvpr16</figDesc><table>Filter type 
MR O 

−2 

ACF [5] 

44.2 

SCF [3] 

34.8 

LDCF [15] 

24.8 
RotatedFilters 19.2 
Checkerboards 18.5 
Table 1: The filter 
type determines the 
ICF methods quality. 

Base detector 
MR O 
−2 +Context +Flow 

Orig. 2Ped [16] 48~5pp 
/ 
Orig. SDt [18] 45 
/ 
8pp 
SCF [3] 
35 
5pp 
4pp 
Checkerboards 19~0 
1pp 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>we introduce new annotations, and mark evaluations done there as MR N −2 and MR N −4 . We expect the MR −4 metric to become more important as detectors get stronger.</figDesc><table>M R N In the standard Caltech evaluation [7] the 
miss rate (MR) is averaged over the low precision range 
of [10 −2 , 10 0 ] FPPI (false positives per image). This met-
ric does not reflect well improvements in localisation er-
rors (lowest FPPI range). Aiming for a more complete 
evaluation, we extend the evaluation FPPI range from tra-
ditional [10 −2 , 10 0 ] to [10 −4 , 10 0 ], we denote these MR O 

−2 

and MR O 
−4 . O stands for "original annotations". In section 
3.3 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Median IoU of true positives for detectors trained on different data, evaluated on original and new Caltech test. Models trained on INRIA align well with our new annotations, confirming that they are more precise than previous ones. Curves for other detectors in the supplement.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Effects of different training annotations on detec-
tion quality on validation set (1× training set). Italic num-
bers have matching training and test sets. Both detectors im-
prove on the original annotations, when using the "pruned" 
variant (see  §4.1). 

discrimination of our baseline detector. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Detection quality of RotatedFilters on test 
set when using different aligned training sets. All mod-
els trained with Caltech 10×, composed with different 
1 × +9× combinations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head></head><label></label><figDesc>Further results in the supplementary material.</figDesc><table>.05 

.10 

.20 

.30 

.40 

.50 

.64 

.80 

.100 

miss rate 

27.60(41.92)% ACF-Caltech+ 
23.72(38.27)% LDCF 
22.18(34.56)% Katamari 
21.59(34.61)% AlexNet 
21.56(35.96)% SpatialPooling+ 
18.75(34.26)% TA-CNN 
16.69(30.78)% Ours-RotatedFilters 
15.81(28.57)% Checkerboards 
12.96(22.20)% Ours-RotatedFilters-New10x 
10.00(20.77)% Ours-RotatedFilters-New10x+VGG 
0.88% Ours-HumanBaseline 

Figure 10: Detection quality on Caltech test set (reasonable 
subset), evaluated on the new annotations (MR N 
−2 MR N 
−4 ). 
Detector aspect 
MR O 
−2 (MR O 
−4 )M R N 
−2 (MR N 
−4 ) 
RotatedFilters 
19.20 (34.28) 
17.22 (31.65) 
+ Alignment  §4.1 
16.97 (28.01) 
14.54 (25.06) 
+ New annotations  §4.1 16.77 (29.76) 
12.96 (22.20) 
+ VGG  §4.2 
16.61 (34.79) 
11.74 (28.37) 
+ bbox reg &amp; NMS 
14.16 (28.39) 
10.00 (20.77) 
Checkerboards 
18.47 (33.20) 
15.81 (28.57) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Step by step improvements from previous best method Checkerboards to Rotated-Filters-New10x+VGG.same time generate a large number of high scoring false positives. The ICF detectors are able to provide high recall proposals, where false positives around the objects have low scores (see</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Except for IoU ≥ 0.8. This is due to issues with the ground truth, discussed in section 3.3.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, CVRSUAD workshop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single-pedestrian detection aided by multi-pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring weak stabilization for motion feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A performance evaluation of single and multi-feature people detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Informed haar-like features improve pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cremers. Exploring human vision driven features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
