<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Metric Learning as Convex Combinations of Local Models with Generalization Guarantees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Emonet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">UJM-Saint-Etienne</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>Institut</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Optique Graduate School</orgName>
								<orgName type="laboratory" key="lab1">Laboratoire Hubert Curien</orgName>
								<orgName type="laboratory" key="lab2">UMR 5516</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>SAINT-ETIENNE</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Metric Learning as Convex Combinations of Local Models with Generalization Guarantees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past ten years, metric learning allowed the improvement of numerous machine learning approaches that manipulate distances or similarities. In this field, local metric learning has been shown to be very efficient, especially to take into account non linearities in the data and better capture the peculiarities of the application of interest. However, it is well known that local metric learning (i) can entail overfitting and (ii) face difficulties to compare two instances that are assigned to two different local models. In this paper, we address these two issues by introducing a novel metric learning algorithm that linearly combines local models (C2LM). Starting from a partition of the space in regions and a model (a score function) for each region, C2LM defines a metric between points as a weighted combination of the models. A weight vector is learned for each pair of regions, and a spatial regularization ensures that the weight vectors evolve smoothly and that nearby models are favored in the combination. The proposed approach has the particularity of working in a regression setting, of working implicitly at different scales, and of being generic enough so that it is applicable to similarities and distances. We prove theoretical guarantees of the approach using the framework of algorithmic robustness. We carry out experiments with datasets using both distances (perceptual color distances, using Mahalanobis-like distances) and similarities (semantic word similarities, using bilinear forms), showing that C2LM consistently improves regression accuracy even in the case where the amount of training data is small.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many machine learning tasks, like classification, clustering or ranking, decisions are based on distance or similarity functions. In order to capture the peculiarities of the data of the applications at hand, a lot of work has gone during the past ten years into automatically optimiz- <ref type="figure">Figure 1</ref>: Limitation of local metric learning: While two points belonging to the same region (e.g. in R1) can be managed by the corresponding locally-learned metric (depicted as an ellipse), two points from different regions (e.g. in R2 and R4) cannot be accurately compared using a single local metric.</p><p>ing those functions, topic referred to as metric learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Most of the time, a unique global metric is learned over the input space, typically taking the form of a (linear) geometric transformation. This is the case for most of the Mahalanobis-like metric learning approaches, such as LMNN <ref type="bibr" target="#b22">[22]</ref> or ITML <ref type="bibr" target="#b4">[5]</ref>. However, it turns out that for data that present multi-modalities and/or non-linearities, local metric learning has been shown to be very efficient because of its flexibility to capture well geometric variations of the input space. On the other hand, a major problem of local metric learning is that it can entail overfitting. Some recent solutions have been proposed based on feature space dimensionality reduction <ref type="bibr" target="#b7">[8]</ref>, manifold regularization <ref type="bibr" target="#b21">[21]</ref> or generative models <ref type="bibr" target="#b14">[15]</ref>. However, those approaches mainly focus on improving the results locally, i.e. while comparing instances of the "same region" of the input space. Therefore, they are not suited to compare points far from each other. This limitation is illustrated in <ref type="figure">Figure 1</ref>.</p><p>One of the main objectives of our paper is to address this pitfall by learning convex combinations of local metrics that are not only good locally, but also globally relevant. Our algorithm, called Convex Combinations of Local Models (C2LM), basically optimizes for any pair of regions a vector of weights corresponding to the contribution of each local model while computing the distance or similarity between two points of those regions (see <ref type="figure" target="#fig_3">Figure 2</ref>). By means of manifold and vector similarity regularization, we constrain the convex combinations to reflect the topological characteristics of the input space and to vary smoothly. Since our main aim is to learn the influence of each local metric, we will assume in the rest of this paper that the input space has been previously partitioned into regions and that on each region a local metric has been learned to express its underlying geometry.</p><p>Our approach has another particularity: unlike the current trend in metric learning, it lies in a regression setting rather than in a classification framework. Indeed, it is worth noticing that most metric learning methods use side information brought by pairs of training examples in the form of must-link/cannot-link constraints (also called positive/negative pairs) or relative constraints (also called training triplets). A metric learning method typically aims to optimize the parameters of the metric such that it best agrees with those constraints. It turns out that in some applications, the side information provided by the problem of interest simply relies on pairs of examples associated to a target score of (dis)similarity. This is the case in color distance perception (that will constitute one of our two series of experiments), where training data take the form of pairs of color patches and their reference perceptual distance ∆E 00 <ref type="bibr" target="#b18">[18]</ref>. This is also the case for databases made of pairs of strings and their corresponding semantic distance (see, e.g., the well known WordSim353 dataset 1 ). A last example comes from temporal sequence alignments, where training data can be made of pairs of acoustic signals and their corresponding optimal alignment (e.g. see <ref type="bibr" target="#b9">[10]</ref>). In such contexts, state of the art metric learning algorithms face difficulties to accurately capture the idiosyncrasies of the data. Indeed, the price to pay often implies a dramatic increase of the number of constraints to satisfy. Here, we overcome this issue by dealing with metric learning in a regression setting that allows us to directly fit the target scores.</p><p>When proposing a new algorithm for metric learning, it is fundamental to prove that it is theoretically well-founded. In this paper, a lot of work has gone into deriving theoretical guarantees for our method through the algorithmic robustness framework introduced in <ref type="bibr" target="#b25">[25]</ref>. We show that this setting is particularly adapted to our framework because it is based on a partition of the input space as we defined it for our problem (see Section 3).</p><p>To recapitulate, our contributions are three fold:</p><p>1. We improve local metrics by learning linear combinations of local models that (i) allow one to accurately compare any pair of points, (ii) guarantees a certain <ref type="figure" target="#fig_3">Figure 2</ref>: Illustration of the influence of the local models based on region distances: the more influent a local metric for the learned metric, the lighter the color of the associated region. For example, the local models of regions R6, R5, R7, R1 and R11 are more influent than those of the other regions, while computing the distance between the two points of regions R6 and R5.</p><p>continuity of the distances in the entire input space, and (iii) do not overfit;</p><p>2. We develop our metric learning approach in a regression setting that is not usual in this field;</p><p>3. We derive theoretical guarantees for our method through the algorithmic robustness framework.</p><p>The remainder of this paper is organized as follows: in Section 2, we introduce a short state of the art on metric learning; Section 3 is devoted to the presentation of our algorithm for which, in Section 4, we derive a generalization bound based on algorithmic robustness; In order to show that C2LM is able to deal with not only distance functions but also similarity functions, we instantiate the local models as Mahalanobis-like distances and as bilinear similarities; Lastly, Section 5 is dedicated to the experiments. We conduct two series of experiments: a first one in color distance perception, and a second one in string semantic similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A classic metric learning approach consists in learning a unique Mahalanobis-like metric of the form d A (x 1 , <ref type="bibr" target="#b3">[4]</ref>. If A = I, the metric is an Euclidean distance. If a Cholesky decomposition is applied to A (then A = L T L), the distance function corresponds to computing an Euclidean distance in a new space, where the data are linearly rescaled. For instance, the authors of <ref type="bibr" target="#b24">[24]</ref>, using pair-wise information, learn a metric that minimizes the distance between similar examples and maximizes the distance between dissimilar ones and show that it improves results in clustering tasks. Other common metric learning frameworks are LMNN (Large-Margin Nearest Neighbors) proposed in <ref type="bibr" target="#b22">[22]</ref> for improving k-nearest neighbor (kNN) classification and ITML (Information-Theoric Metric Learning) introduced in <ref type="bibr" target="#b4">[5]</ref> for handling constraints and prior knowledge on the metric by means of the LogDet regularization.</p><formula xml:id="formula_0">x 2 ) = (x 1 − x 2 ) T A(x 1 − x 2 ), with A positive semi-definite (A 0)</formula><p>On the other hand, a global and linear metric may not necessarily perform well for all problems, especially for data that present multimodality and non-linearities. In those cases, non-linear methods are more suitable, such as kernel learning and local metric learning approaches. For instance, in <ref type="bibr" target="#b23">[23]</ref>, Weinberger et al. have shown that learning simultaneously a set of local metrics, one for each region of the input space or class label, improved their LMNN framework.</p><p>If local metric learning approaches can adapt well to variations on the input space, they are also quite sensitive to overfitting, especially when local metrics are learned independently from each other. In order to overcome this problem, linear or non-linear combinations of local metrics (instead of only one metric) or kernels (see Multiple Kernel Learning <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b6">[7]</ref>) can be used to compare instances and auxiliary information can be taken into account by means of regularization terms. For instance, the authors of <ref type="bibr" target="#b21">[21]</ref> proposed a regularization based on the geometric characteristics of the instance space: they learn jointly linear combinations of basis metrics (one local metric per region and one linear combination per input instance) and constrain them to vary smoothly over the instances. The weight vectors of close instances are then similar and reflect the geometric characteristics of the input space. However, the learned metrics are no longer symmetric and they are accurate only when comparing instances relatively close to each other. Another example of regularization is proposed in <ref type="bibr" target="#b7">[8]</ref>, where the authors control the rank of the matrix of the learned combinations of metrics, i.e. the total number of parameters of the problem. Doing so, they penalize too complex solutions, which are probably too specialized to the training instances and have lost generalization power on unseen instances. Their approach is based on the pair-wise information about the similarity between instances and the geometric structure of the input space is not taken into account.</p><p>Both frameworks <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b7">8]</ref> are not suited for regression tasks and their choice of defining a linear combination of metrics for each input instance affects the complexity of their problems: the number of parameters to be learned increases with the size of the dataset. We claim that the potential gained accuracy is not enough to justify the computational cost and, in any case, it entails some approximations when testing on unseen data (they both assign the weight vector of the closest training instance in term of Euclidean distance).</p><p>As we will see, our approach (C2LM) is simple, theoretically founded, and accurate: it makes use of the geometric characteristics of the input space and weight vectors are learned on each pair of regions instead of each input instance. Moreover, it can be applied for modeling both distances and similarities; it is theoretically robust and has good performances in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Convex Combinations of Local Models</head><p>In this section, we present our optimization problem for learning convex combinations of local models which takes the form of a least absolute errors regression problem. For the sake of clarity, we first give the few notations we will employ in the rest of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations</head><p>Let X be the instance-pair space, i.e. the set of pairs (x 1 , x 2 ) ∈ U 2 , and y : X → Y ⊂ R a metric function (the ground truth metric that can be a distance or a similarity function). We assume that U is a compact <ref type="bibr" target="#b5">[6]</ref> convex metric space w.r.t. a norm . so that U ⊂ R d . Thus, there exists a constant R such that ∀x ∈ U, x ≤ R. We will refer to Z = X × Y as the set of all possible valued pairs p = (x 1 , x 2 , y(x 1 , x 2 )), where (x 1 , x 2 ) ∈ X is a pair of instances and y(x 1 , x 2 ) is the associated target value. We also denote P = {p i } n i=1 ⊂ Z the set of n training pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization Problem</head><p>Let us suppose that the instance space U has been decomposed in K clusters or regions (one could perform a Kmeans according to the Euclidean distance), denoted {R z } K z=1 and, on each cluster, a local model s z : X → R has been defined in order to compare instances belonging to that specific cluster. Let S = {s z (.)} K z=1 be the set of metric functions related to the local models (which can be distance functions, s z : U 2 → R + , or similarity functions, s z : U 2 → R). Our aim is to define on each pair of regions (R i , R j ) = R ij a metric function t ij : X → R as a convex combination of S and that is symmetric. The problem we are trying to solve is how to compare instances potentially belonging to different clusters. For each pair of regions R ij we will learn a vector W ij of positive weights representing the contribution of each local model while estimating the similarity between an instance x 1 ∈ R i and an instance x 2 ∈ R j . Therefore, the new metric function t ij (x 1 , x 2 ) related to that pair of regions can be expressed as follows:</p><formula xml:id="formula_1">t ij (x 1 , x 2 ) = K z=1 W ijz s z (x 1 , x 2 ).<label>(1)</label></formula><p>Notice that, as we want the new function to be a metric,</p><formula xml:id="formula_2">∀i, j = 1, ..., K t ij (x 1 , x 2 ) = t ji (x 2 , x 1 ): the K × K ma- trix of vectors W = [W 11 W 12 ...W KK ] is symmetric, thus ∀i, j = 1...K, W ij = W ji .</formula><p>We define a loss function l : Z → R over the training set P , corresponding to the gap between t ij and the ground truth metric valued on each pair p = (</p><formula xml:id="formula_3">x 1 ∈ R i , x 2 ∈ R j , y(x 1 , x 2 )): l (W, p) = l (W ij , (x 1 ∈ R i , x 2 ∈ R j , y(x 1 , x 2 ))) = |t ij (x 1 , x 2 ) − y(x 1 , x 2 )| .<label>(2)</label></formula><p>Among all the possible norms, we choose to define our loss function as a L1-norm, i.e. the least absolute deviations, because of its robustness to outliers. This loss is assumed to be uniformly upper-bounded by a constant B, i.e. for any pair p ∈ Z the deviation of the predicted value from the expected one is finite. We define our optimization problem, called C2LM, as follows:</p><formula xml:id="formula_4">arg min W F P (W ) =R l + λ 1 D(W ) + λ 2 S(W ) s.t. ∀i, j = 1, ..., K : K z=1 W ijz = 1 and W ij ≥ 0 (3) wherê R l = 1 n i,j,p∈Rij l(W, p) = = 1 n K i=1 i j=1 p∈Rij K z=1 W ijz s z (x 1 , x 2 ) − y(x 1 , x 2 ) (4)</formula><p>is the mean loss over all training pairs, and</p><formula xml:id="formula_5">D(W ) = K i=1 i j=1 E T ij W ij 2 F (5) S(W ) = K i=1 i j=1 K i ′ =1 i ′ j ′ =1 K iji ′ j ′ W ij − W i ′ j ′ 2 2<label>(6)</label></formula><p>are two regularizers used to avoid overfitting and λ 1 and λ 2 are the corresponding regularization parameters that have to be tuned by cross-validation. The first term, D(W ) takes into account the prior influence of each local model in the computing of a weight vector. For instance, for a vector W ij related to the pair of regions (R i , R j ), we penalize a solution that has big weights associated to the local models that should not be influent in the computing of the associated metric. As a matter of fact, E ij is a 1 × K vector whose component E ijz represents the prior influence of the metric s z . E ijz can be estimated in different ways. In our work, we base this estimation on the topological characteristics of the decomposition of the space U . As we can see in <ref type="figure" target="#fig_3">Figure 2</ref>, a local model defined on a region close to the pair of regions is more influent than one far from it.</p><p>The second term, S(W ), expresses the correlations between different weights' vectors. Through it, we force the space of weights' vectors to be smooth. In other words, we constrain the vectors defined on close pairs of regions to be similar. As for the prior influence, we base the estimation of the similarity between two vectors W ij and W i ′ ,j ′ ,  expressed by the parameter K iji ′ j ′ , on the geometric characteristics of the instance space U (see <ref type="figure" target="#fig_0">Figure 3</ref>).</p><p>In order to evaluate the prior influence of local models and the similarity between vectors of weights, we need to define a distance function between regions. We chose to build the Minimum Spanning Tree of the complete graph of region centroids (computed using the Euclidean distance), then to express the distance between two regions as the number of edges of the shortest path connecting them (see <ref type="figure" target="#fig_1">Figure 4</ref>). Therefore, for our experiments, we will consider E ijz directly proportional to</p><formula xml:id="formula_6">dist(R ij , R z ) = dist(R i , R z ) + dist(R j , R z ) and the similarity K iji ′ j ′ = exp(−dist(R ij , R ′ i ′ j )) exponentially decreasing with dist(R ij , R i ′ j ′ ) = min(dist(R i , R i ′ ) + dist(R j , R j ′ ), dist(R i , R j ′ ) + dist(R j , R i ′ )).</formula><p>The learned combinations of local models are convex, as we fix their weights to be non-negative and to sum up to 1, and the resulting optimization problem is convex. Note that the number of parameters to learn depends on the number of regions K defined on the input space and is directly proportional to K 3 , then the number of constraints is also directly proportional to K 3 . This is a main advantage of applying C2LM to problems providing pairs of instances and their target score, if we consider the fact that K ≪ n: in order to adapt the state of the art approaches (meant for classification tasks) to this kind of problems, a number of constraints directly proportional to the number of instances of the dataset has to be added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Robustness and Generalization Bound</head><p>In this section, we study the generalization ability of our algorithm according to the notion of algorithmic robustness introduced in <ref type="bibr" target="#b25">[25]</ref>. This framework allows us to derive generalization bounds when the variation in the loss associated with two nearby training and testing examples is bounded. The closeness of two examples is based on the notion of covering number. By making use of the Bretagnolle-Huber-Carol inequality and proving that the metric functions s z (.) are lipschitz continuous, we can can derive a PAC generalization bound for C2LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Theoretical guarantees</head><p>Let us define a partition of the space Z = X × Y of all possible valued pairs p = (x, x ′ , y(x, x ′ )) in order to establish if two pairs of instances are close. The partition is based on the notion of covering number.</p><p>Definition 1 (Covering Number <ref type="bibr" target="#b20">[20]</ref>) For a metric space (S, ρ), and T ⊂ S, we say thatT ⊂ T is a γ-cover of T if ∀t ∈ T , ∃t ⊂T such that ρ(t, t ′ ) ≤ γ. The γ-covering number of T is N (γ, T, ρ) = min{|T | :T is a γ-covering of T }. <ref type="formula">(7)</ref> In other words, the γ-covering number of a metric space corresponds to the minimum number of regions of radius at most γ &gt; 0 needed to cover it. In order to define the closeness between instances of a metric space Z = X × Y , both the input X and the target Y spaces have to be partitioned. In most works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>, Y is the finite set of labels, so its covering number is exactly equal to |Y | and two instances are considered close if they have the same label. In our setting, we partition the space X into N (γ 1 /2, X, . 2 ) subsets and the space Y into N (γ 2 /2, Y, |.|), so that any region of X (resp. Y ) has a diameter smaller than γ 1 (resp. γ 2 ). In this way, if p = (x 1 , x 2 , y(x 1 , x 2 )) and</p><formula xml:id="formula_7">p ′ = (x ′ 1 , x ′ 2 , y(x ′ 1 , x ′ 2 )) belong to the same subset of Z, then x 1 − x ′ 1 2 ≤ γ 1 , x 2 − x ′ 2 2 ≤ γ 1 and |y(x 1 , x 2 ) − y(x ′ 1 , x ′ 2 )| ≤ γ 2 .</formula><p>In the rest of this paper, we will refer to H = N (γ 1 /2, X, . 2 )N (γ 2 /2, Y, |.|) as the covering number of Z.</p><p>Definition 2 (Algorithmic Robustness <ref type="bibr" target="#b25">[25]</ref>). An algorithm A is said (H, ǫ(.))-robust, for H ∈ N and ǫ : Z → R if Z can be partitioned into H disjoint subsets, denoted by {C i } H i=1 , such that the following holds for all samples P ∈ Z: ∀p ∈ P, ∀p ′ ∈ Z, ∀i = 1, ..., H if p, p ′ ∈ C i then |l (A, p) − l (A, p ′ )| ≤ ǫ(P ).</p><p>The following concentration inequality provides a probability bound on the deviation of a multinomial random variable from its expected value. We will use it for obtaining information about the theoretical distribution of the valued pairs p ∈ Z over the regions of the partition. <ref type="figure">(|N 1 |)</ref>, ..., |N H |) an IID (Independent and Identically Distributed) multinomial random variable with parameters n and (p(C 1 ), ..., p(C H )). By the Bretagnolle-Huber-Carol inequality we have:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1([20]) Let</head><formula xml:id="formula_9">P( H i=1 |Ni| n − p(C i ) ≥ λ) ≤ 2 H exp −nλ 2 2 , hence with probability at least 1 − δ, H i=1 p(C i ) − |N i | n ≤ 2H ln 2 + 2 ln(1/δ) n .<label>(9)</label></formula><p>We denote R l the true loss R l = E p∼Z l(W, p) andR l the empirical lossR l = E p∼P l(W, p) .</p><p>We can now derive a PAC generalization bound for C2LM. We first prove that our algorithm is robust that requires to prove that ∀z = 1, ..., K : s z (.) is θ z -lipschitz. According to the nature of the local metric functions s z (.), the proof of θ z -lipschitzness varies. In Sections 4.2 and 4.3, we will instantiate s z (.) with Mahalanobis-like distances and bilinear similarities.</p><formula xml:id="formula_10">Lemma 1 If ∀z = 1, ..., K, s z (.) is θ z -lipschitz w.r.t. the norm . 2 , the optimization problem (3) is (H, θ √ 2γ 1 + γ 2 )-robust, with θ = max z=1..K θ z .</formula><p>Proof. We can partition Z into H = N (γ 1 /2, X, . 2 )N (γ 2 /2, Y, |.|) disjoint subsets, such that if p = (x 1 , x 2 , y(x 1 , x 2 )) and p = (x ′ 1 , x ′ 2 , y(x ′ 1 , x ′ 2 )) belong to the same subset C h , then</p><formula xml:id="formula_11">x 1 , x ′ 1 ∈ R i so x 1 − x ′ 1 2 ≤ γ 1 , also x 2 , x ′ 2 ∈ R j so x 2 − x ′ 2 2 ≤ γ 1 and |y(x 1 , x 2 ) − y(x ′ 1 , x ′ 2 )| ≤ γ 2 .</formula><p>We have, then:</p><formula xml:id="formula_12">|l (Wij, p) − l (Wij, p ′ )| = (10) K z=1 Wijzsz(x1, x2)−y(x1, x2) − K z=1 Wijzsz(x ′ 1 , x ′ 2 )−y(x ′ 1 , x ′ 2 ) ≤ K z=1 Wijzsz(x1, x2)− K z=1 Wijzsz(x ′ 1 , x ′ 2 )−y(x1, x2)+y(x ′ 1 , x ′ 2 ) (11) ≤ K z=1 Wijz(sz(x1, x2)−sz(x ′ 1 , x ′ 2 )) + y(x1, x2)−y(x ′ 1 , x ′ 2 ) ≤ K z=1 |Wijz| sz(x1, x2) − sz(x ′ 1 , x ′ 2 ) + γ2 (12) ≤ K z=1 |Wijz| θz x1 x2 − x ′ 1 x ′ 2 2 + γ2 (13) ≤ θ x1 x2 − x ′ 1 x ′ 2 2 K z=1 Wijz + γ2<label>(14)</label></formula><p>≤ θ √ 2γ1 + γ2 . <ref type="bibr" target="#b14">(15)</ref> Eq. 11 is due to the reverse triangle inequality. Inequality 13 is valid because s z is multi-variate θ z -lipschitz continuous w.r.t. the norm . 2 (see below). In Eq. 14, we define θ = max ∀z=1..K θ z and recall that ∀i, j = 1, ..., K : W ij ≥ 0. Eq. 15 is due to K z=1 W ij = 1. √ 2γ 1 is the maximum . 2 distance between the two vectors.</p><p>In the previous proof, we made use of the notion of Multi-variate Lipschitz continuity. Definition 3 (Multi-variate Lipschitz continuity). A function f :</p><formula xml:id="formula_13">U 2 ⊂ R d × R d → R, with U a convex space, is said θ-lipschitz w.r.t. the norm . 2 if ∃ θ ∈ R, θ &gt; 0 that ∀x 1 , x 2 , x ′ 1 , x ′ 2 ∈ U : f (x 1 , x 2 ) − f (x ′ 1 , x ′ 2 ) 2 ≤ θ x 1 x 2 − x ′ 1 x ′ 2 2 .<label>(16)</label></formula><p>Roughly speaking, a function that is lipschitz continuous varies slightly within a certain interval. This property is fundamental for the robustness of our algorithm: the fact that the functions S = {s z (.)} K z=1 are θ z -lipschitz continuous implies that any linear combination of them returns similar values when evaluated on instances belonging to the same region of the partition. According to <ref type="bibr" target="#b26">[26]</ref>, the constant θ can be estimated considering the fact that</p><formula xml:id="formula_14">θ = max ∀x1,x2,x ′ 1 ,x ′ 2 ∈U f (x 1 , x 2 ) − f (x ′ 1 , x ′ 2 ) 2 x1 x2 − x ′ 1 x ′ 2 2 = = max ∀x1,x2∈U ∇f (x 1 , x 2 ) 2 .<label>(17)</label></formula><p>We can now derive the generalization bound of C2LM.</p><p>Lemma 2 As F P (W ) is (H, θ √ 2γ 1 + γ 2 )-robust and the training set P is obtained from n IID draws according to a multinomial random variable, for any δ &gt; 0 with probability at least 1 − δ, we have:</p><formula xml:id="formula_15">|R l −R l | ≤ θ √ 2γ1 + γ2 + B 2H ln 2 + 2 ln 1/δ n .<label>(18)</label></formula><p>Proof: See Supplementary Material.</p><p>It is worth noting that this bound tends to zero as the covering number H increases (γ 1 → 0 and γ 2 → 0) and the number of samples n → ∞. In the following subsections, we will instantiate s z (.) with two different metric functions: first as a Mahalanobis-like distance and then as a bilinear similarity. For both of them, we will need to prove their θ zlipschitz continuity and estimate their constant θ z as defined in Def. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Derivation for Mahalanobis-like Local Models</head><p>The Mahalanobis distance of a pair (x 1 , x 2 ) valued for a local model z can be written as s z (x 1 ,</p><formula xml:id="formula_16">x 2 ) = d Mz (x 1 , x 2 ) = (x 1 − x 2 ) T M z (x 1 − x 2 )</formula><p>with M z the corresponding (learned) PSD matrix. Thus, our objective function takes the following form:</p><formula xml:id="formula_17">F P (W ) = 1 n K i=1 i j=1 p∈Rij K z=1 W ijz d Mz (x 1 , x 2 ) − y(x 1 , x 2 ) + λ 1 D(W ) + λ 2 S(W )<label>(19)</label></formula><p>where M = {M 1 , .., M K } is a set of Mahalanobis metrics.</p><formula xml:id="formula_18">Lemma 3 ∀z = 1, ..., K the Mahalanobis distance d Mz (x 1 , x 2 ) is θ z -lipschitz w.r.t. the norm . 2 , with θ z = √ 2 L z 2 . Proof: See [26].</formula><p>Lemma 4 F P (W ) is (H, 2γ 1 L 2 + γ 2 )-robust and for any δ &gt; 0 with probability at least 1 − δ, we have:</p><formula xml:id="formula_19">|R l −R l | ≤ 2γ 1 L 2 + γ 2 + B 2H ln 2 + 2 ln 1/δ n .<label>(20)</label></formula><p>The constant L 2 corresponds to max ∀z=1..</p><formula xml:id="formula_20">K L z 2 so that θ = √ 2 L 2 , because θ z = √ 2 L z 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Derivation for Bilinear Similarity Local Models</head><p>The bilinear similarity of a pair (x 1 , x 2 ) can be written as s z (x 1 , x 2 ) = x T 1 M z x 2 . Thus, our problem becomes:</p><formula xml:id="formula_21">F P (W ) = 1 n K i=1 i j=1 p∈Rij K z=1 W ijz x T 1 M z x 2 − y(x 1 , x 2 ) + λ 1 D(W ) + λ 2 S(W )<label>(21)</label></formula><p>where M = {M 1 , .., M K } is a set of bilinear similarities.</p><p>Lemma 5 ∀z = 1, ..., K the bilinear similarity</p><formula xml:id="formula_22">s z (x 1 , x 2 ) = x T 1 M z x 2 is θ z -lipschitz w.r.t. the norm . 2 , with θ z = √ 2 M z 2 R. Proof: See [26].</formula><p>Lemma 6 F P (W ) is (H, 2γ 1 M 2 R)-robust and for any δ &gt; 0 with probability at least 1 − δ, we have:</p><formula xml:id="formula_23">|R l −R l | ≤ 2γ 1 M 2 R + γ 2 + B</formula><p>2H ln 2 + 2 ln 1/δ n</p><formula xml:id="formula_24">M 2 = max ∀z=1..K M z 2 so that θ = √ 2 M 2 R, because θ z = √ 2 M z 2 R.<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we aim at showing that C2LM is well suited to deal with both distance and similarity functions. Therefore, we empirically evaluate our method on two applications: first on the estimation of perceptual color distances and then on the estimation of semantic similarities between words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Applications and Datasets</head><p>Modeling perceptual color distances It is known that a human observer cannot distinguish all the shades corresponding to the different mixtures of light wavelengths. He is more sensitive to medium wavelengths (to green/yellow colors) than to short and large wavelengths of the visible spectrum. Moreover, human perception strongly depends on variations of visual conditions, such as brightness, luminance, background changes, and so on. The perceived difference between colors cannot be modeled using an additive color space as the RGB space, because the corresponding distance is not proportional to the Euclidean distance on that space. In the past, several perceptual color spaces have been proposed to better model the human color perception : CIELuv and CIELab (see <ref type="bibr" target="#b19">[19]</ref>) are two examples of such efforts to model uniform perceptual spaces. However, these spaces are still sensitive to some visual variations and can be used only under standard image acquisition conditions. This is because the camera configuration, such as white balance, demosaicing and gamma correction, have a huge impact on the final perception of the color distances. We claim that, by means of C2LM, we can model a perceptual color distance that is invariant to acquisition conditions. For our experiments, we use the dataset built by Perrot et al. <ref type="bibr" target="#b16">[17]</ref>. We have at our disposal 29580 color patches, expressed in their RGB coordinates and uniformly distributed in the RGB cube, and 41800 pairs of color patches, taken under several viewing conditions and with 4 different cameras, with their reference perceptual distance ∆E 00 . Such a target distance corresponds to the perceptual color distance and has been computed using the CIEDE2000 color-difference formula <ref type="bibr" target="#b18">[18]</ref> based on CIELab space. However, it is reliable only under standard viewing conditions (illuminant D65, illuminance of 1000 lx, etc. defined by the International Commission on Illumination CIE) so it cannot be used in all circumstances. Our proposal is to approximate the true perceptual distance between two colors no matter the viewing conditions. For this aim, the color patches are clustered using k-means (using the Euclidean distance on the RGB space) and on each so-found region a local model is learned as a Mahalanobis-like distance (using the color pairs whose patches both belong to that region). We then apply our method for learning linear combinations of those distance functions with manifold regularization, as detailed in section 3. We compare our method to <ref type="bibr" target="#b16">[17]</ref>, where the authors learn a set of Mahalanobis-like metrics independently from each other: they cluster the color patches using k-means and learn a local metric on each cluster and a global one with the color pairs whose patches belong to different clusters; they compute the distance between two colors using the local distance if they belong to the same cluster or the global distance if they do not. As <ref type="bibr" target="#b16">[17]</ref>, we evaluate our method on two different tasks (testing on unseen colors and on color pairs from unseen cameras).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling semantic similarities</head><p>The semantic similarity between words is defined as the measure of closeness in meaning between two terms. It is a measure defined by human perception and it cannot be expressed by exact rules. Nevertheless, it can be estimated by representing the words as vectors of a continuous space (word embedding) and computing their distance or similarity, for instance the Euclidean distance or the cosine similarity. We show how a word embedding can be enhanced using our method. As in the previous application, we learn a local model on each cluster of words (the clustering procedure accomplished using k-means with the Euclidean distance on the word embedding) and then we apply C2LM on the learned local models, which, in this case, are bilinear forms (see 4.3) computed independently using the following optimization problem:</p><formula xml:id="formula_25">arg min Bz 1 n p∈Rzz x T 1 Bzx2 − y(x1, x2) + Bz F .<label>(23)</label></formula><p>For our experiments, we extracted the word embedding from the Reuters News stories 2 text corpus using the Hellinger PCA as presented in <ref type="bibr" target="#b10">[11]</ref>. We then evaluate different methods on the WordSim353-similarity dataset: it is composed of 353 pairs of english words and for each pair we have at our disposal its semantic similarity as estimated by a human expert. We will compare our method with computing the cosine similarity directly on the embedding and with learning a set of local bilinear similarities and a global one. Because the cosine similarity is capable of predicting scores only in the interval [−1, 1] and the similarity scores of the dataset are between 0 and 10, we first normalized the target scores into the interval [−1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation and results</head><p>We implemented our algorithm using the Cvxpy library 3 and its SCS solver (see <ref type="bibr" target="#b15">[16]</ref>). For our experiments, we computed the best values for parameters λ 1 and λ 2 executing a grid search hyperparameter optimization by cross-validation: we fixed them to λ 1 = 0.01 and  For the application on unseen colors, we show the mean results of a 6-fold cross validation of the color patches set, iterated five times. In <ref type="figure" target="#fig_2">Figure 5a</ref>, we represent the variation of the test loss over the number of clusters. We notice that as the number of clusters increases the empirical test loss decreases: a set of local metrics captures much better the underlying geometry of the color space than a unique global metric (K = 1). Moreover, with a small number of clusters, the learned linear combinations are more expressive than the local metrics: thanks to the prior influence and similarity regularizations, we successfully prevent the model from overfitting the training instances. This trend is more and more important as the number of clusters grows. For the application on unseen cameras, <ref type="figure" target="#fig_2">Figure 5b</ref> shows the mean results of a 4-fold cross validation (leave one camera out) of the color pairs set, iterated 3 times. Once again, our method outperforms the state of the art. For both tasks, we can note that with a very limited number of clusters, that is only 5, our test loss is always smaller than every test loss the approach of <ref type="bibr" target="#b16">[17]</ref> could attain, even with 30 clusters. In addition, we use the learned color metrics to perform image segmentation and provide illustrations in the supplementary material.</p><p>Concerning the application on semantic similarity, Figure 5c presents the mean results of a 6-fold cross validation, iterated five times. We can note that learning metrics on the word embedding gives better results than applying directly the cosine similarity, but also that the local metrics fail to improve the test error with respect to a global bilinear form. On the contrary, C2LM converges with a limited number of clusters to an enhanced test error. We also notice that, against the trend, the test error increases when passing from one to two clusters. This can be explained by the fact that the quality of the local models is so poor that the learned convex combinations of them cannot be good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a new method for learning convex combinations of local models given a prior knowledge on their correlations. We proved that our learning algorithm is theoretically founded w.r.t. the algorithmic robustness framework. Empirically, our approach has better results than the state of the art to estimate perceptual color distances and semantic word similarities. So far, we assumed that the local models were provided. A possible perspective of this work is to jointly learn the local metrics and their linear combinations. The optimization problem would take the form of a double regression, one over the points belonging to the same region and one for all the others. In this way, we could guarantee that the local models perform well both locally and globally speaking by means of regularization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Similarity of a pair of regions: based on proximity, the vector W56 should be more similar to the vector W11 than to the vector W49.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Minimum Spanning Tree: the distance between two regions corresponds to the number of edges of the shortest path connecting them. E.g., dist(R5, R7) = 1, dist(R56, R4) = dist(R5, R4) + dist(R6, R4) = 4 and dist(R56, R49) = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of our method and local metric learning approaches, such as Perrot et al.'s method, for the application on perceptual color distances (5a and 5b) and for the application on word semantic similarities (5c). The used criterion is the loss over the test instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>λ 2</head><label>2</label><figDesc>= 10000 for the first application and to λ 1 = 0.0001 and λ 2 = 100 for the second one.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://alfonseca.org/eng/research/wordsim353. html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://about.reuters.com/researchandstandards/ corpus/ 3 cvxpy.readthedocs.org/en/latest/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>We wish to thank the reviewers for their valuable comments and suggestions contribute to improve the final version of the paper. We also thank the ANR projects SOL-STICE (ANR-13-BS02-01) and LIVES (ANR-15-CE23-0026-03).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple kernel learning, conic duality, and the smo algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robustness and Generalization for Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="267" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey on metric learning for feature vectors and structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.6709</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Morgan &amp; Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sur les fonctionnelles continues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fréchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales Scientifiques de L&apos;Ecole Normale Superieure</title>
		<imprint>
			<date type="published" when="1910" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="193" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A geometric take on metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2024" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anagnostopoulos. Reduced-rank local distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgiopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="224" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Metric learning: A survey. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="287" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Metric learning for temporal sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arlot</surname></persName>
		</author>
		<idno>De- cember 8-13</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1817" to="1825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5542</idno>
		<title level="m">Word emdeddings through hellinger pca</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="380" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parsimonious unsupervised and semi-supervised domain adaptation with good similarity functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Morvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="349" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Algorithmic robustness for semisupervised (ǫ, γ, τ )-good metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-I</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-R</forename><surname>Amini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6452</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative local metric learning for nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1822" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Conic optimization via operator splitting and homogeneous self-dual embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Odonoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Modeling perceptual color differences by local metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>In</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="96" to="111" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The ciede2000 colordifference formula: Implementation notes, supplementary test data, and mathematical observations. Color research and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Colour spaces: perceptual, historical and applicational background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tkalcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Tasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurocon</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="304" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weak convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Vaart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Weak Convergence and Empirical Processes</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="16" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parametric local metric learning for nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Woznica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1601" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Robustness and generalization. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="391" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01376</idno>
		<title level="m">Lipschitz continuity of mahalanobis distances and bilinear forms</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
