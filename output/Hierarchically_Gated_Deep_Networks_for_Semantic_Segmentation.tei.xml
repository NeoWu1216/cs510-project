<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchically Gated Deep Networks for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
							<email>guojun.qi@ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchically Gated Deep Networks for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation aims to parse the scene structure of images by annotating the labels to each pixel so that images can be segmented into different regions. While image structures usually have various scales, it is difficult to use a single scale to model the spatial contexts for all individual pixels. Multi-scale Convolutional Neural Networks (CNNs) and their variants have made striking success for modeling the global scene structure for an image. However, they are limited in labeling fine-grained local structures like pixels and patches, since spatial contexts might be blindly mixed up without appropriately customizing their scales. To address this challenge, we develop a novel paradigm of multiscale deep network to model spatial contexts surrounding different pixels at various scales. It builds multiple layers of memory cells, learning feature representations for individual pixels at their customized scales by hierarchically absorbing relevant spatial contexts via memory gates between layers. Such Hierarchically Gated Deep Networks (HGDNs) can customize a suitable scale for each pixel, thereby delivering better performance on labeling scene structures of various scales. We conduct the experiments on two datasets, and show competitive results compared with the other multi-scale deep networks on the semantic segmentation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1]</ref>i st os e gment images into different regions usually by assigning one of semantic labels to each pixel. It is a crucial step towards understanding image scene structures. The label of an image pixel cannot be determined by its local features extracted from a small sliding windows or neighborhood surrounding it. Rather, pixel labels are usually defined in spatial contexts, whose scales often have large variation in sizes. For example, sky and sea have a large scale of spatial context, but vessel and pedestrian are more localized to a relatively small scale of context. Moreover, even the re-gions of the same category may have various sizes of spatial contexts, making it impossible to fix a scale to model each pixel. This inspires us to develop a model that is capable of learning to customize spatial contexts and their scales for individual pixels in an image.</p><p>To model the spatial context of a pixel, a typical approach is to model the dependencies between the adjacent local image structures based on 2D Markov Random Fields <ref type="bibr" target="#b3">[ 4]</ref>[12] <ref type="bibr" target="#b12">[13]</ref> and Conditional Random Fields <ref type="bibr" target="#b6">[7]</ref>[21] <ref type="bibr" target="#b8">[9]</ref>. These models usually capture the local similarity between adjacent image structures of various scales, ranging from pixels, patches to regions. Then, the scene labeling is performed by maximizing the consistency between the similar neighbors which are considered as being in the same spatial context.</p><p>On the other hand, the success of deep learning framework on ImageNet challenge <ref type="bibr" target="#b10">[11]</ref> has inspired us to apply hierarchical neural networks to build the spatial context on various scales. Convolutional Neural Networks (C-NNs) <ref type="bibr" target="#b13">[14]</ref>, among all deep learning models, have shown their striking performances on modeling the image structures on different levels with multiple layered convolutional kernels.</p><p>The CNN models have been generalized to scene labeling. For example, multi-scale CNNs are proposed in <ref type="bibr" target="#b2">[3]</ref>, which produce and concatenate the feature maps of all scales. Usually, the learned features have to be postprocessed by up-sampling coarser-scale maps to match with finer-grained image pixels, and the global contextual coherence and spatial consistency are imposed by CRFs and segmentation trees. Long et al. <ref type="bibr" target="#b15">[16]</ref> propose an alternative paradigm of fully convolutional networks to annotate images pixel-wise. They present an approach to de-convolve coarser scale of output maps to label the image pixels. Unlike these two CNN-based models, we are interested in a hierarchical network, which not only explores multi-scale structures of input images, but also avoids producing coarse results that have to be up-sampled for labeling pixels. Such a model is highlighted with customized scale of the spatial context for each individual pixels so that the local structures can be modeled on a suitable scale.</p><p>Recently, Long Short Term Memory (LSTM) recurrent neural networks <ref type="bibr" target="#b7">[8]</ref> has been applied for scene labeling <ref type="bibr" target="#b0">[1]</ref>. The LSTM networks are originally used to model sequential data, such as sentences <ref type="bibr" target="#b18">[19]</ref> and videos <ref type="bibr" target="#b21">[22]</ref>. The networks are composed of a series of recurrently connected memory cells, which are able to capture long-range dependencies between different time frames. All the information entering and leaving memory cells are controlled by several types of gates, which ensures only the information relevant to the task would be maintained in their memory spaces.</p><p>Recently, Byeon et al. <ref type="bibr" target="#b0">[1]</ref> adapt the conventional LSTM networks to model 2D images along four directions -lefttop, left-bottom, right-top and right-bottom, and a hidden feedforward layer is built upon 2D LSTM layer to combine the LSTM cells at each image location. Several 2D LSTM layers and feedforward layers are interleaved to capture the spatial contexts along different directions. However, unlike the CNNs, each memory cell is not hierarchically connected with a receptive field of pixels, making it incapable of modeling various scales of spatial contexts like the CNNs.</p><p>In this paper, we present a novel paradigm of Hierarchically Gated Deep Networks (HGDNs) to address the challenge of customizing spatial contexts for different pixels at suitable scales. The proposed network combines the advantage of both the CNNs and the LSTMs. Like CNNs, the networks have multiple layers of memory cells that model a growing scale of image structures in bottom up fashion. However, unlike CNNs, between two layers, there are memory gates which control whether spatial contexts from the lower layer should be annexed to form a larger scale of spatial context at the higher layer. In this way, at each layer, a memory cell takes the customized spatial context at its location. Going up the HGDN layers, the spatial context of a pixel could gradually grow up to an arbitrary shape by merging the relevant neighbors until it reaches a suitable scale.</p><p>The HGDN model has an intrinsic hierarchical structure, making it adequate in modeling multiple scales of spatial contexts. Unlike the 2D LSTMs, the HGDN model does not have any horizontal connections between memory cells in one layer, and all the gates are deployed to control the flows of spatial patterns moving vertically across layers. In this way, the HGDN can accurately label pixels by avoiding the risk of blindly mixing up different scales of spatial patterns. The HGDN model inherits the ability of the CNNs modeling various scales of image details, as well as the advantage of the LSTMs on capturing the long-range spatial dependency between relevant pixels.</p><p>The remainder of this paper is organized as follows. In the following section, we review the existing work related to the proposed method. In Section 3, we formulate the problem of scene labeling that motivates our model. We present the proposed network architecture in Section 4, followed by a discussion on the implementation details in Section 6.W e present the experiment results in Section 7, and conclude the paper in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most of existing scene labeling approaches fall into one of three major categories.</p><p>The first category uses probabilistic graphical models to reveal the dependencies between adjacent pixels, patches or regions. Tighe and Lazebnik <ref type="bibr" target="#b19">[20]</ref> present a nonparametric approach to label superpixels by incorporating neighborhood context with efficient MRF optimization. On the contrary, Russell et al. <ref type="bibr" target="#b17">[18]</ref> point out that there is no common optimal level like pixels and segments for labeling images, which is suitable for all scene categories. Thus they propose to use a hierarchical CRF model to integrate the features computed at different levels.</p><p>To explore the multiple scales of image spaces, Convolutional Neural Networks and their variants become successful and scalable in many computer vision applications. Among them, as aforementioned in the last section, multi-scale CNNs <ref type="bibr" target="#b1">[2]</ref> and fully convolutional networks <ref type="bibr" target="#b15">[16]</ref> have been proposed for scene labeling tasks. Both produce coarse-scaled output layer which has to be up-sampled to a higher resolution for labeling individual pixels. Kekeç et al. <ref type="bibr" target="#b9">[10]</ref> propose to use two separate CNNs to combine the visual features and the contextual information for scene labeling.</p><p>Recently, LSTM recurrent neural networks have been applied to model the spatial contexts for labeling scenes <ref type="bibr" target="#b0">[1]</ref>. This work is based on multi-dimensional LSTMs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, which model N -dimensional data along different directions. Specifically, 2D LSTMs are unfolded horizontally in four different directions (left-top, left-bottom, right-top and right-bottom) to model images. However, unlike the CN-N models, multi-dimensional LSTMs do not model various scales of input images in a bottom-up fashion. This makes them inadequate in capturing flexible ranges of spatial contexts. Moreover, the 2D LSTMs work on pixel patches rather than the individual pixels. Thus, the labeling result on patches has to be interpolated to annotate individual pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem and Motivation</head><p>In this section, we discuss the proposed Hierarchically Gated Deep Network (HGDN) and its application on scene labelling. Suppose we have an image X = {x i,j |i = 1, ··· ,M,j =1 , ··· ,N} of size M × N , where x i,j represents the local features extracted for the pixel located at (i, j) of the image. Then the goal of scene labeling is to assign each pixel with a label y i,j from a finite set <ref type="figure">Figure 1</ref>: An one-dimensional example of the proposed Hierarchically Gated Deep Networks (HGDNs). The solid orange line represents the allowed information flows being gated between multiple successive layers. They eventually enter the highlighted node at the topmost layer.</p><formula xml:id="formula_0">C = {1, 2, ··· ,C}.</formula><p>The difficulty of labeling pixels lies in that we do not know a suitable scale of a pixel's spatial context containing a set of relevant pixels that form a local structure, such as object parts and scenery areas. Many existing methods attempt to explore the smoothness assumption over the labels assigned to the pixels in a local neighborhood. However, this assumption might not be able to explore the large variations in the scales of the spatial contexts. For example, some pixels have large scales of context like grass and sky; on the contrary, some pixels belong to an object part with a small scale of spatial context such as wheel and window. While multi-scale deep networks such as CNNs model various scales in a bottom-up fashion, however, they are inadequate in capturing location-variant scales surrounding pixels belonging to different scales of local structures. This inspires us to develop a model which can automatically handle flexible spatial contexts corresponding to different local structures.</p><p>To address this challenge, in this paper we propose a novel paradigm of multi-scale deep network, where a suitable scale of spatial context for each pixel is determined by gating varying sizes of neighborhood between two successive layers. It creates multi-layered gates to control the information flows between multiple successive layers. The gate only allows the information flows from the same spatial context to update the hidden memory state corresponding to each pixel at the higher layer. A suitable scale of the spatial context for each pixel gradually grows by annexing more and more relevant pixels bottom up through the network layers. <ref type="figure">Figure 1</ref> illustrates this idea with an one-dimensional example. The highlighted node at the topmost layer absorbs information flows from a spatial context of four nodes at the bottom layer. The solid arrows represent the allowed paths of information flows entering the highlighted node. In the <ref type="figure">Figure 2</ref>: An illustration of the network structure between a memory cell at an upper layer and its connected cells in a neighborhood of the lower layer. Each memory cell has an internal state s l i,j representing the spatial patterns extracted from a context, and it produces an output h l i,j fed into the higher layer. Between two connected cells, there is a gate g l−1 i,j;m,n (square nodes) controlling whether to allow information flows of lower-layer outputs to enter the memory cells at the higher layer. Only the information flows from the same spatial context should be allowed to pass through the gate.</p><p>next section, we will formalize this idea of gating information flows in the mathematical details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Proposed Approach</head><p>The proposed deep network has multiple layers of neurons which have the same size as the input image. This enables labeling each pixel directly at the output layer without having to up-sample the labels. Each layer is composed of a two-dimensional grid of memory cells indexed by (i, j) for i =1 , ··· ,M and j =1 , ··· ,N, each corresponding to a pixel of input image.</p><p>As illustrated in <ref type="figure">Figure 2</ref>, between two layers, each memory cell (i, j) at an upper layer l is connected to a neighborhood N l−1 i,j of memory cells around the same location (i, j) at the lower layer l − 1. Each memory cell has an internal state s l i,j , which memorizes the patterns obtained from the spatial contexts up to the level l. Meanwhile, the memory cell produces the output h l i,j , which is fed into the upper layer. All outputs are gated so that only the relevant patterns are allowed to update the memory cells at the upper layer.</p><p>In the following, we will explain different components of the proposed network architecture in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Gates</head><p>First, we define an input gate to control the information flow between a memory cell (i, j) and its connected cells (m, n) ∈N l−1 i,j at the lower layer. The role of this gate is to filter out those irrelevant patterns to the pixel labels. Specifically, for any connected cell (m, n) ∈N l−1 i,j \ {(i, j)} in the neighborhood of (i, j), we have an input gate defined as</p><formula xml:id="formula_1">g l−1 i,j;m,n = σ(U (i−m,j−n) g s l−1 i,j + W (i−m,j−n) g h l−1 m,n + b g ) (1) where σ(·) is the sigmoid activation over the range of [0, 1]; U (i−m,j−n) g and W (i−m,j−n) g</formula><p>are transformation matrices from the state and input to the gate respectively, and b g is the bias vector 1 . These parameters are functions of the relative disposition (i − m, j − n) between (i, j) at the center and (m, n) in the neighborhood N l−1 i,j \{(i, j)}. So these parameters are recurrent across different neighborhoods at the same layer, which can save a huge amount of parameters.</p><p>The gate defines a spatial context in the neighborhood of (i, j) -its value decides to what extent the output from a memory cell (m, n) of a lower layer is contextually connected to a memory cell at (i, j) of the upper layer. It is a multiplicative gate, where a large value of the gate implies the output h l−1 m,n from (m, n) can pass through it to update the memory state s l i,j . In this sense, the gate can be viewed as an indicator of whether (m, n) belongs to the spatial context of (i, j).</p><p>It is worth noting that g l−1 i,j;m,n is not a symmetric function of two locations (i, j) and (m, n). In other words, the assertion that (m, n) belongs to the spatial context of (i, j) does not necessarily imply (i, j) also belongs to the spatial context of (m, n). For example, for labeling a ship, the spatial context might include surrounding pixels of sea as they provide useful clues to label ship. However, on the converse, it is not necessary to include ship as the spatial context when labeling sea. This gives more flexibility to model the spatial contexts for different pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Cell States</head><p>With the input gate defined above, the state s l i,j of a memory cell can be updated by combining all the outputs from the connected memory cells at the lower layer through the relevant spatial context defined by the gates of g l−1 i,j;m,n</p><formula xml:id="formula_2">s l i,j = s l−1 i,j + (m,n)∈N l−1 i,j \{(i,j)} {g l−1 i,j;m,n ⊙ i l−1 i,j;m,n }<label>(2)</label></formula><p>where ⊙ is the element-wise multiplication, and i l−1 i,j;m,n is the input modulating h l−1 m,n entering the memory cell (i, j):</p><formula xml:id="formula_3">i l−1 i,j;m,n = tanh(W (i−m,j−n) s h l−1 m,n + b s )<label>(3)</label></formula><p>where the transformation matrix and bias vector W cation of (i − m, j − n), and the hyperbolic tangent tanh(·) is the activation function for this modulated input.</p><p>Note that, the first term of the RHS of Eq. (2) suggests that the memory state s l−1 i,j from the lower layer does not need to be gated before entering the memory cell at the same location. This is based on the assumption that each memory cell must belong to its own spatial context at this same location. Thus, it allows the memory state to go straight up to label the pixel at that location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Cell Outputs</head><p>Once the state of a memory cell is updated, it is ready to produce the following output fed into the higher layer.</p><formula xml:id="formula_4">h l i,j = tanh(W h s l i,j + b h )</formula><p>where W h and b h are the parameters for the memory cell output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topmost Output Layer</head><p>At the top of the network, we have an output layer where a softmax function is applied at each location (i, j) to classify the corresponding pixel to one of C labels:</p><formula xml:id="formula_5">Pr(y ij = c|h L i,j )= exp((w c * h L ) i,j + b c ) exp( C c=1 (w c * h L ) i,j + b c )</formula><p>where * denotes the convolution between a kernel w c of label c and the hidden output h L from the last memory cell layer L, and b c is bias for each label c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training the Deep Network</head><p>The training of the proposed deep network can be performed by back-propagating the errors up down through the whole network.</p><p>In particular, given a training image X = {x i,j } with the pixel labels Y = {y i,j }, the loss function can be defined as the negative log likelihood of softmax outputs</p><formula xml:id="formula_6">(Θ)= M,N i,j=1 − log Pr(y i,j |h L i,j )</formula><p>where Θ contains all model parameters.</p><p>It is worth noting that this loss function is separable between the pixel labels at different locations. Thus we can parallelize the calculation of the loss derivatives across different locations for an input image. This can speed up the back-propagation procedure on GPU with many cores.</p><p>Also, when we back-propagate the errors down the network, we will truncate those errors leaving a memory cell down to the lower layer, except the errors back-propagated along the states of the memory cells at the same location. This truncated back-propagation algorithm has worked well on training Long Short-Term Memory (LSTM) machine <ref type="bibr" target="#b7">[8]</ref> to prevent the vanishing or exploding errors.</p><p>It is also worth noting that all the model parameters W * and b * are recurrent over each layer. Like the convolutional kernels used in Convolutional Neural Networks (CNNs), this recurrent structure significantly reduces the size of the parameters that have to be estimated. This makes it possible to have the whole network fit in the GPU's device memory, without frequent swap of the network parameters between the host and device memory spaces. On the Nvidea Tesla K40 GPUs we used for the experiment, the model can fit into the device memory. This further speeds up the training of the proposed HGDN on the GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Further Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">GPU Implementation</head><p>Most computations of the model arise from the calculation of multiple layered memory cells and their gates. They can be accelerated by the GPUs. For example, each term in the argument of the memory gate function <ref type="formula">(1)</ref> can be parallelized on GPGPU. The first term can be vectorized by copying s l i,j to tile over the neighborhood and applying the matrix multiplication with U g simultaneously; the second term can also be parallelized over the whole neighborhood, which can be conducted on GPUs very fast.</p><p>Then, in calculating the memory state in Eq. (2), the summation between the gate function g l−1 i,j;m,n and the modulated input i l−1 i,j;m,n can be considered as a variant of convolution 2 between these two terms. These multiplication and summation operations can be parallelized together with the computations of gate function (1) over each neighborhood in the similar way as the standard convolution. In our implementation on Nvidia Tesla K40 GPUs, an image of size 256 × 256 can be processed within 0.03 seconds on average, including both feed-forward pixel labeling for prediction and backward error propagation for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">How many layers should be added?</head><p>One of interesting questions is how many layers we should build for labeling an input image through the HGDN. Clearly, the answer depends on the size of the image, as well as the size of neighborhood.</p><p>Each memory cell at a particular layer has a maximal size of spatial context over the input image, if we assume all the gates are completely open. For example, in <ref type="figure">Figure 1</ref>, the highlighted memory cell at the top layer can cover a maximal number of five nodes in its spatial context at the input layer. But the actual scale of the spatial context for a memory cell can be much smaller than its possible maximal scale when not all gates on its path to input layers are open, as shown in <ref type="figure">Figure 1</ref> where the highlighted cell only covers four rather than five nodes of input layer in its spatial context. As more layers are added, assuming all gates are open, the maximal scale of a spatial context will gradually grow until it covers the whole input image. How fast the scale of spatial context will grow will depend on the size of neighborhood -the larger the neighborhood, the more cells from the lower layer may be involved, and thus a spatial context can grow faster. Then the maximum number of layers required for labeling pixels can be defined as the one when the whole input image is covered by the memory cells at the output layer.</p><p>However, in practice, we only need spatial contexts to cover a relatively small part of an input image rather than the entire image, and thus the actual number of layers is often much smaller than the theoretical maximum number. For example, on the datasets we used in the experiments, we find with neighborhood sizes ranging from 11 × 11 to 5 × 5 pixels, five hidden layers (excluding input and output layers) should be enough to give a satisfactory performance on labeling pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>In this section, we present our experiment results on scene labeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Datasets</head><p>We test the proposed HGDN on two datasets -Stanford Background dataset <ref type="bibr" target="#b3">[4]</ref> and the SIFT Flow dataset <ref type="bibr" target="#b14">[15]</ref>. Both datasets have been fully labeled on individual pixel level.</p><p>The Stanford background dataset contains 715 images, which are annotated with 8 scene labels. The dataset is split into a training set of 572 images, and a test set of 143 images. It has a special foreground label, which denotes unknown objects. Each individual image has a different resolution, with an average size of 320 × 240 pixels.</p><p>On the other hand, SIFT Flow dataset has 2, 488 images, each having a fixed resolution of 256 × 256 pixels. The dataset is split into a training set of 2, 488 images and a test of the rest 200 images. It contains 33 labels annotated by LabelMe users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Network Architecture</head><p>The HGDN model used in the experiment has seven layers, including one input layer, five hidden layers and one output layer. As shown in <ref type="table" target="#tab_0">Table 1</ref>, five hidden layers are created to hierarchically gate the information flows originated from the input images. At the first hidden layer, convolutional filters of size 11 × 11 are applied to input image to initialize the states of memory cells at all locations. In other words, each memory cell at the first hidden layer is connected to a neighborhood (receptive field) of 11 × 11 pixels of the input image. From the second hidden layer, each memory cell receives the gated inputs from a varying-sized neighborhood at the lower layer. The memory cells at the second hidden layer are connected to a neighborhood of 9 × 9 memory cells of the lower layer; from the third hidden layer, each memory cell is connected to a neighborhood of 5 × 5 memory cells from the lower layer. Finally, a convolutional kernel of size 5 × 5 is applied to the last memory cell layer, which generates the pixel labels at the topmost output layer.</p><p>All hidden layers have the same number of states for each memory cell, which ensures the representation of spatial contexts reside in the same state space across layers. A rule of thumb to choose the number of memory states is to use a multiple of 16 closest to four times of scene labels -on Stanford background dataset, we use 32 states for each memory cell, while 128 states are used for SIFT Flow dataset. Choosing the multiple of 16 for the number of memory states increases the computing performance on GPUs thanks to the data alignment. We find this rule works well for both datasets. Also, each layer is padded with the zeros to ensure all the layer to have a full size of resolution after being processed. All the layers of memory cells, from the input to the output layer, have the same size and thus we do not need to up-sample the output layer to label every pixel.</p><p>We adopt stochastic gradient method to train the model. All the model parameters are initialized with a zero-mean Gaussian distribution with a standard deviation of 0.1. The learning rate is fixed to 0.001, with a momentum of 0.9. With the GPU implementation, each epoch can be finished within less than 1.5 seconds with a batch of 128 training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Evaluation Metrics</head><p>The evaluation of scene labeling task is usually performed based on two metrics. The first is the pixel-wise accuracy, which measures the ratio of pixel-wise true positives over all the pixels. The second is the average label accuracy, which computes the average of label accuracies over all labels. The latter metric is more challenging because on both datasets, different labels are imbalanced, where some labels (e.g., sky) cover a much larger number of pixels than the other labels (e.g., pedestrian). To make a fair comparison with the other algorithms, we use all the pixel labels in their natural frequencies and do not balance them to train the model. We report both metrics when comparing with different algorithms. <ref type="table" target="#tab_1">Table 2a and Table 2b</ref> report the pixel-wise accuracy and average label accuracy on Stanford Background dataset and SIFT Flow dataset respectively. We also compare the average computing time to process each image by different algorithms. Our implementation is very fast in processing images as we parallelize the computation across neighborhoods in each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Results</head><p>We compare with both CNN and LSTM paradigms of state-of-the-art methods. The results show that the proposed HGDN model achieves very competitive accuracy on both datasets, outperforming both CNN and LSTM paradigms when no extra training data are involved in pretraining the model. Note that the 2D LSTM model in <ref type="bibr" target="#b0">[1]</ref> divided images into patches as inputs and has reported the performances with varying sizes of input patches. In the table, we report the best result achieved by the optimal size of input patch on two datasets. <ref type="figure">Figure 3</ref> illustrates the output maps of scene labels on some image examples. The brightness of output maps represents the activations of a pixel in respondence to different scene categories. It reflects the probability of each pixel belonging to those scene categories. The results are obtained from the last HGDN output layer. We can see that different scene categories occupy various scales of spatial contexts -for example, the sky and road usually span a larger scale of context, while the tree is often restricted to relatively s- <ref type="table" target="#tab_1">Table 2</ref>: Comparison of pixel-wise accuracy and label average accuracy on (a) Stanford Background and (b) SIFT FLOW datasets. We also report the average computing time to process each image by different algorithms. The proposed HGDN is implemented on Tesla K40 GPU, which reaches very fast computing speed. Not all compared models are implemented on the GPUs, and we denote them in the parenthesis after the reported computing time. Some results by the compared algorithms are missing in the literature, where we denote with N/A. We do not balance the label frequencies to improve the label average accuracy. † It is noted that the FCN uses extra ILSVRC data to pretrain a reference model before it is fine-tuned on the SIFT Flow dataset, whereas no extra data are involved to pretrain the HGDN. maller scale. Even for the same scene category, its scale of spatial context can vary a lot between different images. This confirms the necessity to customize the spatial context to various scales.</p><p>We also illustrate the feature maps of 32 memory states at 2-5 hidden layers in <ref type="figure">Figure 4</ref>. It shows that how image structures of various scales are captured through these layers -at the lower layers, small structures are captured, while at the higher layers, complex structures are modeled by gradually annexing the small structures from the lower layers. It is also worth noting that the activations of these 32 memory states are much sparse -some of memory states produce zero or saturated activations. This makes sense since not every memory state need to respond to all scene labels. Actually, the sparseness decouples the activations between different memory states, which can reduce the over-fitting risk.</p><p>Also, to verify the importance of gating the scales of spatial contexts across layers, we test a compared HGDN model by removing the multi-layered gates between layers.</p><p>We found that the pixel-wise accuracy on the two datasets would drop to 63.84% and 56.75% respectively, and the average label accuracy would decline to 65.92% and 43.22%. This shows that without these gates, different scales of the spatial contexts might be mixed up, leading to bad performance on labeling pixels. This result justifies the necessity of adding gates to control the spatial scales.</p><p>Finally, it is worth noting that our pixel labeling result is the direct output from the HGDNs, without any postprocessing procedure like label smoothing or interpolation. Although we have found that postprocessing the labeling result via CRFs, over-segmented patches, and segmentation trees can further improve the performance, we do not report the post-processing results to ensure fair comparison with the other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we develop a novel paradigm of deep network which explores various scales of spatial contexts adjusted to pixels at different locations. Compared with the other multi-scale deep network, the proposed model constructs multiple layers of memory cells, whose outputs are hierarchically gated on different scales before recursively feeding to higher layers. Then the pixel labels at differen-t locations are decided based on the spatial contexts of the customized scales. The experiment results show that its architecture is more adequate in modeling the scene structures on different scales than the other compared deep networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>i−m,j−n) s and b s are functions of the relative lo-1 These parameters should differ between different layers. However, for notational simplicity, we do not explicitly distinguish them for different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Illustration of output maps of scene labels on Stanford Background dataset, where the brightness represents the activations of a pixel in respondence to different categories. The results are obtained from the last HGDN output layer. Illustration of the learned feature maps from the top four layers of memory cells (Layer 2-5). The input image corresponds to the first one inFigure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Proposed HGDN architecture. Except the number of memory states, the same HGDN architecture is applied to both datasets. This table shows the neighborhood size of memory cells for each hidden layer. Among them, the first hidden layer initializes the memory cell state at each location by applying a convolution kernel of size 11 × 11 to input images. From the second hidden layer through the fifth hidden layer, varying sizes of neighborhoods are applied to connect the memory cells between two layers. Fi-</figDesc><table>Layer 
Conv. Kernel Size 
No. of output labels 
Stanford SIFT Flow 

Output 
5 × 5 
8 
33 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>( a )</head><label>a</label><figDesc>Stanford Background Algorithm Pixel-wise Accu. Label Avg Accu. Comp. Time (sec.) Algorithm Pixel-wise Accu. Label Avg Accu. Comp. Time (sec.)</figDesc><table>Superparsing 2010 [20] 
77.5 
N/A 
10 to 300 
Singlescale ConvNet 2013 [2] 
66 
56.5 
0.35 (GPU) 
Multiscale net [2] 
78.8 
72.4 
0.6 (GPU) 
Multiscale net + superpixels [2] 
80.4 
74.56 
0.7 (CPU) 
Multiscale net + gPb + cover [2] 
80.4 
75.24 
61 (CPU) 
Multiscale net + CRF on gPb [2] 
81.4 
76.0 
60.5 
Augmented CNNs 2014 [10] 
71.97 
66.16 
N/A 
Recurrent CNNs [17] 
76.2 
67.2 
1.1 (GPU) 
2D LSTM networks [1] 
78.56 
68.26 
1.3 (CPU) 
Proposed HGDN 
82.41 
72.98 
0.02 (GPU) 

(b) SIFT Flow dataset 

Multi-scale net (balanced frequency) [2] 
72.3 
50.8 
N/A 
Multi-scale net (natural frequency) [2] 
78.5 
29.6 
N/A 
Augmented CNNs 2014 [10] 
49.39 
44.54 
N/A 
Recurrent CNNs [17] 
65.5 
20.8 
N/A 
FCN  † [16] 
85.2 
51.7 
0.175 (GPUs) 
2D LSTM networks [1] 
70.11 
22.59 
1.2-3.1 (CPU) 
Proposed HGDN 
79.68 
51.26 
0.03 (GPU) 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here the convolutional kernel g is location-variant unlike the standard convolutional kernel that is location invariant. However, the locationvariant convolution makes it possible to customize unique scale of spatial context for each pixel at different locations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">nally, the topmost output layer generates the pixel labels by applying a convolution computation to the last memory cell layer (i.e., the first layer) with a kernel of size 5 × 5.(a) Hidden Layers Layer Neighborhood Size</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32">256 (b) Output Layer</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Netwroks ICANN 2007</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="519" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Á</forename><surname>Carreira-Perpiñán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">695</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hierarchical conditional random field model for labeling and segmenting images of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1953" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Contextually constrained deep networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kekeç</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trémeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficiently selecting regions for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining appearance models and markov random fields for category level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>1995. 1</idno>
		<imprint>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>arX- iv:1411.4038</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Associative hierarchical crfs for object class image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene segmentation with crfs learned from partially labeled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1553" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
