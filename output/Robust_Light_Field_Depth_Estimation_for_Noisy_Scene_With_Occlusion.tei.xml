<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Light Field Depth Estimation for Noisy Scene with Occlusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williem</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Information and Communication Engineering</orgName>
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">Kyu</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Information and Communication Engineering</orgName>
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Light Field Depth Estimation for Noisy Scene with Occlusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Light field depth estimation is an essential part of many light field applications. Numerous algorithms have been developed using various light field characteristics. However, conventional methods fail when handling noisy scene with occlusion. To remedy this problem, we present a light field depth estimation method which is more robust to occlusion and less sensitive to noise. Novel data costs using angular entropy metric and adaptive defocus response are introduced. Integration of both data costs improves the occlusion and noise invariant capability significantly. Cost volume filtering and graph cut optimization are utilized to improve the accuracy of the depth map. Experimental results confirm that the proposed method is robust and achieves high quality depth maps in various scenes. The proposed method outperforms the state-of-the-art light field depth estimation methods in qualitative and quantitative evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>4D light field camera has become a potential technology in image acquisition due to its rich information captured at once. It does not capture the accumulated intensity of a pixel but captures the intensity for each light direction. Commercial light field cameras, such as Lytro <ref type="bibr" target="#b15">[16]</ref> and Raytrix <ref type="bibr" target="#b17">[18]</ref>, trigger the consumer and researcher interests on light field because of its practicability compared to the conventional light field camera arrays <ref type="bibr" target="#b25">[26]</ref>. A light field image allows wider application to explore than a conventional 2D image. Various applications have been presented in the recent literatures, such as refocusing <ref type="bibr" target="#b16">[17]</ref>, depth estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, saliency detection <ref type="bibr" target="#b13">[14]</ref>, matting <ref type="bibr" target="#b4">[5]</ref>, calibration <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, editing <ref type="bibr" target="#b9">[10]</ref>, etc.</p><p>Depth estimation from a light field image has become a challenging and active problem for the last few years. Many researchers utilize various characteristics of light field (e.g. epipolar plane image, angular patch, and focal stack) to develop the algorithms. However, the state-of-theart techniques mostly fail on occlusion because it breaks the photo consistency assumption. Chen et al. <ref type="bibr" target="#b3">[4]</ref> introduced a Jeon's data cost <ref type="bibr" target="#b10">[11]</ref>; (c) Chen's data cost <ref type="bibr" target="#b3">[4]</ref>.</p><p>method that is robust to occlusion but their method is sensitive to noise. Wang et al. <ref type="bibr" target="#b21">[22]</ref> proposed an occlusion-aware depth estimation method but it is limited to a single occluder and highly depends on the edge detection result. It remains difficult for a depth estimation method to perform well on real data because of the occlusion and noise presence. Note that recent works mostly evaluate the results after the global optimization method is applied. Thus, the discrimination power of each data cost is not evaluated deeply since the final results depend on the individual optimization method.</p><p>In this paper, we introduce novel data costs based on our observation on the light field. Following the idea of <ref type="bibr" target="#b18">[19]</ref>, we utilize two different cues: correspondence and defocus cues. An angular entropy metric is proposed as the correspondence cue, which measures the pixel color randomness of the angular patch quantitatively. Adaptive defocus response is the modified version of the conventional defocus response <ref type="bibr" target="#b19">[20]</ref> that is robust to occlusion. We perform cost volume filtering and graph cut for optimization. An extensive comparison between the proposed and the conventional data costs is done to measure the discrimination power of each data cost. In addition, to evaluate the proposed method in a fair manner, we optimize the state-of-the-art data costs with the identical method. As seen in <ref type="figure" target="#fig_0">Figure 1</ref>, the proposed method achieves more accurate results in challenging scenes (with both occlusion and noise). Experimental results show that the proposed data costs significantly outperform the conventional approaches. The contribution of this paper is summarized as follows.</p><p>-Keen observation on the light field angular patch and the refocus image.</p><p>-Novel angular entropy metric and adaptive defocus response for occlusion and noise invariant light field depth estimation.</p><p>-Intensive evaluation of the existing cost functions for light field depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Depth estimation using light field images has been investigated for last a few years. Wanner and Goldluecke <ref type="bibr" target="#b22">[23]</ref> measured the local line orientation in the epipolar plane image (EPI) to estimate the depth. They utilized the structure tensor to calculate the orientation with its reliability and introduced the variational method to optimize the depth information. However, their method was not robust because of the dependency on the angular line. Tao et al. <ref type="bibr" target="#b18">[19]</ref> combined correspondence and defocus cues to obtain accurate depth. They utilized the variance in the angular patch as the correspondence data cost and the sharpness value in the generated refocus image as the defocus data cost. It was extended by Tao et al. <ref type="bibr" target="#b19">[20]</ref> by adding a shading constraint as the regularization term and by modifying the original correspondence and defocus measure. Instead of variance based correspondence data cost, they employed the standard multi-view stereo data costs (sum of absolute differences). In addition, the defocus data cost was designed as the average of intensity difference between patches in the refocus and center pinhole images. Jeon et al. <ref type="bibr" target="#b10">[11]</ref> proposed the method based on the phase shift theorem to deal with narrow baseline multi-view images. They utilized both the sum of absolute differences and gradient differences as the data costs. Although those methods could obtain accurate depth information, they would fail in the presence of occlusion.</p><p>Chen et al. <ref type="bibr" target="#b3">[4]</ref> adopted the bilateral consistency metric on the angular patch as the data cost. It was shown that the data cost was robust to handle occlusion but it is sensitive to noise. Recently, Wang et al. <ref type="bibr" target="#b21">[22]</ref> assumed that the edge orientation in angular and spatial patches were invariant. They separated the angular patch into two regions based on the edge orientation and utilized conventional correspondence and defocus data costs on each region to find the minimum cost. In addition, occlusion aware regularization term was introduced in <ref type="bibr" target="#b21">[22]</ref>. However, their method is limited to a single large occluder in an angular patch and the performance is affected by how well the angular patch is divided. Lin et al. <ref type="bibr" target="#b14">[15]</ref> analyzed the color symmetry in light field focal stack. Their work introduced the novel infocus and consistency measure that were integrated with traditional depth estimation data costs. However, there was no extensive comparison for each data cost independently without global optimization.</p><p>Several works in multi-view stereo matching have already addressed the occlusion problem. Kolmogorov and Zabih <ref type="bibr" target="#b12">[13]</ref> utilized the visibility constraint to model the occlusion which was optimized by graph cut. Instead of adding new term, Wei and Quan <ref type="bibr" target="#b24">[25]</ref> handled the occlusion cost in the smoothness term. Bleyer et al. <ref type="bibr" target="#b0">[1]</ref> proposed a soft segmentation method to apply the occlusion model in <ref type="bibr" target="#b24">[25]</ref>. Those methods observed the visibility of a pixel in corresponding images to design the occlusion cost. However, it remains difficult to address the method in a huge number of views, such as light field. Kang et al. <ref type="bibr" target="#b11">[12]</ref> utilized a shiftable windows to refine the data cost in occluded pixels. The method could be applied for the conventional defocus cost <ref type="bibr" target="#b19">[20]</ref> but it might have ambiguity between occluder and occluded pixels. Vaish et al. <ref type="bibr" target="#b20">[21]</ref> proposed the binned entropy data cost to reconstruct occluded surface. They measured the entropy value of a binned 3D color histogram that could lead to incorrect depth estimation, especially in smooth surfaces.</p><p>In this paper, we propose a novel depth estimation algorithm that is robust to occlusion by modelling the occlusion in the data costs directly. None of visibility constraint or edge orientation is required in the proposed data costs. In addition, the data costs are less sensitive to noise compared to the conventional ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Light Field Depth Estimation for Noisy</head><p>Scene with Occlusion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Light Field Images</head><p>We observe new characteristics from light field images which are useful for designing the data cost. To measure the data cost for each depth candidate, we need to generate the angular patch for each pixel and the refocus image. Thus, each pixel in light field L(x, y, u, v) is remapped to sheared light field image L α (x, y, u, v) based on the depth label candidate α as follows. <ref type="bibr" target="#b1">(2)</ref> where (x, y) and (u, v) are the spatial and angular coordinates, respectively. The center pinhole image position is denoted as (u c , v c ). ∇ x and ∇ y are the shift value in x and y direction with the unit disparity label k. The shift value increases as the distance between light field subaperture image and the center pinhole image increases. We can generate an angular patch for each pixel (x, y) by extracting the pixels in the angular images from the sheared light field. Refocus imageL α is generated by averaging the angular patch for all pixels.</p><formula xml:id="formula_0">Lα(x, y, u, v) = L(x + ∇x(u, α), y + ∇y(v, α), u, v) (1) ∇x(u, α) = (u − uc)αk ; ∇y(v, α) = (v − vc)αk</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Light Field Stereo Matching</head><p>The proposed light field depth estimation is modeled on MAP-MRF framework <ref type="bibr" target="#b2">[3]</ref> as follows.</p><formula xml:id="formula_1">E = ∑ p E unary (p, α(p))+ λ ∑ p ∑ q∈N (p) E binary (p, q, α(p), α(q))<label>(3)</label></formula><p>where α(p) and N (p) are the depth label and the neighborhood pixels of pixel p, respectively. E unary (p, α(p)) is the data cost that measures how proper the label α of a given pixel p is. E binary (p, q, α(p), α(q)) is the smoothness cost that forces the consistency between neighborhood pixels. λ is the weighting factor.</p><p>We propose two novel data costs for correspondence and defocus cues. For the correspondence response C(p, α(p)), we measure the pixel color randomness in the angular patch by calculating the angular entropy metric. Then, we calculate the adaptive defocus response D(p, α(p)) to obtain robust performance in the presence of occlusion. Each data cost is normalized and integrated to become a final data cost. The final data and smoothness costs are defined as follows.</p><formula xml:id="formula_2">E unary (p, α(p)) = C(p, α(p)) + D(p, α(p)) (4) E binary (p, q, α(p), α(q)) = ∇I(p, q) min(|α(p) − α(q)|, τ )<label>(5)</label></formula><p>where ∇I(p, q) is the intensity difference between pixel p and q. τ is the threshold value. Every slice in the final data cost volume is filtered with edge-preserving filter <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Then, we perform graph cut to optimize the energy function <ref type="bibr" target="#b2">[3]</ref>. The detail of each data cost is described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Angular Entropy</head><p>Conventional correspondence data costs are designed to measure the similarity between pixels in the angular patch, but without considering the occlusion. When an occluder affects the angular patch, the photo consistency assumption is not satisfied for the pixels in the angular patch. However, a majority of pixels are still photo consistent. Therefore, we design a novel occlusion-aware correspondence data cost to capture this property by utilizing the intensity probability of the dominant pixels.</p><p>The first column in <ref type="figure" target="#fig_1">Figure 2</ref> shows the angular patch of a pixel and its intensity histograms for several depth candidates. Without occlusion, the angular patch with the correct depth value (α = 21) has uniform color and the intensity histogram has sharper and higher peaks as shown in <ref type="figure" target="#fig_1">Figure 2(b)</ref>. Based on the observation, we measure the entropy in the angular patch, which is called angular entropy metric, to evaluate the randomness of photo consistency. Since light field has much more views than the conventional multi-view stereo setup, the angular patch has enough pixels to compute the entropy reliably. The angular entropy metric H is formulated as follows.</p><formula xml:id="formula_3">H(p, α) = − ∑ i h(i) log(h(i))<label>(6)</label></formula><p>where h(i) is the probability of intensity i in the angular patch A(p, α). In our approach, the entropy metric is computed for each color channel independently. To integrate the costs from three channels, we cooperate two methods, max pooling C max and averaging C avg , which are formulated as follows.  <ref type="figure" target="#fig_1">Figure 2;</ref> (b) Occluded pixel in the second column of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><formula xml:id="formula_4">C avg (p, α) = H R (p, α) + H G (p, α) + H B (p, α) 3<label>(8)</label></formula><p>where {R, G, B} denotes the color channels. The max pooling C max achieves better result when there is an object with a dominant color channel (e.g. red object has high intensity in the red channel and approximately zero intensity in the green and blue channels). Otherwise, the averaging C avg performs better. To deal with various imaging conditions, the final data cost C(p, α) is designed as follows.</p><formula xml:id="formula_5">C(p, α) = βC max (p, α) + (1 − β)C avg (p, α) (9)</formula><p>where β ∈ [0 ∼ 1] is the weight parameter. <ref type="figure" target="#fig_2">Figure 3</ref> (a) shows the comparison of the data cost curves for the angular patch in the first column of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The angular entropy metric is also robust to the occlusion because it relies on the intensity probability of the dominant pixels. As long as the non-occluded pixels prevail in the angular patch, the metric gives low response. The second column in <ref type="figure" target="#fig_1">Figure 2</ref> shows the angular patches when the occluders exist. Note that the proposed data cost yields the minimum response although there are multi-occluders in the angular patch. <ref type="figure" target="#fig_2">Figure 3</ref> (b) shows the comparison of the data cost curves of the proposed angular entropy metric and the state-of-theart correspondence data costs. It is shown that the proposed data cost achieves the minimum cost together with Chen's bilateral data cost <ref type="bibr" target="#b3">[4]</ref>. However, Chen's data cost is highly sensitive to noise. We compare both data costs on noisy data, which is shown in <ref type="figure" target="#fig_3">Figure 4</ref> that Chen's data cost does not produce any meaningful disparity. On the contrary, the angular metric produces fairly promising disparity map on the noisy occluded regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Adaptive Defocus Response</head><p>Conventional defocus responses for the light field depth estimation are robust to noisy scene but fail on the occluded region <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. To solve the problem, we propose the adaptive defocus response that is robust to not only noise but also occlusion. We observe that the blurry artifact from the occluder in the refocus image causes the ambiguity in the conventional data costs. <ref type="figure" target="#fig_5">Figure 5</ref> (a) and (c)∼(f) show the spatial patches in the center pinhole image and refocus images, respectively. Conventional defocus data costs fail to produce optimal response on the patches. We compute the difference maps between the patches in the center image and refocus images to show clearer observation, as exemplified in <ref type="figure" target="#fig_5">Figure 5</ref> (g)∼(j). It is shown that the large patch in non-ground truth label (α = 35) obtains smaller difference than the ground truth (α = 21). Based on the difference map observation, the idea of the adaptive defocus response is developed to find the minimum response among the neighborhood regions. Instead of measuring the response in the whole region (15 × 15) which is affected by the blurry artifact, we look for a subregion without blur, i.e. a subregion which is not affected by the occluder. To find the clear subregion, the original patch (15 × 15) is divided into 9 subpatches (5 × 5). Then, we measure the defocus response D c (p, α) of each subpatch N c (p) independently as follows.</p><formula xml:id="formula_6">D c (p, α) = 1 |N c (p)| ∑ q∈Nc(p) |L α (q) − P (q)|<label>(10)</label></formula><p>where c is the index of subpatch and P is the center pinhole image. The adaptive defocus response is computed as the minimum patch response at the subpatch c ⋆ (i.e. c ⋆ = min c D c (p, α)). However, the initial cost still leads to the ambiguity between occluder and occluded regions as shown in <ref type="figure" target="#fig_5">Figure 5 (b)</ref>. To discriminate the data cost between two cases, we introduce the additional color similarity constraint D col . The constraint is the difference between the mean color of the minimum subpatch and the center pixel color, which is formulated as follows.  Now, the final adaptive defocus response is formed as follows.</p><formula xml:id="formula_7">D col (p, α) = |{ 1 |N c ⋆ (p)| ∑ q∈N c ⋆ (p)L α (q)} − P (p)| (11)</formula><formula xml:id="formula_8">(b) (c) (d) (e) (f) (g) (h) (i) (j)</formula><formula xml:id="formula_9">D(p, α) = D c ⋆ (p, α) + γ D col (p, α)<label>(12)</label></formula><p>where γ (= 0.1) is the influence parameter of the constraint. <ref type="figure" target="#fig_5">Figure 5</ref> (b) shows the comparison of the data cost curves of the proposed adaptive defocus response and Tao's defocus data cost <ref type="bibr" target="#b19">[20]</ref>. It is shown that the proposed method finds the correct disparity in the occluded region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Data Cost Integration</head><p>Both data costs are combined together to accommodate individual strength. <ref type="figure" target="#fig_6">Figure 6</ref> shows the effect of data cost integration for clean and noisy images. Note that the angular entropy metric is robust to occlusion region and less sensitive to noise, while the adaptive defocus response is robust to noise and less sensitive to occlusion. Therefore, the combination of both data costs yields an improved data cost that is robust to both occlusion and noise. The integration leads to smaller error as evaluated in the following section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>The proposed algorithm is implemented on an Intel i7 4770 @ 3.4 GHz with 12GB RAM. We compare the performance of the proposed data costs with the recent light field depth estimation data costs. To this end, we use the code shared by the authors (Jeon et al. <ref type="bibr" target="#b10">[11]</ref>, Tao et al. <ref type="bibr" target="#b18">[19]</ref>, and Wang et al. <ref type="bibr" target="#b21">[22]</ref>) and implement the other methods that are not available. For fair comparison, we first compare the depth estimation result without global optimization to identify the discriminate power of each data cost. Then, globally optimized depth is compared for a variety of challenging scenes.</p><p>4D light field benchmark is used for the synthetic dataset <ref type="bibr" target="#b23">[24]</ref>. The real light field images are captured using the original Lytro and Lytro Illum <ref type="bibr" target="#b15">[16]</ref>. To extract the 4D real light field image, we utilize the toolbox provided by Dansereau et al. <ref type="bibr" target="#b6">[7]</ref>. We set the parameters as follows: λ = 0.5, β = 0.5, and τ = 10. For the cost slice filtering, the parameter setting is r = 5 and ϵ = 0.0001. The depth search range is 75 for all dataset. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison of the computational time for the data cost volume generation. We measure the runtime for each method with different image size. The proposed method has comparably fast computation. Note that our work is occlusion and noise aware depth estimation <ref type="figure">Figure 7</ref>: Comparison of the disparity maps generated from the local data cost; (a) Center pinhole image; (b) Ground truth; (c) Proposed angular entropy cost; (d) Proposed adaptive defocus cost; (e) Chen's bilateral cost <ref type="bibr" target="#b3">[4]</ref>; (f) Tao's correspondence cost <ref type="bibr" target="#b18">[19]</ref>; (g) Tao's defocus cost <ref type="bibr" target="#b18">[19]</ref>; (h) Tao's correspondence cost <ref type="bibr" target="#b19">[20]</ref>; (i) Tao's defocus cost <ref type="bibr" target="#b19">[20]</ref>; (j) Jeon's data cost <ref type="bibr" target="#b10">[11]</ref>; (k) Kang's shiftable window <ref type="bibr" target="#b11">[12]</ref>; (l) Vaish's binned entropy cost <ref type="bibr" target="#b20">[21]</ref>; (m) Lin's defocus cost <ref type="bibr" target="#b14">[15]</ref>; (n) Wang's correspondence cost <ref type="bibr" target="#b21">[22]</ref>; (o) Wang's defocus cost <ref type="bibr" target="#b21">[22]</ref>.  <ref type="bibr" target="#b19">[20]</ref>, and Wang et al. <ref type="bibr" target="#b21">[22]</ref> have two data costs in each method. <ref type="figure">Figure 7</ref> shows the non-optimized depth comparison for the synthetic dataset from Wanner et al. <ref type="bibr" target="#b23">[24]</ref>. Since the proposed data costs consider the occlusion, it is shown that they outperform the conventional data costs, yielding less fattening effect in the occluded region (i.e. leaves or branches). Similar to the proposed method, Wang et al. <ref type="bibr" target="#b21">[22]</ref> also model the occlusion in their data cost, but their method highly de- pends on the edge detection result. Furthermore, we also evaluate the optimized results of the selected methods ( <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>) using clean and noisy light field data, as shown in <ref type="figure">Figure 8</ref>. The noisy image is generated by adding additive Gaussian noise with standard deviation σ = 10. For fair comparison, we per- <ref type="figure">Figure 8</ref>: Comparison of the optimized disparity maps (synthetic data); (a) Proposed method; (b) Tao's method <ref type="bibr" target="#b18">[19]</ref>; (c) Chen's method <ref type="bibr" target="#b3">[4]</ref>; (d) Tao's method <ref type="bibr" target="#b19">[20]</ref>; (e) Jeon's method <ref type="bibr" target="#b10">[11]</ref>; (First row) Clear light field image; (Second row) Noisy light field image (σ = 10). form the same optimization technique for all methods. As the initial data costs fail to produce the minimum cost on the occluded region, the conventional methods produce the artifacts around the object boundary even after global optimization. Chen et al. <ref type="bibr" target="#b3">[4]</ref> achieves comparable performance on the clean data. However, it produces significant error on the noisy data. It is shown that the proposed method achieves stable performance in both environments. Next, mean squared error is measured to evaluate the computed depth accuracy. <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> show the comparison of the mean squared error for various data with multiple noise levels. On the clean data, the proposed method and Chen's method <ref type="bibr" target="#b3">[4]</ref> achieve comparable performance. However, Chen's method fails on the noisy data (c) Tao's method <ref type="bibr" target="#b18">[19]</ref>; (d) Chen's method <ref type="bibr" target="#b3">[4]</ref>; (e) Tao's method <ref type="bibr" target="#b19">[20]</ref>; (f) Jeon's method <ref type="bibr" target="#b10">[11]</ref>.</p><formula xml:id="formula_10">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Clean and Noisy Data</head><formula xml:id="formula_11">(a) (b) (c) (d) (e)</formula><p>while the proposed method obtains the minimum error for most cases. The proposed method obtains the best performance among all the conventional data costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Noisy Real Data</head><p>Light field image captured by commercial light field camera contains noise due to its small sensor size. We evaluate the proposed method with several scenes captured inside a room with limited lighting, which degrades the signal-to-noise ratio. The typical light level in the room (c) Tao's method <ref type="bibr" target="#b18">[19]</ref>; (d) Chen's method <ref type="bibr" target="#b3">[4]</ref>; (e) Tao's method <ref type="bibr" target="#b19">[20]</ref>; (f) Jeon's method <ref type="bibr" target="#b10">[11]</ref>. The first and second rows are captured by Lytro Illum camera while the others are captured by original Lytro camera.</p><p>(≈ 400 lux) is much lower than the light level of outdoor under daylight (≈ 10000 lux). <ref type="figure" target="#fig_7">Figure 9</ref> and <ref type="figure" target="#fig_0">Figure 10</ref> show the disparity maps generated from the real light field images. Conventional approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> exhibit blurry or fattening effect around the object boundaries, as shown in <ref type="figure" target="#fig_7">Figure 9</ref>. On the other hand, the proposed method shows sharp and unfattened boundary. Note that only the proposed method can estimate the correct shape of the house entrance and the spokes of the wheel as shown in <ref type="figure" target="#fig_0">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Limitation and Future Work</head><p>The angular entropy metric becomes less reliable when the noise/occluder is more dominant than the clean/nonoccluded pixels in the angular patch. Thus, it performs better when there are lots of subaperture images. As an example, it performs better on the Lytro Illum images than the images captured by original Lytro camera, as shown in <ref type="figure" target="#fig_0">Figure 10</ref>. This problem might be addressed by using spatial neighborhood information to increase the probability of the dominant pixels or capturing more angular images. In addition, it is also useful to find the reliability measure for the entropy data cost. To extend the current work, we also plan to perform exhaustive comparison and informative benchmarking on the state-of-the-art data costs for light field depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed an occlusion and noise aware light field depth estimation framework. Two novel data costs were proposed to obtain robust performance in the occluded region. Angular entropy metric was introduced to measure the pixel color randomness in the angular patch. In addition, adaptive defocus response was determined to gain robust performance against occlusion. Both data costs were integrated in the MRF framework and further optimized using graph cut. Experimental results showed that the proposed method significantly outperformed the conventional approaches in both occluded and noisy scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of disparity maps of various algorithms on a noisy light field image (σ = 10). (First row) Data cost only. (Second row) Data cost + global optimization. (a) Proposed data cost with less fattening effect; (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Angular patch analysis. (a) The center pinhole image with a spatial patch; (b) Angular patch and its histogram (α = 1); (c) Angular patch and its histogram (α = 21); (d) Angular patch and its histogram (α = 41); (First column) Non-occluded pixel (Entropy costs are 3.09, 0.99, 3.15, respectively) ; (Second column) Multi-occluded pixel (Entropy costs are 3.42, 2.34, 3.05, respectively). Ground truth α is 21</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>CFigure 3 :</head><label>3</label><figDesc>max (p, α) = max(H R (p, α), H G (p, α), H B (p, α)) (7) Data cost curve comparison. (a) Non-occluded pixel in the first column of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Disparity maps of noisy light field image generated from the local data cost (σ = 10). (a) Proposed angular entropy metric; (b) Chen et al.<ref type="bibr" target="#b3">[4]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Defocus cost analysis. (a) The center pinhole image with a spatial patch; (b) Data cost curve comparison; (c)∼(f) Spatial patch from refocus image (α = 1, 21, 35, 41); (g)∼(j) Different map of patches in (c)∼(f) We multiply the different map by 10 for better visualization. Red box shows the minimum small patch. Ground truth α is 21.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Data cost integration analysis. (a) Clean images; (b) Noisy images (σ = 10); (First row) Disparity maps from angular entropy metric; (Second row) Disparity maps from adaptive defocus response; (Third row) Disparity maps from integrated data cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Comparison of the optimized disparity maps (real data); (a) Center pinhole image; (b) Proposed method;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Additional comparison of the optimized disparity maps (real data);(a) Center pinhole image; (b) Proposed method;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Computational time for the cost volume generation 
(in seconds). 

Data Type 
Synthetic Data 
Original Lytro 
Lytro Illum 

(9 × 9 × 768 × 768) (9 × 9 × 379 × 379) (11 × 11 × 625 × 434) 

Chen et al. [4] 
892 
260 
612 

Jeon et al. [11] 
2,590 
689 
1,406 

Kang et al. [12] 
102 
31 
89 

Lin et al. [15] 
167 
40 
93 

Tao et al. [19] 
878 
110 
246 

Tao et al. [20] 
252 
68 
149 

Vaish et al. [21] 
598 
173 
386 

Wang et al. [22] 
256 
76 
183 

Proposed method 
528 
115 
207 

while others are general or only occlusion aware. Further-
more, the proposed method, Tao et al. [19], Tao et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>The mean squared error of various dataset.</figDesc><table>Data Type 
Buddha Noisy Buddha StillLife Noisy StillLife 

Chen's bilateral cost [4] 
0.0129 
1.0654 
0.3918 
4.9483 

Jeon's data cost [11] 
0.0730 
0.2575 
0.0759 
0.2264 

Kang's shiftable window [12] 
0.0139 
0.1944 
0.0787 
0.1818 

Lin's defocus cost [15] 
0.2750 
0.3374 
2.4554 
2.4624 

Tao's correspondence cost [19] 
0.1198 
0.1657 
0.2314 
0.2490 

Tao's defocus cost [19] 
0.2267 
0.2220 
0.5728 
0.5394 

Tao's correspondence cost [20] 
0.0186 
0.2908 
0.0473 
0.2422 

Tao's defocus cost [20] 
0.0189 
0.0713 
0.0538 
0.0711 

Vaish's binned entropy cost [21] 
0.1646 
0.1500 
0.0377 
0.0980 

Wang's correspondence cost [22] 0.0304 
0.8980 
0.0624 
5.3913 

Wang's defocus cost [22] 
0.2085 
0.7173 
0.9148 
2.3622 

Proposed correspondence cost 
0.0058 
0.1174 
0.0176 
0.0608 

Proposed defocus cost 
0.0266 
0.2500 
0.0634 
0.1961 

Proposed integrated cost 
0.0047 
0.0989 
0.0206 
0.0532 

Chen's cost + optimization [4] 
0.0041 
0.1646 
0.0126 
0.1386 

Jeon's cost + optimization [11] 
0.0149 
0.0250 
0.0283 
0.0432 

Tao's cost + optimization [19] 
0.0270 
0.0300 
0.0572 
0.0593 

Tao's cost + optimization [20] 
0.0154 
0.0282 
0.0491 
0.0587 

Proposed cost + optimization 
0.0036 
0.0170 
0.0142 
0.0251 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>The mean squared error of results from Mona dataset with different noise levels.</figDesc><table>Data Type 
σ = 0 
σ = 5 σ = 10 σ = 15 

Chen's bilateral cost [4] 
0.0233 0.6834 0.8754 0.9373 

Jeon's data cost [11] 
0.0801 0.1588 0.2665 0.3791 

Kang's shiftable window cost [12] 0.0259 0.0799 0.1772 0.2610 

Lin's defocus cost [15] 
0.0626 0.0851 0.1162 0.1453 

Tao's correspondence cost [19] 
0.0601 0.0772 0.1191 0.1755 

Tao's defocus cost [19] 
0.1350 0.1246 0.1252 0.1395 

Tao's correspondence cost [20] 
0.0183 0.1300 0.2752 0.4025 

Tao's defocus cost [20] 
0.0244 0.0409 0.0806 0.0350 

Vaish's binned entropy cost [21] 
0.0928 0.1170 0.1480 0.2115 

Wang's correspondence cost [22] 
0.0207 0.3072 0.5914 0.8130 

Wang's defocus cost [22] 
0.1301 0.4357 0.5809 0.6809 

Proposed correspondence cost 
0.0118 0.0381 0.1152 0.2043 

Proposed defocus cost 
0.0217 0.0954 0.1962 0.2788 

Proposed integrated cost 
0.0081 0.0337 0.1011 0.1792 

Chen's cost + optimization [4] 
0.0052 0.0308 0.1466 0.8547 

Jeon's cost + optimization [11] 
0.0122 0.0179 0.0224 0.0274 

Tao's cost + optimization [19] 
0.0238 0.0251 0.0227 0.0226 

Tao's cost + optimization [20] 
0.0203 0.0245 0.0301 0.0350 

Proposed cost + optimization 
0.0045 0.0072 0.0125 0.0185 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Surface stereo with soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1570" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric calibration of micro-lens-based light-field cameras using line features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="47" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Light field stereo matching using bilateral statistics of surface cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1518" to="1525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Consistent matting for light field images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="90" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling the calibration pipeline of the Lytro camera for high quality lightfield image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3280" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decoding, calibration and rectification for lenselet-based plenoptic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Dansereau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="504" to="511" />
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How do people edit light fields?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jarabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Masia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pellacini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate depth map estimation from a lenslet light field camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1547" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Handling occlusions in dense multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-camera scene reconstruction via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="82" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2806" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth recovery from light field using focal stack symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Lytro camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lytro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fourier slice photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="735" to="744" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">3D light field camera technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raytrix</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth from combining defocus and correspondence using lightfield cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="673" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Depth from shading, defocus, and correspondence using light-field angular coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1940" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reconstructing occluded surfaces using synthetic apertures: Stereo, focus and robust measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Occlusionaware depth estimation using light-field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Globally consistent depth labelling of 4D lightfields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Datasets and benchmarks for densely sampled 4D light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Vision, Modeling &amp; Visualization</title>
		<meeting>of Vision, Modeling &amp; Visualization</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Asymmetrical occlusion handling using graph cut for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="902" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High perofrmance imaging using large camera arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Artunez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="765" to="779" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
