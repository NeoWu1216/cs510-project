<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Cross-Dataset Transfer Learning for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
							<email>yaoweiwang@bit.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
							<email>massimiliano.pontil@iit.it</email>
							<affiliation key="aff3">
								<orgName type="institution">Italian Institute of Technology</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<email>s.gong@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
							<email>tjhuang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Cross-Dataset Transfer Learning for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing person re-identification (Re-ID) approaches follow a supervised learning framework, in which a large number of labelled matching pairs are required for training. This severely limits their scalability in realworld applications. To overcome this limitation, we develop a novel cross-dataset transfer learning approach to learn a discriminative representation. It is unsupervised in the sense that the target dataset is completely unlabelled. Specifically, we present an multi-task dictionary learning method which is able to learn a dataset-shared but targetdata-biased representation. Experimental results on five benchmark datasets demonstrate that the method significantly outperforms the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (Re-ID) is the problem of matching people across non-overlapping camera views. It has become one of the most studied problems in video surveillance due to its great potentials for security and safety management applications. Despite the best efforts from the computer vision researchers <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b9">10]</ref>, it remains an unsolved problem. This is because a person's appearance often changes dramatically across camera views due to changes in body pose, view angle, occlusion and illumination conditions.</p><p>To address these challenges, most existing research efforts on Re-ID <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b1">2]</ref> are based on supervised learning. Specifically, they require a large number of labelled matching pairs across each two camera views to learn a representation or matching function that is invariant to the appearance changes. However, relying on manually labelled data for each camera-pair leads to poor scalability. This is due to two reasons: (1) For each pair of cameras, eye-balling the two views to annotate correctly matching pairs among hundreds of imposters is a tough job even for humans. (2) Given a camera network of even a moderate size, e.g. one installed in an underground station, there can be easily over one hundred cameras and thousands of camera pairs. Since hundreds of labelled image pairs are typically needed from each camera pair for supervised learning, the labelling cost would be prohibitively high. This scalability issue thus severely limits the applicability of the existing methods.</p><p>In order to make a person Re-ID model scalable, one solution is to utilise the unlabelled data, which are abundant in the context of Re-ID -in a busy public space, thousands of people pass by in each camera view everyday. There are a few existing efforts on exploiting unlabelled data for unsupervised Re-ID modelling <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6]</ref>. However, compared to supervised learning approaches, the matching performance of unsupervised models are typically much weaker, rendering them less effective. The reason is that without labelled matching pairs across camera views, existing unsupervised models are unable to learn what makes a person recognisable under severe appearance changes.</p><p>Different from existing unsupervised Re-ID methods, we propose to solve the Re-ID problem without any labelled matching pairs of target data using cross-dataset transfer learning. The idea is that labelling matching pairs for a set of given target camera views is tedious for practical applications. However, there already exist labelled datasets collected elsewhere from other camera networks; it is therefore possible to learn a representation that captures the view-invariant aspects of a person's appearance and transfer it to the target dataset for matching. Since the target views/dataset contains no label, this is an unsupervised learning problem <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. It is thus an extremely challenging problem because not only the source and target domains are different (different camera views), more critically they also have different recognition tasks (different sets of person identities to be matched in each domain), in contrast to most existing transfer learning assumptions.</p><p>To solve the above unsupervised cross-dataset transfer learning problem, we propose a novel asymmetric multitask learning approach which is able to transfer a viewinvariant representation from a number of existing labelled source datasets, each consisting of camera pairs with different viewing conditions, to an unlabelled target dataset containing people who never appeared in the source datasets. Our method is based on dictionary learning, that is, we assume that a person's appearance can be represented as a linear combination of latent factors each corresponding to a dictionary atom. Furthermore, we assume that some of the atoms are view/dataset-independent, thus shared across different datasets/tasks, whilst others are unique to each dataset and may or may not be useful for Re-ID in a new unlabelled target dataset. This results in three types of dictionaries being jointly learned using all datasets.</p><p>The key strength of our method, which also distinguishes it from existing multi-task learning methods <ref type="bibr" target="#b29">[30]</ref>, is that it is able to learn from unlabelled target data. This is precisely why a dictionary learning model is adopted -it is originally designed for unsupervised learning and can thus be naturally reformulated for unsupervised transfer learning. To this end, graph Laplacian regularisation terms with iterative updating are introduced in our formulation in order to learn from both the labelled information from the source data and the unlabelled data from the target data. In addition, to make the learned dictionary biased towards the target dataset, different decompositions of dictionaries are introduced for the source and target datasets respectively to reflect the fact that our multi-task learning model is asymmetric, i.e. the multitask joint learning only aims to benefit the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most existing person Re-ID models are supervised, based on either distance metric learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>, discriminative subspace learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23]</ref>, learning to rank <ref type="bibr" target="#b30">[31]</ref>, or deep learning <ref type="bibr" target="#b1">[2]</ref>. These models are thus unscalable as they need a large number of labelled data (cross-view matched image pairs) to train for each given camera pair. In particular, each learned model is camerapair-specific thus cannot be directly applied to another new camera pair due to the view condition changes, as verified by our experiments (Sec. 4).</p><p>To address the scalability issue, there have been a num-ber of unsupervised Re-ID methods proposed in the literature, including two types: those designing hand-crafted appearance features <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28]</ref> and those modelling localised saliency statistics <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34]</ref>. However, compared to supervised learning approaches, both approaches yield much weaker performance, since without pairwise identity labels they cannot exploit cross-view identity-discriminative information that is critical for matching. To strike a balance between scalability and matching accuracy, a semisupervised approach <ref type="bibr" target="#b25">[26]</ref> is proposed. Nevertheless, it still requires a fairly substantial amount of pairwise labelling which is not possible for large scale deployment in realworld applications.</p><p>Recently, cross-dataset transfer learning has been adopted for Re-ID in the hope that labelled data from other camera views/datasets can provide transferable identitydiscriminative information for a given target dataset. Note that this cross-dataset transfer learning problem is very different from the same-dataset cross-identity or same-dataset cross-view problems tackled in some early transfer Re-ID works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44]</ref>. When both the dataset/domain and the identities are different, the transfer learning problem considered in this work is much harder. Among the existing cross-dataset transfer learning works, <ref type="bibr" target="#b18">[19]</ref> adopted an SVM multi-kernel learning transfer strategy, and both <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b34">[35]</ref> employed multi-task metric learning models. All of theses works are supervised and they need labelled data in the target dataset.</p><p>As far as we know, the only existing unsupervised crossdataset transfer learning model for Re-ID is the work in <ref type="bibr" target="#b26">[27]</ref>. The model proposed in <ref type="bibr" target="#b26">[27]</ref> utilises cross-domain ranking SVMs. Unlike the dictionary learning model employed in this work, an SVM-based model does not naturally learns from completely unlabelled data. As a result, their target dataset is not exactly unlabelled: it is assumed that negative image pairs are given for the target dataset. Therefore, strictly speaking, the model in <ref type="bibr" target="#b26">[27]</ref> is a weaklysupervised rather than unsupervised model. In contrast, our model is completely unsupervised without requiring any labelled data from the target dataset. Our experiments show that our method significantly outperforms that of <ref type="bibr" target="#b26">[27]</ref>, even with less supervision.</p><p>Beyond person Re-ID, dictionary learning for sparse coding has been extensively studied <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref>. Graph Laplacian regularisation has also been explored in a sparse coding formulation before, for problems such as unsupervised clustering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>, or supervised face verification/recognition <ref type="bibr" target="#b12">[13]</ref>. Our model differs in that (1) dictionary learning is performed under an asymmetric multi-task learning framework, hence the unique design of different decompositions of dictionaries for the source and target tasks; and (2) the Laplacian regularisation terms are updated iteratively to adapt transferable knowledge learned from the labelled source data to the unlabelled target data.</p><p>Note that a number of works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32]</ref> have exploited domain adaptation for cross-view classification or verification of faces/actions, based on dictionary learning and/or sparse representation models. They are thus related to our work. But there are significant differences. In particular, some of them <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40]</ref> are supervised and require labelled training data from the target domains. The work in <ref type="bibr" target="#b31">[32]</ref> is unsupervised and based on unsupervised domain adaptation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. Nevertheless they tackle a within-dataset cross-camera view domain adaptation problem. This is fundamentally different to our cross-dataset transfer learning problem: the domain change is much greater across datasets, and importantly the images from cross-domain/view but same dataset contain people of the same identities, whilst a completely different set of people are captured in different datasets. In our experiments, we demonstrate that these unsupervised domain adaptation methods do not work on our cross-dataset transfer learning task because the target dataset contains different classes.</p><p>Contributions The main contributions of this work are: (1) We formulate the Re-ID problem as an unsupervised cross-dataset transfer learning problem and do not require any labelled data from the target dataset, and (2) a novel asymmetric multi-task dictionary learning model is proposed to learn view-invariant and identity-discriminative information from unlabelled target data. Extensive experiments are carried out on five widely used Re-ID benchmark datasets. The results show that the proposed model significantly outperforms the state-of-the-art Re-ID approaches, as well as existing transfer learning models, under both unsupervised and semi-supervised settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Assume a number of source datasets are collected from different camera networks each consisting of two camera views 1 . The images in the source datasets (domains) are paired across camera views by the person's identity, i.e. they are pairwise labelled. An unlabelled target dataset is captured from an entirely different domain (camera view/location) and contains a completely different set of identities/classes. Therefore, the unsupervised transfer learning for Re-ID problem is defined as the problem of learning the optimal representation/matching function for the target dataset/domain using knowledge transferred from the labelled source datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Formulation</head><p>Taking a multi-task learning approach, we consider learning a Re-ID model for each dataset as a task. We wish to learn all tasks jointly so that they can benefit each other. Importantly, since we are only concerned with the target task, the multi-task model is asymmetric and biased towards the target task. Formally, assume X t ∈ R M ×Nt is a feature matrix with each column x t,i corresponding to an M -dimensional feature vector representing the i-th person's appearance in the dataset t (t = 1, ..., T ) consisting of N t samples.</p><p>Assume task T is the target task and the others are source tasks. Adopting a dictionary learning model, for each task/dataset, the goal is to learn a shared dictionary D ∈ R M ×k using all datasets {X 1 , ..., X T }. With this dictionary, each M -dimensional feature vector, regardless which view it comes from, is projected into a lower kdimensional subspace spanned by the k dictionary atoms (columns of D) so that the corresponding coefficients (code vectors) can be matched by the cosine distance in this subspace. The idea is that each atom or the dimension of the subspace corresponds to a latent appearance attribute which is invariant to the camera view changes, thus useful for cross-view matching. <ref type="figure" target="#fig_0">Figure 1</ref> shows some examples of latent attributes learned by the proposed model. In a multi-task dictionary learning model, it is necessary to decompose the dictionary into two parts: the one shared between the tasks, which captures latent attributes that are invariant against any view changes, and a task-specific one that captures dataset-unique aspects of human appearance <ref type="bibr" target="#b4">[5]</ref>. In addition, it is important to note that apart from the latent attributes that can contribute to Re-ID, there are also other aspects of appearance that are variant to view changes. These appearance aspects must be modelled as part of the dictionary as well. Furthermore, the decomposition should be different for the source and target datasets as we only care about the target one. Based on these consider-ations, three types of dictionaries are introduced in our Unsupervised Multi-task Dictionary Learning (UMDL) model: (1) Task-shared dictionary D s which is used to encode the dataset/view invariant latent attributes, and is shared by all tasks, (2) dictionary unique to the target task D u T that is view-invariant , and (3) task-specific residual dictionary D r t (t = 1, ..., T ) which is task-specific and used to encode the residual parts of features which cannot be modelled by D s (source tasks) or D s and D u T (target task). It is clear that the source and target tasks are treated differently: for the target task, an additional third dictionary D u T is needed to account for view-invariant but dataset-variant latent attributes unique to the target views. Now we can formulate our UMDL method as:</p><formula xml:id="formula_0">[D s , D u T , D r 1 , · · · , D r T ] = arg min T −1 t=1 η 2 t { Xt − D s A s t 2 F + Xt − D s A s t − D r t A r t 2 F }+ XT − D s A s T 2 F + XT − D s A s T − D u T A u T 2 F + XT − D s A s T − D u T A u T − D r T A r T 2 F + λ T t=1 N t i,j=1 wt,i,j a s t,i − a s t,j 2 + λ N T i,j=1 wT,i,j a u T,i − a u T,j 2 , s.t. d s i 2 2 ≤ 1, d u T,i 2 2 ≤ 1, d r t,i 2 2 ≤ 1, ∀i, t<label>(1)</label></formula><p>where matrices A s t , A r t and A u T are codes corresponding to dictionaries D s , D r t and D u T respectively; d s i , d r t,i and d u T,i are the i th column of D s , D r t and D u T respectively; a s t,i is the i th column of A s t and a u T,i is the i th column of A u T ; η t and λ are weights of various cost function terms; and W t is the affinity matrix for the task t indicating the relationships among different training samples. Specifically, for the labelled source datasets, w t,i,j = 1 if x t,i and x t,j are of the same person across views and w t,i,j = 0 otherwise. For the target task, W T is initialised as a zero matrix because the target task are unlabelled.</p><p>There are seven terms in this cost function and they fall into two categories: the first five are reconstruction error terms that make sure that the learned dictionaries can encode the feature matrices well, and the last two are graph Laplacian regularisation terms that enforce that similar codes are obtained for instances of the same person across camera views. Note that these two regularisation terms are put on the codes obtained using D s and D u T only. As for those obtained using D r t , they are not subject to the graph Laplacian regularisation because they are either untransferrable to the target task or are view-variant thus useless for Re-ID. Note that since W T is a zero matrix, it seems to make no sense to have the seventh term for the target task T . However, we shall see later that W T will be updated iteratively once a better representation for Re-ID is learned.</p><p>The last two terms can be rewritten using the Laplacian matrix as</p><formula xml:id="formula_1">Nt i,j=1 w t,i,j a s t,i − a s t,j 2 = Tr(A s t L t A s t ′ ),</formula><p>where L t = D t − W t and D t is a diagonal matrix whose diagonal elements are the sums of the row elements of W t . Now we explain how the first five reconstruction error terms are designed. First, we note that the reconstruction error terms are formulated stepwise by the priority of different dictionaries. Let us consider the first two terms</p><formula xml:id="formula_2">X t − D s A s t 2 F + X t − D s A s t − D r t A r t 2 F</formula><p>for the source task t. The minimisation of the first reconstruction error term enables D s to encode X t as much as possible while the minimisation of the second reconstruction error term enables D r t to encode and align the residual part of X t that cannot be coded using D s . This stepwise reconstruction formulation is also applied to the target task T resulting in terms 3-5. However, as an asymmetric multi-task learning model, the target task is biased with three dictionaries rather than two, hence the three terms. We shall see in our experiments that both the stepwise reconstruction terms and asymmetric design contribute positively to the final performance of the model.</p><p>Note that unlike conventional dictionary learning for sparse coding models, in our model, there is no L 1 sparsity penalty term. This is because (1) empirically, we find that less-sparse codes contain richer information for Re-ID, and (2) removing these L 1 terms leads to a simpler optimisation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimisation</head><p>Next we describe how the optimisation problem in (1) can be solved. This optimisation problem is divided into the following subproblems:</p><p>(1) Computing A s t and A u T Given fixed D s , D r t , A r t and D u T , the coding problem of the task t (t = 1, ..., T ) becomes:</p><formula xml:id="formula_3">min X t −DÃt 2 F + λTr(ÃtLtÃ ′ t ),<label>(2)</label></formula><p>where, for the target task:</p><formula xml:id="formula_4">XT =   XT XT XT − D r T A r T   ,D =   D s , 0 D s , D u T D s , D u T   ,ÃT = A s T A u T ,</formula><p>and for the source tasks:</p><formula xml:id="formula_5">Xt = ηtXt ηt (Xt − D r t A r t ) ,D = ηtD s ηtD s ,Ãt = A s t .</formula><p>Let the derivative of (2) equals to 0 and the analytical solution ofã t,i (the i th column ofÃ t ) can be obtained as:</p><formula xml:id="formula_6">at,i = D ′D + 2λlt,i,iI −1  D ′x t,i − 2λ k =ix t,k l t,k,i   ,</formula><p>where l t,k,i is the (k, i) element of L t . I is the identity matrix andx t,i is the i th column ofX t .</p><p>(2) Computing A r t Fix other terms and A r t is solved as:</p><p>For the target task:</p><formula xml:id="formula_7">min XT − D s A s T − D u T A u T − D r T A r T 2 F</formula><p>, and for the source tasks:</p><formula xml:id="formula_8">min Xt − D s A s t − D r t A r t 2 F .<label>(3)</label></formula><p>Let the derivative of (3) equals to 0 and the analytical solution of A r t can be obtained as:</p><p>For the target task:</p><formula xml:id="formula_9">A r T = D r T ′ D r T −1 D r T ′ (XT − D s A s T − D u T A u T )</formula><p>, and for the source tasks:</p><formula xml:id="formula_10">A r t = D r t ′ D r t −1 D r t ′ (Xt − D s A s t ) .</formula><p>(3) Updating dictionaries When D r t , A s t , A r t (t = 1, ..., T ), D u T and A u T are given, D s is optimised as:</p><formula xml:id="formula_11">min X − D s A 2 F , s.t. d s i 2 2 ≤ 1, (∀i),<label>(4)</label></formula><p>where</p><formula xml:id="formula_12">X = [η1X1, ..., ηT −1XT −1, η1(X1 − D r 1 A r 1 ), ..., ηT −1(XT −1− D r T −1 A r T −1 ), XT , XT − D u T A u T , XT − D u T A u T − D r T A r T ], A = [η1A s 1 , ..., ηT −1A s T −1 , η1A s 1 , ..., ηT −1A s T −1 , A s T , A s T , A s T ].<label>(5)</label></formula><p>(4) can be optimised by the Lagrange dual and the analytical solution of D s can be computed as: D s = (X A ′ ) (AA ′ + Λ) −1 , where Λ is a diagonal matrix constructed from all the dual variables. Then, for the target task, fix the dictionaries D s , D r T and codes A s T , A u T , A r T , then D u T can be updated by:</p><formula xml:id="formula_13">min X u T − D u T A u T 2 F , s.t. d u T,i 2 2 ≤ 1, (∀i),<label>(6)</label></formula><p>where</p><formula xml:id="formula_14">X u T = [XT , XT − D s A s T , XT − D s A s T − D r T A r T ] , A u T = [A u T , A u T , A u T ] .<label>(7)</label></formula><p>At last, fix D s , D u T , A s t , A u T and A r t , the objective function to solve D r t is (t = 1, ..., T ) :</p><formula xml:id="formula_15">min X r t − D r t A r t 2 F , s.t. d r t,i 2 2 ≤ 1(∀i),<label>(8)</label></formula><p>where For the target task:</p><formula xml:id="formula_16">X r T = XT − D s A s T − D u T A u T</formula><p>, and for the source tasks: (6) and <ref type="formula" target="#formula_15">(8)</ref> can be solved similarly as <ref type="formula" target="#formula_11">(4)</ref>:</p><formula xml:id="formula_17">X r t = Xt − D s A s t .<label>(9)</label></formula><formula xml:id="formula_18">D u T = X u T A u T ′ A u T A u T ′ + Λ −1 , D r t = X r t A r t ′ A r t A r t ′ + Λ −1 .</formula><p>Alg. 1 summarises our optimisation algorithm. It converges after a few (&lt; 30) iterations in our experiments. Iterative Updating W T After running Alg. 1, each training sample x T,i from the target task will be coded by (10) (detailed below) and the code is a T,i = a s T,i , a u T,i , a r T,i . With this code, we can measure the similarity between each pair of target data samples across views and recompute W T . This matrix now captures the soft relationships among the training samples from the target tasks which we aim to preserve in the lower dimensional space spanned by the dictionary columns. Specifically, if a T,j is among the k-nearest-neighbours of a T,i and a T,i is among the knearest-neighbours of a T,j , w T,i,j = a T ,i ·a T ,j a T ,i a T ,j , otherwise, w T,i,j = 0. With the updated W T , we re-run Alg. 1 to enter the next iteration. The iterations terminate when a stopping criterion is met, and the number of iterations is typically &lt; 5 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Application to Re-ID</head><p>Re-ID for the Target Task After training the model, each test sample x T,i from the target task T can be encoded via D s , D u T and D r T as a s T,i , a u T,i , a r T,i by solving the fol-lowing problem:</p><formula xml:id="formula_19">a s T,i , a u T,i , a r T,i = min xT,i − D s a s T,i − D u T a u T,i − D r T a r T,i 2 2 + γ a s T,i 2 2 + γ a u T,i 2 2 + γ a r T,i 2 2 ,<label>(10)</label></formula><p>which can be solved easily by a linear system. With this new representation, Re-ID is done simply by computing the cosine distance between the code vectors of a probe and a gallery sample.</p><p>Extension to Semi-Supervised Re-ID If the target task are partially labelled, our model can be readily extended with minimal modification. Specifically, for the labelled data, w T,i,j will be set to 1 if x T,i and x T,j are from same individual, otherwise w T,i,j = 0. For the unlabelled data, the corresponding part of W T is initialised and iteratively updated as in the unsupervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Settings</head><p>Datasets Five widely used benchmark datasets are chosen in our experiments. The VIPeR dataset <ref type="bibr" target="#b11">[12]</ref> contains 1,264 images of 632 individuals from two distinct camera views (two images per individual) featured with large viewpoint changes and varying illumination conditions ( <ref type="figure">Fig. 2(a)</ref>). The PRID dataset <ref type="bibr" target="#b13">[14]</ref> consists of images extracted from multiple person trajectories recorded from two surveillance static cameras ( <ref type="figure">Fig. 2(b)</ref>). Camera view A contains 385 individuals, camera view B contains 749 individuals, with 200 of them appearing in both views. The CUHK01 dataset <ref type="bibr" target="#b20">[21]</ref> contains 971 individuals captured from two camera views in a campus environment ( <ref type="figure">Fig. 2(c)</ref>). Each person has two images in each camera view. We follow the single-shot setting, that is, we randomly select one image for each individual in each camera view for both training and testing in our experiments. The iLIDS dataset <ref type="bibr" target="#b42">[43]</ref> has 476 images of 119 individuals captured in an airport terminal from three cameras with different viewpoints <ref type="figure">(Fig. 2(d)</ref>). It contains large occlusions caused by people and luggage. The CAVIAR dataset <ref type="bibr" target="#b2">[3]</ref> includes 1,220 images of 72 individuals from two cameras in a shopping mall <ref type="figure">(Fig. 2(e)</ref>). Each person has 10 to 20 images. The image sizes of this dataset vary significantly (from 141 × 72 to 39 × 17). By examining <ref type="figure">Fig. 2</ref>, it is clear that the obvious variations of visual scenes and crossview conditions between the five benchmark datasets make the transfer learning task extremely challenging.</p><p>Settings A single-shot experiment setting is adopted similar to <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref>. In each experiment, one dataset is chosen as the target dataset and the other four are used as the source datasets. All the individuals in the source data are labelled <ref type="figure">Figure 2</ref>: Image samples of the five datasets. Images in the same column are from the same person across two views. Better viewed in colour. and used for model training, while the individuals in the target dataset are randomly divided into two equal-sized subsets as the training and test sets, with no overlapping on person identities. This process is repeated 10 times, and the averaged performance is reported as the final results. For datasets with only two camera views (VIPeR, PRID and CHUK01), we randomly select one view as probe and the other as gallery. While for the multi-view dataset (iLIDS), one image of each individual in the test set is randomly selected as the gallery image and the rest of the images are used as probe images. Results are obtained by averaging with 10 trials. For the CAVIAR dataset, the same setting as iLIDS is used in the unsupervised setting. However, for fair comparison with published results under the semisupervised setting, we follow <ref type="bibr" target="#b25">[26]</ref> and randomly choose 14 of the 50 individuals appearing in two cameras as the labelled training data, and the remaining 36 individuals as testing data. The 22 people appearing in one camera are used as the unlabelled training data. Also, final results are obtained by averaging with 10 trials. All images are normalized to 128 × 48 pixels and the colour+HOG+LBP histogram-based 5138-D feature representation in <ref type="bibr" target="#b24">[25]</ref> is used. As for the number of dictionary atoms, the size of the task-shared dictionary D s is the same as the residual dictionary D r t (t = 1, 2..., T ) , which is half of the size of the unique dictionary D u T . The size of D s is set to 150 for all experiments. We found that the model's performance is insensitive to the different dictionary sizes. Other parameters (η t and λ in Eq. (1)) in our model are set automatically using four-fold cross-validation with one of the four source datasets as the validation set and the other three as training set 2 .</p><formula xml:id="formula_20">(a) (b) (c) (d) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Unsupervised Re-ID Results</head><p>Under this setting, the target dataset is unlabelled. The compared methods can be categorised into two groups:</p><p>(1) Single-task methods.</p><p>Without transfer learning, the training data of these unsupervised methods are <ref type="bibr" target="#b1">2</ref>   only the unlabelled data from the target dataset. Some state-of-the-art unsupervised Re-ID methods are selected for comparison, including the hand-crafted-feature-based method SDALF <ref type="bibr" target="#b5">[6]</ref>, the saliency-learning-based eSDC <ref type="bibr" target="#b37">[38]</ref>, the graphical-model-based GTS <ref type="bibr" target="#b33">[34]</ref> and the sparserepresentation-classification-based ISR <ref type="bibr" target="#b23">[24]</ref>. We also report results of the single-task version of proposed model by removing all source data related terms in Eq. (1), denoted as Ours S.</p><p>(2) Multi-task methods. There are few multi-task learning methods, or unsupervised transfer learning methods in general, available for the unsupervised setting. AdaRSVM <ref type="bibr" target="#b26">[27]</ref> is the only unsupervised cross-data transfer learning work that we are aware of, and it is also designed for person Re-ID. As discussed in Sec. 2, the main difference is that they assume the availability of negative pairs in the target dataset, thus using more supervision than our method. We also use the subspace alignment based unsupervised domain adaptation method SA DA <ref type="bibr" target="#b6">[7]</ref> to align the data distributions of the source and target datasets first. Then a supervised Re-ID model, kLFDA <ref type="bibr" target="#b35">[36]</ref>, is trained on the labelled source datasets and applied to the aligned target dataset. This method is denoted as SA DA+kFLDA. Note that as an unsupervised domain adaptation method, SA DA assumes that the source and target domains have the same classes, which is invalid for cross-dataset transfer learning. In addition, we compare with a naive transfer approach, that is, learning kFLDA on source datasets first, and applying it directly to the target dataset without any model adaptation. This is denoted as kLFDA N.  <ref type="bibr" target="#b2">3</ref> . From these results, it is evident that: (1) Compared with existing unsupervised methods including SDALF, eSDC, GTS and ISR, our model is significantly better. This shows that transfer learning indeed helps for unsupervised Re-ID. <ref type="bibr" target="#b1">(2)</ref> The difference in performance between "Ours S" and "Ours" models shows exactly how much the target dataset has benefited from the source datasets using our unsupervised asymmetric multitask transfer learning. (3) The results of kLFDA N is very poor, showing that the knowledge learned from the labelled source datasets cannot be directly used to help match target data. This is due to the drastically different viewing conditions and condition changes across views in the target dataset compared to those in the source (see <ref type="figure">Fig. 2</ref>). A naive transfer learning approach such as kLFDA N would not be able to cope with the domain shift/difference of this magnitude. (4) Importantly it is noted that when an existing unsupervised domain-adaptation based transfer learning model is applied to alleviate the domain shift problem (SA DA+kLFDA), the result is even worse. This is not surprising as existing unsupervised domain adaptation methods are designed under the assumption that the source and target domains have the same recognition tasks (i.e. having the same set of classes) -an invalid assumption for our unsupervised Re-ID problem as different datasets contain different person identities. <ref type="formula" target="#formula_12">(5)</ref> The results of the only existing cross-dataset unsupervised Re-ID method AdaRSVM is actually the worst. Note that since the code is not available, these are the reported results in <ref type="bibr" target="#b26">[27]</ref>. Since different feature representation and two instead of four source datasets were used, this comparison is only indicative. However, by examining some additional results in <ref type="table" target="#tab_2">Table 2</ref>, we can still conclude that AdaRSVM is able to transfer very little useful information from the source datasets even when they use more supervision on the target dataset than our model. More specifically, in <ref type="table" target="#tab_2">Table 2</ref>, Fea AdaRSVM (Fea Ours) means the matching accuracy by L1-Norm distance of the features used in AdaRSVM (Ours). The results in <ref type="table" target="#tab_2">Table 2</ref> show that our transfer model can improve 8%-15% matching accuracy over non-learning based L1-Norm distance. In contrast, the increase for AdaRSVM is 1%-2%. <ref type="bibr" target="#b5">(6)</ref> It is noted that on three of five datasets (PRID,CAVIAR and iLIDS), our unsupervised results is close or surpasses the best reported results using existing supervised methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b30">31]</ref>. This shows the clear advantage of our unsupervised transfer learning model over existing models (supervised and unsupervised) on both scalability and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semi-supervised Re-ID Results</head><p>In this experiment, one third of the training set from the target dataset is labelled as in <ref type="bibr" target="#b25">[26]</ref>. Again, we compare with two groups of methods:</p><p>(1) Single-task methods. SSCDL <ref type="bibr" target="#b25">[26]</ref> is the most relevant semi-supervised Re-ID method because it is also based on dictionary learning. In addition, with the target data partially labelled, we can now compare with the existing fully-supervised models by training them using the labelled target data only. These include kLFDA <ref type="bibr" target="#b35">[36]</ref> and KCCA <ref type="bibr" target="#b24">[25]</ref>. The same features are used for fair comparison.</p><p>(2) Multi-task methods. cAMT <ref type="bibr" target="#b34">[35]</ref> is the latest multi-task learning method for person Re-ID to our knowledge. Based on a constrained asymmetric multi-task discriminant component analysis model, it also attempts to transfer knowledge from source tasks to target task. However the key difference is that it needs labelled data in both source datasets and the target dataset; it thus can only be compared under this semi-supervised setting. We also compare with a naive transfer learning method denoted as kLFDA N, that is, we learn kFLDA using a mix of the labelled source data and the labelled target data. Again, the same features are used for fair comparison.  <ref type="table">Table 3</ref>: Semi-supervised Re-ID results</p><p>The results are shown in <ref type="table">Table 3</ref>, from which we note that: (1) Compared to our results in <ref type="table" target="#tab_0">Table 1</ref>, all results improve, albeit by a moderate margin. This means on one hand, our model does benefit from additional labelled data from the target task; on the other hand, they are the ice on the cake as the transferred knowledge from the source task is already very discriminative for the target task. <ref type="bibr" target="#b1">(2)</ref> Compared to SSCDL, our result is much better on VIPeR and slightly worse on CAVIAR. Overall, our model is better because as a transfer learning model it can take advantage more labelled data from the source datasets. <ref type="formula" target="#formula_8">(3)</ref> The results of supervised models (kLFDA and KCCA) are much weaker than ours indicating that they require much more labelled data than the one-third to function properly 4 . (4) The naive transfer model kLFDA N failed miserably. Again this is due to the untreated domain shift problem. (5) The existing multi-task transfer Re-ID method cAMT fares even worse. This shows that a dictionary learning based multi-task model is more appropriate. This is because being originally designed for unsupervised learning, dictionary learning can exploit the unlabelled target data more naturally than the discriminant component analysis model in <ref type="bibr" target="#b34">[35]</ref> which is originally designed for supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions of Model Components</head><p>The two key model design components are evaluated: (1) the asymmetric treatment of the target task by including an additional dictionary D u T ; and (2) the stepwise reconstruction error formulation. For the former, we remove D u T in Eq. (1), and for the latter, we remove Terms 1, 3 and 4 in Eq. (1). <ref type="table" target="#tab_6">Table 4</ref> shows clearly that both components contribute positively to the final performance of the model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have developed a novel unsupervised cross-dataset transfer learning approach based on asymmetric multi-task dictionary learning. It differs significantly from existing methods in that it can exploit labelled datasets collected elsewhere whilst requiring no labelling on a target dataset. Extensive experiments show that our model is superior to existing Re-ID methods with or without transfer learning and has great potentials for real-world applications due to its high scalability, low running cost, and high matching accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Some samples of latent attributes discovered by the proposed Unsupervised Multi-task Dictionary Learning (UMDL) model. It is clear that the latent attributes are visually and semantically meaningful. (a) Upper body red. (b) Lower body black. (c) Short trousers. (d) Jeans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Algorithm 1 :</head><label>1</label><figDesc>Unsupervised Multi-task learning</figDesc><table>Input: Xt; initialise D s , D r 
t and D u 
T randomly; A r 
t → 0; 
Output: D s , D u 
T , D r 
t , A s 
t , A u 
T and A r 
t ; (t = 1, ..., T ). 
while Non-convergence do 
for t = 1 → T do 
if Source tasks then 
Fix D s ,D r 
t , and A r 
t , then calculate A s 
t by (2). 
Fix D s ,D r 
t , and A s 
t , then calculate A r 
t by (3). 
if Target task then 
Fix D s ,D r 
T , D u 
T and A r 
T , then calculate A s 

T 

and A u 
T by (2). 
Fix D s ,D r 
T , D u 
T , A s 
T and A u 
T , then calculate 
A r 
T by (3). 
Fix other terms, update D s by (4). 
for t = 1 → T do 
if Source tasks then 
Update D r 
t with fixed D s , A s 
t and A r 
t by (8). 
if Target task then 
Update D u 
T with fixed D s ,D r 
T , A s 
T , A r 
T and 
A u 
T by (6). 
Update D r 
T with fixed D s ,D u 
T , A s 
T , A r 
T and 
A u 
T by (8). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>More detailed comparisons with AdaRSVM.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1</head><label>1</label><figDesc>reports the results measured with the Rank 1</figDesc><table>matching accuracy (%) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Evaluation under unsupervised setting on the model components Running Cost On a desktop PC with two 3.20 GHz CPUs and 4G RAM running in Matlab, our model takes 12 minutes to train and 0.78 seconds to match 312 images against 312 images when VIPeR is used as the target dataset. It is thus extremely efficient during testing as a linear model.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is for simplification of notations; datasets of more than two views can be easily modelled by our model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The CMC curves of the proposed method can be found at http://pkuml.com/resources/code.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We note that when trained using fully labelled target set, their results are close to ours under the same setting showing the advantage of being a transfer model diminishes when labels are abundant.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pedestrian recognition with a learned metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetrydriven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local features are not lonely laplacian sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The re-identification challenge. Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</title>
		<meeting>IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative dictionary learning with pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person reidentification by efficient impostor-based metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AVSS</title>
		<meeting>AVSS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relaxed pairwise learned metric for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dictionary learning algorithms for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kjersti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Te-Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Terrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain transfer for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ARTEMIS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Re-id: Hunting attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Person reidentification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person re-identification by iterative reweighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matching people across camera views using kernel canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bilmbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDSC</title>
		<meeting>ICDSC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised coupled dictionary learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-domain person reidentification using domain adaptation ranking svms. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1599" to="1613" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bicov: a novel image representation for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Person re-identification over camera networks using multi-task distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3656" to="3670" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse coding for multitask and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ven Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dictionary-based domain adaptation methods for the re-identification of faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Person Re-Identification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="269" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">People reidentification in surveillance and forensics: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of generative topic saliency for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Crossscenario transfer person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Person re-identification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning midlevel filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning view-invariant sparse representations for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3176" to="3183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via a transferable dictionary pair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph regularized sparse coding for image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1327" to="1336" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Associating groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transfer reidentification: From person to set-based verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
