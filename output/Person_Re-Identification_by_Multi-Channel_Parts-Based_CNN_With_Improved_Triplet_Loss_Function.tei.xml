<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Person Re-Identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Robotics Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Robotics Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Robotics Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Robotics Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Robotics Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Person Re-Identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification across cameras remains a very challenging problem, especially when there are no overlapping fields of view between cameras. In this paper, we present a novel multi-channel parts-based convolutional neural network (CNN) model under the triplet framework for person re-identification. Specifically, the proposed CNN model consists of multiple channels to jointly learn both the global full-body and local body-parts features of the input persons. The CNN model is trained by an improved triplet loss function that serves to pull the instances of the same person closer, and at the same time push the instances belonging to different persons farther from each other in the learned feature space. Extensive comparative evaluations demonstrate that our proposed method significantly outperforms many state-of-the-art approaches, including both traditional and deep network-based ones, on the challenging i-LIDS, VIPeR, PRID2011 and CUHK01 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification is the problem of matching the same individuals across multiple cameras, or across time within a single camera. It is attracting rapidly increased attentions in the computer vision and pattern recognition research community due to its importance for many applications such as video surveillance, human-computer interaction, robotics, content-based video retrieval, etc. Despite years of efforts, person re-id remains challenging due to the following reasons: 1) dramatic variations in visual appearance and ambient environment caused by different viewpoints from different cameras; 2) significant changes in human pose across time and space; 3) background clutter and occlusions; and 4) different individuals that share similar appearances. Moreover, with little or no visible faces, in many cases the use of biometric and soft-biometric approaches is not applicable. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates some examples of the matched pairs in four challenging person re-id benchmark datasets i-LIDS <ref type="bibr" target="#b37">[38]</ref>,VIPeR <ref type="bibr" target="#b12">[13]</ref>,PRID2011 <ref type="bibr" target="#b16">[17]</ref> and CUHK01 <ref type="bibr" target="#b23">[24]</ref>. Images in each red bounding box are from the same person.</p><p>Given a query person's image, in order to find the correct matches among a large set of candidate images captured by different cameras, two crucial problems must be addressed. First, good image features are required to represent both the query and the gallery images. Second, suitable distance metrics are indispensable to determine whether a gallery image contains the same individual as the query image. Many existing studies consider the two problems separately and have focused more on the first one, that is, developing more discriminative and robust feature representations to describe a person's visual appear-ance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref>. Once the feature extraction is completed, these methods usually choose a standard distance measure such as l 1 -norm based distance (L1norm) <ref type="bibr" target="#b39">[40]</ref>, Bhattacharyya distance (Bhat) <ref type="bibr" target="#b13">[14]</ref>, or Mahalanobis distance (Maha) <ref type="bibr" target="#b32">[33]</ref> to determine the similarity between pairs.</p><p>The situation has motivated us to consider the feature and distance metric learning problems jointly to improve the person re-id performance. To extract better features for raw person images, we propose a new, multi-channel CNN model that learns features for both the input person's full body and the body parts. The full body and body parts features are concatenated together and fed into the top full-connection layer to produce the final representation of the input person. We also borrow the idea from Wang's et al. <ref type="bibr" target="#b38">[39]</ref> and the FaceNet work <ref type="bibr" target="#b33">[34]</ref> to use triplet training examples and the improved triplet loss function to further enhance the discriminative power of the learned features. In contrast to the original triplet loss function that only requires the intra-class feature distances to be less than the inter-class ones, the improved loss function further requires the intra-class feature distances to be less than a predefined margin. Our experimental evaluations show that the use of the improved triplet loss function alone can improve the person re-id accuracy by up to 4%, compared to the same DC-NN model using the original triplet loss function.</p><p>Given a person's image, the proposed CNN model outputs an 800 dimension feature representation of the input image. The proposed CNN model together with the improved triplet loss function can be considered as learning a mapping function that maps each raw image into a feature space where the difference between images of the same person is less than that of different persons. Therefore, the proposed framework can learn the optimal feature and distance metric jointly for the person re-id task.</p><p>The main contributions of this paper are twofold: 1) a novel, multi-channel CNN model that learns both the global full-body and the local parts features, and integrates them together to produce the final feature representation of the input person; 2) an improved triplet loss function that requires the intra-class feature distances to be less than not only the inter-class ones, but also a predefined threshold. Experimental evaluations results show that the proposed method achieves the state-of-the-art performances on several widely adopted person re-id benchmark test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Typical person re-id systems consist of two major components: a feature extraction method to describe the query image and the gallery images, and a distance metric for comparing those features across images. Research on person re-id problems usually focuses either on constructing robust and discriminative features, or finding an improved similarity metric for comparing features, or a combination of both.</p><p>There are a great amount of research efforts for developing better features that are at least partially invariant to lighting, pose, and viewpoint variations. Features that have been used for the person re-id task include color histograms and their variants <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46]</ref>, local binary patterns(LBP) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46]</ref>, Gabor features <ref type="bibr" target="#b22">[23]</ref>, color names <ref type="bibr" target="#b43">[44]</ref>, and other visual appearance or contextual cues <ref type="bibr" target="#b2">[3]</ref>. Quite some works have also investigated combinations of multiple visual features, including <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>A large number of metric learning and ranking algorithms have also been applied to the person re-id problem <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b30">31]</ref>. The basic idea behind metric learning is to find a mapping function from the feature space to the distance space with certain merits, such as feature vectors from the same person being closer than those from different ones. These metric learning methods mainly include Mahalanobis metric learning(KISSME) <ref type="bibr" target="#b20">[21]</ref>, Local Fisher Discriminant Analysis(LFDA) <ref type="bibr" target="#b40">[41]</ref>, Marginal Fisher Analysis(MFA) <ref type="bibr" target="#b40">[41]</ref>, large margin nearest neighbour (LMNN) <ref type="bibr" target="#b40">[41]</ref>, Locally Adaptive Decision Functions(LADF) <ref type="bibr" target="#b25">[26]</ref>, and attribute consistent matching <ref type="bibr" target="#b19">[20]</ref>. Inspired by the great success of deep learning networks in various computer vision and pattern recognition tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b15">16]</ref>, it becomes increasingly popular to apply deep convolution neural network(DCNN) models to the person re-id problem. It is worth noting that, recent state-of-the-art performances on widely used person reid benchmark datasets, such as i-LIDS, VIPeR, CUHK01, etc, are all obtained by DCNN-based methods. In the following, we briefly introduce those deep learning based approaches related to, or to be compared with our work. Wang et al. <ref type="bibr" target="#b38">[39]</ref> used triplet training examples and the triplet loss function to learn fine grained image similarity metrics. FaceNet <ref type="bibr" target="#b33">[34]</ref> and Ding et al. <ref type="bibr" target="#b5">[6]</ref> applied this triplet framework to the face and person re-identification problems, respectively. In this paper, we also borrow the idea from <ref type="bibr" target="#b38">[39]</ref> and propose an improved triplet loss function for the person re-id task. DeepReID <ref type="bibr" target="#b24">[25]</ref> proposed a novel Filter Pairing Neural Network (FPNN) that jointly handles the problems of misalignment, photometric and geometric transforms, occlusion and black cluster, etc, by using the patch matching layers to match the filter responses of local patches across views, and other convolution and maxpooling layers to model body parts displacements. mFilter <ref type="bibr" target="#b47">[48]</ref> also used the local patch matching method that learns the mid-level filters to get the local discriminative features for the person re-id task. Ahmed et al. <ref type="bibr" target="#b0">[1]</ref> proposed an improved deep learning architecture which takes pair-wise images as its inputs, and outputs a similarity value indicating whether the two input images depict the same person or not. Novel elements in their model include a layer that computes cross-input neighborhood differences to capture local relationships between the two input images based on their mid-level features, and a patch summary layer to get high-level features. Yi et al. <ref type="bibr" target="#b44">[45]</ref> constructed a siamese neural network (denoted as DeepM in our paper) to learn pairwise similarity, and also used body parts to train their CNN models. In their work, person images are cropped into three overlapped parts which are used to train three independent networks. Finally the three networks are fused at the score level.</p><p>Our CNN model differs from the above deep network based approaches in both the network architecture and the loss function. More specifically, We use a single network that consists of multiple channels to learn both the global full-body and local body-parts features. We use different convolution kernel sizes in different types of channels to look at full-body and body-parts with different resolutions, which is similar to the idea of the root/part filters in a DP-M model <ref type="bibr" target="#b8">[9]</ref>. In addition, we use an improved triplet loss function to make the features from the same person closer, meanwhile features from different persons farther away from each other. In Section 4, performance comparisons with some of the above methods will be made in our experimental evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Person Re-Id Method</head><p>In this section, we present the proposed person re-id method in details. We first describe the overall framework of our person re-id method, then elaborate the network architecture of the proposed multi-channel CNN model. Finally, we present the improved triplet loss function used to train the proposed CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Overall Framework</head><p>As illustrated in figure 2, similar to the works in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b33">34]</ref>, the proposed person re-id method uses triplet examples to train the network. Denote by I i =&lt; I o i , I + i , I − i &gt; the three input images forming the i-th triplet, where I o i and I + i are from the same person, while I − i is from a different person. Through the three CNNs that share the parameter set w, i.e., weights and biases, we map triplets I i from the raw image space into a learned feature space, where I i is represented</p><formula xml:id="formula_0">as φ w (I i ) =&lt; φ w (I o i ), φ w (I + i ), φ w (I − i ) &gt;.</formula><p>Each CNN in the figure is a proposed multi-channel CNN model that is able to extract both the global full-body and local bodyparts features. When the proposed CNN model is trained using the improved triplet loss function, the learned feature space will have the property that the distance between φ w (I o i ) and φ w (I + i ) is less than not only the distance between φ w (I o i ) and φ w (I − i ), but also a predefined margin. The improved loss function aims to pull the instances of the same person closer, and at the same time push the instances . Triplet training framework. Triplet training images are fed into three network models with the shared parameter set. The triplet loss function is used to train the network models, which makes the distance between the matched pairs less than not only a predefined threshold, but also that of the mismatched pairs in the learned feature space.</p><p>belonging to different persons farther from each other in the learned feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Channel Parts-based CNN Model</head><p>The proposed multi-channel CNN model mainly consists of the following distinct layers: one global convolution layer, one full-body convolution layer, four body-part convolution layers, five channel-wise full connection layers, and one network-wise full connection layer. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the global convolution layer is the first layer of the proposed CNN model. It consists of 32 feature maps with the convolution kernel of 7 × 7 × 3 and the stride of 3 pixels. Next, this global convolution layer is divided into four equal parts P i , i = {1, . . . , 4}, and each part P i forms the first layer of an independent body-part channel that aims to learn features for the respective body part. A full-body channel with the entire global convolution layer as its first layer is also established to learn global full-body features of the input persons. The four body-part channel together with the full-body channel constitute five independent channels that are trained separately from each other.</p><p>The full-body channel is configured as follows: The global convolution layer, max pooling, the full-body convolution layer, another max pooling, and a full-connection layer. The kernel size for max pooling is 3 × 3, and the full-connection layer generates an output of 400 dimensions. The four body-part channels have the same configuration as follows: The copy of one of the four equally divided parts of the global convolution layer, the body-part convolution Each Pi-conv1 forms the first layer of an independent body-part channel, which is followed by a body-part convolution layer denoted as Pi-conv2, and a channel-wise full connection layer denoted as Pi-fc. The fullbody channel consists of max pooling of G-conv1 denoted as B-pool1, full-body convolution layer denoted as B-conv2, another max pooling denoted as B-pool2, and a channel-wise fully connection layer denoted as B-fc. The network-wise full connection layer is denoted as N-fc.</p><p>layer, no max pooling, and a full-connection layer. The fullconnection layer generates an output of 100 dimensions. Because the full-body convolution layer and the four bodypart convolution layers aim to learn the global full-body and the local body-parts features, respectively, we use the convolution size of 5 × 5 for the former and a smaller size of 3 × 3 for the latter. This serves to learn finer grain local features for persons' body parts. Both types of convolution layers use the stride of 1. Note that all the convolution layers in our CNN model contain a relu layer to produce their outputs.</p><p>The above network configuration achieves state-of-theart person re-id accuracies on relatively small benchmark datasets. In our experiments, we found that for some larger datasets such as CUHK01, constructing each of the five separate channels with two convolution layers lead to a much better result. Therefore, we use two network configurations to handle small and large benchmark datasets, respectively. The two network configurations are mostly the same except for the number of convolution layers (one or two) in each separate channel.</p><p>At the final stage, the outputs of the channel-wise full connection layers from the five separate channels are concatenated into one vector, and is fed into the final networkwise full connection layer. The multi-channel structure described above enables learning of the global full-body and local body-parts features jointly, and the fusion of these two types of features at the final stage leads to remarkable improvements of person re-id accuracies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Improved Triplet Loss Function</head><formula xml:id="formula_1">φ w (I i ) =&lt; φ w (I o i ), φ w (I + i ), φ w (I − i ) &gt;.</formula><p>The similarities between the triplet images</p><formula xml:id="formula_2">I o i , I + i , I − i are measured by the L 2 -norm distances between φ w (I o i ), φ w (I + i ), φ w (I − i )</formula><p>. The original triplet loss function requires that distance of the pair (φ w (I o i ), φ w (I − i )) be larger than that of the pair (φ w (I o i ), φ w (I + i )) by a predefined margin, and uses the following equation to enforce this requirement:</p><formula xml:id="formula_3">d n (I o i , I + i , I − i , w) = (1) d(φ w (I o i ), φ w (I + i )) − d(φ w (I o i ), φ w (I − i )) ≤ τ 1 .</formula><p>In the equation τ 1 is negative. However, since this loss function does not stipulate how close the pair (φ w (I o i ), φ w (I + i )) should be, as a consequence, instances belonging to the same person may form a large cluster with a relatively large average intra-class distance in the learned feature space. Clearly, this is not a desired outcome, and will inevitably hurt the person re-id performance.</p><p>Based on the above observation, we add a new term to the original triplet loss function to further require that distance of the pair (φ w (I o i ), φ w (I + i )) be less than a second margin τ 2 , and that τ 2 be much smaller than |τ 1 |. Translating this statement into equation, we have:</p><formula xml:id="formula_4">d p (I o i , I + i , w) = d(φ w (I o i ), φ w (I + i )) ≤ τ 2 .<label>(2)</label></formula><p>The improved loss function aims to pull the instances of the same person closer, and at the same time push the instances belonging to different persons farther from each other in the learned feature space. This is more consistent with the principal used by many data clustering and discriminative analysis methods.</p><p>In summary, the improved triplet loss function is defined as follows:</p><formula xml:id="formula_5">L(I, w) = 1 N N ∑ i=1 (max{d n (I o i , I + i , I − i , w), τ 1 } inter−class−constraint + β max{d p (I o i , I + i , w), τ 2 } intra−class−constraint ),<label>(3)</label></formula><p>where N is the number of triplet training examples, β is a weight to balance the inter-class and intra-class constraints. In our implementation, the distance function d(., .) is defined as the L 2 -norm distance,</p><formula xml:id="formula_6">d(φ w (I o i ), φ w (I + i )) = ||φ w (I o i ) − φ w (I + i )|| 2 . (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The Training Algorithm</head><p>We use the stochastic gradient decent algorithm to train the proposed CNN achitecture model with the improved triplet loss function. The derivatives of Eq.(3) can be computed as follows: </p><formula xml:id="formula_7">∂L(I, w) ∂w = 1 N N ∑ i=1 h 1 (I i , w) + 1 N N ∑ i=1 h 2 (I i , w) (5) h 1 (I i , w) =    ∂d n (I o i , I + i , I − i , w) ∂w d n (I o i , I + i , I − i , w) &gt; τ 1 0 d n (I o i , I + i , I − i , w) ≤ τ 1 .<label>(6)</label></formula><formula xml:id="formula_8">h 2 (I i , w) =    β ∂d p (I o i , I + i , w) ∂w d p (I o i , I + i , w) &gt; τ 2 0, d p (I o i , I + i , w) ≤ τ 2 .<label>(7)</label></formula><formula xml:id="formula_9">∂d n ∂w = 2(φ w (I o i ) − φ w (I + i )) ∂φ w (I o i ) − ∂φ w (I + i ) ∂w − 2(φ w (I o i ) − φ w (I − i )) ∂φ w (I o i ) − ∂φ w (I − i ) ∂w .<label>(8)</label></formula><formula xml:id="formula_10">∂d p ∂w = 2(φ w (I o i ) − φ w (I + i )) ∂φ w (I o i ) − ∂φ w (I + i ) ∂w ,<label>(9)</label></formula><p>From the above derivations, it is clear that the gradient on each input triplet can be easily computed given the values of</p><formula xml:id="formula_11">φ w (I o i ), φ w (I + i ), φ w (I − i ) and ∂φw(I o i ) ∂w , ∂φw(I + i ) ∂w , ∂φw(I − i ) ∂w</formula><p>, which can be obtained by separately running the standard forward and backward propagations for each image in the triplet examples. As the algorithm needs to go though all the triplets in each batch to accumulate the gradients for each iteration, we call it the triplet-based stochastic gradient descent algorithm. Algorithm 1 shows the main procedures of the training algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Data augmentation: Data augmentation is an important mean for increasing the volume of training data, and for alleviating the over-fitting problem. In our implementation, we resize all the images into 100 × 250 pixels. During the training process, we crop a center region of 80 × 230 pixels with a small random perturbation from each image to augment the training data.</p><p>Setting training parameters: The weights are initialized from two zero-mean Gaussian distributions with the standard deviations of 0.01 and 0.001, respectively. The bias terms are set to 0. We generate the triplets as follows: For each batch of 100 instances, we select 5 persons and generate 20 triplets for each person in each iteration. In each triplet, the matched reference is randomly selected from the same class, and the mismatched one is also randomly selected, but from the remaining classes. In our experiments, the parameters τ 1 , τ 2 , β in Eq.(3) are set to −1, 0.01 and 0.002, respectively.</p><p>Datasets: We use four popular person re-id benchmark datasets, i-LIDS, PRID2011, VIPeR and CUHK01, for performance evaluations. All the datasets contain a set of persons, each of whom has several images captured by different cameras. The following is a brief description of these four datasets:</p><p>i-LIDS dataset: It is constructed from video images shooting a busy airport arrival hall. It contains 479 images from 119 persons, which are normalized to 128 × 64 pixels. Each person has four images in average. These images are captured by non-overlapping cameras, and are subject to large illumination changes and occlusions. Evaluation protocol We adopt the widely used cumulative match curve (CMC) metric for quantitative evaluations. For each dataset, we randomly select about half of the persons for training, and the remaining half for testing. For datasets with two cameras, we randomly select one image of a person from camera A as a query image and one image of the same person from camera B as a gallery image. For multi-camera datasets, two images of the same individual are chosen: one is used as a query and the other as a gallery image. The gallery set comprises one image for each person. For each image in the query set, we first compute the distance between the query image and all the gallery images using the L2 distance with the features produced by the trained network, and then return the top n nearest images in the gallery set. If the returned list contains an image featuring the same person as that in the query image at k-th position, then this query is considered as success of rank k. We repeat the procedure 10 times, and use the average rate as the evaluation result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Evaluations</head><p>Our proposed person re-id method contains two novel ingredients: 1) the multi-channel CNN model that is able to learn both the global full-body and the local body-parts features, 2) the improved triplet loss function that serves to pull the instances of the same person closer, and at the same time push the instances belonging to different persons farther from each other in the learned feature space. To reveal how each ingredient contributes to the performance improvement, we implemented the following four variants   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top1 Top10 Top20 Top50 Top100 KISSME <ref type="bibr" target="#b20">[21]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top1 Top5 Top10 Top15 Top20 Top30 mFilter <ref type="bibr" target="#b47">[48]</ref> 34 Note that, since the CUHK01 dataset is much larger than the other three datasets, we choose to model it using a larger configuration with an additional convolution layer in each of the five channels. The derived models corresponding to Variant 1 to 4 are denoted as Ours3T, Ours3TC, Ours3TP, and Ours3TPC, respectively. <ref type="table" target="#tab_2">Table 1</ref>, 2, 3, and 4 show the evaluation results on the four benchmark datasets, respectively, using the top 1, 5, 10, 15, 20, and 30 ranking accuracies. Each table includes 11 to 14 representative methods that have reported evaluation results on the corresponding dataset. Some of the works in these tables, such as Ding's method <ref type="bibr" target="#b5">[6]</ref>, FPPN  <ref type="bibr" target="#b24">[25]</ref>, DeepM <ref type="bibr" target="#b44">[45]</ref>, mFilter <ref type="bibr" target="#b47">[48]</ref> and Ejaz's <ref type="bibr" target="#b0">[1]</ref> all used DC-NN models to learn features for the person re-id task, and their performance accuracies are near the top in the list. Among these works, DeepM also used body parts to train their CNN models. In contrast to our single network with multiple channels, this work divides person images into three overlapped parts, and uses them to train three independent networks. The three networks are fused at the score level. There are also some works, such as Sakrapee's method <ref type="bibr" target="#b30">[31]</ref>, mFilter+LADF <ref type="bibr" target="#b47">[48]</ref>, that combine several different approaches to boost the performance accuracies. These ensemble methods have achieved state-of-the-art performances so far.</p><p>Compared to the above representative works, the OursTCP model has achieved the top performances on all the four datasets, with all the six ranking measurements. The evaluation results shown in the four tables can be summarized as follows.</p><p>• Compared to Sakrapee's ensemble-based method, which is the state-of-the-art method so far, the OursTCP model is slightly better on CUHK01 dataset, but remarkably outperforms the former on the remaining three datasets by a margin of 2% to 10%.</p><p>• The improved triplet loss function is able to improve the performance accuracies for both the single and multi-channel models. Training a model with this loss function can get up to 4% performance improvement compared to the same model trained with the original triplet loss function.</p><p>• The multi-channel model that explores both the global full-body and local body-parts features is very powerful and effective for improving the performance accuracies. Compared to the model with no parts information in the structure, it can boost the person re-id accuracy by up to 13%.</p><p>As defined by Eq.(3), the improved triplet loss function contains two terms: the intra-class and the inter-class constraints. To investigate the effect of the parameter β on the performance accuracy, we conducted experiments using cross validation method on the VIPeR dataset, and the results are shown in <ref type="table" target="#tab_5">Table 5</ref>. We can clearly see that our proposed person re-id method yields the best performances </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of different body parts</head><p>To understand the contribution of different body regions to the person re-id performance accuracy, we trained four different network models that contains the full-body channel and one body-part channel which corresponds to the bodypart 1, 2, 3 and 4, respectively. These four models are denoted as Ours-Part1, Ours-Part2, Ours-Part3, and Ours-Part4, respectively. We also included the models of OursT and OursTP for comparisons. The experiments are performed on the VIPeR dataset, and the performance accuracies are shown in <ref type="figure" target="#fig_8">Figure 4</ref>. It is interesting to observe that the body part 1, which includes the face and shoulder of a person, leads to the largest performance improvement. When we move down the body, the performance improvement gradually decreases, with the body part 4, which includes the legs and feet of a person, providing the least performance improvement. This result is not surprising, because legs and feet are the moving parts of a person, which change dramatically in shape and pose. Such parts provide the least reliable features, and hence contribute little to the person re-id task.</p><p>We have visualized the features learned by each convolution layer, which are shown in <ref type="figure" target="#fig_9">Figure 5</ref>. We can see that the second convolution layer of the full-body channel captures the global information of each person, while the second convolution layers of the four body-parts channels capture the detailed local body-parts features of a person. Therefore, such a joint representation and learning framework for the global full-body and local body-parts features can achieve superior performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel multi-channel partsbased convolutional network for person re-identification problem, which is formulated under a triplet framework via an improved triplet loss function. In this framework, we Input G-conv1 B-conv2 Pi-conv2 constructed a CNN architecture including both global body convolution layer and local parts convolution layers. Thus the feature representations learned by our model can contain global information and local detailed properties. The architecture is trained by a set of triplets to produce features that aims to pull the instances of the same person closer, meanwhile push the instances belonging to different persons farther from each other in the learned feature space via the organized triplet samples. And our model got stateof-the-art performance on most benchmark datasets. In the future, we will extend our framework and approach to other task such as image and video retrieval problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Matched examples in datasets i-LIDS, VIPeR, CUHK01 and PRID2011. Each row shows matched examples from the same dataset. Images in a red bounding box contain the same person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2. Triplet training framework. Triplet training images are fed into three network models with the shared parameter set. The triplet loss function is used to train the network models, which makes the distance between the matched pairs less than not only a predefined threshold, but also that of the mismatched pairs in the learned feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Network Architecture of the proposed multi-channel C-NN model. The first layer is called the global convolution layer donated as G-conv1. It is then divided into four equal parts, denoted as Pi-conv1, where i = {1, . . . , 4}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>As described in 3.1, we use triplet examples to train the network model. Given a triplet I i =&lt; I o i , I + i , I − i &gt;, the network model maps I i into a learned feature space with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>proposed person re-id method, and compared them with a dozen of representative methods in the literature: Variant 1 (denoted as OursT): We remove the four bodypart channels from the proposed CNN model and use the original triplet loss function to train the network. Variant 2 (denoted as OursTC): We use the same network model as OursT, but use the improved triplet loss function to train the network instead. Variant 3 (denoted as OursTP): We use the full version of the proposed multi-channel CNN model and train it with the original triplet loss function. Variant 4 (denoted as OursTPC): We use the same network model as OursTP, but train it with the improved triplet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Analysis of different body parts on VIPeR dataset. when β is in the range of 0.001 to 0.003. Based on this observation, we set β to 0.002 in all our experimental evaluations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Learned feature maps of the network. G-conv1 shows features learned by the global convolution layer. B-conv2 represents the features learned by the full-body convolution layer which captures salient global full-body features, while Pi-conv2 capture salient local body-parts features learned by the body-part convolution layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Experimental evaluations on i-LIDS dataset.</figDesc><table>Method 
Top1 Top5 Top10 Top15 Top20 Top30 
Adaboost[14]29.6 55.2 68.1 77.0 82.4 92.1 
LMNN[41] 28.0 53.8 66.1 75.5 82.3 91.0 
ITML[5] 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Experimental evaluations on PRID2011 dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Experimental evaluations on VIPeR dataset.</figDesc><table>Method 
Top1 Top5 Top10 Top15 Top20 Top30 
MtMCML[29] 
28.8 59.3 75.8 83.4 88.5 93.5 
SDALF[8] 
19.9 38.4 49.4 58.5 66.0 74.4 
eBiCov[28] 
20.7 42.0 56.2 63.3 68.0 76.0 
eSDC[47] 
26.3 46.4 58.6 66.6 72.8 80.5 
PRDC[49] 
15.7 38.4 53.9 63.3 70.1 78.5 
aPRDC[27] 
16.1 37.7 51.0 59.5 66.0 75.0 
PCCA[30] 
19.3 48.9 64.9 73.9 80.3 87.2 
KISSME[21] 
19.6 48.0 62.2 70.9 77.0 83.7 
SalMatch[46] 
30.2 52.3 66.0 73.4 79.2 86.0 
LMLF[48] 
29.1 52.3 66.0 73.9 79.9 87.9 
Ding[6] 
40.5 60.8 70.4 78.3 84.4 90.9 
mFilter+LADF[48]43.4 −− −− −− −− −− 
Sakrapee[31] 
45.9 −− −− −− −− −− 
OurT 
34.3 55.6 65.1 71.7 74.4 81.7 
OurTC 
37.2 55.6 67.1 76.5 75.3 83.9 
OurTP 
43.8 69.5 79.7 81.0 85.4 90.2 
OurTCP 
47.8 74.7 84.8 89.2 91.1 94.3 

Table 4. Experimental evaluations on CUHK01 dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Analysis the parameter β on VIPeR dataset. 45.9 73.4 81.9 87.0 93.0 95.6 0.002 47.8 74.7 84.8 89.2 91.1 94.3 0.003 45.6 75.3 85.4 87.6 90.5 94.6 0.004 43.7 73.1 81.5 87.9 91.1 93.4</figDesc><table>β 
Top1 Top5 Top10 Top15 Top20 Top30 
0 
43.8 69.5 79.7 81.0 85.4 90.2 
0.001 </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person re-identification using spatial covariance regions of human body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Corvee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
	<note>Seventh IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Complex event detection using semantic saliency and nearly-isotonic svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature mining for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1528" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Metric learning by collapsing classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</title>
		<meeting>IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Is that you? metric learning approaches for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="498" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Twostage learning to predict human eye fixations via sdaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person reidentification by efficient impostor-based metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal-Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
	<note>IEEE Ninth International Conference on</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Principal axis-based correspondence between multiple cameras for people tracking. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="663" to="671" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint learning for attribute-consistent person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="134" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3594" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3610" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Person reidentification: What features are important</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="391" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bicov: a novel image representation for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Person re-identification over camera networks using multi-task distance metric learning. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3656" to="3670" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01543</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vise: Visual search engine using multiple networked cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kitahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kogure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hagita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1204" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mahalanobis distance learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Person Re-Identification</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="247" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03832</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning discriminative appearance-based models using partial least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="322" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Home office i-lids multiple camera tracking scenario definition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shape and appearance context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
