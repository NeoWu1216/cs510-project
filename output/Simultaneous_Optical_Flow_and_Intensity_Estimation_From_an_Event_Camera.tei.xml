<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simultaneous Optical Flow and Intensity Estimation from an Event Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bardow</surname></persName>
							<email>p.bardow14@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="laboratory">Dyson Robotics Laboratory at Imperial College</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
							<email>a.davison@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="laboratory">Dyson Robotics Laboratory at Imperial College</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
							<email>s.leutenegger@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="laboratory">Dyson Robotics Laboratory at Imperial College</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simultaneous Optical Flow and Intensity Estimation from an Event Camera</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Event cameras are bio-inspired vision sensors which mimic retinas to measure per-pixel intensity change rather than outputting an actual intensity image. This proposed paradigm shift away from traditional frame cameras offers significant potential advantages: namely avoiding high data rates, dynamic range limitations and motion blur. Unfortunately, however, established computer vision algorithms may not at all be applied directly to event cameras. Methods proposed so far to reconstruct images, estimate optical flow, track a camera and reconstruct a scene come with severe restrictions on the environment or on the motion of the camera, e.g. allowing only rotation. Here, we propose, to the best of our knowledge, the first algorithm to simultaneously recover the motion field and brightness image, while the camera undergoes a generic motion through any scene. Our approach employs minimisation of a cost function that contains the asynchronous event data as well as spatial and temporal regularisation within a sliding window time interval. Our implementation relies on GPU optimisation and runs in near real-time. In a series of examples, we demonstrate the successful operation of our framework, including in situations where conventional cameras suffer from dynamic range limitations and motion blur.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The 'silicon retina' or event camera appears to offer enormous potential for a new level of performance in realtime geometric vision, and in the longer term a drive towards dramatically more efficient algorithms. It discards the frame-based paradigm of standard cameras and instead adopts a bio-inspired approach of independent and asynchronous pixel brightness change measurement. The highly appealing promise is that all of the information contained in a standard video stream of tens or more Megabytes of data per second is present in a natural and much compressed event stream of only tens or hundreds of kilobytes per sec- ond -all of the redundancy of sending regular, unchanging values where motion and intensity change is small is removed. As systems like LSD-SLAM <ref type="bibr" target="#b7">[8]</ref> very clearly highlight, pixels where edges move are the only ones which give information useful for tracking and reconstruction, and it is precisely these locations which are highlighted in hardware by an event camera. On top of this, the pixels of an event camera with their low latency, microsecond-timestamped response and shutter-free independent intensity measurement offer the prospect of extreme high speed tracking and management of high dynamic range scenes. However, since the original invention of the silicon retina <ref type="bibr" target="#b13">[14]</ref> and the release by iniLabs of the Dynamic Vision Sensor (DVS) <ref type="bibr" target="#b12">[13]</ref> as a research device several years ago, adoption of these cameras by the wider computer vision community has been minor. Standard vision algorithms simply do not transfer to an event camera. Traditional cameras capture images: synchronous intensity measurements; but events are asynchronous reports of local intensity difference, and there is no global or persistent measurement of intensity which would permit normal approaches to correspondence to be used (feature extraction, patch alignment).</p><p>In this paper we take a significant step towards proving the general potential of event cameras for motion and structure estimation by presenting the first algorithm which simultaneously estimates scene intensity and motion with minimal assumptions about the type of scene and motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>'Optical Flow' is in the title of our paper, because a key goal is to recover a generic motion field from camera data, but it is important to clarify the difference between the common understanding of this term in computer vision and the rather different type of estimation our algorithm achieves. Optical flow is normally understood as the correspondence field between two temporally close intensity images -an estimation of two parameters per pixel, the horizontal and vertical displacement between one frame and the next. In our case, we use the time stream of event data to estimate a continuously varying motion field at any time resolution. Therefore, it is more precise to say that we estimate a continuously time-varying velocity field in image coordinates.</p><p>Measuring velocity as a camera observes an arbitrary moving scene requires and implies knowledge of the correspondence between entities at different points in time, and this is where we face a particular challenge when the only input is event data which does not directly record image intensity or even oriented edges. We can only interpret an event as a measurement of motion if we know about the intensity gradient in the scene; but on the contrary we can only interpret it as a measurement of intensity gradient if we know the motion. Therefore, we must formulate a joint estimation problem to recover both motion and intensity together. We will show that weak assumptions about regularity in the overall solutions for motion and intensity are enough to allow this.</p><p>Previous computer vision work using event cameras, both the DVS and the related Asynchronous Time-based Image Sensor (ATIS) <ref type="bibr" target="#b18">[19]</ref>, has not aimed at such generic estimation, but instead focused on reduced problems. The first examples presented of the use of the DVS were for the tracking of simple objects to enable reactive robotic control (e.g.ball blocking <ref type="bibr" target="#b6">[7]</ref> or pencil balancing <ref type="bibr" target="#b4">[5]</ref>), highlighting the new level of performance enabled by the event camera's extremely low latency. Object tracking tasks like these have generally been tackled from a static camera using template fitting, or even more simply by finding the mean pixel coordinates of current event activity. These methods do not extend to tracking the independent motion of multiple, variable and highly textured objects.</p><p>Increasingly, authors have attempted to apply event cameras to more sophisticated vision tasks. In <ref type="bibr" target="#b14">[15]</ref> an event camera was used to estimate the rapid motion of a quadcopter by using events to track a known target at low latency. Extending this, in <ref type="bibr" target="#b2">[3]</ref> the image from a standard CMOS camera was used to provide a target for event tracking to produce a visual odometry system. Neither of these systems performed intensity reconstruction from event data.</p><p>The pieces of work which are most closely related to our approach are Benosman et al.'s optical flow estimation technique <ref type="bibr" target="#b0">[1]</ref> and Kim et al.'s work on Simultaneous mosaicing and tracking <ref type="bibr" target="#b11">[12]</ref>. In <ref type="bibr" target="#b0">[1]</ref> the authors recovered a motion field without explicitly estimating image intensity, by assuming that events which fire spatially and temporally close to each other can be put into correspondence and locally fitting spatiotemporal planes to these. In a scene with sharp edges and monochromatic blocks this works well, but this approach has trouble in more complicated environments.</p><p>After some initial work by Cook et al. on integrating events into interacting maps <ref type="bibr" target="#b5">[6]</ref>, Kim et al. <ref type="bibr" target="#b11">[12]</ref> demonstrated the first true high quality joint estimation of scene intensity and motion from event data, but under the strong assumption that the only movement is due to pure cam-era rotation in an otherwise static scene. They were able to demonstrate high quality intensity recovery, including super-resolution and HDR aspects, from purely event data. It was these results in particular which inspired us to work towards more generic estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our algorithm is formulated as sliding window variational optimisation, and has much in common with well known variational methods for estimating two-view optical flow from standard video <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>. We pre-define an optimisation time window T , and within this a fine time discretisation δ t . We take all of the events in time window T as input and solve jointly for the velocity field u and log intensity L at all cells in the associated spatio-temporal volume. We then slide the optimisation forward to a highly overlapping position, initialising the values of all cells to either previous estimates or predictions, and solve again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Event Camera Definitions</head><p>Each pixel of an event camera independently measures intensity and reports an event when that intensity changes by a pre-defined threshold amount relative to a saved value. The pixels operate asynchronously but each event is timestamped relative to a global clock. The electronics of the camera gather events from all pixels and transmit them to a computer as a serial stream.</p><p>Each event is therefore defined as a tuple e i = (x i , t i , ρ i ) ⊤ , where x i ∈ Ω is the position of the event in the image domain, t i is its time-stamp to microsecond resolution and ρ i ± 1 is its polarity (sign of the brightness change). An event is fired when a change in that log intensity exceeds threshold θ:</p><formula xml:id="formula_0">|L(x, t) − L(x, t p (x, t))| ≥ θ ,<label>(1)</label></formula><p>where L(x, t) is the log intensity at pixel x at time t and t p (x, t) is the time when the previous event occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Estimation Preliminaries</head><p>We aim to estimate continously varying image velocity u and log intensity L at all image pixels over the duration of our input event sequence. The log intensity is related to image intensity as follows: L := log(I + b), where b is a positive offset constant. Since event cameras do not come with any notion of frames, we note that u is in velocity units of pixels/second rather than a frame-to-frame displacement. For brevity we will write partial derivatives as u x := ∂u ∂x . As in well-known two-view methods for optical flow estimation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, a key assumption of our algorithm is brightness constancy, which asserts that the brightness value of a moving pixel is unchanged. In differential form this is:</p><formula xml:id="formula_1">I(x + δ t u, t + δ t ) = I(x, t).<label>(2)</label></formula><p>On a per-pixel basis, this equation is under-determined. As in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref> and many other optical flow methods, we need to introduce regularisation and perform global optimisation in order to achieve a well-defined solution across the whole image domain simultaneously. But in the case of the input data from an event camera, Equation (2) cannot be directly applied since event measurements do not provide absolute intensity information but only differences. To proceed we must formulate our problem as simultaneous estimation of both intensity and velocity. The details of our approach follow in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Variational Formulation</head><p>As mentioned in the previous section, we add regularisers to (2) to determine the system and to handle the sparse measurements from the event camera. These are, in essence, smoothness priors which have been applied in many image processing application for standard cameras, such as optical flow <ref type="bibr" target="#b22">[23]</ref>, image denoising <ref type="bibr" target="#b20">[21]</ref> and SLAM <ref type="bibr" target="#b15">[16]</ref>. Variational methods have been very successful to include smoothness priors such as TV-L 1 <ref type="bibr" target="#b17">[18]</ref>, which approximates natural image statistics <ref type="bibr" target="#b19">[20]</ref>, while leaving the optimisation problem convex.</p><p>With event cameras, smoothness priors allow us to estimate image regions in between events -both spatially and temporally. In other words, we have sensor regions where events are firing and giving information about gradients, and we regions with no data and no events firing, but we can assume smoothness in the absence of events.</p><p>Since an event relates an intensity in the past, i.e. previous event to an intensity at the current event time-stamp, we opt for assimilating the event measurement data to the spatio-temporal smoothness and photometric consistency (optical flow constraint) within a time window. As opposed to traditional optical flow estimation, intensities are unknown, and so is optical flow. Since both quantities are coupled by the optical flow constraint (2), we need to estimate both jointly. We assume that the intensity change at a pixel is induced only by optical flow.</p><p>We therefore propose the following minimisation:</p><formula xml:id="formula_2">min u, L Ω T λ 1 u x 1 + λ 2 u t 1 + λ 3 L x 1 + λ 4 L x , δ t u + L t 1 + λ 5 h θ (L − L(t p )) dt dx + Ω |P (x)| i=2 L(t i ) − L(t i−1 ) − θρ i 1 dx,<label>(3)</label></formula><p>where the individual λs are positive scalar weights. For brevity we omit the parameters x and t. Readers who are familiar with optical flow and image denoising will recognise the first four terms, which regularise the smoothness of the flow (both spatially and temporally) and the smoothness of the intensities. The fourth term is the first order Taylor approximation of (2), with ·, · being the inner product, which ensures temporal consistency in our intensity estimates. The last two terms of (3) are the data terms of the event camera: The event data term and the no-event data term. The event data term is derived from (1), where P (x) is the set of all events fired at x, with t i and ρ i being the timestamp and polarity of the i-th element in P (x). For this term we assume that events are fired as soon as the threshold θ in log intensity is reached. While this term models events which have been fired, the no-event data term models the case of no events occurring on a certain pixel: the absence of events gives us the information that the log intensity, after the last event, has not changed more than the given threshold θ. Therefore, we constrain the intensity between two events with an L 1 -norm cost term containing a dead-zone, which is denoted by h θ . h θ is defined as</p><formula xml:id="formula_3">h θ (x) = |x| − θ, if |x| &gt; θ 0, otherwise,<label>(4)</label></formula><p>and takes as input the difference between L and L(t p ), which is the log intensity at last event relative to L. By using the dead zone, the term does not add any cost when the difference is in bound of [−θ, θ], but penalises deviation beyond. We use an L 1 -norm, in both terms, as we anticipate outliers. Events may be missed by the chip specifically in a short period just after an event has fired -and we anticipate randomly firing events (background noise), which occur in the sensor due to leakage <ref type="bibr" target="#b12">[13]</ref>. Next we will describe the discretisation and minimisation of (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discretisation</head><p>As described in the previous section, we estimate L and u over a time period T and over the image domain Ω. For the minimisation we discretise Ω into a regular pixel grid of size M × N , and T into K cells each of length δ t microseconds, which forms a spatio-temporal volume. For each element in the volume created we estimate L and the motion u by minimising the discretised version of (3). After the minimisation, we then slide the window in time by δ t and minimise again. <ref type="figure">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>visualises this scheme.</head><p>Important here is the choice of δ t , since a larger choice of δ t allows to estimate slower motions, while a smaller value is good for fast motions. We use a constant value for δ t , but in future work this choice may be automated by adopting it to the rate of incoming events. To adapt our event data term to a lower temporal resolution we linearly interpolate the intensity at the time of each event, as described in <ref type="figure">Figure 4</ref>.</p><p>After shifting the sliding window, the oldest estimates and related event data terms are dropping out while we add new incoming events to the window. These new elements in our volume are initialised by assuming a constant motion <ref type="figure">Figure 3</ref>: The sliding window (green box) bins the incoming positive events (red bars) and negative events (blue bars) into a regular grid (dashed lines). When the minimisation converges, the window is shifts to the right. from our previous estimate. For this we "copy" the previous estimates of u into the new grid cells and use the newly "copied" motion vectors u to bilinearly interpolate from the previous log intensity estimate.</p><p>Unfortunately, by deleting estimates from the oldest band of grid cells, we lose information about that period. To compensate this loss, we constrain the oldest estimates in our sliding window with a prior image and penalise deviations from those values in the next minimisation. The idea is that consecutive estimates by the sliding window should be discouraged to discard previous estimates, since they are based on measurements, which are not included anymore. To include those priors, we add λ 6 L(x, t 1 ) −L(x) 2 2 to (3), whereL ∈ Ω is the prior image and t 1 is the first event time-stamp at x in our sliding window. If there is no such t 1 at x, then we use instead of t 1 the minimum of T . Before each minimization, we updateL(x) by copying the values of L(x, t 1 ) for all pixels and if there are no events at x, because they dropped out of the sliding window, we leave the value inL unchanged. By using this prior image scheme, we can mitigate the loss of information to a certain extent.</p><p>We also want to point out that the prior image scheme would allow to include intensity measurements from other sources, e.g. with DAVIS240 <ref type="bibr" target="#b1">[2]</ref>, which provides a standard camera image besides an event stream. However, in this work we focus on exclusively analysing the event stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Optimisation</head><p>To minimise (3) we use the preconditioned primal-dual algorithm <ref type="bibr" target="#b16">[17]</ref>. The advantage of that scheme is its optimal convergence and that it is easily parallelisable. To use the primal dual algorithm we use the duality principle and replace individual L 1 -norms of (3) by their conjugate using the Legendre-Fenchel transform <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_4">min u, L max |a|∞≤λ1 |b|∞≤λ2 |c|∞≤λ3 |d|∞≤λ4 |y|∞≤1 D x u, a − δ λ1A (a) + D t u, b − δ λ2B (b) + D ′ x L, c − δ λ3C (c) + D ′ x L, u δ t + D ′ t L, d − δ λ4D (d) + λ 6 h θ (L−L(t p ))+ EL−z, y −δ Y (y),<label>(5)</label></formula><p>where D x , D ′</p><p>x represent the finite difference matrices with respect to x and D t , D ′ t are the difference matrices with respect to t. The individual δ(.) terms are the indicator functions regards to the dual variables, such that for example δ λ1A (a) = 0 if a ≤ λ 1 , otherwise ∞. The other indicator functions are defined likewise. Note that we reformulate the event data term to the matrix expression EL − z, where z is our measurement vector containing signs of all observed events scaled by θ and E is the event matrix which transforms the intensity estimate to pairwise differences of linearly interpolated intensities of the observed events.</p><p>The optical flow term in <ref type="formula" target="#formula_4">(5)</ref> is biconvex, due to the inner product of L x and u. We following here the minimisation strategy for a biconvex function in <ref type="bibr" target="#b8">[9]</ref> by minimising this term by alternating between the estimation for L and u. This gives us the following minimisation scheme of (5):</p><formula xml:id="formula_5">                                                   L n+1 = (I + T 1 λ 6 ∂h θ ) −1 (L n − T 1 (D ′ x ⊤ c n − D ′ x ⊤ (ū n d n ) − D ′ t ⊤ d n − E ⊤ y n )) L n+1 = 2L n+1 −L n u n+1 = (I + T 2 λ 4 ∂G) −1 (ū n − T 2 (D x ⊤ a n − D ⊤ t b n ))) u n+1 = 2u n+1 −ū n a n+1 = (I + Σ 1 ∂F * 1 ) −1 (a n + Σ 1 D xū n+1 ) b n+1 = (I + Σ 2 ∂F * 2 ) −1 (b n + Σ 2 D tū n+1 ) c n+1 = (I + Σ 3 ∂F * 3 ) −1 (c n + Σ 3 D ′ xL n+1 ) d n+1 = (I + Σ 4 ∂F * 4 ) −1 (d n + Σ 4 ( D ′ xL n+1 ,ū n+1 + D ′ tL n+1 )) y n+1 = (I + Σ 5 ∂F * 5 )(d n + Σ 5 (EL n+1 − z))</formula><p>, where Σ i and T i are diagonal pre-conditioning matrices as described in <ref type="bibr" target="#b16">[17]</ref> and I is the identity matrix. Following the notation in <ref type="bibr" target="#b3">[4]</ref>, F * i represents the indicator functions and G is the optical flow term. Their respective resolvent operators are defined as in <ref type="bibr" target="#b16">[17]</ref>. (I + T 1 ∂h θ ) −1 is the resolvent operator with respect to h θ , which can be solved by a softthresholding scheme for each log intensity estimate L i :</p><formula xml:id="formula_6">(I + λ 6 τ i h θ ) −1 (L i ) =L i +      −λ 6 τ i , if (L i − L(t p )) &gt; θ + λ 6 τ i λ 6 τ i , if (L i − L(t p )) &lt; −θ − λ 6 τ i 0, otherwise,<label>(6)</label></formula><p>x x x x <ref type="figure">Figure 4</ref>: Approximation of the intensity for two given events e 1 and e 2 between two intensity estimates L 1 and L 2 . For the discrete data term, we use the linear approximations f 1 and f 2 at the time of each event η 1 and η 2 .</p><p>where we fix L(t p ) during the minimisation step. A fixed L(t p ) can slow down the convergences of the algorithm, but in practise we have not experienced such behavior. Next we discuss the result of this minimisation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present the capabilities of our method in a series of experiments with the DVS128 camera <ref type="bibr" target="#b12">[13]</ref>. For these experiments we choose a spatial discretisation of 128 × 128, which corresponds to the resolution of the actual camera. To highlight these results in this paper, we show log intensity and velocity field estimates at certain time-stamps to highlight how they resemble images and optical flow fields from standard cameras. However, we believe that these results are best viewed in the accompanying video on our project webpage 1 , where also show that we can estimate super-resolution log-intensity and velocity, via a simple extension to our formulation based on the work of Unger et al. for standard cameras <ref type="bibr" target="#b21">[22]</ref>. Although we do not discuss the super-resolution method in detail here, it is important to highlight that event camera data allows us to perform intensity and velocity estimation at sub-pixel resolution.</p><p>For the following experiments, we set the sliding window depth K to 128, which we have found to be an appropriate choice for most sequences to capture a large amount of events. If not specified otherwise, δ t is set to 15 milliseconds. For all sequences we set θ = 0.22, λ 1 = 0.02, λ 2 = 0.05, λ 3 = 0.02, λ 4 = 0.2, λ 5 = 0.1 and λ 6 = 1.0. All sequences are initialised by assuming no initial motion and only a uniform gray scale intensity distribution, which includes the prior image as well. For comparison, we mounted the DVS128 next to a standard frame-based camera with standard 640 × 480 resolution, 30 fps and global shutter settings for all sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Benefits of Simultaneous Estimation</head><p>We begin with an experiment to argue for the simultaneous estimation of intensities and optical flow with event cameras. In this sequence we compare the intensity estimate of our method with and without using of the optical flow term as defined in (3) <ref type="figure" target="#fig_2">(Figure 5</ref>).</p><p>From these results we can see that our method, without the optical flow, can still estimate the intensities well in regions with strong gradient, but between those regions artefacts occur which do not correspond with the real intensities in the scene. Also we see that the term enforces temporal consistency and without it areas become brighter or darker from frame to frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Face Sequence</head><p>In this sequence we show intensity and velocity reconstruction of a moving face while the camera is in motion as well. The results are shown in <ref type="figure" target="#fig_3">Figure 6</ref>.</p><p>At the beginning of the sequence (left), the intensities have not been properly estimated yet, because only a few events have been captured. We see that more details become visible in the following frames as more events are processed. Velocity visualisation shows clear motion boundaries between the head and the background, proving that our approach can handle motion discontinuities. However, both the intensities and optical flow show noise, presumably caused by outlier events and/or missing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">High Dynamic Range Scene</head><p>In this example, we show the comparison between our method and a traditional camera in a high dynamic range scene. In our experiment, we point the cameras out a window from a dim room, which is a challenging case for a traditional camera, as can be seen in <ref type="figure" target="#fig_4">Fig. 7</ref>.</p><p>We see that our method recovers details both inside the room and outside the window, while the traditional camera, because of its low dynamic range, can only show either the room or the outside at one time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Rapid Motion</head><p>Here we present the capability of our method to estimate fast motion in front of a cluttered background. We throw a ball in front a desktop scene, while the camera is also in motion ( <ref type="figure" target="#fig_5">Figure 8)</ref>.</p><p>For this sequence, we set δ t to a smaller value of 4 milliseconds. We see how the traditional camera is affected by motion blur, while our method is able to recover clear motion boundaries. However, due to the small δ t intensity details are not estimated as well as in the previous examples. This gives the impression that the ball is transparent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Full Body Motion</head><p>In our last example we show a person performing jumping jacks. For this sequence we set δ t to 7 ms and reduce λ 1 and λ 3 to 0.01, which preserves smaller regions from being smoothed out in the optical flow and intensity estimate. In <ref type="figure">Fig. 9</ref> we see that our method can estimate arm motion well, even though it occupies a small image region. However, with the decreased smoothness weight, the influence of noisy events is stronger, which becomes visible in the motion field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have shown that event data from a standard DVS128 sensor can be used to reconstruct high dynamic range intensity frames jointly with a dense optical flow field. As demonstrated experimentally, our sliding window optimisation based method does neither impose any restrictions on camera motion nor scene content. In particular we have shown tracking and reconstruction of extreme, rapid motion and high dynamic range scenes which are beyond the capabilities of frame-based cameras. We thus believe that this work is important in supporting the claim that event cameras will play a major role in real-time geometric visionthe information in a low bit-rate event stream really does contain all of the content of continuous video, and more. Having proven a very general capability, we plan to furthermore investigate specific efficient estimation algorithms for particular problems such as model-based 3D tracking, 3D reconstruction and SLAM.</p><p>We also believe that this work strengthens the argument that research on embodied real-time vision needs to look at all of the components of a hardware/software vision system together (sensors, algorithms and ultimately also processors) in order to reach for maximum performance. Progress in the specific technology of event cameras must be compared with alternatives, always with an eye on the rapidly improving and strongly market-driven performance of standard frame-based cameras.    In the intensity reconstruction (middle) we observe good reconstruction of the monitor which was traversed by the ball, causing many events. <ref type="figure">Figure 9</ref>: Jumping jacks. In the sequence from a standard camera the arms are blurred, while our reconstruction from events allows clear delineation of the arms in both the intensity and velocity fields.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Results from our method: (a) integrated output of a DVS128 event camera; (b) the same scene from a standard camera. (c) and (d) show a snapshots of the intensity and velocity fields we estimate jointly only from event data. A color-wheel is used to show the velocity per pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Desktop scene captured by an event camera (a) and a standard camera (b). The event camera image is an accumulation of all events during a period of 33 ms, where white and black pixels represents positive and negative events. Areas where no event has fired are grey.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of intensity estimation without (a) and with (b) the optical flow term. As usual in (c) we show an image from a standard camera for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Hand-held event camera moving around a person's head. Here the face is moving from left to right, while the background is moving in the opposite direction, which can be seen in the estimated velocity field (top row). The middle row shows high quality and consistent reconstructed intensity. Bottom row: standard video for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>High dynamic range scene. Intensity reconstructions from events (left) contrasted with standard video images (right). The camera moves from observing the bright outside scene to point towards a shelf inside the dim room.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Ball thrown across a desktop scene. In the estimated velocity field (top) we see clear segmentation of the ball, while video from a standard camera (bottom) is heavily blurred.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.imperial.ac.uk/dyson-robotics-lab/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Research presented in this paper has been supported by Dyson Technology Ltd. We are grateful to Jan Jachnik and Ankur Handa for very useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<title level="m">Event-Based Visual Flow. IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="407" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A 240×180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brandli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits (JSSC)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2333" to="2341" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-Latency Event-Based Visual Odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="145" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A pencil balancing robot using a pair of AER dynamic vision sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interacting maps for fast visual interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gugelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Krautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast sensory motor control based on event-based hybrid neuromorphic-procedural system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Largescale direct monocular SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schoeps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dense variational reconstruction of non-rigid surfaces from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1272" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Davison. Applications of the Legendre-Fenchel transformation to computer vision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename></persName>
		</author>
		<idno>DTR11-7</idno>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Imperial College London</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Davison. Simultaneous Mosaicing and Tracking with an Event Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A 128×128 120 dB 15 µs Latency Asynchronous Temporal Contrast Vision Sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits (JSSC)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Silicon Retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="76" to="82" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Event-based , 6-DOF Pose Tracking for High-Speed Maneuvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DTAM: Dense Tracking and Mapping in Real-Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Diagonal preconditioning for first order primal-dual algorithms in convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A convex relaxation approach for computing minimal partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A QVGA 143 dB Dynamic Range Frame-Free PWM Image Sensor With Lossless Pixel-Level Video Compression and Time-Domain CDS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wohlgenannt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JSSC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Natural image statistics for computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</author>
		<idno>UUCS-01-002</idno>
		<imprint>
			<date type="published" when="2001-03" />
		</imprint>
	</monogr>
	<note type="report_type">Univ. Utah Tech Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Convex Approach for Variational Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DAGM Symposium on Pattern Recognition</title>
		<meeting>the DAGM Symposium on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Improved Algorithm for TV-L1 Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Dagstuhl Seminar on Statistical and Geometrical Approaches to Visual Motion Analysis</title>
		<meeting>the Dagstuhl Seminar on Statistical and Geometrical Approaches to Visual Motion Analysis</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DAGM Symposium on Pattern Recognition</title>
		<meeting>the DAGM Symposium on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
