<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sample-Specific SVM Learning for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116023</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baohua</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116023</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116023</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atshushi</forename><surname>Irie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OMRON Corporation</orgName>
								<address>
									<settlement>Kusatsu</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OMRON Corporation</orgName>
								<address>
									<settlement>Kusatsu</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sample-Specific SVM Learning for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification addresses the problem of matching people across disjoint camera views and extensive efforts have been made to seek either the robust feature representation or the discriminative matching metrics. However, most existing approaches focus on learning a fixed distance metric for all instance pairs, while ignoring the individuality of each person. In this paper, we formulate the person re-identification problem as an imbalanced classification problem and learn a classifier specifically for each pedestrian such that the matching model is highly tuned to the individual's appearance. To establish correspondence between feature space and classifier space, we propose a Least Square Semi-Coupled Dictionary Learning (LSSCDL) algorithm to learn a pair of dictionaries and a mapping function efficiently. Extensive experiments on a series of challenging databases demonstrate that the proposed algorithm performs favorably against the state-of-the-art approaches, especially on the rank-1 recognition rate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a fundamental task in multi-camera surveillance system, person re-identification aims to match people observed from different cameras or across different time with a single camera. Although has gained much attention among researchers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref> in recent years, person re-identification remains a challenging problem since a person's appearance can change significantly when large variations in view angle, illumination, background clutter and occlusion are involved.</p><p>To address these challenges, a lot of approaches have been proposed to develop robust feature representations which are discriminative for identity, such as Ensemble of Localized Features (ELF) <ref type="bibr" target="#b10">[11]</ref>, Symmetry-Driven Accumulation of Local Features (SDALF) <ref type="bibr" target="#b7">[8]</ref>, Covariance descriptor based on Bio-inspired Features (gBiCov) <ref type="bibr" target="#b28">[29]</ref> and Local Maximal Occurrence (LOMO) <ref type="bibr" target="#b19">[20]</ref>.</p><p>On the other hand, there are many efforts attempting to learn optimal matching metrics under which instances be-longing to the same person are closer than different persons, like Probabilistic Relative Distance Comparison (PRD-C) <ref type="bibr" target="#b43">[44]</ref>, Keep It Simple and Straightforward Metric Learning (KISSME) <ref type="bibr" target="#b15">[16]</ref>, Local Fisher Discriminant Analysis (LFDA) <ref type="bibr" target="#b31">[32]</ref>, Cross-view Quadratic Discriminant Analysis (XQDA) <ref type="bibr" target="#b19">[20]</ref>, etc. Taking the person re-identification as a relative ranking problem, some researchers <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b23">24]</ref> employ the Support Vector Machine (SVM) model to learn a ranking function such that the scores of matched image pairs are larger than unmatched ones.</p><p>However, most of the existing methods focus on learning a fixed distance metric or ranking function to measure the similarity between all images, while ignoring the fact that different instances have different feature representations, and the metric or function derived for matching all objects may not be optimal for every single person. Since the primary goal of person re-identification is to seek the optimal match for each pedestrian, a sample-specific distance metric or ranking function should be more investigated. Zheng et al. <ref type="bibr" target="#b42">[43]</ref> utilized the initial rank scores of each sample to compute the adaptive weights for feature fusion. The Query based Adaptive Re-Ranking (QARR) algorithm <ref type="bibr" target="#b26">[27]</ref> was developed to learn a weighted combination of a base score function and a perturbation linear function for each query. Nevertheless, both of the query-adaptive methods learn the new weighting scheme in the re-ranking stage, and the effectiveness of the model may be easily affected by the initial matching results.</p><p>In this paper, we propose a novel framework for person re-identification, where a sample-specific SVM is learned for each pedestrian to seek the optimal match. The matching function parameterized by the classifier weight vector is highly tuned to the individual's appearance, which can provide more discriminative measurements for finding the best candidate. To investigate the intrinsic relationship between the feature space and weight space, we propose a Least Square Semi-Coupled Dictionary Learning (LSSCDL) algorithm to learn a dictionary pair and a mapping function simultaneously, through which the weight parameters of a new sample can be easily inferred by its feature patterns. <ref type="figure" target="#fig_1">Figure 1</ref> shows the overview of our approach.  Overview of the proposed approach for person re-identification. We first learn matching classifiers for every training individuals by sample-specific SVMs. The classifier weight vectors are used to further learn a pair of dictionary and a mapping matrix, by which a weight vector of a test probe image can be easily inferred from its feature representation. The re-identification is then performed based on our proposed matching criterion with the learned weight vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mainstream Methods for Re-identification</head><p>Most existing approaches to tackle the person reidentification problem are mainly carried on from two aspects: developing distinctive feature representations and seeking discriminative distance metrics. Both of them aim to compute the matching distances (or scores) which are optimal for matched image pairs from the gallery and probe set respectively.</p><p>For feature representation, a number of approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17]</ref> have been proposed to design robust descriptors against background and illumination variations. For instance, Gray et al. <ref type="bibr" target="#b10">[11]</ref> presented the ELF by fusing 8 color channels with 19 texture channels, while Farenzena et al. <ref type="bibr" target="#b7">[8]</ref> employed the weighted color histograms, Maximally Stable Color Regions (MSCR), Recurrent High-Structured Patches (RHSP) to capture different image properties. Ma et al. <ref type="bibr" target="#b28">[29]</ref> proposed an image representation based on the combination of Biologically Inspired Features (BIF) and Covariance descriptors. LO-MO <ref type="bibr" target="#b19">[20]</ref> extracted the maximal pattern of joint HSV color histogram coupled with Scale Invariant Local Ternary Pattern (SILTP) <ref type="bibr" target="#b21">[22]</ref>, and it is worth mentioning that LOMO based pedestrian representation has shown impressive robustness against viewpoint changes. Chen et al. <ref type="bibr" target="#b3">[4]</ref> proposed a zero-padding based feature transformation strategy to enables alignment of the feature distributions across disjoint views, which can significantly enhance the performance of existing matching models. Shi et al. <ref type="bibr" target="#b34">[35]</ref> learned mid-level semantic attributes such as hair-style, shoe-type or clothing-style to achieve more powerful representation.</p><p>For metric learning, numerous research works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20]</ref> aim to learn a metric matrix under which the distance between images of the same pedestrian is smaller than different ones. Zheng et al. <ref type="bibr" target="#b43">[44]</ref> proposed the PRDC based method where the probability of a pair of true match having a smaller distance than that of a wrong match pair is maximized. Pedagadi et al. <ref type="bibr" target="#b31">[32]</ref> employed the LF-DA algorithm to maximize the inter-class separability while preserving the multiclass modality. Li et al. <ref type="bibr" target="#b18">[19]</ref> developed the Locally-Adaptive Decision Functions (LADF), which combines the distance metric with a locally adaptive thresholding rule for each pair of sample images. KISSME <ref type="bibr" target="#b15">[16]</ref> derived a Mahalanobis metric by computing the difference between the intra-class and inter-class covariance matrix. As an improvement, XQDA <ref type="bibr" target="#b19">[20]</ref> learned a more discriminative distance metric and a low-dimensional subspace simultaneously. Although has achieved inspiring re-identification reults, these methods did not give sufficient consideration to the individuality of each pedestrian when learning a generic distance metric for all instances. In this paper we propose to learn a matching metric specifically for every person, such that each individual could have the matching model that best suits his or her appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">SVM learning for Re-identification</head><p>Some researchers formulated the person re-identification problem as a ranking problem and managed to learn a ranking function parameterized by a weight vector to order relevant images pairs before irrelevant ones. Prosser et al. <ref type="bibr" target="#b32">[33]</ref> developed the Ensemble RankSVM where a set of weak RankSVMs were learned on different subsets and then combined into a stronger ranker using a boosting principle. In <ref type="bibr" target="#b23">[24]</ref>, the structural SVM was employed to score correct matches higher than all incorrect ones by a margin.</p><p>Although based on SVM learning, our approach differs substantially from these two methods. Instead of computing a fixed weight vector for all pedestrians, we learn specific weight parameters such that the ranking function is highly tuned to the individual's appearance. Furthermore, the previous methods employ the relative ranking relationships for SVM learning, while the proposed algorithm tackles person re-identification as an absolute classification problem, which greatly enlarges the gap between matched and unmatched image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dictionary Learning Methods</head><p>SCDL The Semi-Coupled Dictionary Learning (SCDL) algorithm <ref type="bibr" target="#b37">[38]</ref> was originally intended to solve the crossstyle image synthesis problems, and it assumed that there exists a dictionary pair over which the coding coefficients of the two representations have a stable mapping.</p><p>SSCDL To bridge the appearance variations across cameras, Liu et al. <ref type="bibr" target="#b22">[23]</ref> developed the Semi-Supervised Coupled Dictionary Learning (SSCDL) algorithm where the coupled dictionaries for gallery and probe images are learned jointly. SLD 2 L Based on the observation that gallery images are high-resolution (HR) while probe images are lowresolution (LR), Jing et al. <ref type="bibr" target="#b14">[15]</ref> proposed the Semi-coupled Low-rank Discriminant Dictionary Learning (SLD 2 L) algorithm to learn a pair of HR and LR dictionaries and a matrix to map the feature from LR to HR.</p><p>However, the SCDL model is designed for photo-sketch synthesis and requires large time consumption to solve the sparse coding problem, therefore we propose the LSSCDL algorithm to solve the cross-modal problem with higher efficiency. In contrast with SSCDL and SLD 2 L which directly learn a dictionary pair for two camera views or two resolutions, the LSSCDL model attempts to investigate the intrinsic relationship between feature patterns and ranking parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sample-Specific SVM Learning</head><p>Given the probe set D p = {x p i } N i=1 and the gallery set D g = {x g j } N j=1 , we respectively denote i and j to be the identity label of pedestrians from the two groups. A probegallery image pair with a matching label is constructed as</p><formula xml:id="formula_0">{(x p i , x g j ), y p j }, where y p j = +1 represents that (x p i , x g j )</formula><p>is a correct matching pair, while y p j = −1 indicates the incorrect matches.</p><p>We explicitly consider the problem of person reidentification as a binary classification problem. Given a probe image x p i , we attempt to learn a sample-specific clas-</p><formula xml:id="formula_1">sifier F p i on the probe-gallery set {(x p i , x g 1 ), ..., (x p i , x g N )} such that F i (x p i , x g j ) = ≥ 0, y p j = +1 &lt; 0, y p j = −1 j = 1, ..., N (1)</formula><p>We define the classification function with the following form</p><formula xml:id="formula_2">F i (x p i , x g j ) = w p i · φ(x p i , x g j ) + b i<label>(2)</label></formula><p>where w p i denotes the weight vector of</p><formula xml:id="formula_3">x p i and b i is the bias. φ(x p i , x g j )</formula><p>is defined as a feature map of the image pair with the following form <ref type="bibr" target="#b43">[44]</ref> and d is the feature dimension. This feature map not only takes the image difference into consideration, but also exploits the nature of image itself to enhance the distinctiveness of an image pair.</p><formula xml:id="formula_4">φ(x p i , x g j ) = [(x p i ) ⊤ , |x p i − x g j | ⊤ , (x g j ) ⊤ ] ⊤ (3) where x p i − x g j = (|x p i (1) − x g j (1)|, ..., |x p i (d) − x g j (d)|) ⊤ is the absolute difference vector</formula><p>The traditional SVM model <ref type="bibr" target="#b4">[5]</ref> is employed to solve the binary classification problem, where the relevant image pairs are separated from all the irrelevant ones by the largest possible margin. Consider that the number of correct matches (positive set) is much smaller than incorrect ones (negative set), we impose different penalty parameters <ref type="bibr" target="#b35">[36]</ref> to handle the imbalance. Learning the samplespecific classifier is equivalent to optimizing the following objective function:</p><formula xml:id="formula_5">min w p i 1 2 w p i 2 + C + y p j =+1 ξ j + C − y p j =−1 ξ j s.t. y p j (w p i · φ(x p i , x g j ) + b i ) ≥ 1 − ξ j , ξ j ≥ 0, j = 1, ..., N<label>(4)</label></formula><p>where C + &gt; 0 and C − &gt; 0 are regularization parameters for positive and negative classes, respectively, and ξ j is the slack variable.</p><p>Generally, the linear SVM model has good interpretability in the sense that it seeks a direction that can explain the biggest difference between the two classes. Therefore, the binary classification strategy for pedestrian matching actually attempts to find the weight vector maximally enlarging the gap between matched and unmatched pairs to improve the rank-1 recognition rate <ref type="bibr" target="#b41">[42]</ref>, which is consistent with the primary goal of person re-identification. Moreover, the positive image pair is separated from its corresponding negative pairs in each sample-specific classifier, under which the instance-level information is effectively utilized to make the matching model highly tuned to the individual's appearance, leading to more powerful discrimination.</p><p>Note that the weight vector w p i plays a key role in ordering the relevant image pairs before irrelevant ones, here we define a score function without changing the ranks of matching candidates by omitting the bias in <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_6">f i (x p i , x g j ) = w p i · φ(x p i , x g j )<label>(5)</label></formula><p>where f i (x p i , x g j ) denotes the matching score of x p i and x g j . The higher the score is, the more likely that the two images represent the same person. In the following sections we only focus on the discussion of weight vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Least Square Semi-Coupled Dictionary Learning</head><p>Since there is a one-to-one correspondence between the feature vector and weight vector, it is reasonable to assume that the feature patterns and weight parameters of a specific person should have similar intrinsic structures, and there exists a mapping function through which one type of the representation can be inferred by the other. It is hard to define the mapping function between the two styles of representation directly, while the linear reconstruction pattern of each pair of samples in their respective space can be related to some extent. As suggested in <ref type="bibr" target="#b37">[38]</ref>, for two different styles of representations indicating the same scene, there exist coupled dictionaries over which the coding coefficients of two styles have a stable mapping.</p><p>In this paper, we propose a Least Square Semi-Coupled Dictionary Learning (LSSCDL) algorithm to learn a pair of dictionaries and a mapping function efficiently, where the two dictionaries respectively depict the intrinsic structures of the feature space and weight space, and the mapping function characterizes the relationship between the two spaces.</p><p>Given the training probe set X p = (x p 1 , x p 2 , ..., x p N ) ∈ R d×N with each column representing a probe image, and the corresponding learned weight set</p><formula xml:id="formula_7">W p = (w p 1 , w p 2 , ..., w p N ) ∈ R 3d×N , we denote D x ∈ R d×k , D w ∈ R 3d×k</formula><p>and M ∈ R k×k to be the feature dictionary, the weight dictionary, and the mapping matrix, respectively. Here k indicates the dictionary size. Then the problem of jointly optimizing the dictionaries and mapping function can be formulated as follows.</p><formula xml:id="formula_8">min {Dx,Dw,M} Φ(D x , D w , M, Λ x , Λ w ) (6) with Φ = E data (D x , X p ) + E data (D w , W p ) + E map (M) + E reg (Λ x , Λ w , M, D x , D w )<label>(7)</label></formula><p>where Λ x and Λ w denote the coding coefficients. E data (·, ·) is the representation fidelity term indicating the reconstruction error, E map (·) is the mapping fidelity term to represent the mapping error between the coding coefficients, E reg is the regularization term to regularize the coding coefficients, mapping matrix and dictionaries. Following the assumption that dictionaries are overcomplete, many previous methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b14">15]</ref> impose ℓ 1 -norm regularization on coding coefficients to select a few atoms of the learned dictionary to describe a sample. In person reidentification, however, feature dimension is usually much larger than the number of samples, and the sparse representation may be insufficient to capture the correlation structure of data with large variations. Furthermore, it is not efficient to solve the ℓ 1 -minimization problem for each instance. To address these issues, we present the Least Square Semi-Coupled Dictionary Learning (LSSCDL) algorithm with the following form.</p><formula xml:id="formula_9">min {Dx,Dw,M} X p − D x Λ x 2 F + W p − D w Λ w 2 F +λ Λ w − MΛ x 2 F +λ Λ Λ x 2 F +λ Λ Λ w 2 F +λ M M 2 F +λ D D x 2 F +λ D D w 2 F<label>(8)</label></formula><p>where · F indicates the Frobenius norm, λ, λ Λ , λ D , and λ M are regularization parameters to balance the terms in the objective function.</p><p>Note that <ref type="formula" target="#formula_9">(8)</ref> is a non-convex optimization problem with all the matrices D x , D w , M, Λ x , Λ w , but it is convex with respect to each of the variables when the others are fixed. Therefore the optimization problem can be solved by conducting the following steps iteratively until convergency.</p><formula xml:id="formula_10">1. Fix D x , D w , M, Λ w , let ∂Φ ∂Λx = 0 , we have Λx =(D ⊤ x Dx +λ M ⊤ M+λΛI) −1 (D ⊤ x X p +λ M ⊤ Λw)<label>(9)</label></formula><p>2. Fix D x , D w , M, Λ x , let ∂Φ ∂Λw = 0 , we obtain</p><formula xml:id="formula_11">Λw =(D ⊤ w Dw + (λ + λΛ)I) −1 (D ⊤ w W p + λ MΛx)<label>(10)</label></formula><p>3. Fix Λ x , Λ w , M, let ∂Φ ∂Dx = 0 and ∂Φ ∂Dw = 0, we get</p><formula xml:id="formula_12">D x = X p Λ ⊤ x (Λ x Λ ⊤ x + λ D I) −1<label>(11)</label></formula><formula xml:id="formula_13">D w = W p Λ ⊤ w (Λ w Λ ⊤ w + λ D I) −1<label>(12)</label></formula><formula xml:id="formula_14">4. Fix D x , D w , Λ x , Λ w , let ∂Φ ∂M = 0, we have M = Λ w Λ ⊤ x (Λ x Λ ⊤ x + λ M λ I) −1<label>(13)</label></formula><p>The optimization algorithm for solving (8) is summarized in Algorithm 1.</p><p>We can see from (8) that the learned dictionaries D x and D w transfer the representations from two different spaces into a common coding space, and the coding coefficients of X p and W p can be related by the mapping matrix M. Therefore, the weight vector for a new sample can be easily inferred from its feature pattern with the learned dictionary pair and mapping function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: The Optimization of LSSCDL</head><p>Input: probe image matrix X p , weight matrix W p , parameters λ, λ Λ , λ D , and λ M . Output: feature dictionary D x , weight dictionary D w , mapping matrix M. <ref type="formula" target="#formula_12">(11)</ref> and <ref type="bibr" target="#b11">(12)</ref>. <ref type="bibr" target="#b12">(13)</ref>. Until: convergency.</p><formula xml:id="formula_15">Initialize: D x , D w , M, Λ x and Λ w . Repeat: 1: Fix D x , D w , M, Λ w , update Λ x by (9). 2: Fix D x , D w , M, Λ x , update Λ w by (10) 3: Fix Λ x , Λ w , M, update D x , D w by</formula><formula xml:id="formula_16">4: Fix D x , D w , Λ x , Λ w , update M by</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pedestrian Matching</head><p>Assuming that we have the test probe set and test gallery set denoted as</p><formula xml:id="formula_17">T p = {x p t } M t=1 and T g = {x g t ′ } M ′ t ′ =1</formula><p>, where t and t ′ indicate the identity label, respectively. Given a test probe image x p t , the corresponding weight vector w p t can be derived with the learned dictionary pair D w , D x and mapping matrix M.</p><p>We first compute the coding coefficients α x of x p t by solving the following problem.</p><formula xml:id="formula_18">min {αx} x p t − D x α x 2 F +λ Λ α x 2 F<label>(14)</label></formula><p>then the coding vector α w of w p t is derived by α w = Mα x (15) and the weight vector w p t of x p can be reconstructed by w p t = D w α w (16) Finally, we compute the matching score of a test probgallery image pair (x p t , x g t ′ ) with (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we show the performance of the proposed person re-identification algorithm on the VIPER datatset <ref type="bibr" target="#b9">[10]</ref>, the QMUL GRID dataset <ref type="bibr" target="#b25">[26]</ref>, the PRID 450S dataset <ref type="bibr" target="#b33">[34]</ref>, the CUHK01 dataset <ref type="bibr" target="#b41">[42]</ref>, the CUHK03 dataset <ref type="bibr" target="#b17">[18]</ref> and the OpeRID dataset <ref type="bibr" target="#b20">[21]</ref>. Comparisons of the Cumulative Matching Characteristic (CMC) <ref type="bibr" target="#b9">[10]</ref> results demonstrate that our approach performs favorably against other state-of-the-art methods, especially on the rank-1 recognition rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Extraction and Parameter Settings</head><p>Feature Extraction For each image, we extract the LO-MO descriptors to represent the human appearance. The LOMO extractor has shown impressive robustness against viewpoint changes and illumination variations by concatenating the maximal pattern of joint HSV histogram and SILTP descriptor. Consider that the dimensionality of LO-MO is extremely high, we employ <ref type="bibr" target="#b19">[20]</ref> for dimensionality reduction, which can greatly save the time and memory.</p><p>Parameter Settings There are 7 parameters in our approach, C + , C − , k, λ, λ Λ , λ M and λ D . In our experiments, we set C + = 300, C − = 0.1C + , k = N , λ = 0.1, λ Λ = 0.01, λ M = 0.01, λ D = 0.01 for all the databases. We find that our experimental results are not sensitive to parameter changes, and please refer to the supplementary material for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">VIPeR Dataset</head><p>The VIPER datatset <ref type="bibr" target="#b9">[10]</ref> contains 632 pairs of pedestrian images captured from two different cameras in outdoor academic environment, with only one image per person in each view and all the images normalized to 128 × 48 pixels. Suffering from significant viewpoint changes, pose variation, and illumination difference across cameras, it is one of the most challenging database for person re-identification.</p><p>Following the experimental protocol of <ref type="bibr" target="#b19">[20]</ref>, we randomly divide all the pedestrian pairs into two equal parts, with one part for training and the other for testing. These procedures are repeated for 10 trails and the average matching rates are summarized in <ref type="table" target="#tab_0">Table 1</ref>. We can see that the proposed approach achieves comparable performance with other methods. Although the matching rates are a little inferior to MirrorRep <ref type="bibr" target="#b3">[4]</ref> and Semantic <ref type="bibr" target="#b34">[35]</ref> , the proposed approach achieves the second best rank-1 recognition rate of 42.66%.  <ref type="bibr" target="#b32">[33]</ref> 14.00 51.00 67.00 ELF <ref type="bibr" target="#b10">[11]</ref> 12.00 44.00 61.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">QMUL GRID Dataset</head><p>The QMUL GRID dataset <ref type="bibr" target="#b25">[26]</ref> consists of person images recorded from 8 disjoint cameras installed in an underground station. The probe set contains 250 pedestrians, with each one having a matching image in the gallery set. Besides, there are 775 additional images in the gallery set that do not match any person in the probe set, which increases the difficulty of seeking the optimal match for each probe image. We normalize all the images to 128 × 48 pixels, and adopt the experimental setting of 10 random trials for this dataset. In each trial, we randomly select 125 image pairs for training and use the remaining 125 pairs coupled with 775 irrelevant images for testing. <ref type="table" target="#tab_1">Table 2</ref> compares the matching rates of our approach with previous methods. The comparison shows that the proposed algorithm improves the rank-1 recognition rate from 16.56% to 22.40% and produces about 10 percents improvement on rank-10 and rank-20 matching rate, showing significant advantage in person re-identification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">PRID 450S Dataset</head><p>The PRID 450S dataset <ref type="bibr" target="#b33">[34]</ref> includes 450 single-shot pedestrian image pairs captured from two disjoint camera views. It is also a challenging person re-identification dataset due to the background interference, partial occlusion and viewpoint changes. In our experiments, all the images are normalized to the size of 128 × 64 pixels. We randomly select half of the dataset for training and the remaining for testing, and repeat the procedures for 10 times to report the average performance.</p><p>We also implement the LOMO+XQDA <ref type="bibr" target="#b19">[20]</ref> algorithm on the PRID 450S dataset under the same protocol, and the results comparison are summarized in <ref type="table" target="#tab_2">Table 3</ref>, from which one can see that the proposed approach performs well against the existing methods and achieves the second best one of 60.49% on the rank-1 recognition rate, showing competitive performance on this dataset.  <ref type="bibr" target="#b39">[40]</ref> 26.90 64.20 74.90 KISSME <ref type="bibr" target="#b15">[16]</ref> 33.00 71.00 79.00 EIML <ref type="bibr" target="#b12">[13]</ref> 35.00 68.00 77.00 ELF <ref type="bibr" target="#b10">[11]</ref> 30.60 73.60 84.20</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">CUHK01 Dataset</head><p>The CUHK01 Dataset <ref type="bibr" target="#b41">[42]</ref> is captured in a campus environment with two camera views. It contains 971 individuals and each of them has two images in every camera view. Taking the evaluation method in <ref type="bibr" target="#b41">[42]</ref>, we normalized all the images to 160×60 pixels, and conduct the experiments over 10 random partitions for this dataset, where 485 persons are randomly sampled for training and the rest are utilized for testing. <ref type="figure" target="#fig_3">Figure 2</ref> (a) plots the CMC curves of our method and existing state-of-the-art algorithms. Our approach reports the best rank-1 recognition rate of 65.97%, with an improvement of 2.76% over LOMO+XQDA <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">CUHK03 Dataset</head><p>The CUHK03 dataset <ref type="bibr" target="#b17">[18]</ref> consists of 13,164 images of 1,360 pedestrians captured with six surveillance cameras. Each individual is observed by two disjoint camera views, and there are 4.8 images on average for each identity in each view. Apart from the manually labeled pedestrian bounding boxes, this database also provides the samples detected with a pedestrian detector <ref type="bibr" target="#b8">[9]</ref>, which causes some misalignments and body part missing for a more realistic setting.</p><p>Following the experimental settings in <ref type="bibr" target="#b17">[18]</ref>, we partition the dataset into a training set of 1,160 and a test set of 100 persons. All the experiments are conducted with 20 random splits and the average results are presented. <ref type="figure" target="#fig_3">Figure 2</ref> (b) plots the CMC curves of all the methods on the CUHK03 dataset with the labeled bounding boxes. we can see that proposed algorithm achieves comparable results with XQDA <ref type="bibr" target="#b19">[20]</ref>, and IDLA <ref type="bibr" target="#b0">[1]</ref> reports the best performance from rank-2 to rank-30. The best rank-1 recognition rate reported to date is 54.74%, while we have achieved 57.00% with an improvement of 2.26%. <ref type="figure" target="#fig_3">Figure 2</ref> (c) compares the performance of our approach with other state-ofthe art methods using automatically detected bounding boxes. Although the performance on detected CUHK03 is inferior to labeled CUHK03 due to the misalignment and in-   completeness caused by the detector, the proposed algorithm still shows great advantages over previous method, with a rank-1 recognition rate of 51.20% compared with the second best one of 46.25%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">OPeRID Dataset</head><p>The OpeRID dataset <ref type="bibr" target="#b20">[21]</ref> contains 7,413 images of 200 persons collected from a real outdoor surveillance scenario with a network setting of 6 cameras. Each of the person has at least 2 associated camera views and a person may have up to 5 camera views. Lighting changes, viewpoint variations, low resolution and blur can be observed from the images, which are segmented from a interactive labeling software <ref type="bibr" target="#b36">[37]</ref>. According to the experimental settings and evaluation protocols in <ref type="bibr" target="#b20">[21]</ref>, we scale all the images to 128 × 48 pixels, calculate the detection and identification rates (DIR) and false accept rate (FAR) for evaluation. All the procedures are repeated for 10 trials, and the average performance are reported. <ref type="table" target="#tab_4">Table 4</ref> summarizes the DIR values at rank 1, 10 under FAR=1% and FAR=10%. We can see that the proposed algorithm is a little inferior to the rate of 3.99% and 4.35% of RRDA <ref type="bibr" target="#b20">[21]</ref> at FAR=1%, which achieves the best performance of 15.08% and 18.17% at FAR=10%. The DIR values on open-set dataset are pretty low for all the methods, and there are still much work to do for real applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparative Experiments</head><p>To better understand the function of each part in the proposed algorithm, we conducted further comparative experiments in two aspects: sample-specific SVM vs fixed SVM, feature map vs feature difference and LSSCDL vs SCDL. The complexity analysis in terms of running time and convergence performance are also presented in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Sample-specific SVM vs Fixed SVM</head><p>We compare the results of our approach with the method of learning a fixed SVM. Specifically, the second method employs all matched and unmatched pairs as positive and negative class respectively, and learns a common weight vector for all pedestrians. <ref type="figure" target="#fig_4">Figure 3 (a)</ref> shows the rank-1 recognition rate comparisons of the two methods. From the results one can easily confirm that learning specific weight vector for each sample significantly outperforms the method of learning fixed weight parameters, by improving the rank-1 accuracy from 34.64% to 42.66% on the VIPER database, 16.24% to 22.40% on the GRID database, 52.58% to 60.49% on the PRID 450S database, and 57.34% to 65.97% on the CUHK01 database. This comparison demonstrates that by taking the appearance individuality into consideration, the proposed algorithm can learn more optimal ranking function for each pedestrian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Feature Map vs Feature Difference</head><p>To demonstrate the effectiveness of the proposed feature <ref type="figure" target="#fig_4">Figure 3</ref> (b) we can see that, the re-identification performance can be obviously improved by the proposed feature map, especially with a great improvement of 9.71% on the CUHK01 dataset. This indicates that the proposed feature map is able to learn more distinctive weight parameters by exploiting both the difference information and natural characters of the pairwise feature representation.  </p><formula xml:id="formula_19">map φ(x p i , x g j ) = [(x p i ) ⊤ , |x p i − x g j | ⊤ , (x g j ) ⊤ ] ⊤ , we compare the experimental results with feature map φ(x p i , x g j ) and feature difference φ ′ (x p i , x g j ) = |x p i −x g j |. From</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">LSSCDL vs SCDL</head><p>In our framework, the LSSCDL algorithm aims to capture the intrinsic relationship between feature space and ranking function space. It can be seen as an improved version of SCDL <ref type="bibr" target="#b37">[38]</ref> in person re-identification tasks for higher efficiency. The rank-1 recognition rate comparisons of the two dictionary learning strategies are shown in <ref type="figure" target="#fig_4">Figure 3</ref> (c), from which one can see that SCDL achieves a rank-1 rate of 40.82% on VIPER, 20.88% on GRID, 56.58% on PRID 450S and 59.40% on CUHK01 dataset. In contrast, the LSSCDL improves the performance by 1.84%, 1.52%, 3.91% and 6.57% on the four datasets, respectively. The performance of SCDL on small datasets is a slightly lower than LSSCDL, while the running time of SCDL is about five times slower than LSSCDL, which will be discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Complexity Analysis</head><p>We conduct the proposed approach with Matlab implementation on a desktop PC with Intel i7-4790K @4.00GHz CPU and 32GB RAM, and report the running time of each stage averaged over 10 random trials on the VIPER dataset.</p><p>The computation time of learning sample-specific SVMs for all training images is 4.52 seconds, and it should be noted that learning a fixed weight vector costs 41.79 seconds. This demonstrates that solving multiple small SVM learning problems is actually more efficient than solving a large scale classification problem. The optimization time of LSSCDL is about 2.89 seconds, showing notable acceleration compared to 11.88 seconds of the SCDL algorithm. However, the testing time for one probe image is only 0.001 seconds, which indicates good applicability of the proposed approach in real applications.</p><p>To investigate the convergence effect of the proposed LSSCDL algorithm, we visualize the change of objective function value during optimization on the VIPER dataset in <ref type="figure" target="#fig_6">Figure 4</ref>. We initialize D x , D w , Λ x , Λ w to be random ma-trices, and M to be the unit matrix in all experiments. From the figure one can see that the objective function value decreases quickly at first and then reaches a minimal, which demonstrates the feasibility of the proposed algorithm.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel and effective method for person re-identification. Motivated by the insight that different matching functions should be designed for different individuals, we formulate the person re-identification problem into a binary classification problem and learn a classifier specifically for each pedestrian. To capture the intrinsic relationship between feature patterns and ranking parameters, we propose an efficient LSSCDL algorithm to learn a pair of dictionary and a mapping function simultaneously. Experimental results on five challenging person re-identification datasets demonstrate the superiority of the proposed algorithm over state-of-the-art methods. In the future work, we will focus on applying the proposed algorithm to more matching applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of the proposed approach for person re-identification. We first learn matching classifiers for every training individuals by sample-specific SVMs. The classifier weight vectors are used to further learn a pair of dictionary and a mapping matrix, by which a weight vector of a test probe image can be easily inferred from its feature representation. The re-identification is then performed based on our proposed matching criterion with the learned weight vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>CMC curves and rank-1 identification rates on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Rank-1 recognition rate comparison of (a)Sample-specific SVM vs Fixed SVM; (b) Feature Map vs Feature Difference; (c) LSSCDL vs SCDL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Change of objective function value during LSSCDL optimization on the VIPER dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Comparison of state-of-the-art results on the VIPeR dataset (P=316). The cumulative matching scores (%) at rank 1, 10, and 20 are listed.</figDesc><table>Method 
rank=1 rank=10 rank =20 
Ours 
42.66 
84.27 
91.93 
MirrorRep [4] 
42.97 
87.28 
94.84 
Semantic [35] 
41.60 
86.20 
95.10 
LOMO+XQDA [20] 
40.00 
80.51 
91.08 
IDLA [1] 
34.81 
75.63 
84.49 
PolyMap [2] 
36.80 
83.70 
91.70 
SLD 2 L [15] 
16.86 
58.06 
79.00 
ECM [24] 
38.90 
78.40 
88.90 
QALF [43] 
30.17 
62.44 
73.81 
QARR-RSVM [27] 
22.53 
62.20 
75.82 
SCNCD [40] 
37.80 
81.20 
90.40 
gBiCov [29] 
31.11 
70.71 
82.45 
Mid-level Filter [42] 
29.11 
65.95 
79.87 
MtMCML [30] 
28.83 
75.82 
88.51 
SSCDL [23] 
25.60 
68.10 
83.60 
LADF [19] 
30.22 
78.92 
90.44 
SalMatch [41] 
30.16 
65.54 
79.15 
KISSME [16] 
19.60 
62.20 
77.00 
PCCA [31] 
19.27 
64.91 
80.28 
PRDC [44] 
15.66 
53.86 
70.09 
SDALF [8] 
19.87 
49.37 
65.73 
RankSVM </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Comparison of state-of-the-art results on the QMUL GRID database (P=900). The cumulative matching scores (%) at rank 1, 10, and 20 are listed.</figDesc><table>Method 
rank=1 rank=10 rank =20 
Ours 
22.40 
51.28 
61.20 
LOMO+XQDA [20] 
16.56 
41.84 
52.40 
PolyMap [2] 
16.30 
46.00 
57.60 
MtMCML [30] 
14.08 
45.84 
59.84 
LCRML [3] 
10.68 
35.04 
46.48 
MRank-PRDC [25] 
11.12 
35.76 
46.56 
MRank-RankSVM [25] 
12.24 
36.32 
46.56 
PRDC [44] 
9.68 
32.96 
44.32 
RankSVM [33] 
10.24 
33.28 
43.68 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Comparison of state-of-the-art results on the PRID 450S database (P=225). The cumulative matching scores (%) at rank 1, 10, and 20 are listed.</figDesc><table>Method 
rank=1 rank=10 rank =20 
Ours 
60.49 
88.58 
93.60 
LOMO+XQDA [20] 
61.42 
90.84 
95.33 
MirrorRep [4] 
55.42 
87.82 
93.87 
Semantic [35] 
44.90 
77.50 
86.70 
ECM [24] 
41.90 
76.90 
84.90 
SCNCD </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Comparison of Detection and identification rates (%) on the OPeRID dataset. The DIR values at rank 1, 10 under FAR=1% and FAR=10% are listed.</figDesc><table>FAR=1% 
FAR=10% 
rank=1 rank=10 rank =1 rank =10 
Ours 
3.15 
3.76 
15.08 
18.17 
RRDA [21] 
3.99 
4.35 
14.51 
16.72 
LADF [19] 
1.53 
1.74 
9.11 
10.82 
KISSME [16] 
1.82 
1.92 
9.99 
11.46 
MAHAL [16] 
1.89 
1.99 
10.50 
11.97 
ITML [6] 
1.18 
1.21 
8.39 
9.27 
LMNN [39] 
0.41 
0.41 
3.97 
4.58 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Y. Zhang, B. Li, and H. Lu are supported by the Natural Science Foundation of China #61528101 and #61472060.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Similarity learning on an explicit polynomial kernel feature map for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1565" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relevance metric learning for person re-identification by exploiting global similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Pattern Recognition</title>
		<meeting>IEEE International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1657" to="1662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mirror representation for modeling view-specific transform in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3402" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pedestrian recognition with a learned metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="501" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</title>
		<meeting>IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Is that you? metric learning approaches for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="498" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person reidentification by efficient impostor-based metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Advanced Video and Signal-Based Surveillance</title>
		<meeting>IEEE International Conference on Advanced Video and Signal-Based Surveillance</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relaxed pairwise learned metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="780" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Person reidentification by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3610" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Open-set person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.0872</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling pixel process with scale invariant local patterns for background subtraction in complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kellokumpu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1301" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised coupled dictionary learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3550" to="3557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An ensemble color model for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="868" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person re-identification by manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3567" to="3571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-camera activity correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1988" to="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Query based adaptive re-ranking for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="397" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local descriptors encoded by fisher vectors for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision Workshops and Demonstrations</title>
		<meeting>European Conference on Computer Vision Workshops and Demonstrations</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Covariance descriptor based on bio-inspired features for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="379" to="390" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person re-identification over camera networks using multi-task distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3656" to="3670" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PCCA: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boghossian. Local fisher discriminant analysis for pedestrian reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3318" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person reidentification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>British Machine Vision Association</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mahalanobis distance learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Person Re-Identification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="247" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transferring a semantic representation for person re-identification and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4184" to="4193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficiently scaling up crowdsourced video annotation -A set of best practices for high quality, economical video labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="204" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2216" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Query-adaptive late fusion for image search and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
