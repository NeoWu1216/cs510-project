<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Some like it hot -visual guidance for preference prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>CVL, ETH</roleName><forename type="first">Rasmus</forename><surname>Rothe</surname></persName>
							<email>rrothe@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Radu Timofte CVL</orgName>
								<orgName type="institution" key="instit2">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Radu Timofte CVL</orgName>
								<orgName type="institution" key="instit2">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<email>vangool@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Radu Timofte CVL</orgName>
								<orgName type="institution" key="instit2">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Radu Timofte CVL</orgName>
								<orgName type="institution" key="instit2">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Radu Timofte CVL</orgName>
								<orgName type="institution" key="instit2">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Some like it hot -visual guidance for preference prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For people first impressions of someone are of determining importance. They are hard to alter through further information. This begs the question if a computer can reach the same judgement. Earlier research has already pointed out that age, gender, and average attractiveness can be estimated with reasonable precision. We improve the stateof-the-art, but also predict -based on someone's known preferences -how much that particular person is attracted to a novel face. Our computational pipeline comprises a face detector, convolutional neural networks for the extraction of deep features, standard support vector regression for gender, age and facial beauty, and -as the main novelties -visual regularized collaborative filtering to infer interperson preferences as well as a novel regression technique for handling visual queries without rating history. We validate the method using a very large dataset from a dating site as well as images from celebrities. Our experiments yield convincing results, i.e. we predict 76% of the ratings correctly solely based on an image, and reveal some sociologically relevant conclusions. We also validate our collaborative filtering solution on the standard MovieLens rating dataset, augmented with movie posters, to predict an individuals movie rating. We demonstrate our algorithms on howhot.io which went viral around the Internet with more than 50 million pictures evaluated in the first month.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>'First impressions count' the saying goes. Indeed, psychology confirms that it only takes 0.1 seconds for us to get a first impression of someone <ref type="bibr" target="#b46">[47]</ref>, with the face being the dominant cue. Factors that are relevant for survival seem the ones evolution makes us pick up the fastest. These include age, gender, and attractiveness. We will call those quantifiable properties, such as age and gender, 'demographics'. An everyday demonstration is that people on dating sites often base their decisions mainly on profile images rather than textual descriptions of interests or occupation. Our goal is to let a computer predict someone's preferences, from single facial photos (in the wild). In particular, we try to predict how much a previously unseen face would be attractive for ? <ref type="figure">Figure 1</ref>. Can we infer preferences from a single image? a particular person who has already indicated preferences for people in the system.</p><p>Our main benchmark is a large dataset of more than 13,000 user profiles from a dating site. We have access to their age and gender, as well as more than 17 million 'hot or not' ratings by some users about some other users (their profile photo). The ratings are very sparse when compared to their potential number. For people who have given ratings, we want to predict new ratings for other people in and outside the dataset.</p><p>The visual information, here the profile image, presumably containing a face, is the main basis for any user-to-user rating. Therefore, we employ a face detector and crop the best detected face and its surrounding context (corresponding to body and posture) from which we extract deep features by means of a (fine-tuned) convolutional neural network. In order to make sure that these features are appropriate for the main task -automated attractiveness rating -we first test our features on age, gender, and facial beauty estimation for which previous methods and standard datasets exist. We obtain state-of-the-art results.</p><p>For predicting preferences for users with known ratings for a subset of others in the dataset, collaborative filtering is known to provide top results, i.e. for movie <ref type="bibr" target="#b21">[22]</ref> or advertisement suggestions <ref type="bibr" target="#b35">[36]</ref>. We adapt this framework to take account of visual information, however. As our experiments will show, adding visual information improves the prediction, especially in cases with few ratings per user. In case of a new face, not part of the dataset and thus without a history of preferences, we propose to regress the input im-age to the latent space of the known users. By doing so, we alleviate the need for past ratings for the query and solely rely on the query image.</p><p>The same technique can be applied to different visualsenhanced tasks, such as rating prediction of movies, songs, shopping items, in combination with a relevant image (e.g. movie poster, CD cover, image of the item). We test on the MovieLens dataset augmented with poster images for each movie, a rather weak information, to demonstrate the wider applicability of our approach.</p><p>We demonstrate our algorithms on howhot.io, a website where people can upload a photo of their face and an algorithm will then estimate the age, gender and facial attractiveness of the person.</p><p>The main contributions of our work are:</p><p>• an extensive study on the inference of information from profile images using the largest dating dataset thus far • a novel collaborative filtering approach that includes visual information for rating/preference prediction • a novel regression technique for handling visual queries without rating history which prior work cannot cope with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The focus of our paper is to infer as much information as possible from a single image and to predict subjective preferences based on an image query with possibly a prior rating history. Next we review related works. Image features. Instead of handcrafted features like SIFT, HoG, or Gabor filters, we use learned features obtained using neural networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40]</ref>. The latter have shown impressive performance in recent years. Such features have already been used for age and gender estimation in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>. Demographics estimation. Multiple demographic properties such as age, gender, and ethnicity have been extracted from faces. A survey on age prediction is provided by Fu et al. <ref type="bibr" target="#b9">[10]</ref> and on gender classification by Ng et al. <ref type="bibr" target="#b32">[33]</ref>. Kumar et al. <ref type="bibr" target="#b24">[25]</ref> investigate image 'attribute' classifiers in the context of face verification. Some approaches need face shape models or facial landmarks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, others are meant to work in the wild <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref> but still assume face localization. Generally, the former approaches reach better performance as they use additional information. The errors in model fitting or landmark localization are critical. Moreover, they require supervised training, detailed annotated datasets, and higher computation times. On top of the extracted image features a machine learning approach such as SVM <ref type="bibr" target="#b42">[43]</ref> is employed to learn a demographics prediction model which is then applied to new queries. Subjective property estimation. While age and gender correspond to objective criteria, predicting the attractiveness of a face is more subjective. Nonetheless, facial beauty <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48]</ref> can still be quantified by averaging the ratings by a large group. Benchmarks and corresponding estimation methods have been proposed. In the subjective direction, Dhar et al. <ref type="bibr" target="#b6">[7]</ref> demonstrate the aesthetic quality estimation and predict what they call 'interestingness' of an image, while Marchesotti et al. <ref type="bibr" target="#b29">[30]</ref> discover visual attributes (including subjective ones) to then to use them for prediction. Also, recently Kiapour et al. <ref type="bibr" target="#b20">[21]</ref> inferred complex fashion styles from images. Generally, the features and methods used for age and gender can be adapted to subjective property estimation, and we do the same in this paper. From the literature we can observe three trends: (i) besides Whitehill and Movellan <ref type="bibr" target="#b45">[46]</ref>, most papers focus on predicting facial beauty averaged across all ratings, whereas we aim at predicting the rating by a specific person; (ii) as pointed out in the study by Laurentini and Bottino <ref type="bibr" target="#b25">[26]</ref> usually small datasets are used, sometimes with less than 100 images and with only very few ratings per image -our dataset contains more than 13,000 images with more than 17 million ratings; (iii) most datasets are taken in a constrained environment showing aligned faces. In contrast, our photos contain in many cases also parts of the body and some context in the background. Thus, we focus not just on facial beauty but on general attractiveness of the person -referred to as hotness in the following.</p><p>Preferences/ratings prediction. The Internet brought an explosion of choices. Often, it is difficult to pick suitable songs to hear, books to read, movies to watch, or -in the context of dating sites -persons to contact. Among the best predictors of interest are collaborative filtering approaches that use the knowledge of the crowd, i.e. the known preferences/ratings of other subjects <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref>. The more prior ratings there are, the more accurate the predictions become. Shi et al. <ref type="bibr" target="#b38">[39]</ref> survey the collaborative filtering literature. Matrix factorization lies at the basis of many top collaborative filtering methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>. Given the importance of the visual information in many applications, we derive a matrix factorization formulation regularized by the image information. While others <ref type="bibr" target="#b38">[39]</ref> proposed various regularizations, we are the first to prove that visual guidance helps preference prediction. Moreover, we propose to regress queries without rating history to a latent space derived through matrix factorization for the known subjects and ratings.</p><p>Social networks. The expansion of the internet and the advance of smartphones boosted the (online) social networks worldwide. Networks such as Facebook facilitate interaction, sharing, and display of information and preferences among individuals. Yet, time is precious and hence efforts are made to develop filters and ranking tools to assist users. A recent study by Youyou et al. <ref type="bibr" target="#b48">[49]</ref> shows that accurate predictions about the personality of a user can be made using her/his 'likes'. Contents and ads can then be personalized and this is extremely important for social networks <ref type="figure">Figure 2</ref>. Preferences prediction scheme. For a visual query without past ratings we first regress to the latent Q space (obtained through matrix factorization) to then obtain the collaborative filtering prediction as in the case for the queries with known past ratings and Q factor. and search engines such as Google <ref type="bibr" target="#b40">[41]</ref>. This paper focuses on dating sites and the prediction of attractiveness ratings. Most such works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref> rely on past ratings and cannot cope when there are none or few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Visual features</head><p>Razavian et al. <ref type="bibr" target="#b33">[34]</ref> showed that features extracted from convolutional neural networks (CNN) are very powerful generic descriptors. Inspired by that, for all our experiments we use the VGG-16 <ref type="bibr" target="#b39">[40]</ref> features which are pre-trained on a large ImageNet object dataset and result in a descriptor of length 4,096. We use the implementation by Vedaldi and Lenc <ref type="bibr" target="#b43">[44]</ref>. We reduce the dimensionality using PCA to keep ∼ 99% of the energy. Before we use these feature to predict attractiveness to a particular user, we first confirm that the extracted visual features are powerful enough to capture minor facial differences by predicting age and gender.</p><p>We perform reference experiments on a widely used dataset for age prediction, the MORPH 2 database <ref type="bibr" target="#b34">[35]</ref>. We also test gender estimation on the same MORPH 2 dataset. Unfortunately, besides the dataset provided by Gray et al. <ref type="bibr" target="#b12">[13]</ref> -to the best of our knowledge -there are no other publicly available large datasets on averaged facial beauty. As shown next our features achieve state-of-the-art performance for age, gender, and facial beauty prediction. We believe that this good performance is mostly due to the depth of the model with 16 layers, compared with previous state-of-the-art using only 6 layers <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Predicting age and gender</head><p>Our experiments are conducted on a publicly available dataset, the MORPH 2 database <ref type="bibr" target="#b34">[35]</ref>. We adopt the experimental setup of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45]</ref>, where a set of 5,475 individuals is used whose age ranges from 16 to 77. The dataset is randomly divided into 80% for training and 20% for testing. Following the procedure described in <ref type="bibr" target="#b11">[12]</ref>, our CNN features are fine-tuned on the training set.</p><p>The age is regressed using Support Vector Regression (SVR) <ref type="bibr" target="#b3">[4]</ref> with an RBF kernel and its parameters are set by cross-validation on a subset of the training data. We report the performance in terms of mean absolute error (MAE) be-tween the estimated and the ground truth age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MORPH 2 [35] AGES <ref type="bibr" target="#b10">[11]</ref> 8.83 MTWGP <ref type="bibr" target="#b49">[50]</ref> 6.28 CA-SVR <ref type="bibr" target="#b5">[6]</ref> 5.88 SVR <ref type="bibr" target="#b14">[15]</ref> 5.77 OHRank <ref type="bibr" target="#b4">[5]</ref> 5.69 DLA <ref type="bibr" target="#b44">[45]</ref> 4.77 Proposed Method 3.45 <ref type="table">Table 1</ref>. Age estimation performance in terms of mean absolute error (MAE) on the MORPH 2 dataset. We improve the state-ofthe-art results by more than 1 year.</p><p>Age in MORPH 2, men Beauty in Gray Average beauty is less meaningful, suggesting personalized prediction.</p><p>As shown in <ref type="table">Table 1</ref>, we achieve state-of-the-art performance on the MORPH 2 dataset (3.45 years MAE) by reducing the error of the currently best result (4.77 MAE reported by <ref type="bibr" target="#b44">[45]</ref>) with more than a full year. For gender prediction on the MORPH 2 dataset we keep the same partition as for age and achieve 96.26% accuracy, which, despite the small training set, is on par with other results in the literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. <ref type="figure" target="#fig_1">Fig. 4</ref> shows several good and erroneous predictions of our method on the MORPH 2 dataset. <ref type="figure" target="#fig_0">Fig. 3</ref> shows averages of faces ranked according to age on MORPH 2 and beauty on Gray, resp. On our more challenging Hot-or-Not dataset (Section 5.1.1) we achieve 3.61 MAE for age and 88.96% accuracy for gender prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Predicting facial beauty</head><p>Following a similar procedure as for age prediction, we test our features on the dataset introduced by Gray et al. <ref type="bibr" target="#b12">[13]</ref>. The Gray dataset contains 2056 images with female faces collected from a popular social/dating website 1 . The facial beauty was rated by 30 subjects and the ratings were then normalized as described in <ref type="bibr" target="#b12">[13]</ref>. The dataset is split into 1028 images for training and 1028 for testing. We report the average performance across exactly the same 5 splits from the reference paper in terms of Pearson's correlation coefficient, the metric from the original paper. Also, we report performance with and without face alignment using the same alignment algorithm of Huang et al. <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Correlation Correlation w/o alignment w/ alignment Eigenface 0.134 0.180 Single Layer Model <ref type="bibr" target="#b12">[13]</ref> 0.403 0.417 Two Layer Model <ref type="bibr" target="#b12">[13]</ref> 0.405 0.438 Multiscale Model <ref type="bibr" target="#b12">[13]</ref> 0.425 0.458 Proposed Method 0.470 0.478 <ref type="table">Table 2</ref>. Facial beauty estimation performance on Gray dataset with and without face alignment in terms of correlation.</p><p>As shown in <ref type="table">Table 2</ref> our proposed features achieve stateof-the-art performance on predicting facial beauty as averaged over multiple raters. We improve by more than 10% over the best score reported by <ref type="bibr" target="#b12">[13]</ref> for the raw images. A couple of per image results are depicted in <ref type="figure" target="#fig_1">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Predicting preferences</head><p>Our goal is to make personalized predictions, such as how a specific male subject m ∈ M rates a female subject f ∈ F . The rating R mf is 1 if 'm likes f ', -1 if 'm dislikes f ', and 0 if unknown. f is also called the query user, as at test time we want to predict the individual ratings of all men for that woman. Due to space limitations, we derive the formulation for this case. Yet it is also valid when swapping sexes, i.e. when women are rating men.</p><p>In the following section we phrase the problem as a collaborative filtering problem, assuming that we know past ratings for both men and women. In Section 4.2 we extend the formulation to also consider the visuals of the subjects being rated. In Section 4.3 we present a solution to predict the ratings solely based on the visual information of the subjects, without knowing how they were rated in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model-based collaborative filtering (MF)</head><p>We phrase the problem of a user m rating the image of user f as a model-based collaborative filtering problem. The model learned from known ratings is then used to predict unknown ratings. In its most general form, we have</p><formula xml:id="formula_0">g(P m , Q f ) ⇒ R mf , m = 1, 2, ..., M, f = 1, 2, ..., F,<label>(1)</label></formula><p>where the function g maps the model parameters to the known ratings. P m denotes a set of model parameters describing the preferences of user m. Similarly, Q f describes the appearance of user f , i.e. a low-dimensional representation of how the appearance of a user is perceived. We now estimate the model parameters given the ratings we know.</p><p>In recent years, Matrix Factorization (MF) techniques have gained popularity, especially through the Netflix challenge, where it achieved state-of-the-art performance <ref type="bibr" target="#b21">[22]</ref>. The basic assumption underlying MF models is that we can learn low-rank representations, so-called latent factors, to predict missing ratings between user m and image f . One can approximate the ratings as</p><formula xml:id="formula_1">R ≈ P T Q =R.<label>(2)</label></formula><p>In the most common formulation of MF <ref type="bibr" target="#b38">[39]</ref> we can then frame the minimization as</p><formula xml:id="formula_2">P * , Q * = argmin P,Q 1 2 M m=1 F f =1 I mf (R mf −P T m Q f ) 2 + α 2 ( P 2 + Q 2 )<label>(3)</label></formula><p>where P and Q are the latent factors and P * and Q * their optimal values. I mf is an indicator function that equals 1 if there exists a rating R mf . The last term regularizes the problem to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Visual regularization (MF+VisReg)</head><p>Knowing that the users in the app rate the subjects of the opposite sex solely based on the image, we make the assumption that people with similar visual features have similar latent appearance factors Q. Thus we can extend the formulation by adding the visual features V of the query images to further regularize the optimization</p><formula xml:id="formula_3">L(P, Q) = 1 2 M m=1 F f =1 I mf (R mf − P T m Q f ) 2 + α1 2 ( P 2 + Q 2 ) + α2 2 F f =1 F g=1 (S f g − Q T f Q g ) 2 .<label>(4)</label></formula><p>The visual similarity is defined as</p><formula xml:id="formula_4">S f g = V T f V g V f V g .<label>(5)</label></formula><p>Visually this proves to be a good metric for visual similarity. The optimal latent factors are calculated by gradient descent, where the derivatives are</p><formula xml:id="formula_5">∂L ∂Pm = F f =1 I mf (P T m Q f − R mf )Q f + λP m ∂L ∂Q f = M m=1 I mf (P T m Q f − R mf )P m +2α 2 F g=1 (Q T f Q g − S f g )Q g + λQ f .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visual query</head><p>We now want to predict how user m rates user f without knowing any past ratings of f but knowing her visual feature V f (see <ref type="figure">Fig. 2</ref>). This implies that we do not know the latent factor Q f for f . The goal is to get an estimateQ f of Q f based solely on the visual feature V f . Then we would be able to regress the rating aŝ</p><formula xml:id="formula_6">R mf = P T mQf .<label>(7)</label></formula><p>Learning a global regression led to poor results as attractiveness is highly subjective. Instead our approach is inspired by the recently introduced anchored neighborhood regression (ANR) method for image super-resolution <ref type="bibr" target="#b41">[42]</ref>, where  the problem is formulated as a piece-wise local linear regression of low to high resolution image patches and with offline trained regressors. In contrast to ANR, each sample is an anchor and the neighborhood is spanned over all other training samples and weighted according to its similarity to the anchor. This way we are obtaining more robust local regressors that can cope with the scarcity of the data. As for regularizing MF, we assume that the visual features V and the latent factor Q locally have a similar geometry. Further, we assume that we can locally linearly reconstruct each visual feature or latent factor by its neighbors. Under these assumptions we can reconstruct features and latent factors using the same weights for the neighbors. In the visual space we now aim to find these weights β by phrasing the problem as a ridge regression</p><formula xml:id="formula_7">min βg V g −N Vg β g 2 +λ κ Γ g β g 2 +(1−κ) β g 2 ,<label>(8)</label></formula><p>where N Vg is a matrix of the neighboring visual features of V g stacked column-wise and κ is a scalar parameter. The optimization is regularized by the similarity to its neighbors according to eq. 5, in the sense that greater similarity yields greater influence on β:</p><formula xml:id="formula_8">Γ g = diag(1 − S g1 , 1 − S g2 , ..., 1 − S gF ).<label>(9)</label></formula><p>The closed-form solution of the problem can be written as</p><formula xml:id="formula_9">β k = N T Vg N Vg +λ κΓ T g Γ g +(1−κ)I −1 N T Vg V g .<label>(10)</label></formula><p>As we assume that the latent space behaves similarly locally, we can regress the latent factor Q g as a linear combination of its neighbors using the same β g . Note that N Qg corresponds to the latent factors of N Vg , i.e. the neighbors in the visual space. Plugging in our solution for β g we get</p><formula xml:id="formula_10">Q g = N Qg β g = N Qg N T Vg N Vg +λ κΓ T g Γ g +(1−κ)I −1 N T Vg V g = M g V g .<label>(11)</label></formula><p>Thus we have found a projection M g from a visual feature V g to its latent factor Q g . At test time for a given visual feature V f , we now aim to find the most similar visual feature in the training space,ĝ = arg max g S f g . Then we use the projection matrix ofĝ to obtainQ f to finally estimate the rating of user m for the image of user f aŝ</p><formula xml:id="formula_11">R mf = P T mQf ,Q f = MĝV f .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we present qualitative and quantitative results of our proposed framework on the Hot-or-Not and the MovieLens dataset.  Preferences bias. To investigate the natural bias caused by age, we divide the men and women from our dataset according to their age and gender. For each age group of men we counted the percent of hot vs. not on the ratings towards the age groups of women and vice versa. <ref type="figure" target="#fig_2">Fig. 5</ref> describes the preferences by age as found in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Hot-or-Not</head><p>Women generally prefer slightly older men and give better ratings the older they get. In comparison, men on this app prefer women under 25. Hotness paradox. We notice an interesting phenomenon. Most people have a lower rating than their visually similar neighbors, on average, and this holds for both men and women. In <ref type="figure" target="#fig_3">Fig. 6</ref> we report the percentage of cases where the average hotness of the neighborhood of a subject is greater than that of the subject, and this for a neighborhood size from 1 to 10 3 . We plot also the results when we use our latent Q representations for retrieving neighbors. Surprisingly, this time the situation is reversed, the subjects tend to be hotter than their Q-similar neighborhood. Regardless the choice of similarity we have a strong deviation from the expected value of 50%. We call this phenomenon the 'Hotness paradox'. It relates to the so-called 'Friendship paradox' <ref type="bibr" target="#b8">[9]</ref> in social networks, where most people have fewer friends than their friends have. Visual features. As the space covered by the person varies greatly between images, we run a top face detector <ref type="bibr" target="#b30">[31]</ref> on each image. Then we crop the image to the best scoring face detection and include its surrounding (100% of the width to each side and 50% of the height above and 300% below), to capture the upper-body and some of the background. If the face is too small or the detection score too low, we take the entire image. Then we extract CNN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experimental setup</head><p>For all experiments, 50% of either gender are used for training and the rest for testing. For each user in the testing set, 50% of the received ratings are used for testing. We compare different methods. Baseline predicts the majority rating in the training set. Matrix factorization is applied without and with visual regularization, MF (α 1 = 0.1) and MF+VisReg (α 2 = 0.1), resp. The dimensionality of the latent vector of P and Q is fixed to 20. The other parameters were set through cross-validation on a subset of the training data. We predict the ratings a subject receives based upon different knowledge: For Visual we solely rely on the image of the subject which means that we do not know any ratings the subject has received so far. For 10 Ratings, 100 Ratings, Full History, we instead base the prediction upon a fixed set of known ratings for each query user. We report the average accuracy, i.e. the percentage of correctly predicted ratings of the testing set, and the Pearson's correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Results</head><p>Performance. <ref type="figure" target="#fig_4">Fig. 7</ref> shows how the average accuracy varies with the number of known past ratings for the query user. We report the average performance across all men's ratings. Knowing just the image of the person, we can predict 75.92% of the ratings correctly. Adding past received ratings of the user improves performance to up to 83.64%. Matrix factorization significantly improves as more ratings are known. If only few ratings are available, regularizing the matrix factorization with the visuals boosts performance significantly, i.e. from 72.92% to 78.68% for 10 known ratings. <ref type="table" target="#tab_3">Table 3</ref> summarizes the results for various settings.   Latent space Q vs. preferences. In <ref type="figure" target="#fig_5">Fig. 9</ref> we show the learned latent space Q from the matrix factorization by PCA projecting it to two dimensions and adding the hotness and age properties for both genders with visual regularization. The learned latent factor Q captures appearance and for women there is a clear separation in terms of attractiveness and age, whereas for men the separation is less obvious. According to the preferences P and the learned latent Q one can have a more in-depth view on the appearance of women and men. In <ref type="figure" target="#fig_6">Fig. 10</ref> both men and women are clustered according to their 2D representation of the learned latent factors P (preferences of men) and Q (appearances of   women), respectively. For visualization purposes we used 100 user images for each cluster and 10 clusters. The men are visually more diverse in each of their clusters than the women in their clusters, because the men are clustered according to their preferences, therefore ignoring their visual appearance, while the women are clustered according to their Q factors which are strongly correlated with appearance and hotness, as shown in <ref type="figure" target="#fig_5">Fig. 9</ref>. Visual queries without past ratings. We validate our approach on images outside our dataset, retrieved from the internet for celebrities. By applying the visual query regression to the Q space we can make good predictions for such images. For a visual assessment see <ref type="figure">Fig. 12</ref>. This figure also depicts a number of issues our pipeline faces: too small faces, detector failure, wrongly picked face, or simply a wrong prediction. We also tested our method on cartoons and companion pets with the predictor trained on Hot-or-Not. The results are surprising. Instagram filters or how to sell your image. Images and their hotness prediction also indicate which changes could improve their ratings. Earlier work has aimed at the beautification of a face image by invasive techniques such as physiognomy changes <ref type="bibr" target="#b27">[28]</ref> or makeup <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>. Yet, we found that non-invasive techniques (not altering facial geometry and thus 'fair') can lead to surprising improvements. We have evaluated the most popular Instagram filters 3 for our <ref type="bibr" target="#b2">3</ref> brandongaille.com/10-most-popular-instagram-photo-filters task. We observed that the filters lead to an increase in predicted hotness. In <ref type="figure">Fig. 8</ref> we show a couple of results in comparison to the original image. Note that with our predictor and such Instagram filters a user can easily pick its best profile photo. We demonstrate our algorithms on howhot.io, a website where people can upload a photo of their face and our algorithm will then estimate the age, gender and facial attractiveness of the person (c.f. <ref type="figure" target="#fig_7">Fig. 11</ref>). The CNN was trained on the Hot-or-Not dataset for predicting attractiveness and on the IMDB-WIKI dataset <ref type="bibr" target="#b36">[37]</ref> for age and gender prediction. The website went viral around the Internet with more than 50 million pictures evaluated in the first month.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">howhot.io</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">MovieLens</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">The dataset</head><p>We also perform experiments on the MovieLens 10M 4 dataset. It contains 10,000,054 ratings from 69,878 users for 10,681 movies. Ratings are made on a 5-star scale, with half-star increments. On average, each movie has 3,384 ratings and each user rates 517 movies. Note that even though there are more than 10 million ratings, the rating matrix is sparse with only 1.34% of all ratings known. We augment each movie with the poster image from IMDB and extract the same deep CNN features as for the Hot-or-Not dataset. We will make the poster images publicly available under http://www.vision.ee.ethz.ch/˜rrothe/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Experimental Setup</head><p>The experimental setup in term of training and testing split is identical to the Hot-or-Not dataset. As the movie posters are much less informative regarding the ratings in comparison to the Hot-or-Not images, the visual regularization is reduced to α 2 = 0.001. For a given movie we want to infer the ratings of all users. Again, we evaluate the case where just the poster is known and also cases where a varying number of ratings is known. As a baseline we show how a prediction made at random would perform, assuming that there is no bias in the ratings of the test set.     <ref type="table">Table 4</ref>. Rating prediction results on augmented MovieLens. <ref type="table">Table 4</ref> summarizes the performance. <ref type="figure" target="#fig_1">Fig. 14</ref> shows how the number of known ratings impacts the MAE. Visual regularization of MF improves performance, especially when few ratings are known, i.e. for 10 known ratings the MAE can be reduced by 15% from 1.031 to 0.872. When just the movie poster is known, the MAE is 0.813, which is on par with knowing 30 ratings. <ref type="figure" target="#fig_0">Fig. 13</ref> shows some movie posters. We also show overrated and underrated posters, i.e. posters where our algorithm -based on the poster -predicts a much better or worse score than the actual movie rating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a collaborative filtering method for rating/preference prediction based not only on the rating history but also on the visual information. Moreover, we can accurately handle queries with short or lacking rating history. We evaluated our system on a very large dating dataset and on the MovieLens dataset augmented with poster images. To the best of our knowledge, we are the first to report on such a large dating dataset, and to show that adding weak visual information improves the rating prediction of collaborative filtering methods on MovieLens. We achieved stateof-the-art results on facial beauty, age and gender prediction and give some sociologically interesting insights.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Average faces for 5 clusters based on age or beauty, resp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Examples of accurately and wrongly predicted age, gender, and facial beauty for the MORPH 2 and Gray datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Preferences by age for women and men.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Hotness paradox. The people visually similar to you are on average hotter than you. The situation changes when we compute the similarity based on learned latent Q representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Number of known ratings for a female query user vs. accuracy of predicted male's ratings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Visualization of latent space Q for women and men.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Preferences between clusters of users. The color of the arrow indicates how much the men's cluster likes (green) or dislikes (red) the women's cluster on average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>howhot.io</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 .</head><label>13</label><figDesc>Examples of predicted ratings for various movie posters solely based on the visual information of the poster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .</head><label>14</label><figDesc>Number of known ratings for a movie vs. MAE of the predicted ratings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Preference prediction results on Hot-or-Not dataset for female queries.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Figure 12. Predicted percentage of positive ratings for numerous celebrities by the user base of the Hot-or-Not dataset.</figDesc><table>Helena Bonham Carter 

Natalie Portman 
Charlize Theron 

20% 
35% 
62% 
19% 
35% 
59% 
27% 
31% 
46% 
68% 
Cate Blanchett 
Bette Midler 
Jim Carrey 

26% 
42% 
59% 
20% 
32% 
47% 
19% 
32% 
59% 
Cats 
Dogs 
Wonder Woman 
Some like it hot 

29% 
33% 
32% 
36% 
33% 
47% 
27% 
39% 
54% 
54% 
Melissa McCarthy 
Too small face 
Face detector fails 
Wrong person 
Wrong prediction 

24% 
45% 
32% 
39% 
34% 
39% 
14% 
38% 
18% 
67% 

Correctly predicted 
Overrated poster 
Underrated poster 

Average prediction 
3.9 
4.0 
3.1 
3.5 
2.7 
2.4 
3.8 
3.5 
3.7 
2.4 
2.9 
2.7 
Average rating 
4.1 
4.0 
3.1 
3.5 
2.7 
2.5 
0.6 
1.4 
1.9 
3.9 
3.9 
4.5 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://hotornot.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">www.blinq.ch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">grouplens.org/datasets/movielens</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Blinq for providing the Hot-or-Not data and NVIDIA for donating a Tesla K40 used in our research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Relative ranking of facial attractiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altwaijry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Empirical analysis of predictive algorithms for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Breese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recommender system for online dating service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brozovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Petricek</surname></persName>
		</author>
		<idno>cs/0703042</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIST</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ordinal hyperplanes ranker with cost sensitivities for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facial attractiveness: Beauty and the machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eisenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="142" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Why your friends have more friends than you do</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Feld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Sociology</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1464" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Age synthesis and estimation via faces: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1955" to="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Automatic age estimation based on facial aging patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith-Miles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2234" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting facial beauty without landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Digital face makeup by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-based human age estimation by manifold learning and locally adjusted robust regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1178" to="1188" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simultaneous dimensionality reduction and human age estimation via kernel partial least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint estimation of age, gender and ethnicity: Cca vs. pls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Age estimation from face images: Human vs. machine performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised joint alignment of complex images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multimodal human-computer interaction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="116" to="134" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hipster wars: Discovering elements of fashion styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Interaction-based collaborative filtering methods for recommendation in online dating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krzywicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wobcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahidadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Compton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<idno>WISE. 2010</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Computer analysis of face beauty: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laurentini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bottino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Data-driven enhancement of facial attractiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leyvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Wow! you are so beautiful today! TOMM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discovering beautiful attributes for aesthetic image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="246" to="266" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Computational facial attractiveness prediction by aestheticsaware features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="59" to="64" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recognizing human gender in computer vision: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-M</forename><surname>Goi</surname></persName>
		</author>
		<idno>PRICAI. 2012</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automated collaborative filtering in world wide web advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CSUR</publisher>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adaptive web search based on user profile constructed without any effort from users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hatano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yoshikawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4564</idno>
		<title level="m">Matconvnet -convolutional neural networks for matlab</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeply-learned feature for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Personalized facial attractiveness prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">First impressions: Making up your mind after 100 ms exposure to a face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="592" to="598" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cost-sensitive ordinal regression for fully automatic facial beauty assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="334" to="342" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Computer-based personality judgments are more accurate than those made by humans. National Academy of Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Youyou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stillwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-task warped gaussian process for personalized age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
