<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-quality Depth from Uncalibrated Small Motion Clip</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyowon</forename><surname>Ha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">In So Kweon † † Korea Advanced Institute of Science and Technology ‡ Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">In So Kweon † † Korea Advanced Institute of Science and Technology ‡ Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">In So Kweon † † Korea Advanced Institute of Science and Technology ‡ Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">In So Kweon † † Korea Advanced Institute of Science and Technology ‡ Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">High-quality Depth from Uncalibrated Small Motion Clip</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel approach that generates a highquality depth map from a set of images captured with a small viewpoint variation, namely small motion clip. As opposed to prior methods that recover scene geometry and camera motions using pre-calibrated cameras, we introduce a self-calibrating bundle adjustment tailored for small motion. This allows our dense stereo algorithm to produce a high-quality depth map for the user without the need for camera calibration. In the dense matching, the distributions of intensity profiles are analyzed to leverage the benefit of having negligible intensity changes within the scene due to the minuscule variation in viewpoint. The depth maps obtained by the proposed framework show accurate and extremely fine structures that are unmatched by previous literature under the same small motion configuration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Small motion in a hand-held camera commonly happens when a user moves the device slightly to find a better photographic composition, or even when the user tries to hold the camera steady before pressing the shutter. If we were able to restore the geometry of the scene using the small motion clip captured at that moment, it could be useful for a variety of applications, such as synthetic refocusing or view synthesis. <ref type="figure" target="#fig_1">Figure 1</ref> shows an example of the small motion clip. The averaged image of the entire sequence gives a sense of how small the camera motion is.</p><p>In this paper, we propose an effective pipeline for depth acquisition from a small motion clip. At the core of our approach is the novel bundle adjustment scheme that is specially devised to be applied to the small motion case. Unlike to prior approaches, our algorithm can jointly estimate the intrinsic parameters and poses of the camera from a small motion footage, which imbues the proposed method with practicality and severs the need for camera calibration.</p><p>By virtue of reliably estimating the intrinsic and extrinsic camera parameters, a plane sweeping based dense stereo matching algorithm can be directly applied to produce a dense depth map in a unified framework. A notable benefit  to the small motion clip is that the observed image intensities for a given point in the scene are almost identical along the sequence due to the low variation in viewpoint. The dozens of intensity observations also give a better chance in finding reliable matchings. In order to leverage this benefit, our dense stereo matching algorithm utilizes the variance of the intensity profile as the cost measure for plane sweeping, which is less likely to be affected by the noise of the reference image compared to other pair-wise intensity difference based methods.</p><p>As opposed to previous approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27</ref>] that target the same goal for small motion, the distinctive points of our approach can be summarized as follows:</p><p>• A unified framework for depth from an uncalibrated small motion clip is proposed, which can allow the user to acquire a high-quality depth map from a single instance of capture.</p><p>• Our bundle adjustment can even jointly estimate the camera intrinsic parameters (i.e. focal length and radial distortion) as well as the camera poses and the scene geometry from a single small motion clip.</p><p>• Our dense stereo matching that analyzes the intensity and gradient profiles in plane sweeping can generate depth maps exhibiting extremely fine structures, which have not been demonstrated in previous literature under the same small motion conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D reconstruction from a hand-held camera is a widely studied topic. SfM successfully recovers the sparse 3D geometry and camera poses for wide baseline images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>. The bundle adjustment <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4]</ref> minimizes the reprojection errors using an optimization framework. Other approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12]</ref> use the L ∞ norm instead of the L 2 norm to make the cost function convex, but they are more susceptible to outliers. As opposed to SfM, multiview stereo (MVS) can provide a depth for each pixel via dense matching of the images <ref type="bibr" target="#b15">[16]</ref>. Gallup et al. <ref type="bibr" target="#b4">[5]</ref> present an effective image matching method that selects a proper baseline and image resolution adapted for the scene depth.</p><p>Conventional SfM and MVS approaches can reconstruct accurate 3D geometry using wide-baseline images, but users often cannot capture such images. Yu and Gallup <ref type="bibr" target="#b26">[27]</ref> propose an inspirational method that can estimate camera trajectory even from a clip with hand-shaking motion. They recover a dense depth map from a random depth initialization and perform a plane sweeping <ref type="bibr" target="#b2">[3]</ref> based image matching that incorporates a Markov Random Field <ref type="bibr" target="#b12">[13]</ref>. Although it is a well-known fact that narrow baselines affects the accuracy of the estimated 3D geometry <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, the inverse depth representation <ref type="bibr" target="#b26">[27]</ref> is successfully demonstrated in challenging small motion scenarios. Im et al. <ref type="bibr" target="#b8">[9]</ref> extends <ref type="bibr" target="#b26">[27]</ref> with the consideration of rolling the shutter effect. Instead of performing dense image matching, they propagates the tracked 3D points into the canonical image domain. As the propagation is regularized by smooth surface normal map obtained from sparse depth points, the resulting depth map is also smooth. Joshi and Zitnick <ref type="bibr" target="#b9">[10]</ref> adopts a homography based image warping for dense image matching with the micro baseline assumption. Their algorithm even targets the tremble of a camera mounted on a tripod.</p><p>The proposed approach also targets small motion clips obtained by monocular cameras. To the best of our knowledge, we are the first to demonstrate that even the camera intrinsic and lens parameters can be reasonably estimated from a small motion clip. This allows us to introduce a fully automatic pipeline that performs a self-calibration of the camera and estimation of a high-quality depth map. As opposed to <ref type="bibr" target="#b26">[27]</ref> that computes a pair-wise consistency between the images, our dense stereo measures the consistency of the observed intensities by looking at the intensity  <ref type="figure">Figure 2</ref>. Small motion geometry used in our bundle adjustment for an uncalibrated camera (Sec. 3.1). We adopt the distorted-toundistorted mapping function F to utilize the inverse depth representation in an analytic form.</p><p>distributions. Our high fidelity depth map is more reliable that that of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We introduce the two consecutive stages of the proposed framework. The small motion image sequence is first processed in the bundle adjustment to estimate the camera parameters, then the undistorted images and the acquired parameters are utilized in the dense stereo matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bundle Adjustment</head><p>The key aspect to the image sequences in question is that the baseline between the small motion images is significantly smaller than that of the conventional SfM problem. This makes the feature matching easier, but also results in a much higher depth uncertainty that causes conventional SfM approaches to fail.</p><p>In order to handle this challenging problem, Yu and Gallup <ref type="bibr" target="#b26">[27]</ref> introduce two practical clues: (1) the small angle approximation of the camera rotation matrix and (2) the inverse depth based 3D scene point parameterization. It is shown that the former reduces the complexity of the cost function, and the latter helps to regularize the scales of the variables in the bundle adjustment. This idea is validated well in challenging real-world datasets, but they assume that the calibrated focal length is known a priori and do not account for the effects of the lens distortion.</p><p>Based on the two aforementioned insights, we propose a novel bundle adjustment framework that is carefully designed to estimate the focal length and radial distortion parameters in addition to the 3D scene points and camera poses. Compared to a prior work targeting the same objective for a wide baseline 3D reconstruction <ref type="bibr" target="#b22">[23]</ref>, our approach is tailored for the inverse depth representation and is more effective on small motion clips.</p><p>Conventional   <ref type="bibr" target="#b26">[27]</ref>, but our approach is capable of camera selfcalibration. We use calibrated camera parameters to apply <ref type="bibr" target="#b26">[27]</ref>.</p><p>the reprojection error (we refer this approach as to the U-D model). When it comes to the inverse depth representation, computing the reprojection error becomes rather complex: a point must be back-projected into the 3D space before it is projected onto the other image. However, describing the back-projected point in an analytic form is not straightforward when using the U-D model because it is difficult to get an exact analytic inverse function for the radial distortion model <ref type="bibr" target="#b14">[15]</ref>. Our bundle adjustment scheme is built in a slightly different way to avoid losing its analytic form that is essential for non-linear optimization. We accomplish this by instead adopting the D-U radial distortion model that maps the point in the Distorted image domain into the Undistorted image domain <ref type="bibr" target="#b23">[24]</ref>. While the U-D model measures the reprojection error in the distorted image domain, our D-U model measures the error in the undistorted image domain. This idea is fitting for the inverse depth representation because any feature point can be directly mapped onto the undistorted image domain for the back-projection or comparison with the reprojected point.</p><p>We follow a reasonable approximation of the camera model using one focal length f and two radial distortion parameters k 1 , k 2 , where the principal point and radial distortion center are assume to be equal to the image center, as done in <ref type="bibr" target="#b22">[23]</ref>. The small motion geometry used in our bundle adjustment is depicted in <ref type="figure">Figure 2</ref>.I fu ij is the distorted coordinates for the j-th feature in the i-th image relative to the image center, its undistorted coordinates can be calcu-</p><formula xml:id="formula_0">lated as u ij F uij f , where F is the D-U radial distortion function that is defined by: F (·)=1+k 1 · 2 + k 2 · 4 .<label>(1)</label></formula><p>If i =0for the reference image, the back-projection of the feature u 0j to its 3D coordinates x j is parameterized using its inverse depth w j by:</p><formula xml:id="formula_1">x j = u0j fwj F u0j f 1 wj .</formula><p>(</p><p>We now introduce a projection function π to describe the projection of x j onto the i-th image plane as</p><formula xml:id="formula_3">π (x j , r i , t i )= R (r i ) x j + t i ,<label>(3)</label></formula><formula xml:id="formula_4">R (r i )= ⎡ ⎣ 1 −r i,3 r i,2 r i,3 1 −r i,1 −r i,2 r i,1 1 ⎤ ⎦ ,<label>(4)</label></formula><formula xml:id="formula_5">[x, y, z] ⊺ =[x/z, y/z] ⊺ ,<label>(5)</label></formula><p>where r i ∈ R 3 and t i ∈ R 3 indicate the relative rotation and translation from the reference image to the i-th image, {r i,1 ,r i,2 ,r i,3 } are the elements of r i , and R is the vectorto-matrix function that transforms the rotation vector r i into the small-angle-approximated rotation matrix. The undistorted image domain coordinates of the projected point is then calculated as fπ(x j , r i , t i ). We use the distance between these coordinates and the undistorted coordinates as the reprojection error of u ij . Finally, our bundle adjustment is formulated to minimize the reprojection errors of all the features in the non-reference images by:</p><formula xml:id="formula_6">argmin K,R,T,W n−1 i=1 m−1 j=0 ρ u ij F u ij f − fπ(x j , r i , t i ) ,<label>(6)</label></formula><p>where n is the number of images, m the number of features, ρ (·) the element-wise Huber loss function <ref type="bibr" target="#b7">[8]</ref>, K the set of the intrinsic camera parameters {f, k 1 ,k 2 }, R and T the sets of the rotation and translation vectors for the nonreference images, and W the set of inverse depth values.</p><p>To obtain the feature correspondences, we first extract the local features using the Harris corner detector <ref type="bibr" target="#b5">[6]</ref>i n the reference image and find the corresponding feature locations in the other images by using the Kanade-Lukas-Tomashi (KLT) algorithm <ref type="bibr" target="#b13">[14]</ref>. Each tracking is performed forwards and backwards to reject outlier features with bidirectional error greater than 0.1 pixel.</p><p>For the initial parameters of the bundle adjustment, we set the rotation and translation vectors to zero, which is mentioned to be reasonable for the small motion case <ref type="bibr" target="#b26">[27]</ref>. The focal length is set to the larger value between the image width and height. The two radial distortion parameters  are also set to zero. For the inverse depths, a random value between 0.01 and 1.0 is given to each feature.</p><formula xml:id="formula_7">(b) (c) (a)</formula><p>The proposed bundle adjustment has several benefits: 1) It can successfully handle images captured by conventional cameras having mild lens distortion without any precalibration.</p><p>2) The use of the robust Huber loss function helps disregard the effects of outliers. Therefore, filtering through the use of random sample consensus (RANSAC) based two view relation estimation is not necessary. In practice, the two view approach can be unstable for small baselines <ref type="bibr" target="#b26">[27]</ref>.</p><p>Although our formulation requires a higher order compared to that of <ref type="bibr" target="#b26">[27]</ref> due to the addition of the intrinsic camera parameters, we find that our bundle adjustment successfully converges with a reasonable approximation for the parameters in most of our experiments (see <ref type="figure" target="#fig_3">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dense Stereo Matching</head><p>Once we have obtained the intrinsic and extrinsic camera parameters from the previous stage, we can utilize these parameters in our plane sweeping based dense stereo matching algorithm to recover a dense depth map. The distortion in the input images are rectified using the estimated intrinsic parameters. The rectified images are then used in this step.</p><p>The original idea of the plane sweeping algorithm <ref type="bibr" target="#b2">[3]</ref>is to back-project the tracked features onto an arbitrary virtual plane perpendicular to the z-axis of the canonical view point. If the back-projected points from all viewpoints are gathered in a small region of the virtual plane, we can conclude that the depth of the tracked feature is equivalent to that of the plane. Otherwise, this step repeats using other virtual planes. This simple but powerful idea is extended to dense stereo matching by warping the images onto the sweeping plane and measuring the photo consistency of each pixel of the warped images. The representative approach <ref type="bibr" target="#b1">[2]</ref> computes the absolute intensity differences between the reference image and the other images for the consistency measure.</p><p>Inspired by this reliable framework, our approach takes After applying winner-takes-all (WTA) strategy, depth from SAD based cost (WTA-SAD) shows higher depth noise than that of VAR based cost (WTA-VAR). As a result, WTA-SAD gives incorrect depth when depth refinement algorithm <ref type="bibr" target="#b25">[26]</ref> is applied.</p><p>into account the distribution of the intensities acquired from the pixels in the warped images that correspond to the same point on the virtual plane, which we collectively call the intensity profile. Consider the observed intensities in the profile acquired from the correct sweeping depth; it is reasonable to assume that the captured intensities are almost identical because the camera response function, white-balance, scene illumination, and observed scene radiance are unchanged with the kind of small viewpoint variation we are dealing with. Therefore, the profile will be uniform if the sweeping plane is at the correct depth for that pixel. <ref type="figure" target="#fig_4">Figure 4</ref> shows an example supporting this idea. Now we will introduce our dense stereo algorithm devised for small motion clips, step-by-step.</p><p>Building intensity profile. For the k-th depth in n k sweeping depths, all the images are warped by back-projecting them onto a virtual plane at a given inverse-depth 1 w k from the reference viewpoint, and then projected onto the reference image domain. The plane-induced homography H ik ∈ R 3×3 that describes the transformation from the reference image domain coordinates to the i-th image domain coordinates when passing through the virtual plane at the <ref type="bibr" target="#b0">1</ref> To prevent the abuse of the notation, we view the inverse depth w k = 1 z k as a practical analog to the depth candidate.  k-th sweeping depth can be formulated by:</p><formula xml:id="formula_8">H ik = K ⎡ ⎣ 1 −r i,3 r i,2 + w k t i,1 r i,3 1 −r i,1 + w k t i,2 −r i,2 r i,1 1+w k t i,3 ⎤ ⎦ K −1 ,<label>(7)</label></formula><p>where {t i,1 ,t i,2 ,t i,3 } are the elements of t i and K is defined as</p><formula xml:id="formula_9">f 0 pu 0 fp v 00 1 . (p u ,p v ) in K is the principal point co-</formula><p>ordinates equated to the image center. Using this homography, the i-th undistorted image I u i can be warped into the reference image domain through the operation described by the following formulation:</p><formula xml:id="formula_10">I ik (u)=I u i ( H ik u ),<label>(8)</label></formula><p>where · is the same function defined in Eq. (5) and I ik is the warped i-th image according to the k-th sweeping depth. After warping n images, every pixel u in the reference image domain has an intensity profile P(u,w k )= [I 0k (u), ··· ,I (n−1)k (u)] for the inverse depth candidate w k .</p><p>Compute matching cost volume. Based on earlier discussions, we measure the consistency of the intensity profile to evaluate the alignment of the warped images. Our matching cost C I for pixel u and depth candidate w k is defined as follows:</p><formula xml:id="formula_11">C I (u,w k )=VA R I 0k (u), ··· ,I (n−1)k (u) ,<label>(9)</label></formula><p>where VAR(p) is the variance of vector p. In order to enforce the matching fidelity on the edge regions of the image, we introduce two additional costs C δu and C δv defined as the horizontal and vertical gradients of the images, respectively. C δu is defined as follows:</p><formula xml:id="formula_12">C δu (u,w k )=VA R δI 0k δu (u), ··· , δI (n−1)k δu (u) ,<label>(10)</label></formula><p>where δI δu indicates the image gradient in the horizontal direction. We use the first-order gradient filter F =[−101] to approximate δI δu in the discrete and finite image space. C δv is similarly calculated using the vertical gradients obtained by using F ⊺ . The comprehensive matching cost C is defined as</p><formula xml:id="formula_13">C = C I + λ(C δu + C δv ).<label>(11)</label></formula><p>Although the numerous intensity observations in the profile give a reliable matching cost, the variance operator used in Eq. (9) and (10) may not robustly compute the deviation of the profile in the presence of outliers. The application of a 3 × 3 box filter on C can suppress some of the noise in the costs. The proposed matching scheme is found to recover the fine structures in the depth map results, which will be shown later.</p><p>The proposed cost volume C is analogous to the conventional plane sweeping stereo <ref type="bibr" target="#b1">[2]</ref>, which computes the sum of absolute intensity difference (SAD) between the reference image and the other images. Here, the notable difference to our method is that the pairwise matching costs depend on the reference image; if the reference image includes a large amount of noise, the matching cost will be less reliable. By contrast, our variance operator handles every element equally. <ref type="figure" target="#fig_5">Figure 5</ref> shows the depth map comparison between the SAD based cost and the VAR based cost after applying the winner-takes-all strategy and the successive refinement.</p><p>Depth refinement. After applying the winner-takes-all strategy on the cost volume C, we get the depth map D win . Although D win gives a reasonable depth estimate, some values of D win can be noisy when homogeneous textures are present in that region as may not give an obvious cost minimum. To handle noisy depth, we define a confidence measure described by the formulation M(u)= 1 −C I (u,D win (u))/P(u,D win (u)), whereP is the mean n k = number of label, zmin = nearest depth of x for k =1:n k do -Set w k = k n k z min for i =0:(n − 1) do -Warp I u i using w k , K, Ri, and ti (Eq. <ref type="formula" target="#formula_8">(7)</ref>) -Build intensity/gradient profiles and compute cost volume C (Eq. (9, 10, and 11)) -Winner-takes-all on C and get Dwin -Refine Dwin and get Dout (Sec. <ref type="bibr">3.2)</ref> of the intensity profile P used for normalizing the confidence map scale. If M(u) is smaller than a constant threshold, we declare them as outlier pixels. D win and M(u) are used for the depth refinement algorithm <ref type="bibr" target="#b25">[26]</ref> that enforces smoothness on the depth image using the guidance of the reference color image. <ref type="bibr" target="#b25">[26]</ref> is based on the minimum spanning tree structure, and it requires only 2 addition/subtraction operations and 3 multiplication operations in total for each pixel. <ref type="figure" target="#fig_7">Figure 6</ref> shows an example of the outlier detection and the refinement. The overall pipeline of our approach is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Dataset</head><p>We devise a synthetic experiment to analyze the algorithm's dependence on the magnitude of the baseline motion and on the number of images. We render 11 clips each containing one reference image and 30 non-reference images all captured at a fixed distance around the z-axis of the reference image. The baselines are determined relative to the minimum depth of the scene from the reference image. All the images are rendered using the Blender TM software at a set resolution of 640 × 480. Since our algorithm de-  <ref type="table">Table 1</ref>. Quantitative comparison between the SAD based cost and VAR based cost using one of our synthetic clips (b = −2.1, n = 31). MAD denotes the mean of absolute depth label difference.</p><p>termines the camera pose and scene depth up to a scaled factor, the obtained depth map and the ground truth has to be normalized into the same range, from 1 to 256 in our case, for an accurate assessment. <ref type="figure" target="#fig_8">Fig. 7</ref> shows the depth map results obtained by the proposed WTA scheme and their corresponding difference maps by varying magnitudes of the baseline, shown in <ref type="figure" target="#fig_8">Fig. 7.(a)</ref>, and the number of images used, <ref type="figure" target="#fig_8">Fig. 7.(c)</ref>, respectively. Additionally, it shows the results of the quantitative evaluations, <ref type="figure" target="#fig_8">Fig. 7.(b)</ref> and <ref type="figure" target="#fig_8">Fig. 7.(d)</ref>, using the robustness measure employed by the Middlebury stereo evaluation system <ref type="bibr" target="#b19">[20]</ref>; R5 denotes the percentage of pixels that come within an absolute distance of 5 labels from the ground truth label. These two evaluations demonstrate that our method can produce a depth with an R5 score of at around 80% as long as the magnitude of the baseline is greater than 1% of the nearest scene depth and the number of frames captured exceeds 30 frames. This roughly equates to a 1 second small motion clip.</p><p>We also carried out a quantitative comparison by using either the SAD based cost or the proposed VAR based cost in the plane sweeping stage. In this experiment, the groundtruth camera parameters of our synthetic clip (b = −2.1, n =31) are given to the depth map acquisition pipeline. As shown in <ref type="table">Table 1</ref>, the WTA depth map using the SAD based cost gives worse results than that of the VAR based cost. The reason behind this result is that the SAD based cost is easily affected by quality of the reference image, which may contain noise, whereas the VAR based cost has no bias and considers the importance of all input images equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Real-world Dataset</head><p>Camera setup. We have tested our method using a machine vision camera, Flea3 from Point Grey, Inc. (1280 × 960 resolution at 30fps), and an iPhone 6 with two video modes   <ref type="table">Table 2</ref>. Evaluation on the estimated intrinsic camera parameters (i.e. focal length and radial distortion). We have tested 31 clips (identified by the number in the parenthesis) grabbed by two cameras with different lens settings. The ground truth camera parameters (GT) are acquired using the camera calibration toolbox <ref type="bibr" target="#b27">[28]</ref>.</p><p>(1920 × 1280 at 30fps and 1280 × 720 at 240fps). For the 240 fps videos, 30 frames are uniformly sampled from the first 240 frames. As the proposed method can utilize the full resolution of the images, the estimated depth maps have the same resolution as the inputs. The datasets are captured by multiple users independent to this research. The small motion clips contain various types of motions, such as the onedirectional or waving motion. The maximum distances for the captured scenes range from a wide variety of distances. <ref type="figure" target="#fig_9">Figure 8</ref> shows high-quality depth maps from various types of small motions.</p><p>Computational time. In order to process each small motion footage (30 frames of 1280 × 720 res. images), our unoptimized implementation takes about one minute to perform feature extraction, tracking, and bundle adjustment. We use the Ceres solver for the sparse non-linear optimiza-tion <ref type="bibr" target="#b0">[1]</ref>. The dense stereo matching stage takes about 10 minutes. The reported time is measured without CPU parallelization. We use a desktop computer equipped with an Intel i7-4970K 4.0Ghz CPU and 16GB RAM. The data and program are released on our project website.</p><p>Evaluation on the camera self-calibration. As the proposed bundle adjustment is designed to self-calibrate the intrinsic camera parameters, we devise a quantitative evaluation method for the camera parameters obtained by our approach. For this experiment, we use the calibrated Flea3 and iPhone6 cameras, each while on two different lens settings 2 . <ref type="table">Table 2</ref> compares the estimated focal length and radial distortion against the ground truth. Here, we intentionally set the initial focal length to be significantly different from the ground truth. For measuring the distortion error, we generate a pixel grid and transform their coordinates using the estimated D-U function F. The transformed coordinates are again applied with the ground-truth U-D model found in the camera pre-calibration. If the estimated F is reliable, these sequential transformation should be identity. The distortion error is measured in pixels using the mean of absolute distances. The results show that the estimated parameters are close to the ground truth. Notably, the mean distortion error from the Flea3 datasets is around 0.5 pixels, while the initial parameter (k 1,2 =0) had an error of 5.8 pixels.</p><p>Comparison with <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. <ref type="figure">Figure 9</ref> shows the depth maps acquired by Yu and Gallup <ref type="bibr" target="#b26">[27]</ref>, Im et al. <ref type="bibr" target="#b8">[ 9]</ref>, and our approach. We use the dataset and results provided by their Averaged image of the clips Yu and Gallup <ref type="bibr" target="#b26">[27]</ref>I m et al. <ref type="bibr" target="#b8">[9]</ref> Our results <ref type="figure">Figure 9</ref>. Comparison with state-of-the-art approaches. The amount of camera movement is quite small as indicated by the averaged images on the far left. The results of Yu and Gallup <ref type="bibr" target="#b26">[27]</ref> show an inaccurate depth discontinuity on the shadow and indicate that the brick wall is nearer than the ground. Our result shows a similar depth tendency to <ref type="bibr" target="#b8">[9]</ref> while exhibiting much sharper depth discontinuities. The erroneous region in our approach is also marked in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One of input images Our depth map</head><p>Joshi and Zitnick <ref type="bibr" target="#b9">[10]</ref> Our synthetic refocusing <ref type="figure" target="#fig_1">Figure 10</ref>. Comparison with Joshi and Zitnick <ref type="bibr" target="#b9">[10]</ref> on the application of synthetic refocusing. Our high-quality depth map generates an admirable foreground focused image from the small motion clip. Note that our result shows realistic defocus blurs on the background whilst preserving sharp edges in the foreground (bottom region of the cup). respective websites for the pair comparison. The results by <ref type="bibr" target="#b26">[27]</ref> have inaccurate depth values as marked with the red rectangles. The results of <ref type="bibr" target="#b8">[9]</ref> show better depth values, indicating that the ground plane is nearest to the camera. This is due to their explicit consideration of the rolling shutter effect. However, the depth discontinuity is too smooth, resulting in blurry object boundaries unsuitable for the use of synthetic refocusing. Our results have similar depth values to <ref type="bibr" target="#b8">[9]</ref> but show much sharper edge boundaries. Note that our approach performs a self-calibration of the camera parameters for the displayed results, whereas <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b8">9]</ref> utilize the factory settings for the camera focal length and do not account for the lens distortion. Comparison with <ref type="bibr" target="#b9">[10]</ref>. The synthetic refocusing image obtained by using our depth map is compared with that of <ref type="bibr" target="#b9">[10]</ref>. We also use the dataset and result provided by <ref type="bibr" target="#b9">[10]</ref> for the pair comparison. As shown in <ref type="figure" target="#fig_1">Fig. 10</ref>, our foreground focused image consistently gives realistic defocus blurs. Limitations. Although our algorithm is specially designed for small baselines, the estimated camera poses become unreliable if the motion is unreasonably small. However, according to Sec. 4.1, the required minimum baseline to apply our approach is reasonable, and such failure cases rarely happen in real hand-held scenarios. We observe that the lack of features near the image border results in the erroneous estimation of the radial distortion parameters. Our approach does not explicitly take into account the effects of occlusion/dis-occlusion because the detected outliers in our depth post-processing stage reasonably correspond to such regions. However, any sophisticated occlusion inference algorithm could be applied here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced a practical algorithm that recovers a high-quality depth map from a small motion clip recorded by commercial cameras. Our self-calibrating bundle adjustment estimates the camera parameters that are shown to be close to the ground truth, even if only one second of the clip is used. Our dense stereo matching step analyzes the statistics of intensity profile and shows a superior depth map than that of the previous approaches. In the future, we plan to study the convergence properties of the proposed bundle adjustment scheme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Front and top view of recovered 3D scene and camera poses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>A small motion clip is our sole input. (a) An averaged image showing the overall camera motion. (b) Visualization of recovered 3D scene points and camera poses. (c) Our depth map result. (d) A synthetic refocusing result as an application example. Please note that our method does not require camera calibration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Comparing reconstructed 3D point clouds. Our approach recovers reliable 3D point clouds like the result of Yu and Gallup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>An example of plane sweeping stereo. The image sequences correspond to the small motion clip are warped according to the sweeping depth. (a) One of input images. Three local regions of different scene depth -P 1, P 2, and P 3 are marked for the illustration. (b) The mean image of warped input images and its corresponding intensity/gradient profile are displayed. If the sweeping depth is correct for the local region, its profiles become flat. (c) Recovered depth map after applying winner-takes-all scheme on the computed cost volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Comparison with SAD based cost and VAR based cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>The sequential procedures to get the dense depth map. (a) Rough depth map acquired by applying winner-takes-all strategy on the cost volume, (b) after the removal of unreliable depth pixels through our approach, and (c) after the depth refinement algorithm is applied. (d) The reference image. Note the fine structures observable in the bicycles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Synthetic experiments with respect to the magnitude of the baseline (n =3 1 ) and the number of images (b = −2.1). (a), (c) Depth maps obtained by the proposed WTA and their difference maps. (b), (d) Robustness measure results denoting the percentage of pixels within 3, 5, 7, and 10 label differences from the ground truth. Algorithm 1: Depth from Small Motion Clip Input: Image sequence {Ii} n−1 i=0 Output: Depth map Dout -Compute relative pose {ri, ti} n−1 i=0 , 3D points {x} of the scene, and lens parameters f , k1, k2 (Sec. 3.1) -Undistort {Ii} n−1 i=0 to get {I u i } n−1 i=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Depth maps acquired by our algorithm. For each dataset, the left image shows the averaged image of the footage to indicate the amount of camera movement, and the right image shows the estimated depth map using our algorithm. The clips are captured by a hand-held iPhone 6 device using the default video capturing mode. Note that the capture times of the displayed clips are only one second.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>SfM algorithms map the projected 3D points from the Undistorted image domain coordinates to the Distorted image domain coordinates when measuring</figDesc><table>Front view 

Side view 
Top view 

Yu and Gallup [27] (requires pre-calibration) 

Our self-calibrating bundle adjustment 

Reference view 
Our depth map 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">iPhone can hold a fixed focal length if the user touches the region of interest in the preview screen for a long time.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mierle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename><surname>Ceres</surname></persName>
		</author>
		<ptr target="http://ceres-solver.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards urban 3d reconstruction from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Talton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multi-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="358" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discrete-continuous optimization for large-scale structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3001" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variable baseline/resolution stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alvey vision conference</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High quality structure from small motion for rolling shutter cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Micro-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>MSR-TR-2014-73</idno>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple view geometry and the l-norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1002" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Practical global optimization for multiview geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.5644</idno>
		<title level="m">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="674" to="679" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rational radial distortion models of camera lenses with analytical solution for distortion correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Acquisition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="135" to="147" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A multiple-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="353" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computing the camera heading from multiple frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliensis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page">203</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A multi-frame structure-from-motion algorithm under perspective projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliensis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="163" to="192" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The least-squares error for structure from infinitesimal motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliensis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="299" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recovering camera motion using linfty minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1230" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photo tourism: Exploring photo collections in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIG-GRAPH</title>
		<meeting>ACM SIG-GRAPH<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="835" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling the world from Internet photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="210" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unified approach to image distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ohnishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 16th International Conference on</title>
		<meeting>16th International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="584" to="587" />
		</imprint>
	</monogr>
	<note>Pattern Recognition (ICPR)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bundle adjustmenta modern synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision algorithms: theory and practice</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="298" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A non-local cost aggregation method for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1402" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d reconstruction from accidental motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3986" to="3993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
